<BOF>
from functools import partial
from os import environ

from invocations import travis
from invocations.checks import blacken
from invocations.docs import docs, www, sites, watch_docs
from invocations.pytest import test, integration, coverage
from invocations.packaging import release
from invocations.util import tmpdir

from invoke import Collection, task
from invoke.util import LOG_FORMAT


# Neuter the normal release.publish task to prevent accidents, then reinstate
# it as a custom task that does dual fabric-xxx and fabric2-xxx releases.
# TODO: tweak this once release.all_ actually works right...sigh
# TODO: if possible, try phrasing as a custom build that builds x2, and then
# convince the vanilla publish() to use that custom build instead of its local
# build?
# NOTE: this skips the dual_wheels, alt_python bits the upstream task has,
# which are at the moment purely for Invoke's sake (as it must publish explicit
# py2 vs py3 wheels due to some vendored dependencies)
@task
def publish(
    c,
    sdist=True,
    wheel=False,
    index=None,
    sign=False,
    dry_run=False,
    directory=None,
    check_desc=False,
):
    # TODO: better pattern for merging kwargs + config
    config = c.config.get("packaging", {})
    index = config.get("index", index)
    sign = config.get("sign", sign)
    check_desc = config.get("check_desc", check_desc)
    # Initial sanity check, if needed. Will die usefully.
    # TODO: this could also get factored out harder in invocations. shrug. it's
    # like 3 lines total...
    if check_desc:
        c.run("python setup.py check -r -s")
    with tmpdir(skip_cleanup=dry_run, explicit=directory) as directory:
        # Doesn't reeeeally need to be a partial, but if we start having to add
        # a kwarg to one call or the other, it's nice
        builder = partial(
            release.build, c, sdist=sdist, wheel=wheel, directory=directory
        )
        # Vanilla build
        builder()
        # Fabric 2 build
        environ["PACKAGE_AS_FABRIC2"] = "yes"
        builder()
        # Upload
        release.upload(c, directory, index, sign, dry_run)


# Better than nothing, since we haven't solved "pretend I have some other
# task's signature" yet...
publish.__doc__ = release.publish.__doc__
my_release = Collection(
    "release", release.build, release.status, publish, release.prepare
)

ns = Collection(
    blacken,
    coverage,
    docs,
    integration,
    my_release,
    sites,
    test,
    travis,
    watch_docs,
    www,
)
ns.configure(
    {
        "tests": {
            # TODO: have pytest tasks honor these?
            "package": "fabric",
            "logformat": LOG_FORMAT,
        },
        "packaging": {
            # NOTE: this is currently for identifying the source directory.
            # Should it get used for actual releasing, needs changing.
            "package": "fabric",
            "sign": True,
            "wheel": True,
            "check_desc": True,
            "changelog_file": "sites/www/changelog.rst",
        },
        # TODO: perhaps move this into a tertiary, non automatically loaded,
        # conf file so that both this & the code under test can reference it?
        # Meh.
        "travis": {
            "sudo": {"user": "sudouser", "password": "mypass"},
            "black": {"version": "18.6b4"},
        },
    }
)
<EOF>
<BOF>
#!/usr/bin/env python

import os
import setuptools

# Enable the option of building/installing Fabric 2.x as "fabric2". This allows
# users migrating from 1.x to 2.x to have both in the same process space and
# migrate piecemeal.
#
# NOTE: this requires some irritating tomfoolery; to wit:
# - the repo has a fabric2/ symlink to fabric/ so that things looking for
# fabric2/<whatever> will find it OK, whether that's code in here or deeper in
# setuptools/wheel/etc
# - wheels do _not_ execute this on install, only on generation, so maintainers
# just build wheels with the env var below turned on, and those wheels install
# 'fabric2' no problem
# - sdists execute this _both_ on package creation _and_ on install, so the env
# var only helps with inbound package metadata; on install by a user, if they
# don't have the env var, they'd end up with errors because this file tries to
# look in fabric/, not fabric2/
# - thus, we use a different test that looks locally to see if only one dir
# is present, and that overrides the env var test.
#
# See also sites/www/installing.txt.

env_wants_v2 = os.environ.get("PACKAGE_AS_FABRIC2", False)

here = os.path.abspath(os.path.dirname(__file__))
fabric2_present = os.path.isdir(os.path.join(here, "fabric2"))
fabric_present = os.path.isdir(os.path.join(here, "fabric"))
only_v2_present = fabric2_present and not fabric_present

package_name = "fabric"
binary_name = "fab"
if env_wants_v2 or only_v2_present:
    package_name = "fabric2"
    binary_name = "fab2"
packages = setuptools.find_packages(
    include=[package_name, "{}.*".format(package_name)]
)

# Version info -- read without importing
_locals = {}
with open(os.path.join(package_name, "_version.py")) as fp:
    exec(fp.read(), None, _locals)
version = _locals["__version__"]

# Frankenstein long_description: changelog note + README
long_description = """
To find out what's new in this version of Fabric, please see `the changelog
<http://fabfile.org/changelog.html>`_.

{}
""".format(
    open("README.rst").read()
)

setuptools.setup(
    name=package_name,
    version=version,
    description="High level SSH command execution",
    license="BSD",
    long_description=long_description,
    author="Jeff Forcier",
    author_email="jeff@bitprophet.org",
    url="http://fabfile.org",
    install_requires=[
        "invoke>=1.0,<2.0",
        "paramiko>=2.4",
        "cryptography>=1.1",
    ],
    packages=packages,
    entry_points={
        "console_scripts": [
            "{} = {}.main:program.run".format(binary_name, package_name)
        ]
    },
    classifiers=[
        "Development Status :: 5 - Production/Stable",
        "Environment :: Console",
        "Intended Audience :: Developers",
        "Intended Audience :: System Administrators",
        "License :: OSI Approved :: BSD License",
        "Operating System :: POSIX",
        "Operating System :: Unix",
        "Operating System :: MacOS :: MacOS X",
        "Operating System :: Microsoft :: Windows",
        "Programming Language :: Python",
        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.4",
        "Programming Language :: Python :: 3.5",
        "Programming Language :: Python :: 3.6",
        "Topic :: Software Development",
        "Topic :: Software Development :: Build Tools",
        "Topic :: Software Development :: Libraries",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: System :: Clustering",
        "Topic :: System :: Software Distribution",
        "Topic :: System :: Systems Administration",
    ],
)
<EOF>
<BOF>
try:
    from invoke.vendor.six import StringIO
except ImportError:
    from six import StringIO

from invoke import pty_size, Result

from fabric import Config, Connection, Remote


# On most systems this will explode if actually executed as a shell command;
# this lets us detect holes in our network mocking.
CMD = "nope"

# TODO: see TODO in tests/main.py above _run_fab(), this is the same thing.
def _Connection(*args, **kwargs):
    kwargs["config"] = Config({"run": {"in_stream": False}})
    return Connection(*args, **kwargs)


class Remote_:
    def needs_handle_on_a_Connection(self):
        c = _Connection("host")
        assert Remote(context=c).context is c

    class run:
        def calls_expected_paramiko_bits(self, remote):
            # remote mocking makes generic sanity checks like "were
            # get_transport and open_session called", but we also want to make
            # sure that exec_command got run with our arg to run().
            remote.expect(cmd=CMD)
            c = _Connection("host")
            r = Remote(context=c)
            r.run(CMD)

        def writes_remote_streams_to_local_streams(self, remote):
            remote.expect(out=b"hello yes this is dog")
            c = _Connection("host")
            r = Remote(context=c)
            fakeout = StringIO()
            r.run(CMD, out_stream=fakeout)
            assert fakeout.getvalue() == "hello yes this is dog"

        def pty_True_uses_paramiko_get_pty(self, remote):
            chan = remote.expect()
            c = _Connection("host")
            r = Remote(context=c)
            r.run(CMD, pty=True)
            cols, rows = pty_size()
            chan.get_pty.assert_called_with(width=cols, height=rows)

        def start_sends_given_env_to_paramiko_update_environment(self, remote):
            chan = remote.expect()
            c = _Connection("host")
            r = Remote(context=c)
            r.run(CMD, pty=True, env={"FOO": "bar"})
            chan.update_environment.assert_called_once_with({"FOO": "bar"})

        def return_value_is_Result_subclass_exposing_cxn_used(self, remote):
            c = _Connection("host")
            r = Remote(context=c)
            result = r.run(CMD)
            assert isinstance(result, Result)
            # Mild sanity test for other Result superclass bits
            assert result.ok is True
            assert result.exited == 0
            # Test the attr our own subclass adds
            assert result.connection is c

        def channel_is_closed_normally(self, remote):
            chan = remote.expect()
            # I.e. Remote.stop() closes the channel automatically
            r = Remote(context=_Connection("host"))
            r.run(CMD)
            chan.close.assert_called_once_with()

        def channel_is_closed_on_body_exceptions(self, remote):
            chan = remote.expect()

            # I.e. Remote.stop() is called within a try/finally.
            # Technically is just testing invoke.Runner, but meh.
            class Oops(Exception):
                pass

            class _OopsRemote(Remote):
                def wait(self):
                    raise Oops()

            r = _OopsRemote(context=_Connection("host"))
            try:
                r.run(CMD)
            except Oops:
                chan.close.assert_called_once_with()
            else:
                assert False, "Runner failed to raise exception!"

        def channel_close_skipped_when_channel_not_even_made(self):
            # I.e. if obtaining self.channel doesn't even happen (i.e. if
            # Connection.create_session() dies), we need to account for that
            # case...
            class Oops(Exception):
                pass

            def oops():
                raise Oops

            cxn = _Connection("host")
            cxn.create_session = oops
            r = Remote(context=cxn)
            # When bug present, this will result in AttributeError because
            # Remote has no 'channel'
            try:
                r.run(CMD)
            except Oops:
                pass
            else:
                assert False, "Weird, Oops never got raised..."

        # TODO: how much of Invoke's tests re: the upper level run() (re:
        # things like returning Result, behavior of Result, etc) to
        # duplicate here? Ideally none or very few core ones.

        # TODO: only test guts of our stuff, Invoke's Runner tests should
        # handle all the normal shit like stdout/err print and capture.
        # Implies we want a way to import & run those tests ourselves, though,
        # with the Runner instead being a Remote. Or do we just replicate the
        # basics?

        # TODO: all other run() tests from fab1...
<EOF>
<BOF>
from pytest import fixture

from fabric import Connection
from fabric.transfer import Transfer
from mock import Mock, patch

from _util import MockRemote, MockSFTP


@fixture
def remote():
    """
    Fixture allowing setup of a mocked remote session & access to sub-mocks.

    Yields a `MockRemote` object (which may need to be updated via
    `MockRemote.expect`, `MockRemote.expect_sessions`, etc; otherwise a default
    session will be used) & calls `MockRemote.stop` on teardown.
    """
    remote = MockRemote()
    yield remote
    remote.stop()


@fixture
def sftp():
    """
    Fixture allowing setup of a mocked remote SFTP session.

    Yields a 3-tuple of: Transfer() object, SFTPClient object, and mocked OS
    module.

    For many/most tests which only want the Transfer and/or SFTPClient objects,
    see `sftp_objs` and `transfer` which wrap this fixture.
    """
    mock = MockSFTP(autostart=False)
    client, mock_os = mock.start()
    transfer = Transfer(Connection("host"))
    yield transfer, client, mock_os
    # TODO: old mock_sftp() lacked any 'stop'...why? feels bad man


@fixture
def sftp_objs(sftp):
    """
    Wrapper for `sftp` which only yields the Transfer and SFTPClient.
    """
    yield sftp[:2]


@fixture
def transfer(sftp):
    """
    Wrapper for `sftp` which only yields the Transfer object.
    """
    yield sftp[0]


@fixture
def client():
    """
    Yields a mocked-out SSHClient for testing calls to connect() & co.

    It updates get_transport to return a mock that appears active on first
    check, then inactive after, matching most tests' needs by default:

    - `Connection` instantiates, with a None ``.transport``.
    - Calls to ``.open()`` test ``.is_connected``, which returns ``False`` when
      ``.transport`` is falsey, and so the first open will call
      ``SSHClient.connect`` regardless.
    - ``.open()`` then sets ``.transport`` to ``SSHClient.get_transport()``, so
      ``Connection.transport`` is effectively
      ``client.get_transport.return_value``.
    - Subsequent activity will want to think the mocked SSHClient is
      "connected", meaning we want the mocked transport's ``.active`` to be
      ``True``.
    - This includes ``Connection.close``, which short-circuits if
      ``.is_connected``; having a statically ``True`` active flag means a full
      open -> close cycle will run without error. (Only tests that double-close
      or double-open should have issues here.)

    End result is that:

    - ``.is_connected`` behaves False after instantiation and before ``.open``,
      then True after ``.open``
    - ``.close`` will work normally on 1st call
    - ``.close will behave "incorrectly" on subsequent calls (since it'll think
      connection is still live.) Tests that check the idempotency of ``.close``
      will need to tweak their mock mid-test.

    For 'full' fake remote session interaction (i.e. stdout/err
    reading/writing, channel opens, etc) see `remote`.
    """
    with patch("fabric.connection.SSHClient") as SSHClient:
        client = SSHClient.return_value
        client.get_transport.return_value = Mock(active=True)
        yield client
<EOF>
<BOF>
from itertools import chain, repeat

try:
    from invoke.vendor.six import b
except ImportError:
    from six import b
import errno
from os.path import join
import socket
import time

from mock import patch, Mock, call, ANY
from paramiko.client import SSHClient, AutoAddPolicy
from paramiko import SSHConfig
import pytest  # for mark
from pytest import skip, param
from pytest_relaxed import raises

from invoke.config import Config as InvokeConfig
from invoke.exceptions import ThreadException

from fabric import Config as Config_
from fabric.util import get_local_user

from _util import support, Connection, Config


# Remote is woven in as a config default, so must be patched there
remote_path = "fabric.config.Remote"


def _select_result(obj):
    """
    Return iterator/generator suitable for mocking a select.select() call.

    Specifically one that has a single initial return value of ``obj``, and
    then empty results thereafter.

    If ``obj`` is an exception, it will be used as the sole initial
    ``side_effect`` (as opposed to a return value among tuples).
    """
    # select.select() returns three N-tuples. Have it just act like a single
    # read event happened, then quiet after. So chain a single-item iterable to
    # a repeat(). (Mock has no built-in way to do this apparently.)
    initial = [(obj,), tuple(), tuple()]
    if isinstance(obj, Exception) or (
        isinstance(obj, type) and issubclass(obj, Exception)
    ):
        initial = obj
    return chain([initial], repeat([tuple(), tuple(), tuple()]))


class Connection_:
    class basic_attributes:
        def is_connected_defaults_to_False(self):
            assert Connection("host").is_connected is False

        def client_defaults_to_a_new_SSHClient(self):
            c = Connection("host").client
            assert isinstance(c, SSHClient)
            assert c.get_transport() is None

    class known_hosts_behavior:
        def defaults_to_auto_add(self):
            # TODO: change Paramiko API so this isn't a private access
            # TODO: maybe just merge with the __init__ test that is similar
            assert isinstance(Connection("host").client._policy, AutoAddPolicy)

    class init:
        "__init__"

        class host:
            @raises(TypeError)
            def is_required(self):
                Connection()

            def is_exposed_as_attribute(self):
                assert Connection("host").host == "host"  # buffalo buffalo

            def may_contain_user_shorthand(self):
                c = Connection("user@host")
                assert c.host == "host"
                assert c.user == "user"

            def may_contain_port_shorthand(self):
                c = Connection("host:123")
                assert c.host == "host"
                assert c.port == 123

            def may_contain_user_and_port_shorthand(self):
                c = Connection("user@host:123")
                assert c.host == "host"
                assert c.user == "user"
                assert c.port == 123

            def ipv6_addresses_work_ok_but_avoid_port_shorthand(self):
                for addr in ("2001:DB8:0:0:0:0:0:1", "2001:DB8::1", "::1"):
                    c = Connection(addr, port=123)
                    assert c.user == get_local_user()
                    assert c.host == addr
                    assert c.port == 123
                    c2 = Connection("somebody@{}".format(addr), port=123)
                    assert c2.user == "somebody"
                    assert c2.host == addr
                    assert c2.port == 123

        class user:
            def defaults_to_local_user_with_no_config(self):
                # Tautology-tastic!
                assert Connection("host").user == get_local_user()

            def accepts_config_user_option(self):
                config = Config(overrides={"user": "nobody"})
                assert Connection("host", config=config).user == "nobody"

            def may_be_given_as_kwarg(self):
                assert Connection("host", user="somebody").user == "somebody"

            @raises(ValueError)
            def errors_when_given_as_both_kwarg_and_shorthand(self):
                Connection("user@host", user="otheruser")

            def kwarg_wins_over_config(self):
                config = Config(overrides={"user": "nobody"})
                cxn = Connection("host", user="somebody", config=config)
                assert cxn.user == "somebody"

            def shorthand_wins_over_config(self):
                config = Config(overrides={"user": "nobody"})
                cxn = Connection("somebody@host", config=config)
                assert cxn.user == "somebody"

        class port:
            def defaults_to_22_because_yup(self):
                assert Connection("host").port == 22

            def accepts_configuration_port(self):
                config = Config(overrides={"port": 2222})
                assert Connection("host", config=config).port == 2222

            def may_be_given_as_kwarg(self):
                assert Connection("host", port=2202).port == 2202

            @raises(ValueError)
            def errors_when_given_as_both_kwarg_and_shorthand(self):
                Connection("host:123", port=321)

            def kwarg_wins_over_config(self):
                config = Config(overrides={"port": 2222})
                cxn = Connection("host", port=123, config=config)
                assert cxn.port == 123

            def shorthand_wins_over_config(self):
                config = Config(overrides={"port": 2222})
                cxn = Connection("host:123", config=config)
                assert cxn.port == 123

        class forward_agent:
            def defaults_to_False(self):
                assert Connection("host").forward_agent is False

            def accepts_configuration_value(self):
                config = Config(overrides={"forward_agent": True})
                assert Connection("host", config=config).forward_agent is True

            def may_be_given_as_kwarg(self):
                cxn = Connection("host", forward_agent=True)
                assert cxn.forward_agent is True

            def kwarg_wins_over_config(self):
                config = Config(overrides={"forward_agent": True})
                cxn = Connection("host", forward_agent=False, config=config)
                assert cxn.forward_agent is False

        class connect_timeout:
            def defaults_to_None(self):
                assert Connection("host").connect_timeout is None

            def accepts_configuration_value(self):
                config = Config(overrides={"timeouts": {"connect": 10}})
                assert Connection("host", config=config).connect_timeout == 10

            def may_be_given_as_kwarg(self):
                cxn = Connection("host", connect_timeout=15)
                assert cxn.connect_timeout == 15

            def kwarg_wins_over_config(self):
                config = Config(overrides={"timeouts": {"connect": 20}})
                cxn = Connection("host", connect_timeout=100, config=config)
                assert cxn.connect_timeout == 100

        class config:
            # NOTE: behavior local to Config itself is tested in its own test
            # module; below is solely about Connection's config kwarg and its
            # handling of that value

            def is_not_required(self):
                assert Connection("host").config.__class__ == Config

            def can_be_specified(self):
                c = Config(overrides={"user": "me", "custom": "option"})
                config = Connection("host", config=c).config
                assert c is config
                assert config["user"] == "me"
                assert config["custom"] == "option"

            def if_given_an_invoke_Config_we_upgrade_to_our_own_Config(self):
                # Scenario: user has Fabric-level data present at vanilla
                # Invoke config level, and is then creating Connection objects
                # with those vanilla invoke Configs.
                # (Could also _not_ have any Fabric-level data, but then that's
                # just a base case...)
                # TODO: adjust this if we ever switch to all our settings being
                # namespaced...
                vanilla = InvokeConfig(overrides={"forward_agent": True})
                cxn = Connection("host", config=vanilla)
                assert cxn.forward_agent is True  # not False, which is default

        class gateway:
            def is_optional_and_defaults_to_None(self):
                c = Connection(host="host")
                assert c.gateway is None

            def takes_a_Connection(self):
                c = Connection("host", gateway=Connection("otherhost"))
                assert isinstance(c.gateway, Connection)
                assert c.gateway.host == "otherhost"

            def takes_a_string(self):
                c = Connection("host", gateway="meh")
                assert c.gateway == "meh"

            def accepts_configuration_value(self):
                gw = Connection("jumpbox")
                config = Config(overrides={"gateway": gw})
                # TODO: the fact that they will be eq, but _not_ necessarily be
                # the same object, could be problematic in some cases...
                cxn = Connection("host", config=config)
                assert cxn.gateway == gw

        class initializes_client:
            @patch("fabric.connection.SSHClient")
            def instantiates_empty_SSHClient(self, Client):
                Connection("host")
                Client.assert_called_once_with()

            @patch("fabric.connection.AutoAddPolicy")
            def sets_missing_host_key_policy(self, Policy, client):
                # TODO: should make the policy configurable early on
                sentinel = Mock()
                Policy.return_value = sentinel
                Connection("host")
                set_policy = client.set_missing_host_key_policy
                set_policy.assert_called_once_with(sentinel)

            def is_made_available_as_client_attr(self, client):
                # NOTE: client is SSHClient.return_value
                assert Connection("host").client is client

        class ssh_config:
            def _runtime_config(self, overrides=None, basename="runtime"):
                confname = "{}.conf".format(basename)
                runtime_path = join(support, "ssh_config", confname)
                if overrides is None:
                    overrides = {}
                return Config_(
                    runtime_ssh_path=runtime_path, overrides=overrides
                )

            def _runtime_cxn(self, **kwargs):
                config = self._runtime_config(**kwargs)
                return Connection("runtime", config=config)

            def effectively_blank_when_no_loaded_config(self):
                c = Config_(ssh_config=SSHConfig())
                cxn = Connection("host", config=c)
                # NOTE: paramiko always injects this even if you look up a host
                # that has no rules, even wildcard ones.
                assert cxn.ssh_config == {"hostname": "host"}

            def shows_result_of_lookup_when_loaded_config(self):
                conf = self._runtime_cxn().ssh_config
                expected = {
                    "connecttimeout": "15",
                    "forwardagent": "yes",
                    "hostname": "runtime",
                    "identityfile": ["whatever.key", "some-other.key"],
                    "port": "666",
                    "proxycommand": "my gateway",
                    "user": "abaddon",
                }
                assert conf == expected

            class hostname:
                def original_host_always_set(self):
                    cxn = Connection("somehost")
                    assert cxn.original_host == "somehost"
                    assert cxn.host == "somehost"

                def hostname_directive_overrides_host_attr(self):
                    # TODO: not 100% convinced this is the absolute most
                    # obvious API for 'translation' of given hostname to
                    # ssh-configured hostname, but it feels okay for now.
                    path = join(
                        support, "ssh_config", "overridden_hostname.conf"
                    )
                    config = Config_(runtime_ssh_path=path)
                    cxn = Connection("aliasname", config=config)
                    assert cxn.host == "realname"
                    assert cxn.original_host == "aliasname"
                    assert cxn.port == 2222

            class user:
                def wins_over_default(self):
                    assert self._runtime_cxn().user == "abaddon"

                def wins_over_configuration(self):
                    cxn = self._runtime_cxn(overrides={"user": "baal"})
                    assert cxn.user == "abaddon"

                def loses_to_explicit(self):
                    # Would be 'abaddon', as above
                    config = self._runtime_config()
                    cxn = Connection("runtime", config=config, user="set")
                    assert cxn.user == "set"

            class port:
                def wins_over_default(self):
                    assert self._runtime_cxn().port == 666

                def wins_over_configuration(self):
                    cxn = self._runtime_cxn(overrides={"port": 777})
                    assert cxn.port == 666

                def loses_to_explicit(self):
                    config = self._runtime_config()  # Would be 666, as above
                    cxn = Connection("runtime", config=config, port=777)
                    assert cxn.port == 777

            class forward_agent:
                def wins_over_default(self):
                    assert self._runtime_cxn().forward_agent is True

                def wins_over_configuration(self):
                    # Of course, this "config override" is also the same as the
                    # default. Meh.
                    cxn = self._runtime_cxn(overrides={"forward_agent": False})
                    assert cxn.forward_agent is True

                def loses_to_explicit(self):
                    # Would be True, as above
                    config = self._runtime_config()
                    cxn = Connection(
                        "runtime", config=config, forward_agent=False
                    )
                    assert cxn.forward_agent is False

            class proxy_command:
                def wins_over_default(self):
                    assert self._runtime_cxn().gateway == "my gateway"

                def wins_over_configuration(self):
                    cxn = self._runtime_cxn(overrides={"gateway": "meh gw"})
                    assert cxn.gateway == "my gateway"

                def loses_to_explicit(self):
                    # Would be "my gateway", as above
                    config = self._runtime_config()
                    cxn = Connection(
                        "runtime", config=config, gateway="other gateway"
                    )
                    assert cxn.gateway == "other gateway"

                def explicit_False_turns_off_feature(self):
                    # This isn't as necessary for things like user/port, which
                    # _may not_ be None in the end - this setting could be.
                    config = self._runtime_config()
                    cxn = Connection("runtime", config=config, gateway=False)
                    assert cxn.gateway is False

            class proxy_jump:
                def setup(self):
                    self._expected_gw = Connection("jumpuser@jumphost:373")

                def wins_over_default(self):
                    cxn = self._runtime_cxn(basename="proxyjump")
                    assert cxn.gateway == self._expected_gw

                def wins_over_configuration(self):
                    cxn = self._runtime_cxn(
                        basename="proxyjump", overrides={"gateway": "meh gw"}
                    )
                    assert cxn.gateway == self._expected_gw

                def loses_to_explicit(self):
                    # Would be a Connection equal to self._expected_gw, as
                    # above
                    config = self._runtime_config(basename="proxyjump")
                    cxn = Connection(
                        "runtime", config=config, gateway="other gateway"
                    )
                    assert cxn.gateway == "other gateway"

                def explicit_False_turns_off_feature(self):
                    config = self._runtime_config(basename="proxyjump")
                    cxn = Connection("runtime", config=config, gateway=False)
                    assert cxn.gateway is False

                def wins_over_proxycommand(self):
                    cxn = self._runtime_cxn(basename="both_proxies")
                    assert cxn.gateway == Connection("winner@everything:777")

                def multi_hop_works_ok(self):
                    cxn = self._runtime_cxn(basename="proxyjump_multi")
                    innermost = cxn.gateway.gateway.gateway
                    middle = cxn.gateway.gateway
                    outermost = cxn.gateway
                    assert innermost == Connection("jumpuser3@jumphost3:411")
                    assert middle == Connection("jumpuser2@jumphost2:872")
                    assert outermost == Connection("jumpuser@jumphost:373")

                def wildcards_do_not_trigger_recursion(self):
                    # When #1850 is present, this will RecursionError.
                    conf = self._runtime_config(basename="proxyjump_recursive")
                    cxn = Connection("runtime.tld", config=conf)
                    assert cxn.gateway == Connection("bastion.tld")
                    assert cxn.gateway.gateway is None

                def multihop_plus_wildcards_still_no_recursion(self):
                    conf = self._runtime_config(
                        basename="proxyjump_multi_recursive"
                    )
                    cxn = Connection("runtime.tld", config=conf)
                    outer = cxn.gateway
                    inner = cxn.gateway.gateway
                    assert outer == Connection("bastion1.tld")
                    assert inner == Connection("bastion2.tld")
                    assert inner.gateway is None

                def gateway_Connections_get_parent_connection_configs(self):
                    conf = self._runtime_config(
                        basename="proxyjump",
                        overrides={"some_random_option": "a-value"},
                    )
                    cxn = Connection("runtime", config=conf)
                    # Sanity
                    assert cxn.config is conf
                    assert cxn.gateway == self._expected_gw
                    # Real check
                    assert cxn.gateway.config.some_random_option == "a-value"
                    # Prove copy not reference
                    # TODO: would we ever WANT a reference? can't imagine...
                    assert cxn.gateway.config is not conf

            class connect_timeout:
                def wins_over_default(self):
                    assert self._runtime_cxn().connect_timeout == 15

                def wins_over_configuration(self):
                    cxn = self._runtime_cxn(
                        overrides={"timeouts": {"connect": 17}}
                    )
                    assert cxn.connect_timeout == 15

                def loses_to_explicit(self):
                    config = self._runtime_config()
                    cxn = Connection(
                        "runtime", config=config, connect_timeout=23
                    )
                    assert cxn.connect_timeout == 23

            class identity_file:
                # NOTE: ssh_config value gets merged w/ (instead of overridden
                # by) config and kwarg values; that is tested in the tests for
                # open().
                def basic_loading_of_value(self):
                    # By default, key_filename will be empty, and the data from
                    # the runtime ssh config will be all that appears.
                    value = self._runtime_cxn().connect_kwargs["key_filename"]
                    assert value == ["whatever.key", "some-other.key"]

        class connect_kwargs:
            def defaults_to_empty_dict(self):
                assert Connection("host").connect_kwargs == {}

            def may_be_given_explicitly(self):
                cxn = Connection("host", connect_kwargs={"foo": "bar"})
                assert cxn.connect_kwargs == {"foo": "bar"}

            def may_be_configured(self):
                c = Config(overrides={"connect_kwargs": {"origin": "config"}})
                cxn = Connection("host", config=c)
                assert cxn.connect_kwargs == {"origin": "config"}

            def kwarg_wins_over_config(self):
                # TODO: should this be more of a merge-down?
                c = Config(overrides={"connect_kwargs": {"origin": "config"}})
                cxn = Connection(
                    "host", connect_kwargs={"origin": "kwarg"}, config=c
                )
                assert cxn.connect_kwargs == {"origin": "kwarg"}

    class string_representation:
        "string representations"

        def str_displays_repr(self):
            c = Connection("meh")
            assert str(c) == "<Connection host=meh>"

        def displays_core_params(self):
            c = Connection(user="me", host="there", port=123)
            template = "<Connection host=there user=me port=123>"
            assert repr(c) == template

        def omits_default_param_values(self):
            c = Connection("justhost")
            assert repr(c) == "<Connection host=justhost>"

        def param_comparison_uses_config(self):
            conf = Config(overrides={"user": "zerocool"})
            c = Connection(
                user="zerocool", host="myhost", port=123, config=conf
            )
            template = "<Connection host=myhost port=123>"
            assert repr(c) == template

        def proxyjump_gateway_shows_type(self):
            c = Connection(host="myhost", gateway=Connection("jump"))
            template = "<Connection host=myhost gw=proxyjump>"
            assert repr(c) == template

        def proxycommand_gateway_shows_type(self):
            c = Connection(host="myhost", gateway="netcat is cool")
            template = "<Connection host=myhost gw=proxycommand>"
            assert repr(c) == template

    class comparison_and_hashing:
        def comparison_uses_host_user_and_port(self):
            # Just host
            assert Connection("host") == Connection("host")
            # Host + user
            c1 = Connection("host", user="foo")
            c2 = Connection("host", user="foo")
            assert c1 == c2
            # Host + user + port
            c1 = Connection("host", user="foo", port=123)
            c2 = Connection("host", user="foo", port=123)
            assert c1 == c2

        def comparison_to_non_Connections_is_False(self):
            assert Connection("host") != 15

        def hashing_works(self):
            assert hash(Connection("host")) == hash(Connection("host"))

        def sorting_works(self):
            # Hostname...
            assert Connection("a-host") < Connection("b-host")
            # User...
            assert Connection("a-host", user="a-user") < Connection(
                "a-host", user="b-user"
            )
            # then port...
            assert Connection("a-host", port=1) < Connection("a-host", port=2)

    class open:
        def has_no_required_args_and_returns_None(self, client):
            assert Connection("host").open() is None

        def calls_SSHClient_connect(self, client):
            "calls paramiko.SSHClient.connect() with correct args"
            Connection("host").open()
            client.connect.assert_called_with(
                username=get_local_user(), hostname="host", port=22
            )

        def passes_through_connect_kwargs(self, client):
            Connection("host", connect_kwargs={"foobar": "bizbaz"}).open()
            client.connect.assert_called_with(
                username=get_local_user(),
                hostname="host",
                port=22,
                foobar="bizbaz",
            )

        def refuses_to_overwrite_connect_kwargs_with_others(self, client):
            for key, value, kwargs in (
                # Core connection args should definitely not get overwritten!
                # NOTE: recall that these keys are the SSHClient.connect()
                # kwarg names, NOT our own config/kwarg names!
                ("hostname", "nothost", {}),
                ("port", 17, {}),
                ("username", "zerocool", {}),
                # These might arguably still be allowed to work, but let's head
                # off confusion anyways.
                ("timeout", 100, {"connect_timeout": 25}),
            ):
                try:
                    Connection(
                        "host", connect_kwargs={key: value}, **kwargs
                    ).open()
                except ValueError as e:
                    err = "Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!"  # noqa
                    assert str(e) == err.format(key)
                else:
                    assert False, "Did not raise ValueError!"

        def connect_kwargs_protection_not_tripped_by_defaults(self, client):
            Connection("host", connect_kwargs={"timeout": 300}).open()
            client.connect.assert_called_with(
                username=get_local_user(),
                hostname="host",
                port=22,
                timeout=300,
            )

        def submits_connect_timeout(self, client):
            Connection("host", connect_timeout=27).open()
            client.connect.assert_called_with(
                username=get_local_user(), hostname="host", port=22, timeout=27
            )

        def is_connected_True_when_successful(self, client):
            c = Connection("host")
            c.open()
            assert c.is_connected is True

        def short_circuits_if_already_connected(self, client):
            cxn = Connection("host")
            # First call will set self.transport to fixture's mock
            cxn.open()
            # Second call will check .is_connected which will see active==True,
            # and short circuit
            cxn.open()
            assert client.connect.call_count == 1

        def is_connected_still_False_when_connect_fails(self, client):
            client.connect.side_effect = socket.error
            cxn = Connection("host")
            try:
                cxn.open()
            except socket.error:
                pass
            assert cxn.is_connected is False

        def uses_configured_user_host_and_port(self, client):
            Connection(user="myuser", host="myhost", port=9001).open()
            client.connect.assert_called_once_with(
                username="myuser", hostname="myhost", port=9001
            )

        # NOTE: does more involved stuff so can't use "client" fixture
        @patch("fabric.connection.SSHClient")
        def uses_gateway_channel_as_sock_for_SSHClient_connect(self, Client):
            "uses Connection gateway as 'sock' arg to SSHClient.connect"
            # Setup
            mock_gw = Mock()
            mock_main = Mock()
            Client.side_effect = [mock_gw, mock_main]
            gw = Connection("otherhost")
            gw.open = Mock(wraps=gw.open)
            main = Connection("host", gateway=gw)
            main.open()
            # Expect gateway is also open()'d
            gw.open.assert_called_once_with()
            # Expect direct-tcpip channel open on 1st client
            open_channel = mock_gw.get_transport.return_value.open_channel
            kwargs = open_channel.call_args[1]
            assert kwargs["kind"] == "direct-tcpip"
            assert kwargs["dest_addr"], "host" == 22
            # Expect result of that channel open as sock arg to connect()
            sock_arg = mock_main.connect.call_args[1]["sock"]
            assert sock_arg is open_channel.return_value

        @patch("fabric.connection.ProxyCommand")
        def uses_proxycommand_as_sock_for_Client_connect(self, moxy, client):
            "uses ProxyCommand from gateway as 'sock' arg to SSHClient.connect"
            # Setup
            main = Connection("host", gateway="net catty %h %p")
            main.open()
            # Expect ProxyCommand instantiation
            moxy.assert_called_once_with("net catty host 22")
            # Expect result of that as sock arg to connect()
            sock_arg = client.connect.call_args[1]["sock"]
            assert sock_arg is moxy.return_value

        # TODO: all the various connect-time options such as agent forwarding,
        # host acceptance policies, how to auth, etc etc. These are all aspects
        # of a given session and not necessarily the same for entire lifetime
        # of a Connection object, should it ever disconnect/reconnect.
        # TODO: though some/all of those things might want to be set to
        # defaults at initialization time...

    class connect_kwargs_key_filename:
        "connect_kwargs(key_filename=...)"

        # TODO: it'd be nice to truly separate CLI from regular (non override
        # level) invoke config; as it is, invoke config comes first in expected
        # outputs since otherwise there's no way for --identity to "come
        # first".
        @pytest.mark.parametrize(
            "ssh, invoke, kwarg, expected",
            [
                param(
                    True,
                    True,
                    True,
                    [
                        "configured.key",
                        "kwarg.key",
                        "ssh-config-B.key",
                        "ssh-config-A.key",
                    ],
                    id="All sources",
                ),
                param(False, False, False, [], id="No sources"),
                param(
                    True,
                    False,
                    False,
                    ["ssh-config-B.key", "ssh-config-A.key"],
                    id="ssh_config only",
                ),
                param(
                    False,
                    True,
                    False,
                    ["configured.key"],
                    id="Invoke-level config only",
                ),
                param(
                    False,
                    False,
                    True,
                    ["kwarg.key"],
                    id="Connection kwarg only",
                ),
                param(
                    True,
                    True,
                    False,
                    ["configured.key", "ssh-config-B.key", "ssh-config-A.key"],
                    id="ssh_config + invoke config, no kwarg",
                ),
                param(
                    True,
                    False,
                    True,
                    ["kwarg.key", "ssh-config-B.key", "ssh-config-A.key"],
                    id="ssh_config + kwarg, no Invoke-level config",
                ),
                param(
                    False,
                    True,
                    True,
                    ["configured.key", "kwarg.key"],
                    id="Invoke-level config + kwarg, no ssh_config",
                ),
            ],
        )
        def merges_sources(self, client, ssh, invoke, kwarg, expected):
            config_kwargs = {}
            if ssh:
                # SSH config with 2x IdentityFile directives.
                config_kwargs["runtime_ssh_path"] = join(
                    support, "ssh_config", "runtime_identity.conf"
                )
            if invoke:
                # Use overrides config level to mimic --identity use NOTE: (the
                # fact that --identity is an override, and thus overrides eg
                # invoke config file values is part of invoke's config test
                # suite)
                config_kwargs["overrides"] = {
                    "connect_kwargs": {"key_filename": ["configured.key"]}
                }
            conf = Config_(**config_kwargs)
            connect_kwargs = {}
            if kwarg:
                # Stitch in connect_kwargs value
                connect_kwargs = {"key_filename": ["kwarg.key"]}
            # Tie in all sources that were configured & open()
            Connection(
                "runtime", config=conf, connect_kwargs=connect_kwargs
            ).open()
            # Ensure we got the expected list of keys
            kwargs = client.connect.call_args[1]
            if expected:
                assert kwargs["key_filename"] == expected
            else:
                # No key filenames -> it's not even passed in as connect_kwargs
                # is gonna be a blank dict
                assert "key_filename" not in kwargs

    class close:
        def has_no_required_args_and_returns_None(self, client):
            c = Connection("host")
            c.open()
            assert c.close() is None

        def calls_SSHClient_close(self, client):
            "calls paramiko.SSHClient.close()"
            c = Connection("host")
            c.open()
            c.close()
            client.close.assert_called_with()

        @patch("fabric.connection.AgentRequestHandler")
        def calls_agent_handler_close_if_enabled(self, Handler, client):
            c = Connection("host", forward_agent=True)
            c.create_session()
            c.close()
            # NOTE: this will need to change if, for w/e reason, we ever want
            # to run multiple handlers at once
            Handler.return_value.close.assert_called_once_with()

        def short_circuits_if_not_connected(self, client):
            c = Connection("host")
            # Won't trigger close() on client because it'll already think it's
            # closed (due to no .transport & the behavior of .is_connected)
            c.close()
            assert not client.close.called

        def class_works_as_a_closing_contextmanager(self, client):
            with Connection("host") as c:
                c.open()
            client.close.assert_called_once_with()

    class create_session:
        def calls_open_for_you(self, client):
            c = Connection("host")
            c.open = Mock()
            c.transport = Mock()  # so create_session no asplode
            c.create_session()
            assert c.open.called

        @patch("fabric.connection.AgentRequestHandler")
        def activates_paramiko_agent_forwarding_if_configured(
            self, Handler, client
        ):
            c = Connection("host", forward_agent=True)
            chan = c.create_session()
            Handler.assert_called_once_with(chan)

    class run:
        # NOTE: most actual run related tests live in the runners module's
        # tests. Here we are just testing the outer interface a bit.

        @patch(remote_path)
        def calls_open_for_you(self, Remote, client):
            c = Connection("host")
            c.open = Mock()
            c.run("command")
            assert c.open.called

        @patch(remote_path)
        def calls_Remote_run_with_command_and_kwargs_and_returns_its_result(
            self, Remote, client
        ):
            remote = Remote.return_value
            sentinel = object()
            remote.run.return_value = sentinel
            c = Connection("host")
            r1 = c.run("command")
            r2 = c.run("command", warn=True, hide="stderr")
            # NOTE: somehow, .call_args & the methods built on it (like
            # .assert_called_with()) stopped working, apparently triggered by
            # our code...somehow...after commit (roughly) 80906c7.
            # And yet, .call_args_list and its brethren work fine. Wha?
            Remote.assert_any_call(c)
            remote.run.assert_has_calls(
                [call("command"), call("command", warn=True, hide="stderr")]
            )
            for r in (r1, r2):
                assert r is sentinel

    class local:
        # NOTE: most tests for this functionality live in Invoke's runner
        # tests.
        @patch("invoke.config.Local")
        def calls_invoke_Local_run(self, Local):
            Connection("host").local("foo")
            # NOTE: yet another casualty of the bizarre mock issues
            assert call().run("foo") in Local.mock_calls

    class sudo:
        @patch(remote_path)
        def calls_open_for_you(self, Remote, client):
            c = Connection("host")
            c.open = Mock()
            c.sudo("command")
            assert c.open.called

        @patch(remote_path)
        def basic_invocation(self, Remote, client):
            # Technically duplicates Invoke-level tests, but ensures things
            # still work correctly at our level.
            cxn = Connection("host")
            cxn.sudo("foo")
            cmd = "sudo -S -p '{}' foo".format(cxn.config.sudo.prompt)
            # NOTE: this is another spot where Mock.call_args is inexplicably
            # None despite call_args_list being populated. WTF. (Also,
            # Remote.return_value is two different Mocks now, despite Remote's
            # own Mock having the same ID here and in code under test. WTF!!)
            expected = [call(cxn), call().run(cmd, watchers=ANY)]
            assert Remote.mock_calls == expected
            # NOTE: we used to have a "sudo return value is literally the same
            # return value from Remote.run()" sanity check here, which is
            # completely impossible now thanks to the above issue.

        def per_host_password_works_as_expected(self):
            # TODO: needs clearly defined "per-host" config API, if a distinct
            # one is necessary besides "the config obj handed in when
            # instantiating the Connection".
            # E.g. generate a Connection pulling in a sudo.password value from
            # what would be a generic conf file or similar, *and* one more
            # specific to that particular Connection (perhaps simply the
            # 'override' level?), w/ test asserting the more-specific value is
            # what's submitted.
            skip()

    class sftp:
        def returns_result_of_client_open_sftp(self, client):
            "returns result of client.open_sftp()"
            sentinel = object()
            client.open_sftp.return_value = sentinel
            assert Connection("host").sftp() == sentinel
            client.open_sftp.assert_called_with()

        def lazily_caches_result(self, client):
            sentinel1, sentinel2 = object(), object()
            client.open_sftp.side_effect = [sentinel1, sentinel2]
            cxn = Connection("host")
            first = cxn.sftp()
            # TODO: why aren't we just asserting about calls of open_sftp???
            err = "{0!r} wasn't the sentinel object()!"
            assert first is sentinel1, err.format(first)
            second = cxn.sftp()
            assert second is sentinel1, err.format(second)

    class get:
        @patch("fabric.connection.Transfer")
        def calls_Transfer_get(self, Transfer):
            "calls Transfer.get()"
            c = Connection("host")
            c.get("meh")
            Transfer.assert_called_with(c)
            Transfer.return_value.get.assert_called_with("meh")

    class put:
        @patch("fabric.connection.Transfer")
        def calls_Transfer_put(self, Transfer):
            "calls Transfer.put()"
            c = Connection("host")
            c.put("meh")
            Transfer.assert_called_with(c)
            Transfer.return_value.put.assert_called_with("meh")

    class forward_local:
        @patch("fabric.tunnels.select")
        @patch("fabric.tunnels.socket.socket")
        @patch("fabric.connection.SSHClient")
        def _forward_local(self, kwargs, Client, mocket, select):
            # Tease out bits of kwargs for use in the mocking/expecting.
            # But leave it alone for raw passthru to the API call itself.
            # TODO: unhappy with how much this apes the real code & its sig...
            local_port = kwargs["local_port"]
            remote_port = kwargs.get("remote_port", local_port)
            local_host = kwargs.get("local_host", "localhost")
            remote_host = kwargs.get("remote_host", "localhost")
            # These aren't part of the real sig, but this is easier than trying
            # to reconcile the mock decorators + optional-value kwargs. meh.
            tunnel_exception = kwargs.pop("tunnel_exception", None)
            listener_exception = kwargs.pop("listener_exception", False)
            # Mock setup
            client = Client.return_value
            listener_sock = Mock(name="listener_sock")
            if listener_exception:
                listener_sock.bind.side_effect = listener_exception
            data = b("Some data")
            tunnel_sock = Mock(name="tunnel_sock", recv=lambda n: data)
            local_addr = Mock()
            transport = client.get_transport.return_value
            channel = transport.open_channel.return_value
            # socket.socket is only called once directly
            mocket.return_value = listener_sock
            # The 2nd socket is obtained via an accept() (which should only
            # fire once & raise EAGAIN after)
            listener_sock.accept.side_effect = chain(
                [(tunnel_sock, local_addr)],
                repeat(socket.error(errno.EAGAIN, "nothing yet")),
            )
            obj = tunnel_sock if tunnel_exception is None else tunnel_exception
            select.select.side_effect = _select_result(obj)
            with Connection("host").forward_local(**kwargs):
                # Make sure we give listener thread enough time to boot up :(
                # Otherwise we might assert before it does things. (NOTE:
                # doesn't need to be much, even at 0.01s, 0/100 trials failed
                # (vs 45/100 with no sleep)
                time.sleep(0.015)
                assert client.connect.call_args[1]["hostname"] == "host"
                listener_sock.setsockopt.assert_called_once_with(
                    socket.SOL_SOCKET, socket.SO_REUSEADDR, 1
                )
                listener_sock.setblocking.assert_called_once_with(0)
                listener_sock.bind.assert_called_once_with(
                    (local_host, local_port)
                )
                if not listener_exception:
                    listener_sock.listen.assert_called_once_with(1)
                    transport.open_channel.assert_called_once_with(
                        "direct-tcpip", (remote_host, remote_port), local_addr
                    )
                # Local write to tunnel_sock is implied by its mocked-out
                # recv() call above...
                # NOTE: don't assert if explodey; we want to mimic "the only
                # error that occurred was within the thread" behavior being
                # tested by thread-exception-handling tests
                if not (tunnel_exception or listener_exception):
                    channel.sendall.assert_called_once_with(data)
            # Shutdown, with another sleep because threads.
            time.sleep(0.015)
            if not listener_exception:
                tunnel_sock.close.assert_called_once_with()
                channel.close.assert_called_once_with()
                listener_sock.close.assert_called_once_with()

        def forwards_local_port_to_remote_end(self):
            self._forward_local({"local_port": 1234})

        def distinct_remote_port(self):
            self._forward_local({"local_port": 1234, "remote_port": 4321})

        def non_localhost_listener(self):
            self._forward_local(
                {"local_port": 1234, "local_host": "nearby_local_host"}
            )

        def non_remote_localhost_connection(self):
            self._forward_local(
                {"local_port": 1234, "remote_host": "nearby_remote_host"}
            )

        def _thread_error(self, which):
            class Sentinel(Exception):
                pass

            try:
                self._forward_local(
                    {
                        "local_port": 1234,
                        "{}_exception".format(which): Sentinel,
                    }
                )
            except ThreadException as e:
                # NOTE: ensures that we're getting what we expected and not
                # some deeper, test-bug related error
                assert len(e.exceptions) == 1
                inner = e.exceptions[0]
                err = "Expected wrapped exception to be Sentinel, was {}"
                assert inner.type is Sentinel, err.format(inner.type.__name__)
            else:
                # no exception happened :( implies the thread went boom but
                # nobody noticed
                err = "Failed to get ThreadException on {} error"
                assert False, err.format(which)

        def tunnel_errors_bubble_up(self):
            self._thread_error("tunnel")

        def tunnel_manager_errors_bubble_up(self):
            self._thread_error("listener")

        # TODO: these require additional refactoring of _forward_local to be
        # more like the decorators in _util
        def multiple_tunnels_can_be_open_at_once(self):
            skip()

    class forward_remote:
        @patch("fabric.connection.socket.socket")
        @patch("fabric.tunnels.select")
        @patch("fabric.connection.SSHClient")
        def _forward_remote(self, kwargs, Client, select, mocket):
            # TODO: unhappy with how much this duplicates of the code under
            # test, re: sig/default vals
            # Set up parameter values/defaults
            remote_port = kwargs["remote_port"]
            remote_host = kwargs.get("remote_host", "127.0.0.1")
            local_port = kwargs.get("local_port", remote_port)
            local_host = kwargs.get("local_host", "localhost")
            # Mock/etc setup, anything that can be prepped before the forward
            # occurs (which is most things)
            tun_socket = mocket.return_value
            cxn = Connection("host")
            # Channel that will yield data when read from
            chan = Mock()
            chan.recv.return_value = "data"
            # And make select() yield it as being ready once, when called
            select.select.side_effect = _select_result(chan)
            with cxn.forward_remote(**kwargs):
                # At this point Connection.open() has run and generated a
                # Transport mock for us (because SSHClient is mocked). Let's
                # first make sure we asked it for the port forward...
                # NOTE: this feels like it's too limited/tautological a test,
                # until you realize that it's functionally impossible to mock
                # out everything required for Paramiko's inner guts to run
                # _parse_channel_open() and suchlike :(
                call = cxn.transport.request_port_forward.call_args_list[0]
                assert call[1]["address"] == remote_host
                assert call[1]["port"] == remote_port
                # Pretend the Transport called our callback with mock Channel
                call[1]["handler"](chan, tuple(), tuple())
                # Then have to sleep a bit to make sure we give the tunnel
                # created by that callback to spin up; otherwise ~5% of the
                # time we exit the contextmanager so fast, the tunnel's "you're
                # done!" flag is set before it even gets a chance to select()
                # once.
                time.sleep(0.01)
                # And make sure we hooked up to the local socket OK
                tup = (local_host, local_port)
                tun_socket.connect.assert_called_once_with(tup)
            # Expect that our socket got written to by the tunnel (due to the
            # above-setup select() and channel mocking). Need to do this after
            # tunnel shutdown or we risk thread ordering issues.
            tun_socket.sendall.assert_called_once_with("data")
            # Ensure we closed down the mock socket
            mocket.return_value.close.assert_called_once_with()
            # And that the transport canceled the port forward on the remote
            # end.
            assert cxn.transport.cancel_port_forward.call_count == 1

        def forwards_remote_port_to_local_end(self):
            self._forward_remote({"remote_port": 1234})

        def distinct_local_port(self):
            self._forward_remote({"remote_port": 1234, "local_port": 4321})

        def non_localhost_connections(self):
            self._forward_remote(
                {"remote_port": 1234, "local_host": "nearby_local_host"}
            )

        def remote_non_localhost_listener(self):
            self._forward_remote(
                {"remote_port": 1234, "remote_host": "192.168.1.254"}
            )

        # TODO: these require additional refactoring of _forward_remote to be
        # more like the decorators in _util
        def multiple_tunnels_can_be_open_at_once(self):
            skip()

        def tunnel_errors_bubble_up(self):
            skip()

        def listener_errors_bubble_up(self):
            skip()
<EOF>
<BOF>
from itertools import chain, repeat
from io import BytesIO
import os
import re
import sys

from mock import patch, Mock, PropertyMock, call, ANY
from pytest_relaxed import trap

from fabric import Connection as Connection_, Config as Config_
from fabric.main import make_program
from paramiko import SSHConfig


support = os.path.join(os.path.abspath(os.path.dirname(__file__)), "_support")
config_file = os.path.abspath(os.path.join(support, "config.yml"))


# TODO: revert to asserts
def eq_(got, expected):
    assert got == expected


# TODO: this could become a fixture in conftest.py, presumably, and just yield
# stdout, allowing the tests themselves to assert more naturally
@trap
def expect(invocation, out, program=None, test="equals"):
    if program is None:
        program = make_program()
    program.run("fab {}".format(invocation), exit=False)
    output = sys.stdout.getvalue()
    if test == "equals":
        assert output == out
    elif test == "contains":
        assert out in output
    elif test == "regex":
        assert re.match(out, output)
    else:
        err = "Don't know how to expect that <stdout> {} <expected>!"
        assert False, err.format(test)


class Command(object):
    """
    Data record specifying params of a command execution to mock/expect.

    :param str cmd:
        Command string to expect. If not given, no expectations about the
        command executed will be set up. Default: ``None``.

    :param bytes out: Data yielded as remote stdout. Default: ``b""``.

    :param bytes err: Data yielded as remote stderr. Default: ``b""``.

    :param int exit: Remote exit code. Default: ``0``.

    :param int waits:
        Number of calls to the channel's ``exit_status_ready`` that should
        return ``False`` before it then returns ``True``. Default: ``0``
        (``exit_status_ready`` will return ``True`` immediately).
    """

    def __init__(self, cmd=None, out=b"", err=b"", in_=None, exit=0, waits=0):
        self.cmd = cmd
        self.out = out
        self.err = err
        self.in_ = in_
        self.exit = exit
        self.waits = waits


class MockChannel(Mock):
    """
    Mock subclass that tracks state for its ``recv(_stderr)?`` methods.

    Turns out abusing function closures inside MockRemote to track this state
    only worked for 1 command per session!
    """

    def __init__(self, *args, **kwargs):
        # TODO: worth accepting strings and doing the BytesIO setup ourselves?
        # Stored privately to avoid any possible collisions ever. shrug.
        object.__setattr__(self, "__stdout", kwargs.pop("stdout"))
        object.__setattr__(self, "__stderr", kwargs.pop("stderr"))
        # Stdin less private so it can be asserted about
        object.__setattr__(self, "_stdin", BytesIO())
        super(MockChannel, self).__init__(*args, **kwargs)

    def _get_child_mock(self, **kwargs):
        # Don't return our own class on sub-mocks.
        return Mock(**kwargs)

    def recv(self, count):
        return object.__getattribute__(self, "__stdout").read(count)

    def recv_stderr(self, count):
        return object.__getattribute__(self, "__stderr").read(count)

    def sendall(self, data):
        return object.__getattribute__(self, "_stdin").write(data)


class Session(object):
    """
    A mock remote session of a single connection and 1 or more command execs.

    Allows quick configuration of expected remote state, and also helps
    generate the necessary test mocks used by `MockRemote` itself. Only useful
    when handed into `MockRemote`.

    The parameters ``cmd``, ``out``, ``err``, ``exit`` and ``waits`` are all
    shorthand for the same constructor arguments for a single anonymous
    `.Command`; see `.Command` for details.

    To give fully explicit `.Command` objects, use the ``commands`` parameter.

    :param str user:
    :param str host:
    :param int port:
        Sets up expectations that a connection will be generated to the given
        user, host and/or port. If ``None`` (default), no expectations are
        generated / any value is accepted.

    :param commands:
        Iterable of `.Command` objects, used when mocking nontrivial sessions
        involving >1 command execution per host. Default: ``None``.

        .. note::
            Giving ``cmd``, ``out`` etc alongside explicit ``commands`` is not
            allowed and will result in an error.
    """

    def __init__(
        self,
        host=None,
        user=None,
        port=None,
        commands=None,
        cmd=None,
        out=None,
        in_=None,
        err=None,
        exit=None,
        waits=None,
    ):
        # Sanity check
        params = cmd or out or err or exit or waits
        if commands and params:
            raise ValueError(
                "You can't give both 'commands' and individual "
                "Command parameters!"
            )  # noqa
        # Fill in values
        self.host = host
        self.user = user
        self.port = port
        self.commands = commands
        if params:
            # Honestly dunno which is dumber, this or duplicating Command's
            # default kwarg values in this method's signature...sigh
            kwargs = {}
            if cmd is not None:
                kwargs["cmd"] = cmd
            if out is not None:
                kwargs["out"] = out
            if err is not None:
                kwargs["err"] = err
            if in_ is not None:
                kwargs["in_"] = in_
            if exit is not None:
                kwargs["exit"] = exit
            if waits is not None:
                kwargs["waits"] = waits
            self.commands = [Command(**kwargs)]
        if not self.commands:
            self.commands = [Command()]

    def generate_mocks(self):
        """
        Sets up a mock `.SSHClient` and one or more mock `Channel` objects.

        Specifically, the client will expect itself to be connected to
        ``self.host`` (if given), the channels will be associated with the
        client's `.Transport`, and the channels will expect/provide
        command-execution behavior as specified on the `.Command` objects
        supplied to this `.Session`.

        The client is then attached as ``self.client`` and the channels as
        ``self.channels`.

        :returns:
            ``None`` - this is mostly a "deferred setup" method and callers
            will just reference the above attributes (and call more methods) as
            needed.
        """
        client = Mock()
        transport = client.get_transport.return_value  # another Mock

        # NOTE: this originally did chain([False], repeat(True)) so that
        # get_transport().active was False initially, then True. However,
        # because we also have to consider when get_transport() comes back None
        # (which it does initially), the case where we get back a non-None
        # transport _and_ it's not active yet, isn't useful to test, and
        # complicates text expectations. So we don't, for now.
        actives = repeat(True)
        # NOTE: setting PropertyMocks on a mock's type() is apparently
        # How It Must Be Done, otherwise it sets the real attr value.
        type(transport).active = PropertyMock(side_effect=actives)

        channels = []
        for command in self.commands:
            # Mock of a Channel instance, not e.g. Channel-the-class.
            # Specifically, one that can track individual state for recv*().
            channel = MockChannel(
                stdout=BytesIO(command.out), stderr=BytesIO(command.err)
            )
            channel.recv_exit_status.return_value = command.exit

            # If requested, make exit_status_ready return False the first N
            # times it is called in the wait() loop.
            readies = chain(repeat(False, command.waits), repeat(True))
            channel.exit_status_ready.side_effect = readies

            channels.append(channel)

        # Have our transport yield those channel mocks in order when
        # open_session() is called.
        transport.open_session.side_effect = channels

        self.client = client
        self.channels = channels

    def sanity_check(self):
        # Per-session we expect a single transport get
        transport = self.client.get_transport
        transport.assert_called_once_with()
        # And a single connect to our target host.
        self.client.connect.assert_called_once_with(
            username=self.user or ANY,
            hostname=self.host or ANY,
            port=self.port or ANY,
        )

        # Calls to open_session will be 1-per-command but are on transport, not
        # channel, so we can only really inspect how many happened in
        # aggregate. Save a list for later comparison to call_args.
        session_opens = []

        for channel, command in zip(self.channels, self.commands):
            # Expect an open_session for each command exec
            session_opens.append(call())
            # Expect that the channel gets an exec_command
            channel.exec_command.assert_called_with(command.cmd or ANY)
            # Expect written stdin, if given
            if command.in_:
                eq_(channel._stdin.getvalue(), command.in_)

        # Make sure open_session was called expected number of times.
        eq_(transport.return_value.open_session.call_args_list, session_opens)


class MockRemote(object):
    """
    Class representing mocked remote state.

    Set up for start/stop style patching (so it can be used in situations
    requiring setup/teardown semantics); is then wrapped by the `remote`
    fixture.

    Defaults to a single anonymous `Session`, so it can be used as a "request &
    forget" pytest fixture. Users requiring detailed remote session
    expectations can call methods like `expect`, which wipe that anonymous
    Session & set up a new one instead.
    """

    def __init__(self):
        self.expect_sessions(Session())

    # TODO: make it easier to assume single session w/ >1 command?

    def expect(self, *args, **kwargs):
        """
        Convenience method for creating & 'expect'ing a single `Session`.

        Returns the single `MockChannel` yielded by that Session.
        """
        return self.expect_sessions(Session(*args, **kwargs))[0]

    def expect_sessions(self, *sessions):
        """
        Sets the mocked remote environment to expect the given ``sessions``.

        Returns a list of `MockChannel` objects, one per input `Session`.
        """
        # First, stop the default session to clean up its state, if it seems to
        # be running.
        self.stop()
        # Update sessions list with new session(s)
        self.sessions = sessions
        # And start patching again, returning mocked channels
        return self.start()

    def start(self):
        """
        Start patching SSHClient with the stored sessions, returning channels.
        """
        # Patch SSHClient so the sessions' generated mocks can be set as its
        # return values
        self.patcher = patcher = patch("fabric.connection.SSHClient")
        SSHClient = patcher.start()
        # Mock clients, to be inspected afterwards during sanity-checks
        clients = []
        for session in self.sessions:
            session.generate_mocks()
            clients.append(session.client)
        # Each time the mocked SSHClient class is instantiated, it will
        # yield one of our mocked clients (w/ mocked transport & channel)
        # generated above.
        SSHClient.side_effect = clients
        return list(chain.from_iterable(x.channels for x in self.sessions))

    def stop(self):
        """
        Stop patching SSHClient.
        """
        # Short circuit if we don't seem to have start()ed yet.
        if not hasattr(self, "patcher"):
            return
        # Stop patching SSHClient
        self.patcher.stop()

    def sanity(self):
        """
        Run post-execution sanity checks (usually 'was X called' tests.)
        """
        for session in self.sessions:
            # Basic sanity tests about transport, channel etc
            session.sanity_check()


# TODO: unify with the stuff in paramiko itself (now in its tests/conftest.py),
# they're quite distinct and really shouldn't be.
class MockSFTP(object):
    """
    Class managing mocked SFTP remote state.

    Used in start/stop fashion in eg doctests; wrapped in the SFTP fixtures in
    conftest.py for main use.
    """

    def __init__(self, autostart=True):
        if autostart:
            self.start()

    def start(self):
        # Set up mocks
        self.os_patcher = patch("fabric.transfer.os")
        self.client_patcher = patch("fabric.connection.SSHClient")
        mock_os = self.os_patcher.start()
        Client = self.client_patcher.start()
        sftp = Client.return_value.open_sftp.return_value

        # Handle common filepath massage actions; tests will assume these.
        def fake_abspath(path):
            return "/local/{}".format(path)

        mock_os.path.abspath.side_effect = fake_abspath
        sftp.getcwd.return_value = "/remote"
        # Ensure stat st_mode is a real number; Python 2 stat.S_IMODE doesn't
        # appear to care if it's handed a MagicMock, but Python 3's does (?!)
        fake_mode = 0o644  # arbitrary real-ish mode
        sftp.stat.return_value.st_mode = fake_mode
        mock_os.stat.return_value.st_mode = fake_mode
        # Not super clear to me why the 'wraps' functionality in mock isn't
        # working for this :(
        mock_os.path.basename.side_effect = os.path.basename
        # Return the sftp and OS mocks for use by decorator use case.
        return sftp, mock_os

    def stop(self):
        self.os_patcher.stop()
        self.client_patcher.stop()


# Locally override Connection, Config with versions that supply a dummy
# SSHConfig and thus don't load any test-running user's own ssh_config files.
# TODO: find a cleaner way to do this, though I don't really see any that isn't
# adding a ton of fixtures everywhere (and thus, opening up to forgetting it
# for new tests...)
class Config(Config_):
    def __init__(self, *args, **kwargs):
        wat = "You're giving ssh_config explicitly, please use Config_!"
        assert "ssh_config" not in kwargs, wat
        # Give ssh_config explicitly -> shorter way of turning off loading
        kwargs["ssh_config"] = SSHConfig()
        super(Config, self).__init__(*args, **kwargs)


class Connection(Connection_):
    def __init__(self, *args, **kwargs):
        # Make sure we're using our tweaked Config if none was given.
        kwargs.setdefault("config", Config())
        super(Connection, self).__init__(*args, **kwargs)
<EOF>
<BOF>
import errno
from os.path import join, expanduser

from paramiko.config import SSHConfig

from fabric import Config
from fabric.util import get_local_user

from mock import patch, call

from _util import support


class Config_:
    def defaults_to_merger_of_global_defaults(self):
        # I.e. our global_defaults + Invoke's global_defaults
        c = Config()
        # From invoke's global_defaults
        assert c.run.warn is False
        # From ours
        assert c.port == 22

    def our_global_defaults_can_override_invokes(self):
        "our global_defaults can override Invoke's key-by-key"
        with patch.object(
            Config,
            "global_defaults",
            return_value={
                "run": {"warn": "nope lol"},
                # NOTE: Config requires these to be present to instantiate
                # happily
                "load_ssh_configs": True,
                "ssh_config_path": None,
            },
        ):
            # If our global_defaults didn't win, this would still
            # resolve to False.
            assert Config().run.warn == "nope lol"

    def has_various_Fabric_specific_default_keys(self):
        c = Config()
        assert c.port == 22
        assert c.user == get_local_user()
        assert c.forward_agent is False
        assert c.connect_kwargs == {}
        assert c.timeouts.connect is None
        assert c.ssh_config_path is None

    def overrides_some_Invoke_defaults(self):
        config = Config()
        # This value defaults to False in Invoke proper.
        assert config.run.replace_env is True
        assert config.tasks.collection_name == "fabfile"

    def uses_Fabric_prefix(self):
        # NOTE: see also the integration-esque tests in tests/main.py; this
        # just tests the underlying data/attribute driving the behavior.
        assert Config().prefix == "fabric"


class ssh_config_loading:
    "ssh_config loading"

    # NOTE: actual _behavior_ of loaded SSH configs is tested in Connection's
    # tests; these tests just prove that the loading itself works & the data is
    # correctly available.

    _system_path = join(support, "ssh_config", "system.conf")
    _user_path = join(support, "ssh_config", "user.conf")
    _runtime_path = join(support, "ssh_config", "runtime.conf")
    _empty_kwargs = dict(
        system_ssh_path="nope/nope/nope", user_ssh_path="nope/noway/nuhuh"
    )

    def defaults_to_empty_sshconfig_obj_if_no_files_found(self):
        c = Config(**self._empty_kwargs)
        # TODO: Currently no great public API that lets us figure out if
        # one of these is 'empty' or not. So for now, expect an empty inner
        # SSHConfig._config from an un-.parse()d such object. (AFAIK, such
        # objects work fine re: .lookup, .get_hostnames etc.)
        assert type(c.base_ssh_config) is SSHConfig
        assert c.base_ssh_config._config == []

    def object_can_be_given_explicitly_via_ssh_config_kwarg(self):
        sc = SSHConfig()
        assert Config(ssh_config=sc).base_ssh_config is sc

    @patch.object(Config, "_load_ssh_file")
    def when_config_obj_given_default_paths_are_not_sought(self, method):
        sc = SSHConfig()
        Config(ssh_config=sc)
        assert not method.called

    @patch.object(Config, "_load_ssh_file")
    def config_obj_prevents_loading_runtime_path_too(self, method):
        sc = SSHConfig()
        Config(ssh_config=sc, runtime_ssh_path=self._system_path)
        assert not method.called

    @patch.object(Config, "_load_ssh_file")
    def when_runtime_path_given_other_paths_are_not_sought(self, method):
        Config(runtime_ssh_path=self._runtime_path)
        method.assert_called_once_with(self._runtime_path)

    @patch.object(Config, "_load_ssh_file")
    def runtime_path_can_be_given_via_config_itself(self, method):
        Config(overrides={"ssh_config_path": self._runtime_path})
        method.assert_called_once_with(self._runtime_path)

    def runtime_path_does_not_die_silently(self):
        try:
            Config(runtime_ssh_path="sure/thing/boss/whatever/you/say")
        except IOError as e:
            assert "No such file or directory" in str(e)
            assert e.errno == errno.ENOENT
        else:
            assert False, "Bad runtime path didn't raise IOError!"

    # TODO: skip on windows
    @patch.object(Config, "_load_ssh_file")
    def default_file_paths_match_openssh(self, method):
        Config()
        method.assert_has_calls(
            [call(expanduser("~/.ssh/config")), call("/etc/ssh/ssh_config")]
        )

    def system_path_loads_ok(self):
        c = Config(
            **dict(self._empty_kwargs, system_ssh_path=self._system_path)
        )
        names = c.base_ssh_config.get_hostnames()
        assert names == {"system", "shared", "*"}

    def user_path_loads_ok(self):
        c = Config(**dict(self._empty_kwargs, user_ssh_path=self._user_path))
        names = c.base_ssh_config.get_hostnames()
        assert names == {"user", "shared", "*"}

    def both_paths_loaded_if_both_exist_with_user_winning(self):
        c = Config(
            user_ssh_path=self._user_path, system_ssh_path=self._system_path
        )
        names = c.base_ssh_config.get_hostnames()
        expected = {"user", "system", "shared", "*"}
        assert names == expected
        # Expect the user value (321), not the system one (123)
        assert c.base_ssh_config.lookup("shared")["port"] == "321"

    @patch.object(Config, "_load_ssh_file")
    @patch("fabric.config.os.path.exists", lambda x: True)
    def runtime_path_subject_to_user_expansion(self, method):
        # TODO: other expansion types? no real need for abspath...
        tilded = "~/probably/not/real/tho"
        Config(runtime_ssh_path=tilded)
        method.assert_called_once_with(expanduser(tilded))

    @patch.object(Config, "_load_ssh_file")
    def user_path_subject_to_user_expansion(self, method):
        # TODO: other expansion types? no real need for abspath...
        tilded = "~/probably/not/real/tho"
        Config(user_ssh_path=tilded)
        method.assert_any_call(expanduser(tilded))

    class core_ssh_load_option_allows_skipping_ssh_config_loading:
        @patch.object(Config, "_load_ssh_file")
        def skips_default_paths(self, method):
            Config(overrides={"load_ssh_configs": False})
            assert not method.called

        @patch.object(Config, "_load_ssh_file")
        def does_not_affect_explicit_object(self, method):
            sc = SSHConfig()
            c = Config(ssh_config=sc, overrides={"load_ssh_configs": False})
            # Implicit loading still doesn't happen...sanity check
            assert not method.called
            # Real test: the obj we passed in is present as usual
            assert c.base_ssh_config is sc

        @patch.object(Config, "_load_ssh_file")
        def does_not_skip_loading_runtime_path(self, method):
            Config(
                runtime_ssh_path=self._runtime_path,
                overrides={"load_ssh_configs": False},
            )
            # Expect that loader method did still run (and, as usual, that
            # it did not load any other files)
            method.assert_called_once_with(self._runtime_path)

    class lazy_loading_and_explicit_methods:
        @patch.object(Config, "_load_ssh_file")
        def may_use_lazy_plus_explicit_methods_to_control_flow(self, method):
            c = Config(lazy=True)
            assert not method.called
            c.set_runtime_ssh_path(self._runtime_path)
            c.load_ssh_config()
            method.assert_called_once_with(self._runtime_path)
<EOF>
<BOF>
import fabric
from fabric import _version, connection, runners, group


class init:
    "__init__"

    def version_and_version_info(self):
        for name in ("__version_info__", "__version__"):
            assert getattr(_version, name) == getattr(fabric, name)

    def Connection(self):
        assert fabric.Connection is connection.Connection

    def Remote(self):
        assert fabric.Remote is runners.Remote

    def Result(self):
        assert fabric.Result is runners.Result

    def Config(self):
        assert fabric.Config is connection.Config

    def Group(self):
        assert fabric.Group is group.Group

    def SerialGroup(self):
        assert fabric.SerialGroup is group.SerialGroup

    def ThreadingGroup(self):
        assert fabric.ThreadingGroup is group.ThreadingGroup

    def GroupResult(self):
        assert fabric.GroupResult is group.GroupResult
<EOF>
<BOF>
"""
Tests testing the fabric.util module, not utils for the tests!
"""

from mock import patch

from fabric.util import get_local_user


# Basically implementation tests, because it's not feasible to do a "real" test
# on random platforms (where we have no idea what the actual invoking user is)
class get_local_user_:
    @patch("getpass.getuser")
    def defaults_to_getpass_getuser(self, getuser):
        "defaults to getpass.getuser"
        get_local_user()
        getuser.assert_called_once_with()

    @patch("getpass.getuser", side_effect=KeyError)
    def KeyError_means_SaaS_and_thus_None(self, getuser):
        assert get_local_user() is None

    # TODO: test for ImportError+win32 once appveyor is set up as w/ invoke
<EOF>
<BOF>
from mock import Mock, patch, call
from pytest_relaxed import raises

from fabric import Connection, Group, SerialGroup, ThreadingGroup, GroupResult
from fabric.group import thread_worker
from fabric.exceptions import GroupException


class Group_:
    class init:
        "__init__"

        def may_be_empty(self):
            assert len(Group()) == 0

        def takes_splat_arg_of_host_strings(self):
            g = Group("foo", "bar")
            assert g[0].host == "foo"
            assert g[1].host == "bar"

    class from_connections:
        def inits_from_iterable_of_Connections(self):
            g = Group.from_connections((Connection("foo"), Connection("bar")))
            assert len(g) == 2
            assert g[1].host == "bar"

    def acts_like_an_iterable_of_Connections(self):
        g = Group("foo", "bar", "biz")
        assert g[0].host == "foo"
        assert g[-1].host == "biz"
        assert len(g) == 3
        for c in g:
            assert isinstance(c, Connection)

    class run:
        @raises(NotImplementedError)
        def not_implemented_in_base_class(self):
            Group().run()


def _make_serial_tester(cxns, index, args, kwargs):
    args = args[:]
    kwargs = kwargs.copy()

    def tester(*a, **k):  # Don't care about doing anything with our own args.
        car, cdr = index, index + 1
        predecessors = cxns[:car]
        successors = cxns[cdr:]
        for predecessor in predecessors:
            predecessor.run.assert_called_with(*args, **kwargs)
        for successor in successors:
            assert not successor.run.called

    return tester


class SerialGroup_:
    class run:
        def executes_arguments_on_contents_run_serially(self):
            "executes arguments on contents' run() serially"
            cxns = [Connection(x) for x in ("host1", "host2", "host3")]
            args = ("command",)
            kwargs = {"hide": True, "warn": True}
            for index, cxn in enumerate(cxns):
                side_effect = _make_serial_tester(cxns, index, args, kwargs)
                cxn.run = Mock(side_effect=side_effect)
            g = SerialGroup.from_connections(cxns)
            g.run(*args, **kwargs)
            # Sanity check, e.g. in case none of them were actually run
            for cxn in cxns:
                cxn.run.assert_called_with(*args, **kwargs)

        def errors_in_execution_capture_and_continue_til_end(self):
            cxns = [Mock(name=x) for x in ("host1", "host2", "host3")]

            class OhNoz(Exception):
                pass

            onoz = OhNoz()
            cxns[1].run.side_effect = onoz
            g = SerialGroup.from_connections(cxns)
            try:
                g.run("whatever", hide=True)
            except GroupException as e:
                result = e.result
            else:
                assert False, "Did not raise GroupException!"
            succeeded = {
                cxns[0]: cxns[0].run.return_value,
                cxns[2]: cxns[2].run.return_value,
            }
            failed = {cxns[1]: onoz}
            expected = succeeded.copy()
            expected.update(failed)
            assert result == expected
            assert result.succeeded == succeeded
            assert result.failed == failed

        def returns_results_mapping(self):
            cxns = [Mock(name=x) for x in ("host1", "host2", "host3")]
            g = SerialGroup.from_connections(cxns)
            result = g.run("whatever", hide=True)
            assert isinstance(result, GroupResult)
            expected = {x: x.run.return_value for x in cxns}
            assert result == expected
            assert result.succeeded == expected
            assert result.failed == {}


class ThreadingGroup_:
    def setup(self):
        self.cxns = [Connection(x) for x in ("host1", "host2", "host3")]
        self.args = ("command",)
        self.kwargs = {"hide": True, "warn": True}

    class run:
        @patch("fabric.group.Queue")
        @patch("fabric.group.ExceptionHandlingThread")
        def executes_arguments_on_contents_run_via_threading(
            self, Thread, Queue
        ):
            queue = Queue.return_value
            g = ThreadingGroup.from_connections(self.cxns)
            # Make sure .exception() doesn't yield truthy Mocks. Otherwise we
            # end up with 'exceptions' that cause errors due to all being the
            # same.
            Thread.return_value.exception.return_value = None
            g.run(*self.args, **self.kwargs)
            # Testing that threads were used the way we expect is mediocre but
            # I honestly can't think of another good way to assert "threading
            # was used & concurrency occurred"...
            instantiations = [
                call(
                    target=thread_worker,
                    kwargs=dict(
                        cxn=cxn,
                        queue=queue,
                        args=self.args,
                        kwargs=self.kwargs,
                    ),
                )
                for cxn in self.cxns
            ]
            Thread.assert_has_calls(instantiations, any_order=True)
            # These ought to work as by default a Mock.return_value is a
            # singleton mock object
            expected = len(self.cxns)
            for name, got in (
                ("start", Thread.return_value.start.call_count),
                ("join", Thread.return_value.join.call_count),
            ):
                err = (
                    "Expected {} calls to ExceptionHandlingThread.{}, got {}"
                )  # noqa
                err = err.format(expected, name, got)
                assert expected, got == err

        @patch("fabric.group.Queue")
        def queue_used_to_return_results(self, Queue):
            # Regular, explicit, mocks for Connections
            cxns = [Mock(host=x) for x in ("host1", "host2", "host3")]
            # Set up Queue with enough behavior to work / assert
            queue = Queue.return_value
            # Ending w/ a True will terminate a while-not-empty loop
            queue.empty.side_effect = (False, False, False, True)
            fakes = [(x, x.run.return_value) for x in cxns]
            queue.get.side_effect = fakes[:]
            # Execute & inspect results
            g = ThreadingGroup.from_connections(cxns)
            results = g.run(*self.args, **self.kwargs)
            expected = {x: x.run.return_value for x in cxns}
            assert results == expected
            # Make sure queue was used as expected within worker &
            # ThreadingGroup.run()
            puts = [call(x) for x in fakes]
            queue.put.assert_has_calls(puts, any_order=True)
            assert queue.empty.called
            gets = [call(block=False) for _ in cxns]
            queue.get.assert_has_calls(gets)

        def bubbles_up_errors_within_threads(self):
            # TODO: I feel like this is the first spot where a raw
            # ThreadException might need tweaks, at least presentation-wise,
            # since we're no longer dealing with truly background threads (IO
            # workers and tunnels), but "middle-ground" threads the user is
            # kind of expecting (and which they might expect to encounter
            # failures).
            cxns = [Mock(host=x) for x in ("host1", "host2", "host3")]

            class OhNoz(Exception):
                pass

            onoz = OhNoz()
            cxns[1].run.side_effect = onoz
            g = ThreadingGroup.from_connections(cxns)
            try:
                g.run(*self.args, **self.kwargs)
            except GroupException as e:
                result = e.result
            else:
                assert False, "Did not raise GroupException!"
            succeeded = {
                cxns[0]: cxns[0].run.return_value,
                cxns[2]: cxns[2].run.return_value,
            }
            failed = {cxns[1]: onoz}
            expected = succeeded.copy()
            expected.update(failed)
            assert result == expected
            assert result.succeeded == succeeded
            assert result.failed == failed

        def returns_results_mapping(self):
            # TODO: update if/when we implement ResultSet
            cxns = [Mock(name=x) for x in ("host1", "host2", "host3")]
            g = ThreadingGroup.from_connections(cxns)
            result = g.run("whatever", hide=True)
            assert isinstance(result, GroupResult)
            expected = {x: x.run.return_value for x in cxns}
            assert result == expected
            assert result.succeeded == expected
            assert result.failed == {}
<EOF>
<BOF>
"""
Tests concerned with the ``fab`` tool & how it overrides Invoke defaults.
"""

import os
import sys

from invoke.util import cd
from mock import patch
import pytest  # because WHY would you expose @skip normally? -_-
from pytest_relaxed import raises

from fabric.config import Config
from fabric.main import make_program
from fabric.exceptions import NothingToDo

from _util import expect, Session, support, config_file, trap


# Designate a runtime config file intended for the test environment; it does
# things like automatically mute stdin so test harnesses that care about stdin
# don't get upset.
# NOTE: this requires the test environment to have Invoke 1.1.0 or above; for
# now this is fine as we don't do a big serious matrix, we typically use Invoke
# master to allow testing in-dev changes.
# TODO: if that _changes_ then we may have to rethink this so that it goes back
# to being testable on Invoke >=1.0 instead of >=1.1...
os.environ["INVOKE_RUNTIME_CONFIG"] = config_file


class Fab_:
    class core_program_behavior:
        def version_output_contains_our_name_plus_deps(self):
            expect(
                "--version",
                r"""
Fabric .+
Paramiko .+
Invoke .+
""".strip(),
                test="regex",
            )

        def help_output_says_fab(self):
            expect("--help", "Usage: fab", test="contains")

        def exposes_hosts_flag_in_help(self):
            expect("--help", "-H STRING, --hosts=STRING", test="contains")

        def executes_remainder_as_anonymous_task(self, remote):
            remote.expect(host="myhost", cmd="whoami")
            make_program().run("fab -H myhost -- whoami", exit=False)

        def uses_FABRIC_env_prefix(self, environ):
            environ["FABRIC_RUN_ECHO"] = "1"
            with cd(support):
                make_program().run("fab expect-from-env")

        def basic_pre_and_post_tasks_still_work(self):
            with cd(support):
                # Sanity
                expect("first", "First!\n")
                expect("third", "Third!\n")
                # Real test
                expect("second", "First!\nSecond!\nThird!\n")

    class filenames:
        def loads_fabfile_not_tasks(self):
            "Loads fabfile.py, not tasks.py"
            with cd(support):
                expect(
                    "--list",
                    """
Available tasks:

  basic-run
  build
  deploy
  expect-from-env
  expect-identities
  expect-identity
  expect-mutation
  expect-mutation-to-fail
  expect-vanilla-Context
  first
  mutate
  second
  third

""".lstrip(),
                )

        def loads_fabric_config_files_not_invoke_ones(self):
            for type_ in ("yaml", "yml", "json", "py"):
                with cd(os.path.join(support, "{}_conf".format(type_))):
                    # This task, in each subdir, expects data present in a
                    # fabric.<ext> nearby to show up in the config.
                    make_program().run("fab expect-conf-value")

    class runtime_ssh_config_path:
        def _run(
            self,
            flag="-S",
            file_="ssh_config/runtime.conf",
            tasks="runtime-ssh-config",
        ):
            with cd(support):
                # Relies on asserts within the task, which will bubble up as
                # it's executed in-process
                cmd = "fab -c runtime_fabfile {} {} -H runtime {}"
                make_program().run(cmd.format(flag, file_, tasks))

        def capital_F_flag_specifies_runtime_ssh_config_file(self):
            self._run(flag="-S")

        def long_form_flag_also_works(self):
            self._run(flag="--ssh-config")

        @raises(IOError)
        def IOErrors_if_given_missing_file(self):
            self._run(file_="nope/nothere.conf")

        @patch.object(Config, "_load_ssh_file")
        def config_only_loaded_once_per_session(self, method):
            # Task that doesn't make assertions about the config (since the
            # _actual_ config it gets is empty as we had to mock out the loader
            # method...sigh)
            self._run(tasks="dummy dummy")
            # Called only once (initial __init__) with runtime conf, instead of
            # that plus a few more pairs of calls against the default files
            # (which is what happens when clone() isn't preserving the
            # already-parsed/loaded SSHConfig)
            method.assert_called_once_with("ssh_config/runtime.conf")

    class hosts_flag_parameterizes_tasks:
        # NOTE: many of these just rely on MockRemote's builtin
        # "channel.exec_command called with given command string" asserts.

        def single_string_is_single_host_and_single_exec(self, remote):
            remote.expect(host="myhost", cmd="nope")
            # In addition to just testing a base case, this checks for a really
            # dumb bug where one appends to, instead of replacing, the task
            # list during parameterization/expansion XD
            with cd(support):
                make_program().run("fab -H myhost basic-run")

        def comma_separated_string_is_multiple_hosts(self, remote):
            remote.expect_sessions(
                Session("host1", cmd="nope"), Session("host2", cmd="nope")
            )
            with cd(support):
                make_program().run("fab -H host1,host2 basic-run")

        def multiple_hosts_works_with_remainder_too(self, remote):
            remote.expect_sessions(
                Session("host1", cmd="whoami"), Session("host2", cmd="whoami")
            )
            make_program().run("fab -H host1,host2 -- whoami")

        def host_string_shorthand_is_passed_through(self, remote):
            remote.expect(host="host1", port=1234, user="someuser")
            make_program().run("fab -H someuser@host1:1234 -- whoami")

        # NOTE: no mocking because no actual run() under test, only
        # parameterization
        # TODO: avoiding for now because implementing this requires more work
        # at the Invoke level re: deciding when to _not_ pass in the
        # session-global config object (Executor's self.config). At the moment,
        # our threading-concurrency API is oriented around Group, and we're not
        # using it for --hosts, so it's not broken...yet.
        @pytest.mark.skip
        def config_mutation_not_preserved(self):
            with cd(support):
                make_program().run(
                    "fab -H host1,host2 expect-mutation-to-fail"
                )

        @trap
        def pre_post_tasks_are_not_parameterized_across_hosts(self):
            with cd(support):
                make_program().run(
                    "fab -H hostA,hostB,hostC second --show-host"
                )
                output = sys.stdout.getvalue()
                # Expect pre once, 3x main, post once, as opposed to e.g. both
                # pre and main task
                expected = """
First!
Second: hostA
Second: hostB
Second: hostC
Third!
""".lstrip()
                assert output == expected

    class no_hosts_flag:
        def calls_task_once_with_invoke_context(self):
            with cd(support):
                make_program().run("fab expect-vanilla-Context")

        @raises(NothingToDo)
        def generates_exception_if_combined_with_remainder(self):
            make_program().run("fab -- nope")

        def invokelike_multitask_invocation_preserves_config_mutation(self):
            # Mostly a guard against Executor subclass tweaks breaking Invoke
            # behavior added in pyinvoke/invoke#309
            with cd(support):
                make_program().run("fab mutate expect-mutation")

    class runtime_identity_file:
        def dash_i_supplies_default_connect_kwarg_key_filename(self):
            # NOTE: the expect-identity task in tests/_support/fabfile.py
            # performs asserts about its context's .connect_kwargs value,
            # relying on other tests to prove connect_kwargs makes its way into
            # that context.
            with cd(support):
                make_program().run("fab -i identity.key expect-identity")

        def double_dash_identity_also_works(self):
            with cd(support):
                make_program().run(
                    "fab --identity identity.key expect-identity"
                )

        def may_be_given_multiple_times(self):
            with cd(support):
                make_program().run(
                    "fab -i identity.key -i identity2.key expect-identities"
                )

    class secrets_prompts:
        @patch("fabric.main.getpass.getpass")
        def _expect_prompt(self, getpass, flag, key, value, prompt):
            getpass.return_value = value
            with cd(support):
                # Expect that the given key was found in the context.
                cmd = "fab -c prompting {} expect-connect-kwarg --key {} --val {}"  # noqa
                make_program().run(cmd.format(flag, key, value))
            # Then we also expect that getpass was called w/ expected prompt
            getpass.assert_called_once_with(prompt)

        def password_prompt_updates_connect_kwargs(self):
            self._expect_prompt(
                flag="--prompt-for-login-password",
                key="password",
                value="mypassword",
                prompt="Enter login password for use with SSH auth: ",
            )

        def passphrase_prompt_updates_connect_kwargs(self):
            self._expect_prompt(
                flag="--prompt-for-passphrase",
                key="passphrase",
                value="mypassphrase",
                prompt="Enter passphrase for use unlocking SSH keys: ",
            )

    class configuration_updating_and_merging:
        def key_filename_can_be_set_via_non_override_config_levels(self):
            # Proves/protects against #1762, where eg key_filenames gets
            # 'reset' to an empty list. Arbitrarily uses the 'yml' level of
            # test fixtures, which has a fabric.yml w/ a
            # connect_kwargs.key_filename value of [private.key, other.key].
            with cd(os.path.join(support, "yml_conf")):
                make_program().run("fab expect-conf-key-filename")

        def cli_identity_still_overrides_when_non_empty(self):
            with cd(os.path.join(support, "yml_conf")):
                make_program().run("fab -i cli.key expect-cli-key-filename")

    class completion:
        # NOTE: most completion tests are in Invoke too; this is just an
        # irritating corner case driven by Fabric's 'remainder' functionality.
        @trap
        def complete_flag_does_not_trigger_remainder_only_behavior(self):
            # When bug present, 'fab --complete -- fab' fails to load any
            # collections because it thinks it's in remainder-only,
            # work-without-a-collection mode.
            with cd(support):
                make_program().run("fab --complete -- fab", exit=False)
            # Cherry-picked sanity checks looking for tasks from fixture
            # fabfile
            output = sys.stdout.getvalue()
            for name in ("build", "deploy", "expect-from-env"):
                assert name in output
<EOF>
<BOF>
try:
    from invoke.vendor.six import StringIO
except ImportError:
    from six import StringIO

from mock import Mock, call
from pytest_relaxed import raises
from pytest import skip  # noqa
from paramiko import SFTPAttributes

from fabric import Connection
from fabric.transfer import Transfer


# TODO: pull in all edge/corner case tests from fabric v1


class Transfer_:
    class init:
        "__init__"

        def requires_connection(self):
            # Transfer() -> explodes
            try:
                Transfer()
            except TypeError:
                pass
            else:
                assert False, "Did not raise ArgumentError"
            # Transfer(Connection()) -> happy, exposes an attribute
            cxn = Connection("host")
            assert Transfer(cxn).connection is cxn

    class is_remote_dir:
        def returns_bool_of_stat_ISDIR_flag(self, sftp_objs):
            xfer, sftp = sftp_objs
            # Default mocked st_mode is file-like (first octal digit is 1)
            assert xfer.is_remote_dir("whatever") is False
            # Set mode directory-ish (first octal digit is 4)
            sftp.stat.return_value.st_mode = 0o41777
            assert xfer.is_remote_dir("whatever") is True

        def returns_False_if_stat_raises_IOError(self, sftp_objs):
            xfer, sftp = sftp_objs
            sftp.stat.side_effect = IOError
            assert xfer.is_remote_dir("whatever") is False

    class get:
        class basics:
            def accepts_single_remote_path_posarg(self, sftp_objs):
                transfer, client = sftp_objs
                transfer.get("file")
                client.get.assert_called_with(
                    localpath="/local/file", remotepath="/remote/file"
                )

            def accepts_local_and_remote_kwargs(self, sftp_objs):
                transfer, client = sftp_objs
                transfer.get(remote="path1", local="path2")
                client.get.assert_called_with(
                    remotepath="/remote/path1", localpath="/local/path2"
                )

            def returns_rich_Result_object(self, sftp_objs):
                transfer, client = sftp_objs
                cxn = Connection("host")
                result = Transfer(cxn).get("file")
                assert result.orig_remote == "file"
                assert result.remote == "/remote/file"
                assert result.orig_local is None
                assert result.local == "/local/file"
                assert result.connection is cxn
                # TODO: timing info
                # TODO: bytes-transferred info

        class path_arg_edge_cases:
            def local_None_uses_remote_filename(self, transfer):
                assert transfer.get("file").local == "/local/file"

            def local_empty_string_uses_remote_filename(self, transfer):
                assert transfer.get("file", local="").local == "/local/file"

            @raises(TypeError)
            def remote_arg_is_required(self, transfer):
                transfer.get()

            @raises(ValueError)
            def remote_arg_cannot_be_None(self, transfer):
                transfer.get(None)

            @raises(ValueError)
            def remote_arg_cannot_be_empty_string(self, transfer):
                transfer.get("")

        class file_like_local_paths:
            "file-like local paths"

            def _get_to_stringio(self, sftp_objs):
                transfer, client = sftp_objs
                fd = StringIO()
                result = transfer.get("file", local=fd)
                # Note: getfo, not get
                client.getfo.assert_called_with(
                    remotepath="/remote/file", fl=fd
                )
                return result, fd

            def remote_path_to_local_StringIO(self, sftp_objs):
                self._get_to_stringio(sftp_objs)

            def result_contains_fd_for_local_path(self, sftp_objs):
                result, fd = self._get_to_stringio(sftp_objs)
                assert result.remote == "/remote/file"
                assert result.local is fd

        class mode_concerns:
            def setup(self):
                self.attrs = SFTPAttributes()
                self.attrs.st_mode = 0o100644

            def preserves_remote_mode_by_default(self, sftp):
                transfer, client, mock_os = sftp
                # Attributes obj reflecting a realistic 'extended' octal mode
                client.stat.return_value = self.attrs
                transfer.get("file", local="meh")
                # Expect os.chmod to be called with the scrubbed/shifted
                # version of same.
                mock_os.chmod.assert_called_with("/local/meh", 0o644)

            def allows_disabling_remote_mode_preservation(self, sftp):
                transfer, client, mock_os = sftp
                client.stat.return_value = self.attrs
                transfer.get("file", local="meh", preserve_mode=False)
                assert not mock_os.chmod.called

    class put:
        class basics:
            def accepts_single_local_path_posarg(self, sftp_objs):
                transfer, client = sftp_objs
                transfer.put("file")
                client.put.assert_called_with(
                    localpath="/local/file", remotepath="/remote/file"
                )

            def accepts_local_and_remote_kwargs(self, sftp_objs):
                transfer, sftp = sftp_objs
                # NOTE: default mock stat is file-ish, so path won't be munged
                transfer.put(local="path2", remote="path1")
                sftp.put.assert_called_with(
                    localpath="/local/path2", remotepath="/remote/path1"
                )

            def returns_rich_Result_object(self, transfer):
                cxn = Connection("host")
                result = Transfer(cxn).put("file")
                assert result.orig_remote is None
                assert result.remote == "/remote/file"
                assert result.orig_local == "file"
                assert result.local == "/local/file"
                assert result.connection is cxn
                # TODO: timing info
                # TODO: bytes-transferred info

        class remote_end_is_directory:
            def appends_local_file_basename(self, sftp_objs):
                xfer, sftp = sftp_objs
                sftp.stat.return_value.st_mode = 0o41777
                xfer.put(local="file.txt", remote="/dir/path/")
                sftp.stat.assert_called_once_with("/dir/path/")
                sftp.put.assert_called_with(
                    localpath="/local/file.txt",
                    remotepath="/dir/path/file.txt",
                )

            class file_like_local_objects:
                def name_attribute_present_appends_like_basename(
                    self, sftp_objs
                ):
                    xfer, sftp = sftp_objs
                    sftp.stat.return_value.st_mode = 0o41777
                    local = StringIO("sup\n")
                    local.name = "sup.txt"
                    xfer.put(local, remote="/dir/path")
                    sftp.putfo.assert_called_with(
                        fl=local, remotepath="/dir/path/sup.txt"
                    )

                @raises(ValueError)
                def no_name_attribute_raises_ValueError(self, sftp_objs):
                    xfer, sftp = sftp_objs
                    sftp.stat.return_value.st_mode = 0o41777
                    local = StringIO("sup\n")
                    xfer.put(local, remote="/dir/path")

        class path_arg_edge_cases:
            def remote_None_uses_local_filename(self, transfer):
                assert transfer.put("file").remote == "/remote/file"

            def remote_empty_string_uses_local_filename(self, transfer):
                assert transfer.put("file", remote="").remote == "/remote/file"

            @raises(ValueError)
            def remote_cant_be_empty_if_local_file_like(self, transfer):
                transfer.put(StringIO())

            @raises(TypeError)
            def local_arg_is_required(self, transfer):
                transfer.put()

            @raises(ValueError)
            def local_arg_cannot_be_None(self, transfer):
                transfer.put(None)

            @raises(ValueError)
            def local_arg_cannot_be_empty_string(self, transfer):
                transfer.put("")

        class file_like_local_paths:
            "file-like local paths"

            def _put_from_stringio(self, sftp_objs):
                transfer, client = sftp_objs
                fd = StringIO()
                result = transfer.put(fd, remote="file")
                # Note: putfo, not put
                client.putfo.assert_called_with(
                    remotepath="/remote/file", fl=fd
                )
                return result, fd

            def remote_path_from_local_StringIO(self, sftp_objs):
                self._put_from_stringio(sftp_objs)

            def local_FLOs_are_rewound_before_putting(self, transfer):
                fd = Mock()
                fd.tell.return_value = 17
                transfer.put(fd, remote="file")
                seek_calls = fd.seek.call_args_list
                assert seek_calls, [call(0) == call(17)]

            def result_contains_fd_for_local_path(self, sftp_objs):
                result, fd = self._put_from_stringio(sftp_objs)
                assert result.remote == "/remote/file"
                assert result.local is fd

        class mode_concerns:
            def preserves_local_mode_by_default(self, sftp):
                transfer, client, mock_os = sftp
                # This is a realistic stat for 0o644
                mock_os.stat.return_value.st_mode = 33188
                transfer.put("file")
                client.chmod.assert_called_with("/remote/file", 0o644)

            def allows_disabling_local_mode_preservation(self, sftp_objs):
                transfer, client = sftp_objs
                transfer.put("file", preserve_mode=False)
                assert not client.chmod.called
<EOF>
<BOF>
from invoke import task, Context
from fabric import Connection


@task
def build(c):
    pass


@task
def deploy(c):
    pass


@task
def basic_run(c):
    c.run("nope")


@task
def expect_vanilla_Context(c):
    assert isinstance(c, Context)
    assert not isinstance(c, Connection)


@task
def expect_from_env(c):
    assert c.config.run.echo is True


@task
def expect_mutation_to_fail(c):
    # If user level config changes are preserved between parameterized per-host
    # task calls, this would assert on subsequent invocations...
    assert "foo" not in c.config
    # ... because of this:
    c.config.foo = "bar"


@task
def mutate(c):
    c.foo = "bar"


@task
def expect_mutation(c):
    assert c.foo == "bar"


@task
def expect_identity(c):
    assert c.config.connect_kwargs["key_filename"] == ["identity.key"]


@task
def expect_identities(c):
    assert c.config.connect_kwargs["key_filename"] == [
        "identity.key",
        "identity2.key",
    ]


@task
def first(c):
    print("First!")


@task
def third(c):
    print("Third!")


@task(pre=[first], post=[third])
def second(c, show_host=False):
    if show_host:
        print("Second: {}".format(c.host))
    else:
        print("Second!")
<EOF>
<BOF>
from invoke import task


@task
def expect_connect_kwarg(c, key, val):
    assert c.config.connect_kwargs[key] == val
<EOF>
<BOF>
from invoke import task


@task
def runtime_ssh_config(c):
    # NOTE: assumes it's run with host='runtime' + ssh_configs/runtime.conf
    # TODO: SSHConfig should really learn to turn certain things into ints
    # automatically...
    assert c.ssh_config["port"] == "666"
    assert c.port == 666


@task
def dummy(c):
    pass
<EOF>
<BOF>
from invoke import task


@task
def expect_conf_value(c):
    assert c.it_came_from == "yml"


@task
def expect_conf_key_filename(c):
    expected = ["private.key", "other.key"]
    got = c.connect_kwargs.key_filename
    assert got == expected, "{!r} != {!r}".format(got, expected)


@task
def expect_cli_key_filename(c):
    expected = ["cli.key"]
    got = c.connect_kwargs.key_filename
    assert got == expected, "{!r} != {!r}".format(got, expected)
<EOF>
<BOF>
from invoke import task


@task
def expect_conf_value(c):
    assert c.it_came_from == "json"
<EOF>
<BOF>
from invoke import task


@task
def expect_conf_value(c):
    assert c.it_came_from == "py"
<EOF>
<BOF>
it_came_from = "py"
<EOF>
<BOF>
from invoke import task


@task
def expect_conf_value(c):
    assert c.it_came_from == "yaml"
<EOF>
<BOF>
import os

from invoke import pty_size
from pytest import skip

from fabric import Connection, Config


# TODO: use pytest markers
def skip_outside_travis():
    if not os.environ.get("TRAVIS", False):
        skip()


class Connection_:
    class ssh_connections:
        def open_method_generates_real_connection(self):
            c = Connection("localhost")
            c.open()
            assert c.client.get_transport().active is True
            assert c.is_connected is True
            return c

        def close_method_closes_connection(self):
            # Handy shortcut - open things up, then return Connection for us to
            # close
            c = self.open_method_generates_real_connection()
            c.close()
            assert c.client.get_transport() is None
            assert c.is_connected is False

    class run:
        def simple_command_on_host(self):
            """
            Run command on localhost
            """
            result = Connection("localhost").run("echo foo", hide=True)
            assert result.stdout == "foo\n"
            assert result.exited == 0
            assert result.ok is True

        def simple_command_with_pty(self):
            """
            Run command under PTY on localhost
            """
            # Most Unix systems should have stty, which asplodes when not run
            # under a pty, and prints useful info otherwise
            result = Connection("localhost").run(
                "stty size", hide=True, pty=True
            )
            found = result.stdout.strip().split()
            cols, rows = pty_size()
            assert tuple(map(int, found)), rows == cols
            # PTYs use \r\n, not \n, line separation
            assert "\r\n" in result.stdout
            assert result.pty is True

    class local:
        def wraps_invoke_run(self):
            # NOTE: most of the interesting tests about this are in
            # invoke.runners / invoke.integration.
            cxn = Connection("localhost")
            result = cxn.local("echo foo", hide=True)
            assert result.stdout == "foo\n"
            assert not cxn.is_connected  # meh way of proving it didn't use SSH

    def mixed_use_of_local_and_run(self):
        """
        Run command truly locally, and over SSH via localhost
        """
        cxn = Connection("localhost")
        result = cxn.local("echo foo", hide=True)
        assert result.stdout == "foo\n"
        assert not cxn.is_connected  # meh way of proving it didn't use SSH yet
        result = cxn.run("echo foo", hide=True)
        assert cxn.is_connected  # NOW it's using SSH
        assert result.stdout == "foo\n"

    class sudo:
        def setup(self):
            # NOTE: assumes a user configured for passworded (NOT
            # passwordless)_sudo, whose password is 'mypass', is executing the
            # test suite. I.e. our travis-ci setup.
            config = Config(
                {"sudo": {"password": "mypass"}, "run": {"hide": True}}
            )
            self.cxn = Connection("localhost", config=config)

        def sudo_command(self):
            """
            Run command via sudo on host localhost
            """
            skip_outside_travis()
            assert self.cxn.sudo("whoami").stdout.strip() == "root"

        def mixed_sudo_and_normal_commands(self):
            """
            Run command via sudo, and not via sudo, on localhost
            """
            skip_outside_travis()
            logname = os.environ["LOGNAME"]
            assert self.cxn.run("whoami").stdout.strip() == logname
            assert self.cxn.sudo("whoami").stdout.strip() == "root"

    def large_remote_commands_finish_cleanly(self):
        # Guards against e.g. cleanup finishing before actually reading all
        # data from the remote end. Which is largely an issue in Invoke-level
        # code but one that only really manifests when doing stuff over the
        # network. Yay computers!
        path = "/usr/share/dict/words"
        cxn = Connection("localhost")
        with open(path) as fd:
            words = [x.strip() for x in fd.readlines()]
        stdout = cxn.run("cat {}".format(path), hide=True).stdout
        lines = [x.strip() for x in stdout.splitlines()]
        # When bug present, # lines received is significantly fewer than the
        # true count in the file (by thousands).
        assert len(lines) == len(words)
<EOF>
<BOF>
import codecs

from invoke.vendor.six.moves.queue import Queue
from invoke.vendor.six.moves import zip_longest

from invoke.util import ExceptionHandlingThread
from pytest import skip

from fabric import Connection


_words = "/usr/share/dict/words"


def _worker(queue, cxn, start, num_words, count, expected):
    tail = num_words - start
    cmd = "tail -n {} {} | head -n {}".format(tail, _words, count)
    stdout = cxn.run(cmd, hide=True).stdout
    result = [x.strip() for x in stdout.splitlines()]
    queue.put((cxn, result, expected))


class concurrency:
    # TODO: still useful to use Group API here? Where does this responsibility
    # fall between Group and Executor (e.g. phrasing this specifically as a
    # generic subcase of Invoke level task parameterization)?

    # TODO: spin up multiple temp SSHDs / Paramiko servers / ???

    def setup(self):
        cxn1 = Connection("localhost")
        cxn2 = Connection("localhost")
        cxn3 = Connection("localhost")
        self.cxns = (cxn1, cxn2, cxn3)

    def connections_objects_do_not_share_connection_state(self):
        cxn1, cxn2, cxn3 = self.cxns
        [x.open() for x in self.cxns]
        # Prove no exterior connection caching, socket reuse, etc
        # NOTE: would phrase these as chained 'is not' but pep8 linter is being
        # stupid :(
        assert cxn1 is not cxn2
        assert cxn2 is not cxn3
        assert cxn1.client is not cxn2.client
        assert cxn2.client is not cxn3.client
        ports = [x.transport.sock.getsockname()[1] for x in self.cxns]
        assert ports[0] is not ports[1] is not ports[2]

    def manual_threading_works_okay(self):
        # TODO: needs https://github.com/pyinvoke/invoke/issues/438 fixed
        # before it will reliably pass
        skip()
        # Kind of silly but a nice base case for "how would someone thread this
        # stuff; and are there any bizarre gotchas lurking in default
        # config/context/connection state?"
        # Specifically, cut up the local (usually 100k's long) words dict into
        # per-thread chunks, then read those chunks via shell command, as a
        # crummy "make sure each thread isn't polluting things like stored
        # stdout" sanity test
        queue = Queue()
        # TODO: skip test on Windows or find suitable alternative file
        with codecs.open(_words, encoding="utf-8") as fd:
            data = [x.strip() for x in fd.readlines()]
        threads = []
        num_words = len(data)
        chunksize = len(data) / len(self.cxns)  # will be an int, which is fine
        for i, cxn in enumerate(self.cxns):
            start = i * chunksize
            end = max([start + chunksize, num_words])
            chunk = data[start:end]
            kwargs = dict(
                queue=queue,
                cxn=cxn,
                start=start,
                num_words=num_words,
                count=len(chunk),
                expected=chunk,
            )
            thread = ExceptionHandlingThread(target=_worker, kwargs=kwargs)
            threads.append(thread)
        for t in threads:
            t.start()
        for t in threads:
            t.join(5)  # Kinda slow, but hey, maybe the test runner is hot
        while not queue.empty():
            cxn, result, expected = queue.get(block=False)
            for resultword, expectedword in zip_longest(result, expected):
                err = u"({2!r}, {3!r}->{4!r}) {0!r} != {1!r}".format(
                    resultword, expectedword, cxn, expected[0], expected[-1]
                )
                assert resultword == expectedword, err
<EOF>
<BOF>
from socket import gaierror

from fabric import ThreadingGroup as Group
from fabric.exceptions import GroupException


class Group_:
    def simple_command(self):
        group = Group("localhost", "127.0.0.1")
        result = group.run("echo foo", hide=True)
        outs = [x.stdout.strip() for x in result.values()]
        assert ["foo", "foo"] == outs

    def failed_command(self):
        group = Group("localhost", "127.0.0.1")
        try:
            group.run("lolnope", hide=True)
        except GroupException as e:
            # GroupException.result -> GroupResult;
            # GroupResult values will be UnexpectedExit in this case;
            # UnexpectedExit.result -> Result, and thus .exited etc.
            exits = [x.result.exited for x in e.result.values()]
            assert [127, 127] == exits
        else:
            assert False, "Did not raise GroupException!"

    def excepted_command(self):
        group = Group("nopebadhost1", "nopebadhost2")
        try:
            group.run("lolnope", hide=True)
        except GroupException as e:
            for value in e.result.values():
                assert isinstance(value, gaierror)
        else:
            assert False, "Did not raise GroupException!"
<EOF>
<BOF>
import os
import stat
from io import BytesIO

from py import path

from fabric import Connection


def _support(*parts):
    return os.path.join(os.path.dirname(__file__), "_support", *parts)


class Transfer_:
    class get:
        def setup(self):
            self.c = Connection("localhost")
            self.remote = _support("file.txt")

        def base_case(self, tmpdir):
            # Copy file from support to tempdir
            with tmpdir.as_cwd():
                result = self.c.get(self.remote)

            # Make sure it arrived
            local = tmpdir.join("file.txt")
            assert local.check()
            assert local.read() == "yup\n"
            # Sanity check result object
            assert result.remote == self.remote
            assert result.orig_remote == self.remote
            assert result.local == str(local)
            assert result.orig_local is None

        def file_like_objects(self):
            fd = BytesIO()
            result = self.c.get(remote=self.remote, local=fd)
            assert fd.getvalue() == b"yup\n"
            assert result.remote == self.remote
            assert result.local is fd

        def mode_preservation(self, tmpdir):
            # Use a dummy file which is given an unusual, highly unlikely to be
            # default umask, set of permissions (oct 641, aka -rw-r----x)
            local = tmpdir.join("funky-local.txt")
            remote = tmpdir.join("funky-remote.txt")
            remote.write("whatever")
            remote.chmod(0o641)
            self.c.get(remote=str(remote), local=str(local))
            assert stat.S_IMODE(local.stat().mode) == 0o641

    class put:
        def setup(self):
            self.c = Connection("localhost")
            self.remote = path.local.mkdtemp().join("file.txt").realpath()

        def base_case(self):
            # Copy file from 'local' (support dir) to 'remote' (tempdir)
            local_dir = _support()
            with path.local(local_dir).as_cwd():
                tmpdir = self.remote.dirpath()
                # TODO: wrap chdir at the Connection level
                self.c.sftp().chdir(str(tmpdir))
                result = self.c.put("file.txt")
            # Make sure it arrived
            assert self.remote.check()
            assert self.remote.read() == "yup\n"
            # Sanity check result object
            assert result.remote == self.remote
            assert result.orig_remote is None
            assert result.local == _support("file.txt")
            assert result.orig_local == "file.txt"

        def file_like_objects(self):
            fd = BytesIO()
            fd.write(b"yup\n")
            remote_str = str(self.remote)
            result = self.c.put(local=fd, remote=remote_str)
            assert self.remote.read() == "yup\n"
            assert result.remote == remote_str
            assert result.local is fd

        def mode_preservation(self, tmpdir):
            # Use a dummy file which is given an unusual, highly unlikely to be
            # default umask, set of permissions (oct 641, aka -rw-r----x)
            local = tmpdir.join("funky-local.txt")
            local.write("whatever")
            local.chmod(0o641)
            remote = tmpdir.join("funky-remote.txt")
            self.c.put(remote=str(remote), local=str(local))
            assert stat.S_IMODE(remote.stat().mode) == 0o641
<EOF>
<BOF>
import os
from os.path import join, dirname, abspath
from datetime import datetime

import alabaster


# Alabaster theme + mini-extension
html_theme_path = [alabaster.get_path()]
extensions = ["alabaster", "sphinx.ext.intersphinx"]

# Paths relative to invoking conf.py - not this shared file
html_static_path = [join("..", "_shared_static")]
html_theme = "alabaster"
html_theme_options = {
    "logo": "logo.png",
    "logo_name": True,
    "logo_text_align": "center",
    "description": "Pythonic remote execution",
    "github_user": "fabric",
    "github_repo": "fabric",
    "travis_button": True,
    "codecov_button": True,
    "tidelift_url": "https://tidelift.com/subscription/pkg/pypi-fabric?utm_source=pypi-fabric&utm_medium=referral&utm_campaign=docs",
    "analytics_id": "UA-18486793-1",
    "link": "#3782BE",
    "link_hover": "#3782BE",
    # Wide enough that 80-col code snippets aren't truncated on default font
    # settings (at least for bitprophet's Chrome-on-OSX-Yosemite setup)
    "page_width": "1024px",
}
html_sidebars = {
    "**": ["about.html", "navigation.html", "searchbox.html", "donate.html"]
}

# Enable & configure doctest
extensions.append("sphinx.ext.doctest")
# Import mock tooling from unit tests' _util.py
doctest_path = [abspath(join(dirname(__file__), "..", "tests"))]
doctest_global_setup = r"""
from _util import MockRemote, MockSFTP, Session, Command
"""

on_rtd = os.environ.get("READTHEDOCS") == "True"
on_travis = os.environ.get("TRAVIS", False)
on_dev = not (on_rtd or on_travis)

# Invoke (docs + www)
inv_target = join(
    dirname(__file__), "..", "..", "invoke", "sites", "docs", "_build"
)
if not on_dev:
    inv_target = "http://docs.pyinvoke.org/en/latest/"
inv_www_target = join(
    dirname(__file__), "..", "..", "invoke", "sites", "www", "_build"
)
if not on_dev:
    inv_www_target = "http://pyinvoke.org/"
# Paramiko (docs)
para_target = join(
    dirname(__file__), "..", "..", "paramiko", "sites", "docs", "_build"
)
if not on_dev:
    para_target = "http://docs.paramiko.org/en/latest/"
intersphinx_mapping = {
    "python": ("http://docs.python.org/", None),
    "invoke": (inv_target, None),
    "invoke_www": (inv_www_target, None),
    "paramiko": (para_target, None),
}

# Regular settings
project = "Fabric"
year = datetime.now().year
copyright = "%d Jeff Forcier" % year
master_doc = "index"
templates_path = ["_templates"]
exclude_trees = ["_build"]
source_suffix = ".rst"
default_role = "obj"
<EOF>
<BOF>
# Obtain shared config values
import sys
from os.path import abspath, join, dirname

sys.path.append(abspath(join(dirname(__file__), "..")))
sys.path.append(abspath(join(dirname(__file__), "..", "..")))
from shared_conf import *

# Enable & configure autodoc
extensions.append("sphinx.ext.autodoc")
autodoc_default_flags = ["members", "special-members"]

# Default is 'local' building, but reference the public WWW site when building
# under RTD.
target = join(dirname(__file__), "..", "www", "_build")
if on_rtd:
    target = "http://www.fabfile.org/"
www = (target, None)
# Intersphinx connection to www site
intersphinx_mapping.update({"www": www})

# Sister-site links to WWW
html_theme_options["extra_nav_links"] = {
    "Main website": "http://www.fabfile.org"
}
<EOF>
<BOF>
# Obtain shared config values
import sys
import os
from os.path import abspath, join, dirname

sys.path.append(abspath(join(dirname(__file__), "..")))
from shared_conf import *


# Releases changelog extension
extensions.append("releases")
releases_document_name = ["changelog", "changelog-v1"]
releases_github_path = "fabric/fabric"

# Intersphinx for referencing API/usage docs
extensions.append("sphinx.ext.intersphinx")
# Default is 'local' building, but reference the public docs site when building
# under RTD.
target = join(dirname(__file__), "..", "docs", "_build")
if on_rtd:
    target = "http://docs.fabfile.org/en/latest/"
intersphinx_mapping.update({"docs": (target, None)})

# Sister-site links to API docs
html_theme_options["extra_nav_links"] = {"API Docs": "http://docs.fabfile.org"}
<EOF>
<BOF>
from invoke import Runner, pty_size, Result as InvokeResult


class Remote(Runner):
    """
    Run a shell command over an SSH connection.

    This class subclasses `invoke.runners.Runner`; please see its documentation
    for most public API details.

    .. note::
        `.Remote`'s ``__init__`` method expects a `.Connection` (or subclass)
        instance for its ``context`` argument.

    .. versionadded:: 2.0
    """

    def start(self, command, shell, env):
        self.channel = self.context.create_session()
        if self.using_pty:
            rows, cols = pty_size()
            self.channel.get_pty(width=rows, height=cols)
        # TODO: consider adding an option to conditionally turn this
        # update_environment call into a command-string prefixing behavior
        # instead (e.g. when one isn't able/willing to update remote server's
        # AcceptEnv setting). OR: rely on higher-level generic command
        # prefixing functionality, when implemented.
        # TODO: honor SendEnv from ssh_config
        self.channel.update_environment(env)
        # TODO: pass in timeout= here when invoke grows timeout functionality
        # in Runner/Local.
        self.channel.exec_command(command)

    def read_proc_stdout(self, num_bytes):
        return self.channel.recv(num_bytes)

    def read_proc_stderr(self, num_bytes):
        return self.channel.recv_stderr(num_bytes)

    def _write_proc_stdin(self, data):
        return self.channel.sendall(data)

    @property
    def process_is_finished(self):
        return self.channel.exit_status_ready()

    def send_interrupt(self, interrupt):
        # NOTE: in v1, we just reraised the KeyboardInterrupt unless a PTY was
        # present; this seems to have been because without a PTY, the
        # below escape sequence is ignored, so all we can do is immediately
        # terminate on our end.
        # NOTE: also in v1, the raising of the KeyboardInterrupt completely
        # skipped all thread joining & cleanup; presumably regular interpreter
        # shutdown suffices to tie everything off well enough.
        if self.using_pty:
            # Submit hex ASCII character 3, aka ETX, which most Unix PTYs
            # interpret as a foreground SIGINT.
            # TODO: is there anything else we can do here to be more portable?
            self.channel.send(u"\x03")
        else:
            raise interrupt

    def returncode(self):
        return self.channel.recv_exit_status()

    def generate_result(self, **kwargs):
        kwargs["connection"] = self.context
        return Result(**kwargs)

    def stop(self):
        if hasattr(self, "channel"):
            self.channel.close()

    # TODO: shit that is in fab 1 run() but could apply to invoke.Local too:
    # * command timeout control
    # * see rest of stuff in _run_command/_execute in operations.py...there is
    # a bunch that applies generally like optional exit codes, etc

    # TODO: general shit not done yet
    # * stdin; Local relies on local process management to ensure stdin is
    # hooked up; we cannot do that.
    # * output prefixing
    # * agent forwarding
    # * reading at 4096 bytes/time instead of whatever inv defaults to (also,
    # document why we are doing that, iirc it changed recentlyish via ticket)
    # * TODO: oh god so much more, go look it up

    # TODO: shit that has no Local equivalent that we probs need to backfill
    # into Runner, probably just as a "finish()" or "stop()" (to mirror
    # start()):
    # * channel close()
    # * agent-forward close()


class Result(InvokeResult):
    """
    An `invoke.runners.Result` exposing which `.Connection` was run against.

    Exposes all attributes from its superclass, then adds a ``.connection``,
    which is simply a reference to the `.Connection` whose method yielded this
    result.

    .. versionadded:: 2.0
    """

    def __init__(self, **kwargs):
        connection = kwargs.pop("connection")
        super(Result, self).__init__(**kwargs)
        self.connection = connection

    # TODO: have useful str/repr differentiation from invoke.Result,
    # transfer.Result etc.
<EOF>
<BOF>
from contextlib import contextmanager
from threading import Event

try:
    from invoke.vendor.six import StringIO
    from invoke.vendor.decorator import decorator
    from invoke.vendor.six import string_types
except ImportError:
    from six import StringIO
    from decorator import decorator
    from six import string_types
import socket


from invoke import Context
from invoke.exceptions import ThreadException
from paramiko.agent import AgentRequestHandler
from paramiko.client import SSHClient, AutoAddPolicy
from paramiko.config import SSHConfig
from paramiko.proxy import ProxyCommand

from .config import Config
from .transfer import Transfer
from .tunnels import TunnelManager, Tunnel


@decorator
def opens(method, self, *args, **kwargs):
    self.open()
    return method(self, *args, **kwargs)


class Connection(Context):
    """
    A connection to an SSH daemon, with methods for commands and file transfer.

    **Basics**

    This class inherits from Invoke's `~invoke.context.Context`, as it is a
    context within which commands, tasks etc can operate. It also encapsulates
    a Paramiko `~paramiko.client.SSHClient` instance, performing useful high
    level operations with that `~paramiko.client.SSHClient` and
    `~paramiko.channel.Channel` instances generated from it.

    **Lifecycle**

    `.Connection` has a basic "`create <__init__>`, `connect/open <open>`, `do
    work <run>`, `disconnect/close <close>`" lifecycle:

    * `Instantiation <__init__>` imprints the object with its connection
      parameters (but does **not** actually initiate the network connection).
    * Methods like `run`, `get` etc automatically trigger a call to
      `open` if the connection is not active; users may of course call `open`
      manually if desired.
    * Connections do not always need to be explicitly closed; much of the
      time, Paramiko's garbage collection hooks or Python's own shutdown
      sequence will take care of things. **However**, should you encounter edge
      cases (for example, sessions hanging on exit) it's helpful to explicitly
      close connections when you're done with them.

      This can be accomplished by manually calling `close`, or by using the
      object as a contextmanager::

        with Connection('host') as c:
            c.run('command')
            c.put('file')

    .. note::
        This class rebinds `invoke.context.Context.run` to `.local` so both
        remote and local command execution can coexist.

    **Configuration**

    Most `.Connection` parameters honor :doc:`Invoke-style configuration
    </concepts/configuration>` as well as any applicable :ref:`SSH config file
    directives <connection-ssh-config>`. For example, to end up with a
    connection to ``admin@myhost``, one could:

    - Use any built-in config mechanism, such as ``/etc/fabric.yml``,
      ``~/.fabric.json``, collection-driven configuration, env vars, etc,
      stating ``user: admin`` (or ``{"user": "admin"}``, depending on config
      format.) Then ``Connection('myhost')`` would implicitly have a ``user``
      of ``admin``.
    - Use an SSH config file containing ``User admin`` within any applicable
      ``Host`` header (``Host myhost``, ``Host *``, etc.) Again,
      ``Connection('myhost')`` will default to an ``admin`` user.
    - Leverage host-parameter shorthand (described in `.Config.__init__`), i.e.
      ``Connection('admin@myhost')``.
    - Give the parameter directly: ``Connection('myhost', user='admin')``.

    The same applies to agent forwarding, gateways, and so forth.

    .. versionadded:: 2.0
    """

    # NOTE: these are initialized here to hint to invoke.Config.__setattr__
    # that they should be treated as real attributes instead of config proxies.
    # (Additionally, we're doing this instead of using invoke.Config._set() so
    # we can take advantage of Sphinx's attribute-doc-comment static analysis.)
    # Once an instance is created, these values will usually be non-None
    # because they default to the default config values.
    host = None
    original_host = None
    user = None
    port = None
    ssh_config = None
    gateway = None
    forward_agent = None
    connect_timeout = None
    connect_kwargs = None
    client = None
    transport = None
    _sftp = None
    _agent_handler = None

    # TODO: should "reopening" an existing Connection object that has been
    # closed, be allowed? (See e.g. how v1 detects closed/semi-closed
    # connections & nukes them before creating a new client to the same host.)
    # TODO: push some of this into paramiko.client.Client? e.g. expand what
    # Client.exec_command does, it already allows configuring a subset of what
    # we do / will eventually do / did in 1.x. It's silly to have to do
    # .get_transport().open_session().
    def __init__(
        self,
        host,
        user=None,
        port=None,
        config=None,
        gateway=None,
        forward_agent=None,
        connect_timeout=None,
        connect_kwargs=None,
    ):
        """
        Set up a new object representing a server connection.

        :param str host:
            the hostname (or IP address) of this connection.

            May include shorthand for the ``user`` and/or ``port`` parameters,
            of the form ``user@host``, ``host:port``, or ``user@host:port``.

            .. note::
                Due to ambiguity, IPv6 host addresses are incompatible with the
                ``host:port`` shorthand (though ``user@host`` will still work
                OK). In other words, the presence of >1 ``:`` character will
                prevent any attempt to derive a shorthand port number; use the
                explicit ``port`` parameter instead.

            .. note::
                If ``host`` matches a ``Host`` clause in loaded SSH config
                data, and that ``Host`` clause contains a ``Hostname``
                directive, the resulting `.Connection` object will behave as if
                ``host`` is equal to that ``Hostname`` value.

                In all cases, the original value of ``host`` is preserved as
                the ``original_host`` attribute.

                Thus, given SSH config like so::

                    Host myalias
                        Hostname realhostname

                a call like ``Connection(host='myalias')`` will result in an
                object whose ``host`` attribute is ``realhostname``, and whose
                ``original_host`` attribute is ``myalias``.

        :param str user:
            the login user for the remote connection. Defaults to
            ``config.user``.

        :param int port:
            the remote port. Defaults to ``config.port``.

        :param config:
            configuration settings to use when executing methods on this
            `.Connection` (e.g. default SSH port and so forth).

            Should be a `.Config` or an `invoke.config.Config`
            (which will be turned into a `.Config`).

            Default is an anonymous `.Config` object.

        :param gateway:
            An object to use as a proxy or gateway for this connection.

            This parameter accepts one of the following:

            - another `.Connection` (for a ``ProxyJump`` style gateway);
            - a shell command string (for a ``ProxyCommand`` style style
              gateway).

            Default: ``None``, meaning no gatewaying will occur (unless
            otherwise configured; if one wants to override a configured gateway
            at runtime, specify ``gateway=False``.)

            .. seealso:: :ref:`ssh-gateways`

        :param bool forward_agent:
            Whether to enable SSH agent forwarding.

            Default: ``config.forward_agent``.

        :param int connect_timeout:
            Connection timeout, in seconds.

            Default: ``config.timeouts.connect``.

        :param dict connect_kwargs:
            Keyword arguments handed verbatim to
            `SSHClient.connect <paramiko.client.SSHClient.connect>` (when
            `.open` is called).

            `.Connection` tries not to grow additional settings/kwargs of its
            own unless it is adding value of some kind; thus,
            ``connect_kwargs`` is currently the right place to hand in
            parameters such as ``pkey`` or ``key_filename``.

            Default: ``config.connect_kwargs``.

        :raises ValueError:
            if user or port values are given via both ``host`` shorthand *and*
            their own arguments. (We `refuse the temptation to guess`_).

        .. _refuse the temptation to guess:
            http://zen-of-python.info/
            in-the-face-of-ambiguity-refuse-the-temptation-to-guess.html#12
        """
        # NOTE: parent __init__ sets self._config; for now we simply overwrite
        # that below. If it's somehow problematic we would want to break parent
        # __init__ up in a manner that is more cleanly overrideable.
        super(Connection, self).__init__(config=config)

        #: The .Config object referenced when handling default values (for e.g.
        #: user or port, when not explicitly given) or deciding how to behave.
        if config is None:
            config = Config()
        # Handle 'vanilla' Invoke config objects, which need cloning 'into' one
        # of our own Configs (which grants the new defaults, etc, while not
        # squashing them if the Invoke-level config already accounted for them)
        elif not isinstance(config, Config):
            config = config.clone(into=Config)
        self._set(_config=config)
        # TODO: when/how to run load_files, merge, load_shell_env, etc?
        # TODO: i.e. what is the lib use case here (and honestly in invoke too)

        shorthand = self.derive_shorthand(host)
        host = shorthand["host"]
        err = "You supplied the {} via both shorthand and kwarg! Please pick one."  # noqa
        if shorthand["user"] is not None:
            if user is not None:
                raise ValueError(err.format("user"))
            user = shorthand["user"]
        if shorthand["port"] is not None:
            if port is not None:
                raise ValueError(err.format("port"))
            port = shorthand["port"]

        # NOTE: we load SSH config data as early as possible as it has
        # potential to affect nearly every other attribute.
        #: The per-host SSH config data, if any. (See :ref:`ssh-config`.)
        self.ssh_config = self.config.base_ssh_config.lookup(host)

        self.original_host = host
        #: The hostname of the target server.
        self.host = host
        if "hostname" in self.ssh_config:
            # TODO: log that this occurred?
            self.host = self.ssh_config["hostname"]

        #: The username this connection will use to connect to the remote end.
        self.user = user or self.ssh_config.get("user", self.config.user)
        # TODO: is it _ever_ possible to give an empty user value (e.g.
        # user='')? E.g. do some SSH server specs allow for that?

        #: The network port to connect on.
        self.port = port or int(self.ssh_config.get("port", self.config.port))

        # Gateway/proxy/bastion/jump setting: non-None values - string,
        # Connection, even eg False - get set directly; None triggers seek in
        # config/ssh_config
        #: The gateway `.Connection` or ``ProxyCommand`` string to be used,
        #: if any.
        self.gateway = gateway if gateway is not None else self.get_gateway()
        # NOTE: we use string above, vs ProxyCommand obj, to avoid spinning up
        # the ProxyCommand subprocess at init time, vs open() time.
        # TODO: make paramiko.proxy.ProxyCommand lazy instead?

        if forward_agent is None:
            # Default to config...
            forward_agent = self.config.forward_agent
            # But if ssh_config is present, it wins
            if "forwardagent" in self.ssh_config:
                # TODO: SSHConfig really, seriously needs some love here, god
                map_ = {"yes": True, "no": False}
                forward_agent = map_[self.ssh_config["forwardagent"]]
        #: Whether agent forwarding is enabled.
        self.forward_agent = forward_agent

        if connect_timeout is None:
            connect_timeout = self.ssh_config.get(
                "connecttimeout", self.config.timeouts.connect
            )
        if connect_timeout is not None:
            connect_timeout = int(connect_timeout)
        #: Connection timeout
        self.connect_timeout = connect_timeout

        #: Keyword arguments given to `paramiko.client.SSHClient.connect` when
        #: `open` is called.
        self.connect_kwargs = self.resolve_connect_kwargs(connect_kwargs)

        #: The `paramiko.client.SSHClient` instance this connection wraps.
        client = SSHClient()
        client.set_missing_host_key_policy(AutoAddPolicy())
        self.client = client

        #: A convenience handle onto the return value of
        #: ``self.client.get_transport()``.
        self.transport = None

    def resolve_connect_kwargs(self, connect_kwargs):
        # Grab connect_kwargs from config if not explicitly given.
        if connect_kwargs is None:
            # TODO: is it better to pre-empt conflicts w/ manually-handled
            # connect() kwargs (hostname, username, etc) here or in open()?
            # We're doing open() for now in case e.g. someone manually modifies
            # .connect_kwargs attributewise, but otherwise it feels better to
            # do it early instead of late.
            connect_kwargs = self.config.connect_kwargs
        # Special case: key_filename gets merged instead of overridden.
        # TODO: probably want some sorta smart merging generally, special cases
        # are bad.
        elif "key_filename" in self.config.connect_kwargs:
            kwarg_val = connect_kwargs.get("key_filename", [])
            conf_val = self.config.connect_kwargs["key_filename"]
            # Config value comes before kwarg value (because it may contain
            # CLI flag value.)
            connect_kwargs["key_filename"] = conf_val + kwarg_val

        # SSH config identityfile values come last in the key_filename
        # 'hierarchy'.
        if "identityfile" in self.ssh_config:
            connect_kwargs.setdefault("key_filename", [])
            connect_kwargs["key_filename"].extend(
                self.ssh_config["identityfile"]
            )

        return connect_kwargs

    def get_gateway(self):
        # SSH config wins over Invoke-style config
        if "proxyjump" in self.ssh_config:
            # Reverse hop1,hop2,hop3 style ProxyJump directive so we start
            # with the final (itself non-gatewayed) hop and work up to
            # the front (actual, supplied as our own gateway) hop
            hops = reversed(self.ssh_config["proxyjump"].split(","))
            prev_gw = None
            for hop in hops:
                # Short-circuit if we appear to be our own proxy, which would
                # be a RecursionError. Implies SSH config wildcards.
                # TODO: in an ideal world we'd check user/port too in case they
                # differ, but...seriously? They can file a PR with those extra
                # half dozen test cases in play, E_NOTIME
                if self.derive_shorthand(hop)["host"] == self.host:
                    return None
                # Happily, ProxyJump uses identical format to our host
                # shorthand...
                kwargs = dict(config=self.config.clone())
                if prev_gw is not None:
                    kwargs["gateway"] = prev_gw
                cxn = Connection(hop, **kwargs)
                prev_gw = cxn
            return prev_gw
        elif "proxycommand" in self.ssh_config:
            # Just a string, which we interpret as a proxy command..
            return self.ssh_config["proxycommand"]
        # Fallback: config value (may be None).
        return self.config.gateway

    def __repr__(self):
        # Host comes first as it's the most common differentiator by far
        bits = [("host", self.host)]
        # TODO: maybe always show user regardless? Explicit is good...
        if self.user != self.config.user:
            bits.append(("user", self.user))
        # TODO: harder to make case for 'always show port'; maybe if it's
        # non-22 (even if config has overridden the local default)?
        if self.port != self.config.port:
            bits.append(("port", self.port))
        # NOTE: sometimes self.gateway may be eg False if someone wants to
        # explicitly override a configured non-None value (as otherwise it's
        # impossible for __init__ to tell if a None means "nothing given" or
        # "seriously please no gatewaying". So, this must always be a vanilla
        # truth test and not eg "is not None".
        if self.gateway:
            # Displaying type because gw params would probs be too verbose
            val = "proxyjump"
            if isinstance(self.gateway, string_types):
                val = "proxycommand"
            bits.append(("gw", val))
        return "<Connection {}>".format(
            " ".join("{}={}".format(*x) for x in bits)
        )

    def _identity(self):
        # TODO: consider including gateway and maybe even other init kwargs?
        # Whether two cxns w/ same user/host/port but different
        # gateway/keys/etc, should be considered "the same", is unclear.
        return (self.host, self.user, self.port)

    def __eq__(self, other):
        if not isinstance(other, Connection):
            return False
        return self._identity() == other._identity()

    def __lt__(self, other):
        return self._identity() < other._identity()

    def __hash__(self):
        # NOTE: this departs from Context/DataProxy, which is not usefully
        # hashable.
        return hash(self._identity())

    def derive_shorthand(self, host_string):
        user_hostport = host_string.rsplit("@", 1)
        hostport = user_hostport.pop()
        user = user_hostport[0] if user_hostport and user_hostport[0] else None

        # IPv6: can't reliably tell where addr ends and port begins, so don't
        # try (and don't bother adding special syntax either, user should avoid
        # this situation by using port=).
        if hostport.count(":") > 1:
            host = hostport
            port = None
        # IPv4: can split on ':' reliably.
        else:
            host_port = hostport.rsplit(":", 1)
            host = host_port.pop(0) or None
            port = host_port[0] if host_port and host_port[0] else None

        if port is not None:
            port = int(port)

        return {"user": user, "host": host, "port": port}

    @property
    def is_connected(self):
        """
        Whether or not this connection is actually open.

        .. versionadded:: 2.0
        """
        return self.transport.active if self.transport else False

    def open(self):
        """
        Initiate an SSH connection to the host/port this object is bound to.

        This may include activating the configured gateway connection, if one
        is set.

        Also saves a handle to the now-set Transport object for easier access.

        Various connect-time settings (and/or their corresponding :ref:`SSH
        config options <ssh-config>`) are utilized here in the call to
        `SSHClient.connect <paramiko.client.SSHClient.connect>`. (For details,
        see :doc:`the configuration docs </concepts/configuration>`.)

        .. versionadded:: 2.0
        """
        # Short-circuit
        if self.is_connected:
            return
        err = "Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!"  # noqa
        # These may not be given, period
        for key in """
            hostname
            port
            username
        """.split():
            if key in self.connect_kwargs:
                raise ValueError(err.format(key))
        # These may be given one way or the other, but not both
        if (
            "timeout" in self.connect_kwargs
            and self.connect_timeout is not None
        ):
            raise ValueError(err.format("timeout"))
        # No conflicts -> merge 'em together
        kwargs = dict(
            self.connect_kwargs,
            username=self.user,
            hostname=self.host,
            port=self.port,
        )
        if self.gateway:
            kwargs["sock"] = self.open_gateway()
        if self.connect_timeout:
            kwargs["timeout"] = self.connect_timeout
        # Strip out empty defaults for less noisy debugging
        if "key_filename" in kwargs and not kwargs["key_filename"]:
            del kwargs["key_filename"]
        # Actually connect!
        self.client.connect(**kwargs)
        self.transport = self.client.get_transport()

    def open_gateway(self):
        """
        Obtain a socket-like object from `gateway`.

        :returns:
            A ``direct-tcpip`` `paramiko.channel.Channel`, if `gateway` was a
            `.Connection`; or a `~paramiko.proxy.ProxyCommand`, if `gateway`
            was a string.

        .. versionadded:: 2.0
        """
        # ProxyCommand is faster to set up, so do it first.
        if isinstance(self.gateway, string_types):
            # Leverage a dummy SSHConfig to ensure %h/%p/etc are parsed.
            # TODO: use real SSH config once loading one properly is
            # implemented.
            ssh_conf = SSHConfig()
            dummy = "Host {}\n    ProxyCommand {}"
            ssh_conf.parse(StringIO(dummy.format(self.host, self.gateway)))
            return ProxyCommand(ssh_conf.lookup(self.host)["proxycommand"])
        # Handle inner-Connection gateway type here.
        # TODO: logging
        self.gateway.open()
        # TODO: expose the opened channel itself as an attribute? (another
        # possible argument for separating the two gateway types...) e.g. if
        # someone wanted to piggyback on it for other same-interpreter socket
        # needs...
        # TODO: and the inverse? allow users to supply their own socket/like
        # object they got via $WHEREEVER?
        # TODO: how best to expose timeout param? reuse general connection
        # timeout from config?
        return self.gateway.transport.open_channel(
            kind="direct-tcpip",
            dest_addr=(self.host, int(self.port)),
            # NOTE: src_addr needs to be 'empty but not None' values to
            # correctly encode into a network message. Theoretically Paramiko
            # could auto-interpret None sometime & save us the trouble.
            src_addr=("", 0),
        )

    def close(self):
        """
        Terminate the network connection to the remote end, if open.

        If no connection is open, this method does nothing.

        .. versionadded:: 2.0
        """
        if self.is_connected:
            self.client.close()
            if self.forward_agent and self._agent_handler is not None:
                self._agent_handler.close()

    def __enter__(self):
        return self

    def __exit__(self, *exc):
        self.close()

    @opens
    def create_session(self):
        channel = self.transport.open_session()
        if self.forward_agent:
            self._agent_handler = AgentRequestHandler(channel)
        return channel

    @opens
    def run(self, command, **kwargs):
        """
        Execute a shell command on the remote end of this connection.

        This method wraps an SSH-capable implementation of
        `invoke.runners.Runner.run`; see its documentation for details.

        .. warning::
            There are a few spots where Fabric departs from Invoke's default
            settings/behaviors; they are documented under
            `.Config.global_defaults`.

        .. versionadded:: 2.0
        """
        runner = self.config.runners.remote(self)
        return self._run(runner, command, **kwargs)

    @opens
    def sudo(self, command, **kwargs):
        """
        Execute a shell command, via ``sudo``, on the remote end.

        This method is identical to `invoke.context.Context.sudo` in every way,
        except in that -- like `run` -- it honors per-host/per-connection
        configuration overrides in addition to the generic/global ones. Thus,
        for example, per-host sudo passwords may be configured.

        .. versionadded:: 2.0
        """
        runner = self.config.runners.remote(self)
        return self._sudo(runner, command, **kwargs)

    def local(self, *args, **kwargs):
        """
        Execute a shell command on the local system.

        This method is effectively a wrapper of `invoke.run`; see its docs for
        details and call signature.

        .. versionadded:: 2.0
        """
        # Superclass run() uses runners.local, so we can literally just call it
        # straight.
        return super(Connection, self).run(*args, **kwargs)

    @opens
    def sftp(self):
        """
        Return a `~paramiko.sftp_client.SFTPClient` object.

        If called more than one time, memoizes the first result; thus, any
        given `.Connection` instance will only ever have a single SFTP client,
        and state (such as that managed by
        `~paramiko.sftp_client.SFTPClient.chdir`) will be preserved.

        .. versionadded:: 2.0
        """
        if self._sftp is None:
            self._sftp = self.client.open_sftp()
        return self._sftp

    def get(self, *args, **kwargs):
        """
        Get a remote file to the local filesystem or file-like object.

        Simply a wrapper for `.Transfer.get`. Please see its documentation for
        all details.

        .. versionadded:: 2.0
        """
        return Transfer(self).get(*args, **kwargs)

    def put(self, *args, **kwargs):
        """
        Put a remote file (or file-like object) to the remote filesystem.

        Simply a wrapper for `.Transfer.put`. Please see its documentation for
        all details.

        .. versionadded:: 2.0
        """
        return Transfer(self).put(*args, **kwargs)

    # TODO: yield the socket for advanced users? Other advanced use cases
    # (perhaps factor out socket creation itself)?
    # TODO: probably push some of this down into Paramiko
    @contextmanager
    @opens
    def forward_local(
        self,
        local_port,
        remote_port=None,
        remote_host="localhost",
        local_host="localhost",
    ):
        """
        Open a tunnel connecting ``local_port`` to the server's environment.

        For example, say you want to connect to a remote PostgreSQL database
        which is locked down and only accessible via the system it's running
        on. You have SSH access to this server, so you can temporarily make
        port 5432 on your local system act like port 5432 on the server::

            import psycopg2
            from fabric import Connection

            with Connection('my-db-server').forward_local(5432):
                db = psycopg2.connect(
                    host='localhost', port=5432, database='mydb'
                )
                # Do things with 'db' here

        This method is analogous to using the ``-L`` option of OpenSSH's
        ``ssh`` program.

        :param int local_port: The local port number on which to listen.

        :param int remote_port:
            The remote port number. Defaults to the same value as
            ``local_port``.

        :param str local_host:
            The local hostname/interface on which to listen. Default:
            ``localhost``.

        :param str remote_host:
            The remote hostname serving the forwarded remote port. Default:
            ``localhost`` (i.e., the host this `.Connection` is connected to.)

        :returns:
            Nothing; this method is only useful as a context manager affecting
            local operating system state.

        .. versionadded:: 2.0
        """
        if not remote_port:
            remote_port = local_port

        # TunnelManager does all of the work, sitting in the background (so we
        # can yield) and spawning threads every time somebody connects to our
        # local port.
        finished = Event()
        manager = TunnelManager(
            local_port=local_port,
            local_host=local_host,
            remote_port=remote_port,
            remote_host=remote_host,
            # TODO: not a huge fan of handing in our transport, but...?
            transport=self.transport,
            finished=finished,
        )
        manager.start()

        # Return control to caller now that things ought to be operational
        try:
            yield
        # Teardown once user exits block
        finally:
            # Signal to manager that it should close all open tunnels
            finished.set()
            # Then wait for it to do so
            manager.join()
            # Raise threading errors from within the manager, which would be
            # one of:
            # - an inner ThreadException, which was created by the manager on
            # behalf of its Tunnels; this gets directly raised.
            # - some other exception, which would thus have occurred in the
            # manager itself; we wrap this in a new ThreadException.
            # NOTE: in these cases, some of the metadata tracking in
            # ExceptionHandlingThread/ExceptionWrapper/ThreadException (which
            # is useful when dealing with multiple nearly-identical sibling IO
            # threads) is superfluous, but it doesn't feel worth breaking
            # things up further; we just ignore it for now.
            wrapper = manager.exception()
            if wrapper is not None:
                if wrapper.type is ThreadException:
                    raise wrapper.value
                else:
                    raise ThreadException([wrapper])

            # TODO: cancel port forward on transport? Does that even make sense
            # here (where we used direct-tcpip) vs the opposite method (which
            # is what uses forward-tcpip)?

    # TODO: probably push some of this down into Paramiko
    @contextmanager
    @opens
    def forward_remote(
        self,
        remote_port,
        local_port=None,
        remote_host="127.0.0.1",
        local_host="localhost",
    ):
        """
        Open a tunnel connecting ``remote_port`` to the local environment.

        For example, say you're running a daemon in development mode on your
        workstation at port 8080, and want to funnel traffic to it from a
        production or staging environment.

        In most situations this isn't possible as your office/home network
        probably blocks inbound traffic. But you have SSH access to this
        server, so you can temporarily make port 8080 on that server act like
        port 8080 on your workstation::

            from fabric import Connection

            c = Connection('my-remote-server')
            with c.forward_remote(8080):
                c.run("remote-data-writer --port 8080")
                # Assuming remote-data-writer runs until interrupted, this will
                # stay open until you Ctrl-C...

        This method is analogous to using the ``-R`` option of OpenSSH's
        ``ssh`` program.

        :param int remote_port: The remote port number on which to listen.

        :param int local_port:
            The local port number. Defaults to the same value as
            ``remote_port``.

        :param str local_host:
            The local hostname/interface the forwarded connection talks to.
            Default: ``localhost``.

        :param str remote_host:
            The remote interface address to listen on when forwarding
            connections. Default: ``127.0.0.1`` (i.e. only listen on the remote
            localhost).

        :returns:
            Nothing; this method is only useful as a context manager affecting
            local operating system state.

        .. versionadded:: 2.0
        """
        if not local_port:
            local_port = remote_port
        # Callback executes on each connection to the remote port and is given
        # a Channel hooked up to said port. (We don't actually care about the
        # source/dest host/port pairs at all; only whether the channel has data
        # to read and suchlike.)
        # We then pair that channel with a new 'outbound' socket connection to
        # the local host/port being forwarded, in a new Tunnel.
        # That Tunnel is then added to a shared data structure so we can track
        # & close them during shutdown.
        #
        # TODO: this approach is less than ideal because we have to share state
        # between ourselves & the callback handed into the transport's own
        # thread handling (which is roughly analogous to our self-controlled
        # TunnelManager for local forwarding). See if we can use more of
        # Paramiko's API (or improve it and then do so) so that isn't
        # necessary.
        tunnels = []

        def callback(channel, src_addr_tup, dst_addr_tup):
            sock = socket.socket()
            # TODO: handle connection failure such that channel, etc get closed
            sock.connect((local_host, local_port))
            # TODO: we don't actually need to generate the Events at our level,
            # do we? Just let Tunnel.__init__ do it; all we do is "press its
            # button" on shutdown...
            tunnel = Tunnel(channel=channel, sock=sock, finished=Event())
            tunnel.start()
            # Communication between ourselves & the Paramiko handling subthread
            tunnels.append(tunnel)

        # Ask Paramiko (really, the remote sshd) to call our callback whenever
        # connections are established on the remote iface/port.
        # transport.request_port_forward(remote_host, remote_port, callback)
        try:
            self.transport.request_port_forward(
                address=remote_host, port=remote_port, handler=callback
            )
            yield
        finally:
            # TODO: see above re: lack of a TunnelManager
            # TODO: and/or also refactor with TunnelManager re: shutdown logic.
            # E.g. maybe have a non-thread TunnelManager-alike with a method
            # that acts as the callback? At least then there's a tiny bit more
            # encapsulation...meh.
            for tunnel in tunnels:
                tunnel.finished.set()
                tunnel.join()
            self.transport.cancel_port_forward(
                address=remote_host, port=remote_port
            )
<EOF>
<BOF>
# TODO: this may want to move to Invoke if we can find a use for it there too?
# Or make it _more_ narrowly focused and stay here?
class NothingToDo(Exception):
    pass


class GroupException(Exception):
    """
    Lightweight exception wrapper for `.GroupResult` when one contains errors.

    .. versionadded:: 2.0
    """

    def __init__(self, result):
        #: The `.GroupResult` object which would have been returned, had there
        #: been no errors. See its docstring (and that of `.Group`) for
        #: details.
        self.result = result
<EOF>
<BOF>
# flake8: noqa
from ._version import __version_info__, __version__
from .connection import Config, Connection
from .runners import Remote, Result
from .group import Group, SerialGroup, ThreadingGroup, GroupResult
<EOF>
<BOF>
import copy
import errno
import os

from invoke.config import Config as InvokeConfig, merge_dicts
from paramiko.config import SSHConfig

from .runners import Remote
from .util import get_local_user, debug


class Config(InvokeConfig):
    """
    An `invoke.config.Config` subclass with extra Fabric-related behavior.

    This class behaves like `invoke.config.Config` in every way, with the
    following exceptions:

    - its `global_defaults` staticmethod has been extended to add/modify some
      default settings (see its documentation, below, for details);
    - it triggers loading of Fabric-specific env vars (e.g.
      ``FABRIC_RUN_HIDE=true`` instead of ``INVOKE_RUN_HIDE=true``) and
      filenames (e.g. ``/etc/fabric.yaml`` instead of ``/etc/invoke.yaml``).
    - it extends the API to account for loading ``ssh_config`` files (which are
      stored as additional attributes and have no direct relation to the
      regular config data/hierarchy.)

    Intended for use with `.Connection`, as using vanilla
    `invoke.config.Config` objects would require users to manually define
    ``port``, ``user`` and so forth.

    .. seealso:: :doc:`/concepts/configuration`, :ref:`ssh-config`

    .. versionadded:: 2.0
    """

    prefix = "fabric"

    def __init__(self, *args, **kwargs):
        """
        Creates a new Fabric-specific config object.

        For most API details, see `invoke.config.Config.__init__`. Parameters
        new to this subclass are listed below.

        :param ssh_config:
            Custom/explicit `paramiko.config.SSHConfig` object. If given,
            prevents loading of any SSH config files. Default: ``None``.

        :param str runtime_ssh_path:
            Runtime SSH config path to load. Prevents loading of system/user
            files if given. Default: ``None``.

        :param str system_ssh_path:
            Location of the system-level SSH config file. Default:
            ``/etc/ssh/ssh_config``.

        :param str user_ssh_path:
            Location of the user-level SSH config file. Default:
            ``~/.ssh/config``.

        :param bool lazy:
            Has the same meaning as the parent class' ``lazy``, but additionall
            controls whether SSH config file loading is deferred (requires
            manually calling `load_ssh_config` sometime.) For example, one may
            need to wait for user input before calling `set_runtime_ssh_path`,
            which will inform exactly what `load_ssh_config` does.
        """
        # Tease out our own kwargs.
        # TODO: consider moving more stuff out of __init__ and into methods so
        # there's less of this sort of splat-args + pop thing? Eh.
        ssh_config = kwargs.pop("ssh_config", None)
        lazy = kwargs.get("lazy", False)
        self.set_runtime_ssh_path(kwargs.pop("runtime_ssh_path", None))
        system_path = kwargs.pop("system_ssh_path", "/etc/ssh/ssh_config")
        self._set(_system_ssh_path=system_path)
        self._set(_user_ssh_path=kwargs.pop("user_ssh_path", "~/.ssh/config"))

        # Record whether we were given an explicit object (so other steps know
        # whether to bother loading from disk or not)
        # This needs doing before super __init__ as that calls our post_init
        explicit = ssh_config is not None
        self._set(_given_explicit_object=explicit)

        # Arrive at some non-None SSHConfig object (upon which to run .parse()
        # later, in _load_ssh_file())
        if ssh_config is None:
            ssh_config = SSHConfig()
        self._set(base_ssh_config=ssh_config)

        # Now that our own attributes have been prepared & kwargs yanked, we
        # can fall up into parent __init__()
        super(Config, self).__init__(*args, **kwargs)

        # And finally perform convenience non-lazy bits if needed
        if not lazy:
            self.load_ssh_config()

    def set_runtime_ssh_path(self, path):
        """
        Configure a runtime-level SSH config file path.

        If set, this will cause `load_ssh_config` to skip system and user
        files, as OpenSSH does.

        .. versionadded:: 2.0
        """
        self._set(_runtime_ssh_path=path)

    def load_ssh_config(self):
        """
        Load SSH config file(s) from disk.

        Also (beforehand) ensures that Invoke-level config re: runtime SSH
        config file paths, is accounted for.

        .. versionadded:: 2.0
        """
        # Update the runtime SSH config path (assumes enough regular config
        # levels have been loaded that anyone wanting to transmit this info
        # from a 'vanilla' Invoke config, has gotten it set.)
        if self.ssh_config_path:
            self._runtime_ssh_path = self.ssh_config_path
        # Load files from disk if we weren't given an explicit SSHConfig in
        # __init__
        if not self._given_explicit_object:
            self._load_ssh_files()

    def clone(self, *args, **kwargs):
        # TODO: clone() at this point kinda-sorta feels like it's retreading
        # __reduce__ and the related (un)pickling stuff...
        # Get cloned obj.
        # NOTE: Because we also extend .init_kwargs, the actual core SSHConfig
        # data is passed in at init time (ensuring no files get loaded a 2nd,
        # etc time) and will already be present, so we don't need to set
        # .base_ssh_config ourselves. Similarly, there's no need to worry about
        # how the SSH config paths may be inaccurate until below; nothing will
        # be referencing them.
        new = super(Config, self).clone(*args, **kwargs)
        # Copy over our custom attributes, so that the clone still resembles us
        # re: recording where the data originally came from (in case anything
        # re-runs ._load_ssh_files(), for example).
        for attr in (
            "_runtime_ssh_path",
            "_system_ssh_path",
            "_user_ssh_path",
        ):
            setattr(new, attr, getattr(self, attr))
        # Load SSH configs, in case they weren't prior to now (e.g. a vanilla
        # Invoke clone(into), instead of a us-to-us clone.)
        self.load_ssh_config()
        # All done
        return new

    def _clone_init_kwargs(self, *args, **kw):
        # Parent kwargs
        kwargs = super(Config, self)._clone_init_kwargs(*args, **kw)
        # Transmit our internal SSHConfig via explicit-obj kwarg, thus
        # bypassing any file loading. (Our extension of clone() above copies
        # over other attributes as well so that the end result looks consistent
        # with reality.)
        new_config = SSHConfig()
        # TODO: as with other spots, this implies SSHConfig needs a cleaner
        # public API re: creating and updating its core data.
        new_config._config = copy.deepcopy(self.base_ssh_config._config)
        return dict(kwargs, ssh_config=new_config)

    def _load_ssh_files(self):
        """
        Trigger loading of configured SSH config file paths.

        Expects that ``base_ssh_config`` has already been set to an
        `~paramiko.config.SSHConfig` object.

        :returns: ``None``.
        """
        # TODO: does this want to more closely ape the behavior of
        # InvokeConfig.load_files? re: having a _found attribute for each that
        # determines whether to load or skip
        if self._runtime_ssh_path is not None:
            path = self._runtime_ssh_path
            # Manually blow up like open() (_load_ssh_file normally doesn't)
            if not os.path.exists(path):
                msg = "No such file or directory: {!r}".format(path)
                raise IOError(errno.ENOENT, msg)
            self._load_ssh_file(os.path.expanduser(path))
        elif self.load_ssh_configs:
            for path in (self._user_ssh_path, self._system_ssh_path):
                self._load_ssh_file(os.path.expanduser(path))

    def _load_ssh_file(self, path):
        """
        Attempt to open and parse an SSH config file at ``path``.

        Does nothing if ``path`` is not a path to a valid file.

        :returns: ``None``.
        """
        if os.path.isfile(path):
            old_rules = len(self.base_ssh_config._config)
            with open(path) as fd:
                self.base_ssh_config.parse(fd)
            new_rules = len(self.base_ssh_config._config)
            msg = "Loaded {} new ssh_config rules from {!r}"
            debug(msg.format(new_rules - old_rules, path))
        else:
            debug("File not found, skipping")

    @staticmethod
    def global_defaults():
        """
        Default configuration values and behavior toggles.

        Fabric only extends this method in order to make minor adjustments and
        additions to Invoke's `~invoke.config.Config.global_defaults`; see its
        documentation for the base values, such as the config subtrees
        controlling behavior of ``run`` or how ``tasks`` behave.

        For Fabric-specific modifications and additions to the Invoke-level
        defaults, see our own config docs at :ref:`default-values`.

        .. versionadded:: 2.0
        """
        # TODO: hrm should the run-related things actually be derived from the
        # runner_class? E.g. Local defines local stuff, Remote defines remote
        # stuff? Doesn't help with the final config tree tho...
        # TODO: as to that, this is a core problem, Fabric wants split
        # local/remote stuff, eg replace_env wants to be False for local and
        # True remotely; shell wants to differ depending on target (and either
        # way, does not want to use local interrogation for remote)
        # TODO: is it worth moving all of our 'new' settings to a discrete
        # namespace for cleanliness' sake? e.g. ssh.port, ssh.user etc.
        # It wouldn't actually simplify this code any, but it would make it
        # easier for users to determine what came from which library/repo.
        defaults = InvokeConfig.global_defaults()
        ours = {
            # New settings
            "connect_kwargs": {},
            "forward_agent": False,
            "gateway": None,
            "load_ssh_configs": True,
            "port": 22,
            "run": {"replace_env": True},
            "runners": {"remote": Remote},
            "ssh_config_path": None,
            "tasks": {"collection_name": "fabfile"},
            # TODO: this becomes an override/extend once Invoke grows execution
            # timeouts (which should be timeouts.execute)
            "timeouts": {"connect": None},
            "user": get_local_user(),
        }
        merge_dicts(defaults, ours)
        return defaults
<EOF>
<BOF>
import logging
import sys


# Ape the half-assed logging junk from Invoke, but ensuring the logger reflects
# our name, not theirs. (Assume most contexts will rely on Invoke itself to
# literally enable/disable logging, for now.)
log = logging.getLogger("fabric")
for x in ("debug",):
    globals()[x] = getattr(log, x)


win32 = sys.platform == "win32"


def get_local_user():
    """
    Return the local executing username, or ``None`` if one can't be found.

    .. versionadded:: 2.0
    """
    # TODO: I don't understand why these lines were added outside the
    # try/except, since presumably it means the attempt at catching ImportError
    # wouldn't work. However, that's how the contributing user committed it.
    # Need an older Windows box to test it out, most likely.
    import getpass

    username = None
    # All Unix and most Windows systems support the getpass module.
    try:
        username = getpass.getuser()
    # Some SaaS platforms raise KeyError, implying there is no real user
    # involved. They get the default value of None.
    except KeyError:
        pass
    # Older (?) Windows systems don't support getpass well; they should
    # have the `win32` module instead.
    except ImportError:  # pragma: nocover
        if win32:
            import win32api
            import win32security  # noqa
            import win32profile  # noqa

            username = win32api.GetUserName()
    return username
<EOF>
<BOF>
__version_info__ = (2, 0, 5)
__version__ = ".".join(map(str, __version_info__))
<EOF>
<BOF>
try:
    from invoke.vendor.six.moves.queue import Queue
except ImportError:
    from six.moves.queue import Queue

from invoke.util import ExceptionHandlingThread

from .connection import Connection
from .exceptions import GroupException


class Group(list):
    """
    A collection of `.Connection` objects whose API operates on its contents.

    .. warning::
        **This is a partially abstract class**; you need to use one of its
        concrete subclasses (such as `.SerialGroup` or `.ThreadingGroup`) or
        you'll get ``NotImplementedError`` on most of the methods.

    Most methods in this class mirror those of `.Connection`, taking the same
    arguments; however their return values and exception-raising behavior
    differs:

    - Return values are dict-like objects (`.GroupResult`) mapping
      `.Connection` objects to the return value for the respective connections:
      `.Group.run` returns a map of `.Connection` to `.runners.Result`,
      `.Group.get` returns a map of `.Connection` to `.transfer.Result`, etc.
    - If any connections encountered exceptions, a `.GroupException` is raised,
      which is a thin wrapper around what would otherwise have been the
      `.GroupResult` returned; within that wrapped `.GroupResult`, the
      excepting connections map to the exception that was raised, in place of a
      ``Result`` (as no ``Result`` was obtained.) Any non-excepting connections
      will have a ``Result`` value, as normal.

    For example, when no exceptions occur, a session might look like this::

        >>> group = SerialGroup('host1', 'host2')
        >>> group.run("this is fine")
        {
            <Connection host='host1'>: <Result cmd='this is fine' exited=0>,
            <Connection host='host2'>: <Result cmd='this is fine' exited=0>,
        }

    With exceptions (anywhere from 1 to "all of them"), it looks like so; note
    the different exception classes, e.g. `~invoke.exceptions.UnexpectedExit`
    for a completed session whose command exited poorly, versus
    `socket.gaierror` for a host that had DNS problems::

        >>> group = SerialGroup('host1', 'host2', 'notahost')
        >>> group.run("will it blend?")
        {
            <Connection host='host1'>: <Result cmd='will it blend?' exited=0>,
            <Connection host='host2'>: <UnexpectedExit: cmd='...' exited=1>,
            <Connection host='notahost'>: gaierror(...),
        }

    .. versionadded:: 2.0
    """

    def __init__(self, *hosts):
        """
        Create a group of connections from one or more shorthand strings.

        See `.Connection` for details on the format of these strings - they
        will be used as the first positional argument of `.Connection`
        constructors.
        """
        # TODO: #563, #388 (could be here or higher up in Program area)
        self.extend(map(Connection, hosts))

    @classmethod
    def from_connections(cls, connections):
        """
        Alternate constructor accepting `.Connection` objects.

        .. versionadded:: 2.0
        """
        # TODO: *args here too; or maybe just fold into __init__ and type
        # check?
        group = cls()
        group.extend(connections)
        return group

    def run(self, *args, **kwargs):
        """
        Executes `.Connection.run` on all member `Connections <.Connection>`.

        :returns: a `.GroupResult`.

        .. versionadded:: 2.0
        """
        # TODO: probably best to suck it up & match actual run() sig?
        # TODO: how to change method of execution across contents? subclass,
        # kwargs, additional methods, inject an executor? Doing subclass for
        # now, but not 100% sure it's the best route.
        # TODO: also need way to deal with duplicate connections (see THOUGHTS)
        # TODO: and errors - probably FailureSet? How to handle other,
        # regular, non Failure, exceptions though? Still need an aggregate
        # exception type either way, whether it is FailureSet or what...
        # TODO: OTOH, users may well want to be able to operate on the hosts
        # that did not fail (esp if failure % is low) so we really _do_ want
        # something like a result object mixing success and failure, or maybe a
        # golang style two-tuple of successes and failures?
        # TODO: or keep going w/ a "return or except", but the object is
        # largely similar (if not identical) in both situations, with the
        # exception just being the signal that Shit Broke?
        raise NotImplementedError

    # TODO: how to handle sudo? Probably just an inner worker method that takes
    # the method name to actually call (run, sudo, etc)?

    # TODO: this all needs to mesh well with similar strategies applied to
    # entire tasks - so that may still end up factored out into Executors or
    # something lower level than both those and these?

    # TODO: local? Invoke wants ability to do that on its own though, which
    # would be distinct from Group. (May want to switch Group to use that,
    # though, whatever it ends up being?)

    # TODO: mirror Connection's close()?

    def get(self, *args, **kwargs):
        """
        Executes `.Connection.get` on all member `Connections <.Connection>`.

        :returns: a `.GroupResult`.

        .. versionadded:: 2.0
        """
        # TODO: probably best to suck it up & match actual get() sig?
        # TODO: actually implement on subclasses
        raise NotImplementedError


class SerialGroup(Group):
    """
    Subclass of `.Group` which executes in simple, serial fashion.

    .. versionadded:: 2.0
    """

    def run(self, *args, **kwargs):
        results = GroupResult()
        excepted = False
        for cxn in self:
            try:
                results[cxn] = cxn.run(*args, **kwargs)
            except Exception as e:
                results[cxn] = e
                excepted = True
        if excepted:
            raise GroupException(results)
        return results


def thread_worker(cxn, queue, args, kwargs):
    result = cxn.run(*args, **kwargs)
    # TODO: namedtuple or attrs object?
    queue.put((cxn, result))


class ThreadingGroup(Group):
    """
    Subclass of `.Group` which uses threading to execute concurrently.

    .. versionadded:: 2.0
    """

    def run(self, *args, **kwargs):
        results = GroupResult()
        queue = Queue()
        threads = []
        for cxn in self:
            my_kwargs = dict(cxn=cxn, queue=queue, args=args, kwargs=kwargs)
            thread = ExceptionHandlingThread(
                target=thread_worker, kwargs=my_kwargs
            )
            threads.append(thread)
        for thread in threads:
            thread.start()
        for thread in threads:
            # TODO: configurable join timeout
            # TODO: (in sudo's version) configurability around interactive
            # prompting resulting in an exception instead, as in v1
            thread.join()
        # Get non-exception results from queue
        while not queue.empty():
            # TODO: io-sleep? shouldn't matter if all threads are now joined
            cxn, result = queue.get(block=False)
            # TODO: outstanding musings about how exactly aggregate results
            # ought to ideally operate...heterogenous obj like this, multiple
            # objs, ??
            results[cxn] = result
        # Get exceptions from the threads themselves.
        # TODO: in a non-thread setup, this would differ, e.g.:
        # - a queue if using multiprocessing
        # - some other state-passing mechanism if using e.g. coroutines
        # - ???
        excepted = False
        for thread in threads:
            wrapper = thread.exception()
            if wrapper is not None:
                # Outer kwargs is Thread instantiation kwargs, inner is kwargs
                # passed to thread target/body.
                cxn = wrapper.kwargs["kwargs"]["cxn"]
                results[cxn] = wrapper.value
                excepted = True
        if excepted:
            raise GroupException(results)
        return results


class GroupResult(dict):
    """
    Collection of results and/or exceptions arising from `.Group` methods.

    Acts like a dict, but adds a couple convenience methods, to wit:

    - Keys are the individual `.Connection` objects from within the `.Group`.
    - Values are either return values / results from the called method (e.g.
      `.runners.Result` objects), *or* an exception object, if one prevented
      the method from returning.
    - Subclasses `dict`, so has all dict methods.
    - Has `.succeeded` and `.failed` attributes containing sub-dicts limited to
      just those key/value pairs that succeeded or encountered exceptions,
      respectively.

      - Of note, these attributes allow high level logic, e.g. ``if
        mygroup.run('command').failed`` and so forth.

    .. versionadded:: 2.0
    """

    def __init__(self, *args, **kwargs):
        super(dict, self).__init__(*args, **kwargs)
        self._successes = {}
        self._failures = {}

    def _bifurcate(self):
        # Short-circuit to avoid reprocessing every access.
        if self._successes or self._failures:
            return
        # TODO: if we ever expect .succeeded/.failed to be useful before a
        # GroupResult is fully initialized, this needs to become smarter.
        for key, value in self.items():
            if isinstance(value, BaseException):
                self._failures[key] = value
            else:
                self._successes[key] = value

    @property
    def succeeded(self):
        """
        A sub-dict containing only successful results.

        .. versionadded:: 2.0
        """
        self._bifurcate()
        return self._successes

    @property
    def failed(self):
        """
        A sub-dict containing only failed results.

        .. versionadded:: 2.0
        """
        self._bifurcate()
        return self._failures
<EOF>
<BOF>
from invoke import Call, Executor, Task

from . import Connection
from .exceptions import NothingToDo
from .util import debug


# TODO: come up w/ a better name heh
class FabExecutor(Executor):
    def expand_calls(self, calls, apply_hosts=True):
        # Generate new call list with per-host variants & Connections inserted
        ret = []
        # TODO: mesh well with Invoke list-type args helper (inv #132)
        hosts = []
        host_str = self.core[0].args.hosts.value
        if apply_hosts and host_str:
            hosts = host_str.split(",")
        for call in calls:
            if isinstance(call, Task):
                call = Call(task=call)
            # TODO: expand this to allow multiple types of execution plans,
            # pending outcome of invoke#461 (which, if flexible enough to
            # handle intersect of dependencies+parameterization, just becomes
            # 'honor that new feature of Invoke')
            # TODO: roles, other non-runtime host parameterizations, etc
            # Pre-tasks get added only once, not once per host.
            ret.extend(self.expand_calls(call.pre, apply_hosts=False))
            # Main task, per host
            for host in hosts:
                ret.append(self.parameterize(call, host))
            # Deal with lack of hosts arg (acts same as `inv` in that case)
            # TODO: no tests for this branch?
            if not hosts:
                ret.append(call)
            # Post-tasks added once, not once per host.
            ret.extend(self.expand_calls(call.post, apply_hosts=False))
        # Add remainder as anonymous task
        if self.core.remainder:
            # TODO: this will need to change once there are more options for
            # setting host lists besides "-H or 100% within-task"
            if not hosts:
                raise NothingToDo(
                    "Was told to run a command, but not given any hosts to run it on!"  # noqa
                )

            def anonymous(c):
                # TODO: how to make all our tests configure in_stream=False?
                c.run(self.core.remainder, in_stream=False)

            anon = Call(Task(body=anonymous))
            # TODO: see above TODOs about non-parameterized setups, roles etc
            # TODO: will likely need to refactor that logic some more so it can
            # be used both there and here.
            for host in hosts:
                ret.append(self.parameterize(anon, host))
        return ret

    def parameterize(self, call, host):
        """
        Parameterize a Call with its Context set to a per-host Config.
        """
        debug("Parameterizing {!r} for host {!r}".format(call, host))
        # Generate a custom ConnectionCall that knows how to yield a Connection
        # in its make_context(), specifically one to the host requested here.
        clone = call.clone(into=ConnectionCall)
        # TODO: using bag-of-attrs is mildly gross but whatever, I'll take it.
        clone.host = host
        return clone

    def dedupe(self, tasks):
        # Don't perform deduping, we will often have "duplicate" tasks w/
        # distinct host values/etc.
        # TODO: might want some deduplication later on though - falls under
        # "how to mesh parameterization with pre/post/etc deduping".
        return tasks


class ConnectionCall(Call):
    """
    Subclass of `invoke.tasks.Call` that generates `Connections <.Connection>`.
    """

    def make_context(self, config):
        return Connection(host=self.host, config=config)
<EOF>
<BOF>
"""
CLI entrypoint & parser configuration.

Builds on top of Invoke's core functionality for same.
"""

import getpass

from invoke import Argument, Collection, Program
from invoke import __version__ as invoke
from paramiko import __version__ as paramiko

from . import __version__ as fabric
from . import Config
from .executor import FabExecutor


class Fab(Program):
    def print_version(self):
        super(Fab, self).print_version()
        print("Paramiko {}".format(paramiko))
        print("Invoke {}".format(invoke))

    def core_args(self):
        core_args = super(Fab, self).core_args()
        my_args = [
            Argument(
                names=("S", "ssh-config"),
                help="Path to runtime SSH config file.",
            ),
            Argument(
                names=("H", "hosts"),
                help="Comma-separated host name(s) to execute tasks against.",
            ),
            Argument(
                names=("i", "identity"),
                kind=list,  # Same as OpenSSH, can give >1 key
                # TODO: automatically add hint about iterable-ness to Invoke
                # help display machinery?
                help="Path to runtime SSH identity (key) file. May be given multiple times.",  # noqa
            ),
            # TODO: worth having short flags for these prompt args?
            Argument(
                names=("prompt-for-login-password",),
                kind=bool,
                help="Request an upfront SSH-auth password prompt.",
            ),
            Argument(
                names=("prompt-for-passphrase",),
                kind=bool,
                help="Request an upfront SSH key passphrase prompt.",
            ),
        ]
        return core_args + my_args

    @property
    def _remainder_only(self):
        # No 'unparsed' (i.e. tokens intended for task contexts), and remainder
        # (text after a double-dash) implies a contextless/taskless remainder
        # execution of the style 'fab -H host -- command'.
        # NOTE: must ALSO check to ensure the double dash isn't being used for
        # tab completion machinery...
        return (
            not self.core.unparsed
            and self.core.remainder
            and not self.args.complete.value
        )

    def load_collection(self):
        # Stick in a dummy Collection if it looks like we were invoked w/o any
        # tasks, and with a remainder.
        # This isn't super ideal, but Invoke proper has no obvious "just run my
        # remainder" use case, so having it be capable of running w/o any task
        # module, makes no sense. But we want that capability for testing &
        # things like 'fab -H x,y,z -- mycommand'.
        if self._remainder_only:
            # TODO: hm we're probably not honoring project-specific configs in
            # this branch; is it worth having it assume CWD==project, since
            # that's often what users expect? Even tho no task collection to
            # honor the real "lives by task coll"?
            self.collection = Collection()
        else:
            super(Fab, self).load_collection()

    def no_tasks_given(self):
        # As above, neuter the usual "hey you didn't give me any tasks, let me
        # print help for you" behavior, if necessary.
        if not self._remainder_only:
            super(Fab, self).no_tasks_given()

    def create_config(self):
        # Create config, as parent does, but with lazy=True to avoid our own
        # SSH config autoload. (Otherwise, we can't correctly load _just_ the
        # runtime file if one's being given later.)
        self.config = self.config_class(lazy=True)
        # However, we don't really want the parent class' lazy behavior (which
        # skips loading system/global invoke-type conf files) so we manually do
        # that here to match upstream behavior.
        self.config.load_base_conf_files()
        # And merge again so that data is available.
        # TODO: really need to either A) stop giving fucks about calling
        # merge() "too many times", or B) make merge() itself determine whether
        # it needs to run and/or just merge stuff that's changed, so log spam
        # isn't as bad.
        self.config.merge()

    def update_config(self):
        # Note runtime SSH path, if given, and load SSH configurations.
        # NOTE: must do parent before our work, in case users want to disable
        # SSH config loading within a runtime-level conf file/flag.
        super(Fab, self).update_config(merge=False)
        self.config.set_runtime_ssh_path(self.args["ssh-config"].value)
        self.config.load_ssh_config()
        # Load -i identity file, if given, into connect_kwargs, at overrides
        # level.
        # TODO: this feels a little gross, but since the parent has already
        # called load_overrides, this is best we can do for now w/o losing
        # data. Still feels correct; just might be cleaner to have even more
        # Config API members around this sort of thing. Shrug.
        connect_kwargs = {}
        path = self.args["identity"].value
        if path:
            connect_kwargs["key_filename"] = path
        # Secrets prompts that want to happen at handoff time instead of
        # later/at user-time.
        # TODO: should this become part of Invoke proper in case other
        # downstreams have need of it? E.g. a prompt Argument 'type'? We're
        # already doing a similar thing there for sudo password...
        if self.args["prompt-for-login-password"].value:
            prompt = "Enter login password for use with SSH auth: "
            connect_kwargs["password"] = getpass.getpass(prompt)
        if self.args["prompt-for-passphrase"].value:
            prompt = "Enter passphrase for use unlocking SSH keys: "
            connect_kwargs["passphrase"] = getpass.getpass(prompt)
        self.config._overrides["connect_kwargs"] = connect_kwargs
        # Since we gave merge=False above, we must do it ourselves here. (Also
        # allows us to 'compile' our overrides manipulation.)
        self.config.merge()


# Mostly a concession to testing.
def make_program():
    return Fab(
        name="Fabric",
        version=fabric,
        executor_class=FabExecutor,
        config_class=Config,
    )


program = make_program()
<EOF>
<BOF>
"""
Tunnel and connection forwarding internals.

If you're looking for simple, end-user-focused connection forwarding, please
see `.Connection`, e.g. `.Connection.forward_local`.
"""

import errno
import select
import socket
import time
from threading import Event

from invoke.exceptions import ThreadException
from invoke.util import ExceptionHandlingThread


class TunnelManager(ExceptionHandlingThread):
    """
    Thread subclass for tunnelling connections over SSH between two endpoints.

    Specifically, one instance of this class is sufficient to sit around
    forwarding any number of individual connections made to one end of the
    tunnel or the other. If you need to forward connections between more than
    one set of ports, you'll end up instantiating multiple TunnelManagers.

    Wraps a `~paramiko.transport.Transport`, which should already be connected
    to the remote server.

    .. versionadded:: 2.0
    """

    def __init__(
        self,
        local_host,
        local_port,
        remote_host,
        remote_port,
        transport,
        finished,
    ):
        super(TunnelManager, self).__init__()
        self.local_address = (local_host, local_port)
        self.remote_address = (remote_host, remote_port)
        self.transport = transport
        self.finished = finished

    def _run(self):
        # Track each tunnel that gets opened during our lifetime
        tunnels = []

        # Set up OS-level listener socket on forwarded port
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # TODO: why do we want REUSEADDR exactly? and is it portable?
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # NOTE: choosing to deal with nonblocking semantics and a fast loop,
        # versus an older approach which blocks & expects outer scope to cause
        # a socket exception by close()ing the socket.
        sock.setblocking(0)
        sock.bind(self.local_address)
        sock.listen(1)

        while not self.finished.is_set():
            # Main loop-wait: accept connections on the local listener
            # NOTE: EAGAIN means "you're nonblocking and nobody happened to
            # connect at this point in time"
            try:
                tun_sock, local_addr = sock.accept()
                # Set TCP_NODELAY to match OpenSSH's forwarding socket behavior
                tun_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
            except socket.error as e:
                if e.errno is errno.EAGAIN:
                    # TODO: make configurable
                    time.sleep(0.01)
                    continue
                raise

            # Set up direct-tcpip channel on server end
            # TODO: refactor w/ what's used for gateways
            channel = self.transport.open_channel(
                "direct-tcpip", self.remote_address, local_addr
            )

            # Set up 'worker' thread for this specific connection to our
            # tunnel, plus its dedicated signal event (which will appear as a
            # public attr, no need to track both independently).
            finished = Event()
            tunnel = Tunnel(channel=channel, sock=tun_sock, finished=finished)
            tunnel.start()
            tunnels.append(tunnel)

        exceptions = []
        # Propogate shutdown signal to all tunnels & wait for closure
        # TODO: would be nice to have some output or at least logging here,
        # especially for "sets up a handful of tunnels" use cases like
        # forwarding nontrivial HTTP traffic.
        for tunnel in tunnels:
            tunnel.finished.set()
            tunnel.join()
            wrapper = tunnel.exception()
            if wrapper:
                exceptions.append(wrapper)
        # Handle exceptions
        if exceptions:
            raise ThreadException(exceptions)

        # All we have left to close is our own sock.
        # TODO: use try/finally?
        sock.close()


class Tunnel(ExceptionHandlingThread):
    """
    Bidirectionally forward data between an SSH channel and local socket.

    .. versionadded:: 2.0
    """

    def __init__(self, channel, sock, finished):
        self.channel = channel
        self.sock = sock
        self.finished = finished
        self.socket_chunk_size = 1024
        self.channel_chunk_size = 1024
        super(Tunnel, self).__init__()

    def _run(self):
        try:
            empty_sock, empty_chan = None, None
            while not self.finished.is_set():
                r, w, x = select.select([self.sock, self.channel], [], [], 1)
                if self.sock in r:
                    empty_sock = self.read_and_write(
                        self.sock, self.channel, self.socket_chunk_size
                    )
                if self.channel in r:
                    empty_chan = self.read_and_write(
                        self.channel, self.sock, self.channel_chunk_size
                    )
                if empty_sock or empty_chan:
                    break
        finally:
            self.channel.close()
            self.sock.close()

    def read_and_write(self, reader, writer, chunk_size):
        """
        Read ``chunk_size`` from ``reader``, writing result to ``writer``.

        Returns ``None`` if successful, or ``True`` if the read was empty.

        .. versionadded:: 2.0
        """
        data = reader.recv(chunk_size)
        if len(data) == 0:
            return True
        writer.sendall(data)
<EOF>
<BOF>
"""
File transfer via SFTP and/or SCP.
"""

import os
import posixpath
import stat

from .util import debug  # TODO: actual logging! LOL

# TODO: figure out best way to direct folks seeking rsync, to patchwork's rsync
# call (which needs updating to use invoke.run() & fab 2 connection methods,
# but is otherwise suitable).
# UNLESS we want to try and shoehorn it into this module after all? Delegate
# any recursive get/put to it? Requires users to have rsync available of
# course.


class Transfer(object):
    """
    `.Connection`-wrapping class responsible for managing file upload/download.

    .. versionadded:: 2.0
    """

    # TODO: SFTP clear default, but how to do SCP? subclass? init kwarg?

    def __init__(self, connection):
        self.connection = connection

    @property
    def sftp(self):
        return self.connection.sftp()

    def is_remote_dir(self, path):
        try:
            return stat.S_ISDIR(self.sftp.stat(path).st_mode)
        except IOError:
            return False

    def get(self, remote, local=None, preserve_mode=True):
        """
        Download a file from the current connection to the local filesystem.

        :param str remote:
            Remote file to download.

            May be absolute, or relative to the remote working directory.

            .. note::
                Most SFTP servers set the remote working directory to the
                connecting user's home directory, and (unlike most shells) do
                *not* expand tildes (``~``).

                For example, instead of saying ``get("~/tmp/archive.tgz")``,
                say ``get("tmp/archive.tgz")``.

        :param local:
            Local path to store downloaded file in, or a file-like object.

            **If None or another 'falsey'/empty value is given** (the default),
            the remote file is downloaded to the current working directory (as
            seen by `os.getcwd`) using its remote filename.

            **If a string is given**, it should be a path to a local directory
            or file and is subject to similar behavior as that seen by common
            Unix utilities or OpenSSH's ``sftp`` or ``scp`` tools.

            For example, if the local path is a directory, the remote path's
            base filename will be added onto it (so ``get('foo/bar/file.txt',
            '/tmp/')`` would result in creation or overwriting of
            ``/tmp/file.txt``).

            .. note::
                When dealing with nonexistent file paths, normal Python file
                handling concerns come into play - for example, a ``local``
                path containing non-leaf directories which do not exist, will
                typically result in an `OSError`.

            **If a file-like object is given**, the contents of the remote file
            are simply written into it.

        :param bool preserve_mode:
            Whether to `os.chmod` the local file so it matches the remote
            file's mode (default: ``True``).

        :returns: A `.Result` object.

        .. versionadded:: 2.0
        """
        # TODO: how does this API change if we want to implement
        # remote-to-remote file transfer? (Is that even realistic?)
        # TODO: handle v1's string interpolation bits, especially the default
        # one, or at least think about how that would work re: split between
        # single and multiple server targets.
        # TODO: callback support
        # TODO: how best to allow changing the behavior/semantics of
        # remote/local (e.g. users might want 'safer' behavior that complains
        # instead of overwriting existing files) - this likely ties into the
        # "how to handle recursive/rsync" and "how to handle scp" questions

        # Massage remote path
        if not remote:
            raise ValueError("Remote path must not be empty!")
        orig_remote = remote
        remote = posixpath.join(
            self.sftp.getcwd() or self.sftp.normalize("."), remote
        )

        # Massage local path:
        # - handle file-ness
        # - if path, fill with remote name if empty, & make absolute
        orig_local = local
        is_file_like = hasattr(local, "write") and callable(local.write)
        if not local:
            local = posixpath.basename(remote)
        if not is_file_like:
            local = os.path.abspath(local)

        # Run Paramiko-level .get() (side-effects only. womp.)
        # TODO: push some of the path handling into Paramiko; it should be
        # responsible for dealing with path cleaning etc.
        # TODO: probably preserve warning message from v1 when overwriting
        # existing files. Use logging for that obviously.
        #
        # If local appears to be a file-like object, use sftp.getfo, not get
        if is_file_like:
            self.sftp.getfo(remotepath=remote, fl=local)
        else:
            self.sftp.get(remotepath=remote, localpath=local)
            # Set mode to same as remote end
            # TODO: Push this down into SFTPClient sometime (requires backwards
            # incompat release.)
            if preserve_mode:
                remote_mode = self.sftp.stat(remote).st_mode
                mode = stat.S_IMODE(remote_mode)
                os.chmod(local, mode)
        # Return something useful
        return Result(
            orig_remote=orig_remote,
            remote=remote,
            orig_local=orig_local,
            local=local,
            connection=self.connection,
        )

    def put(self, local, remote=None, preserve_mode=True):
        """
        Upload a file from the local filesystem to the current connection.

        :param local:
            Local path of file to upload, or a file-like object.

            **If a string is given**, it should be a path to a local (regular)
            file (not a directory).

            .. note::
                When dealing with nonexistent file paths, normal Python file
                handling concerns come into play - for example, trying to
                upload a nonexistent ``local`` path will typically result in an
                `OSError`.

            **If a file-like object is given**, its contents are written to the
            remote file path.

        :param str remote:
            Remote path to which the local file will be written.

            .. note::
                Most SFTP servers set the remote working directory to the
                connecting user's home directory, and (unlike most shells) do
                *not* expand tildes (``~``).

                For example, instead of saying ``put("archive.tgz",
                "~/tmp/")``, say ``put("archive.tgz", "tmp/")``.

                In addition, this means that 'falsey'/empty values (such as the
                default value, ``None``) are allowed and result in uploading to
                the remote home directory.

            .. note::
                When ``local`` is a file-like object, ``remote`` is required
                and must refer to a valid file path (not a directory).

        :param bool preserve_mode:
            Whether to ``chmod`` the remote file so it matches the local file's
            mode (default: ``True``).

        :returns: A `.Result` object.

        .. versionadded:: 2.0
        """
        if not local:
            raise ValueError("Local path must not be empty!")

        is_file_like = hasattr(local, "write") and callable(local.write)

        # Massage remote path
        orig_remote = remote
        if is_file_like:
            local_base = getattr(local, "name", None)
        else:
            local_base = os.path.basename(local)
        if not remote:
            if is_file_like:
                raise ValueError(
                    "Must give non-empty remote path when local is a file-like object!"  # noqa
                )
            else:
                remote = local_base
                debug("Massaged empty remote path into {!r}".format(remote))
        elif self.is_remote_dir(remote):
            # non-empty local_base implies a) text file path or b) FLO which
            # had a non-empty .name attribute. huzzah!
            if local_base:
                remote = posixpath.join(remote, local_base)
            else:
                if is_file_like:
                    raise ValueError(
                        "Can't put a file-like-object into a directory unless it has a non-empty .name attribute!"  # noqa
                    )
                else:
                    # TODO: can we ever really end up here? implies we want to
                    # reorganize all this logic so it has fewer potential holes
                    raise ValueError(
                        "Somehow got an empty local file basename ({!r}) when uploading to a directory ({!r})!".format(  # noqa
                            local_base, remote
                        )
                    )

        prejoined_remote = remote
        remote = posixpath.join(
            self.sftp.getcwd() or self.sftp.normalize("."), remote
        )
        if remote != prejoined_remote:
            msg = "Massaged relative remote path {!r} into {!r}"
            debug(msg.format(prejoined_remote, remote))

        # Massage local path
        orig_local = local
        if not is_file_like:
            local = os.path.abspath(local)
            if local != orig_local:
                debug(
                    "Massaged relative local path {!r} into {!r}".format(
                        orig_local, local
                    )
                )  # noqa

        # Run Paramiko-level .put() (side-effects only. womp.)
        # TODO: push some of the path handling into Paramiko; it should be
        # responsible for dealing with path cleaning etc.
        # TODO: probably preserve warning message from v1 when overwriting
        # existing files. Use logging for that obviously.
        #
        # If local appears to be a file-like object, use sftp.putfo, not put
        if is_file_like:
            msg = "Uploading file-like object {!r} to {!r}"
            debug(msg.format(local, remote))
            pointer = local.tell()
            try:
                local.seek(0)
                self.sftp.putfo(fl=local, remotepath=remote)
            finally:
                local.seek(pointer)
        else:
            debug("Uploading {!r} to {!r}".format(local, remote))
            self.sftp.put(localpath=local, remotepath=remote)
            # Set mode to same as local end
            # TODO: Push this down into SFTPClient sometime (requires backwards
            # incompat release.)
            if preserve_mode:
                local_mode = os.stat(local).st_mode
                mode = stat.S_IMODE(local_mode)
                self.sftp.chmod(remote, mode)
        # Return something useful
        return Result(
            orig_remote=orig_remote,
            remote=remote,
            orig_local=orig_local,
            local=local,
            connection=self.connection,
        )


class Result(object):
    """
    A container for information about the result of a file transfer.

    See individual attribute/method documentation below for details.

    .. note::
        Unlike similar classes such as `invoke.runners.Result` or
        `fabric.runners.Result` (which have a concept of "warn and return
        anyways on failure") this class has no useful truthiness behavior. If a
        file transfer fails, some exception will be raised, either an `OSError`
        or an error from within Paramiko.

    .. versionadded:: 2.0
    """

    # TODO: how does this differ from put vs get? field stating which? (feels
    # meh) distinct classes differing, for now, solely by name? (also meh)
    def __init__(self, local, orig_local, remote, orig_remote, connection):
        #: The local path the file was saved as, or the object it was saved
        #: into if a file-like object was given instead.
        #:
        #: If a string path, this value is massaged to be absolute; see
        #: `.orig_local` for the original argument value.
        self.local = local
        #: The original value given as the returning method's ``local``
        #: argument.
        self.orig_local = orig_local
        #: The remote path downloaded from. Massaged to be absolute; see
        #: `.orig_remote` for the original argument value.
        self.remote = remote
        #: The original argument value given as the returning method's
        #: ``remote`` argument.
        self.orig_remote = orig_remote
        #: The `.Connection` object this result was obtained from.
        self.connection = connection

    # TODO: ensure str/repr makes it easily differentiable from run() or
    # local() result objects (and vice versa).
<EOF>
<BOF>
from invoke import Runner, pty_size, Result as InvokeResult


class Remote(Runner):
    """
    Run a shell command over an SSH connection.

    This class subclasses `invoke.runners.Runner`; please see its documentation
    for most public API details.

    .. note::
        `.Remote`'s ``__init__`` method expects a `.Connection` (or subclass)
        instance for its ``context`` argument.

    .. versionadded:: 2.0
    """

    def start(self, command, shell, env):
        self.channel = self.context.create_session()
        if self.using_pty:
            rows, cols = pty_size()
            self.channel.get_pty(width=rows, height=cols)
        # TODO: consider adding an option to conditionally turn this
        # update_environment call into a command-string prefixing behavior
        # instead (e.g. when one isn't able/willing to update remote server's
        # AcceptEnv setting). OR: rely on higher-level generic command
        # prefixing functionality, when implemented.
        # TODO: honor SendEnv from ssh_config
        self.channel.update_environment(env)
        # TODO: pass in timeout= here when invoke grows timeout functionality
        # in Runner/Local.
        self.channel.exec_command(command)

    def read_proc_stdout(self, num_bytes):
        return self.channel.recv(num_bytes)

    def read_proc_stderr(self, num_bytes):
        return self.channel.recv_stderr(num_bytes)

    def _write_proc_stdin(self, data):
        return self.channel.sendall(data)

    @property
    def process_is_finished(self):
        return self.channel.exit_status_ready()

    def send_interrupt(self, interrupt):
        # NOTE: in v1, we just reraised the KeyboardInterrupt unless a PTY was
        # present; this seems to have been because without a PTY, the
        # below escape sequence is ignored, so all we can do is immediately
        # terminate on our end.
        # NOTE: also in v1, the raising of the KeyboardInterrupt completely
        # skipped all thread joining & cleanup; presumably regular interpreter
        # shutdown suffices to tie everything off well enough.
        if self.using_pty:
            # Submit hex ASCII character 3, aka ETX, which most Unix PTYs
            # interpret as a foreground SIGINT.
            # TODO: is there anything else we can do here to be more portable?
            self.channel.send(u"\x03")
        else:
            raise interrupt

    def returncode(self):
        return self.channel.recv_exit_status()

    def generate_result(self, **kwargs):
        kwargs["connection"] = self.context
        return Result(**kwargs)

    def stop(self):
        if hasattr(self, "channel"):
            self.channel.close()

    # TODO: shit that is in fab 1 run() but could apply to invoke.Local too:
    # * command timeout control
    # * see rest of stuff in _run_command/_execute in operations.py...there is
    # a bunch that applies generally like optional exit codes, etc

    # TODO: general shit not done yet
    # * stdin; Local relies on local process management to ensure stdin is
    # hooked up; we cannot do that.
    # * output prefixing
    # * agent forwarding
    # * reading at 4096 bytes/time instead of whatever inv defaults to (also,
    # document why we are doing that, iirc it changed recentlyish via ticket)
    # * TODO: oh god so much more, go look it up

    # TODO: shit that has no Local equivalent that we probs need to backfill
    # into Runner, probably just as a "finish()" or "stop()" (to mirror
    # start()):
    # * channel close()
    # * agent-forward close()


class Result(InvokeResult):
    """
    An `invoke.runners.Result` exposing which `.Connection` was run against.

    Exposes all attributes from its superclass, then adds a ``.connection``,
    which is simply a reference to the `.Connection` whose method yielded this
    result.

    .. versionadded:: 2.0
    """

    def __init__(self, **kwargs):
        connection = kwargs.pop("connection")
        super(Result, self).__init__(**kwargs)
        self.connection = connection

    # TODO: have useful str/repr differentiation from invoke.Result,
    # transfer.Result etc.
<EOF>
<BOF>
from contextlib import contextmanager
from threading import Event

try:
    from invoke.vendor.six import StringIO
    from invoke.vendor.decorator import decorator
    from invoke.vendor.six import string_types
except ImportError:
    from six import StringIO
    from decorator import decorator
    from six import string_types
import socket


from invoke import Context
from invoke.exceptions import ThreadException
from paramiko.agent import AgentRequestHandler
from paramiko.client import SSHClient, AutoAddPolicy
from paramiko.config import SSHConfig
from paramiko.proxy import ProxyCommand

from .config import Config
from .transfer import Transfer
from .tunnels import TunnelManager, Tunnel


@decorator
def opens(method, self, *args, **kwargs):
    self.open()
    return method(self, *args, **kwargs)


class Connection(Context):
    """
    A connection to an SSH daemon, with methods for commands and file transfer.

    **Basics**

    This class inherits from Invoke's `~invoke.context.Context`, as it is a
    context within which commands, tasks etc can operate. It also encapsulates
    a Paramiko `~paramiko.client.SSHClient` instance, performing useful high
    level operations with that `~paramiko.client.SSHClient` and
    `~paramiko.channel.Channel` instances generated from it.

    **Lifecycle**

    `.Connection` has a basic "`create <__init__>`, `connect/open <open>`, `do
    work <run>`, `disconnect/close <close>`" lifecycle:

    * `Instantiation <__init__>` imprints the object with its connection
      parameters (but does **not** actually initiate the network connection).
    * Methods like `run`, `get` etc automatically trigger a call to
      `open` if the connection is not active; users may of course call `open`
      manually if desired.
    * Connections do not always need to be explicitly closed; much of the
      time, Paramiko's garbage collection hooks or Python's own shutdown
      sequence will take care of things. **However**, should you encounter edge
      cases (for example, sessions hanging on exit) it's helpful to explicitly
      close connections when you're done with them.

      This can be accomplished by manually calling `close`, or by using the
      object as a contextmanager::

        with Connection('host') as c:
            c.run('command')
            c.put('file')

    .. note::
        This class rebinds `invoke.context.Context.run` to `.local` so both
        remote and local command execution can coexist.

    **Configuration**

    Most `.Connection` parameters honor :doc:`Invoke-style configuration
    </concepts/configuration>` as well as any applicable :ref:`SSH config file
    directives <connection-ssh-config>`. For example, to end up with a
    connection to ``admin@myhost``, one could:

    - Use any built-in config mechanism, such as ``/etc/fabric.yml``,
      ``~/.fabric.json``, collection-driven configuration, env vars, etc,
      stating ``user: admin`` (or ``{"user": "admin"}``, depending on config
      format.) Then ``Connection('myhost')`` would implicitly have a ``user``
      of ``admin``.
    - Use an SSH config file containing ``User admin`` within any applicable
      ``Host`` header (``Host myhost``, ``Host *``, etc.) Again,
      ``Connection('myhost')`` will default to an ``admin`` user.
    - Leverage host-parameter shorthand (described in `.Config.__init__`), i.e.
      ``Connection('admin@myhost')``.
    - Give the parameter directly: ``Connection('myhost', user='admin')``.

    The same applies to agent forwarding, gateways, and so forth.

    .. versionadded:: 2.0
    """

    # NOTE: these are initialized here to hint to invoke.Config.__setattr__
    # that they should be treated as real attributes instead of config proxies.
    # (Additionally, we're doing this instead of using invoke.Config._set() so
    # we can take advantage of Sphinx's attribute-doc-comment static analysis.)
    # Once an instance is created, these values will usually be non-None
    # because they default to the default config values.
    host = None
    original_host = None
    user = None
    port = None
    ssh_config = None
    gateway = None
    forward_agent = None
    connect_timeout = None
    connect_kwargs = None
    client = None
    transport = None
    _sftp = None
    _agent_handler = None

    # TODO: should "reopening" an existing Connection object that has been
    # closed, be allowed? (See e.g. how v1 detects closed/semi-closed
    # connections & nukes them before creating a new client to the same host.)
    # TODO: push some of this into paramiko.client.Client? e.g. expand what
    # Client.exec_command does, it already allows configuring a subset of what
    # we do / will eventually do / did in 1.x. It's silly to have to do
    # .get_transport().open_session().
    def __init__(
        self,
        host,
        user=None,
        port=None,
        config=None,
        gateway=None,
        forward_agent=None,
        connect_timeout=None,
        connect_kwargs=None,
    ):
        """
        Set up a new object representing a server connection.

        :param str host:
            the hostname (or IP address) of this connection.

            May include shorthand for the ``user`` and/or ``port`` parameters,
            of the form ``user@host``, ``host:port``, or ``user@host:port``.

            .. note::
                Due to ambiguity, IPv6 host addresses are incompatible with the
                ``host:port`` shorthand (though ``user@host`` will still work
                OK). In other words, the presence of >1 ``:`` character will
                prevent any attempt to derive a shorthand port number; use the
                explicit ``port`` parameter instead.

            .. note::
                If ``host`` matches a ``Host`` clause in loaded SSH config
                data, and that ``Host`` clause contains a ``Hostname``
                directive, the resulting `.Connection` object will behave as if
                ``host`` is equal to that ``Hostname`` value.

                In all cases, the original value of ``host`` is preserved as
                the ``original_host`` attribute.

                Thus, given SSH config like so::

                    Host myalias
                        Hostname realhostname

                a call like ``Connection(host='myalias')`` will result in an
                object whose ``host`` attribute is ``realhostname``, and whose
                ``original_host`` attribute is ``myalias``.

        :param str user:
            the login user for the remote connection. Defaults to
            ``config.user``.

        :param int port:
            the remote port. Defaults to ``config.port``.

        :param config:
            configuration settings to use when executing methods on this
            `.Connection` (e.g. default SSH port and so forth).

            Should be a `.Config` or an `invoke.config.Config`
            (which will be turned into a `.Config`).

            Default is an anonymous `.Config` object.

        :param gateway:
            An object to use as a proxy or gateway for this connection.

            This parameter accepts one of the following:

            - another `.Connection` (for a ``ProxyJump`` style gateway);
            - a shell command string (for a ``ProxyCommand`` style style
              gateway).

            Default: ``None``, meaning no gatewaying will occur (unless
            otherwise configured; if one wants to override a configured gateway
            at runtime, specify ``gateway=False``.)

            .. seealso:: :ref:`ssh-gateways`

        :param bool forward_agent:
            Whether to enable SSH agent forwarding.

            Default: ``config.forward_agent``.

        :param int connect_timeout:
            Connection timeout, in seconds.

            Default: ``config.timeouts.connect``.

        :param dict connect_kwargs:
            Keyword arguments handed verbatim to
            `SSHClient.connect <paramiko.client.SSHClient.connect>` (when
            `.open` is called).

            `.Connection` tries not to grow additional settings/kwargs of its
            own unless it is adding value of some kind; thus,
            ``connect_kwargs`` is currently the right place to hand in
            parameters such as ``pkey`` or ``key_filename``.

            Default: ``config.connect_kwargs``.

        :raises ValueError:
            if user or port values are given via both ``host`` shorthand *and*
            their own arguments. (We `refuse the temptation to guess`_).

        .. _refuse the temptation to guess:
            http://zen-of-python.info/
            in-the-face-of-ambiguity-refuse-the-temptation-to-guess.html#12
        """
        # NOTE: parent __init__ sets self._config; for now we simply overwrite
        # that below. If it's somehow problematic we would want to break parent
        # __init__ up in a manner that is more cleanly overrideable.
        super(Connection, self).__init__(config=config)

        #: The .Config object referenced when handling default values (for e.g.
        #: user or port, when not explicitly given) or deciding how to behave.
        if config is None:
            config = Config()
        # Handle 'vanilla' Invoke config objects, which need cloning 'into' one
        # of our own Configs (which grants the new defaults, etc, while not
        # squashing them if the Invoke-level config already accounted for them)
        elif not isinstance(config, Config):
            config = config.clone(into=Config)
        self._set(_config=config)
        # TODO: when/how to run load_files, merge, load_shell_env, etc?
        # TODO: i.e. what is the lib use case here (and honestly in invoke too)

        shorthand = self.derive_shorthand(host)
        host = shorthand["host"]
        err = "You supplied the {} via both shorthand and kwarg! Please pick one."  # noqa
        if shorthand["user"] is not None:
            if user is not None:
                raise ValueError(err.format("user"))
            user = shorthand["user"]
        if shorthand["port"] is not None:
            if port is not None:
                raise ValueError(err.format("port"))
            port = shorthand["port"]

        # NOTE: we load SSH config data as early as possible as it has
        # potential to affect nearly every other attribute.
        #: The per-host SSH config data, if any. (See :ref:`ssh-config`.)
        self.ssh_config = self.config.base_ssh_config.lookup(host)

        self.original_host = host
        #: The hostname of the target server.
        self.host = host
        if "hostname" in self.ssh_config:
            # TODO: log that this occurred?
            self.host = self.ssh_config["hostname"]

        #: The username this connection will use to connect to the remote end.
        self.user = user or self.ssh_config.get("user", self.config.user)
        # TODO: is it _ever_ possible to give an empty user value (e.g.
        # user='')? E.g. do some SSH server specs allow for that?

        #: The network port to connect on.
        self.port = port or int(self.ssh_config.get("port", self.config.port))

        # Gateway/proxy/bastion/jump setting: non-None values - string,
        # Connection, even eg False - get set directly; None triggers seek in
        # config/ssh_config
        #: The gateway `.Connection` or ``ProxyCommand`` string to be used,
        #: if any.
        self.gateway = gateway if gateway is not None else self.get_gateway()
        # NOTE: we use string above, vs ProxyCommand obj, to avoid spinning up
        # the ProxyCommand subprocess at init time, vs open() time.
        # TODO: make paramiko.proxy.ProxyCommand lazy instead?

        if forward_agent is None:
            # Default to config...
            forward_agent = self.config.forward_agent
            # But if ssh_config is present, it wins
            if "forwardagent" in self.ssh_config:
                # TODO: SSHConfig really, seriously needs some love here, god
                map_ = {"yes": True, "no": False}
                forward_agent = map_[self.ssh_config["forwardagent"]]
        #: Whether agent forwarding is enabled.
        self.forward_agent = forward_agent

        if connect_timeout is None:
            connect_timeout = self.ssh_config.get(
                "connecttimeout", self.config.timeouts.connect
            )
        if connect_timeout is not None:
            connect_timeout = int(connect_timeout)
        #: Connection timeout
        self.connect_timeout = connect_timeout

        #: Keyword arguments given to `paramiko.client.SSHClient.connect` when
        #: `open` is called.
        self.connect_kwargs = self.resolve_connect_kwargs(connect_kwargs)

        #: The `paramiko.client.SSHClient` instance this connection wraps.
        client = SSHClient()
        client.set_missing_host_key_policy(AutoAddPolicy())
        self.client = client

        #: A convenience handle onto the return value of
        #: ``self.client.get_transport()``.
        self.transport = None

    def resolve_connect_kwargs(self, connect_kwargs):
        # Grab connect_kwargs from config if not explicitly given.
        if connect_kwargs is None:
            # TODO: is it better to pre-empt conflicts w/ manually-handled
            # connect() kwargs (hostname, username, etc) here or in open()?
            # We're doing open() for now in case e.g. someone manually modifies
            # .connect_kwargs attributewise, but otherwise it feels better to
            # do it early instead of late.
            connect_kwargs = self.config.connect_kwargs
        # Special case: key_filename gets merged instead of overridden.
        # TODO: probably want some sorta smart merging generally, special cases
        # are bad.
        elif "key_filename" in self.config.connect_kwargs:
            kwarg_val = connect_kwargs.get("key_filename", [])
            conf_val = self.config.connect_kwargs["key_filename"]
            # Config value comes before kwarg value (because it may contain
            # CLI flag value.)
            connect_kwargs["key_filename"] = conf_val + kwarg_val

        # SSH config identityfile values come last in the key_filename
        # 'hierarchy'.
        if "identityfile" in self.ssh_config:
            connect_kwargs.setdefault("key_filename", [])
            connect_kwargs["key_filename"].extend(
                self.ssh_config["identityfile"]
            )

        return connect_kwargs

    def get_gateway(self):
        # SSH config wins over Invoke-style config
        if "proxyjump" in self.ssh_config:
            # Reverse hop1,hop2,hop3 style ProxyJump directive so we start
            # with the final (itself non-gatewayed) hop and work up to
            # the front (actual, supplied as our own gateway) hop
            hops = reversed(self.ssh_config["proxyjump"].split(","))
            prev_gw = None
            for hop in hops:
                # Short-circuit if we appear to be our own proxy, which would
                # be a RecursionError. Implies SSH config wildcards.
                # TODO: in an ideal world we'd check user/port too in case they
                # differ, but...seriously? They can file a PR with those extra
                # half dozen test cases in play, E_NOTIME
                if self.derive_shorthand(hop)["host"] == self.host:
                    return None
                # Happily, ProxyJump uses identical format to our host
                # shorthand...
                kwargs = dict(config=self.config.clone())
                if prev_gw is not None:
                    kwargs["gateway"] = prev_gw
                cxn = Connection(hop, **kwargs)
                prev_gw = cxn
            return prev_gw
        elif "proxycommand" in self.ssh_config:
            # Just a string, which we interpret as a proxy command..
            return self.ssh_config["proxycommand"]
        # Fallback: config value (may be None).
        return self.config.gateway

    def __repr__(self):
        # Host comes first as it's the most common differentiator by far
        bits = [("host", self.host)]
        # TODO: maybe always show user regardless? Explicit is good...
        if self.user != self.config.user:
            bits.append(("user", self.user))
        # TODO: harder to make case for 'always show port'; maybe if it's
        # non-22 (even if config has overridden the local default)?
        if self.port != self.config.port:
            bits.append(("port", self.port))
        # NOTE: sometimes self.gateway may be eg False if someone wants to
        # explicitly override a configured non-None value (as otherwise it's
        # impossible for __init__ to tell if a None means "nothing given" or
        # "seriously please no gatewaying". So, this must always be a vanilla
        # truth test and not eg "is not None".
        if self.gateway:
            # Displaying type because gw params would probs be too verbose
            val = "proxyjump"
            if isinstance(self.gateway, string_types):
                val = "proxycommand"
            bits.append(("gw", val))
        return "<Connection {}>".format(
            " ".join("{}={}".format(*x) for x in bits)
        )

    def _identity(self):
        # TODO: consider including gateway and maybe even other init kwargs?
        # Whether two cxns w/ same user/host/port but different
        # gateway/keys/etc, should be considered "the same", is unclear.
        return (self.host, self.user, self.port)

    def __eq__(self, other):
        if not isinstance(other, Connection):
            return False
        return self._identity() == other._identity()

    def __lt__(self, other):
        return self._identity() < other._identity()

    def __hash__(self):
        # NOTE: this departs from Context/DataProxy, which is not usefully
        # hashable.
        return hash(self._identity())

    def derive_shorthand(self, host_string):
        user_hostport = host_string.rsplit("@", 1)
        hostport = user_hostport.pop()
        user = user_hostport[0] if user_hostport and user_hostport[0] else None

        # IPv6: can't reliably tell where addr ends and port begins, so don't
        # try (and don't bother adding special syntax either, user should avoid
        # this situation by using port=).
        if hostport.count(":") > 1:
            host = hostport
            port = None
        # IPv4: can split on ':' reliably.
        else:
            host_port = hostport.rsplit(":", 1)
            host = host_port.pop(0) or None
            port = host_port[0] if host_port and host_port[0] else None

        if port is not None:
            port = int(port)

        return {"user": user, "host": host, "port": port}

    @property
    def is_connected(self):
        """
        Whether or not this connection is actually open.

        .. versionadded:: 2.0
        """
        return self.transport.active if self.transport else False

    def open(self):
        """
        Initiate an SSH connection to the host/port this object is bound to.

        This may include activating the configured gateway connection, if one
        is set.

        Also saves a handle to the now-set Transport object for easier access.

        Various connect-time settings (and/or their corresponding :ref:`SSH
        config options <ssh-config>`) are utilized here in the call to
        `SSHClient.connect <paramiko.client.SSHClient.connect>`. (For details,
        see :doc:`the configuration docs </concepts/configuration>`.)

        .. versionadded:: 2.0
        """
        # Short-circuit
        if self.is_connected:
            return
        err = "Refusing to be ambiguous: connect() kwarg '{}' was given both via regular arg and via connect_kwargs!"  # noqa
        # These may not be given, period
        for key in """
            hostname
            port
            username
        """.split():
            if key in self.connect_kwargs:
                raise ValueError(err.format(key))
        # These may be given one way or the other, but not both
        if (
            "timeout" in self.connect_kwargs
            and self.connect_timeout is not None
        ):
            raise ValueError(err.format("timeout"))
        # No conflicts -> merge 'em together
        kwargs = dict(
            self.connect_kwargs,
            username=self.user,
            hostname=self.host,
            port=self.port,
        )
        if self.gateway:
            kwargs["sock"] = self.open_gateway()
        if self.connect_timeout:
            kwargs["timeout"] = self.connect_timeout
        # Strip out empty defaults for less noisy debugging
        if "key_filename" in kwargs and not kwargs["key_filename"]:
            del kwargs["key_filename"]
        # Actually connect!
        self.client.connect(**kwargs)
        self.transport = self.client.get_transport()

    def open_gateway(self):
        """
        Obtain a socket-like object from `gateway`.

        :returns:
            A ``direct-tcpip`` `paramiko.channel.Channel`, if `gateway` was a
            `.Connection`; or a `~paramiko.proxy.ProxyCommand`, if `gateway`
            was a string.

        .. versionadded:: 2.0
        """
        # ProxyCommand is faster to set up, so do it first.
        if isinstance(self.gateway, string_types):
            # Leverage a dummy SSHConfig to ensure %h/%p/etc are parsed.
            # TODO: use real SSH config once loading one properly is
            # implemented.
            ssh_conf = SSHConfig()
            dummy = "Host {}\n    ProxyCommand {}"
            ssh_conf.parse(StringIO(dummy.format(self.host, self.gateway)))
            return ProxyCommand(ssh_conf.lookup(self.host)["proxycommand"])
        # Handle inner-Connection gateway type here.
        # TODO: logging
        self.gateway.open()
        # TODO: expose the opened channel itself as an attribute? (another
        # possible argument for separating the two gateway types...) e.g. if
        # someone wanted to piggyback on it for other same-interpreter socket
        # needs...
        # TODO: and the inverse? allow users to supply their own socket/like
        # object they got via $WHEREEVER?
        # TODO: how best to expose timeout param? reuse general connection
        # timeout from config?
        return self.gateway.transport.open_channel(
            kind="direct-tcpip",
            dest_addr=(self.host, int(self.port)),
            # NOTE: src_addr needs to be 'empty but not None' values to
            # correctly encode into a network message. Theoretically Paramiko
            # could auto-interpret None sometime & save us the trouble.
            src_addr=("", 0),
        )

    def close(self):
        """
        Terminate the network connection to the remote end, if open.

        If no connection is open, this method does nothing.

        .. versionadded:: 2.0
        """
        if self.is_connected:
            self.client.close()
            if self.forward_agent and self._agent_handler is not None:
                self._agent_handler.close()

    def __enter__(self):
        return self

    def __exit__(self, *exc):
        self.close()

    @opens
    def create_session(self):
        channel = self.transport.open_session()
        if self.forward_agent:
            self._agent_handler = AgentRequestHandler(channel)
        return channel

    @opens
    def run(self, command, **kwargs):
        """
        Execute a shell command on the remote end of this connection.

        This method wraps an SSH-capable implementation of
        `invoke.runners.Runner.run`; see its documentation for details.

        .. warning::
            There are a few spots where Fabric departs from Invoke's default
            settings/behaviors; they are documented under
            `.Config.global_defaults`.

        .. versionadded:: 2.0
        """
        runner = self.config.runners.remote(self)
        return self._run(runner, command, **kwargs)

    @opens
    def sudo(self, command, **kwargs):
        """
        Execute a shell command, via ``sudo``, on the remote end.

        This method is identical to `invoke.context.Context.sudo` in every way,
        except in that -- like `run` -- it honors per-host/per-connection
        configuration overrides in addition to the generic/global ones. Thus,
        for example, per-host sudo passwords may be configured.

        .. versionadded:: 2.0
        """
        runner = self.config.runners.remote(self)
        return self._sudo(runner, command, **kwargs)

    def local(self, *args, **kwargs):
        """
        Execute a shell command on the local system.

        This method is effectively a wrapper of `invoke.run`; see its docs for
        details and call signature.

        .. versionadded:: 2.0
        """
        # Superclass run() uses runners.local, so we can literally just call it
        # straight.
        return super(Connection, self).run(*args, **kwargs)

    @opens
    def sftp(self):
        """
        Return a `~paramiko.sftp_client.SFTPClient` object.

        If called more than one time, memoizes the first result; thus, any
        given `.Connection` instance will only ever have a single SFTP client,
        and state (such as that managed by
        `~paramiko.sftp_client.SFTPClient.chdir`) will be preserved.

        .. versionadded:: 2.0
        """
        if self._sftp is None:
            self._sftp = self.client.open_sftp()
        return self._sftp

    def get(self, *args, **kwargs):
        """
        Get a remote file to the local filesystem or file-like object.

        Simply a wrapper for `.Transfer.get`. Please see its documentation for
        all details.

        .. versionadded:: 2.0
        """
        return Transfer(self).get(*args, **kwargs)

    def put(self, *args, **kwargs):
        """
        Put a remote file (or file-like object) to the remote filesystem.

        Simply a wrapper for `.Transfer.put`. Please see its documentation for
        all details.

        .. versionadded:: 2.0
        """
        return Transfer(self).put(*args, **kwargs)

    # TODO: yield the socket for advanced users? Other advanced use cases
    # (perhaps factor out socket creation itself)?
    # TODO: probably push some of this down into Paramiko
    @contextmanager
    @opens
    def forward_local(
        self,
        local_port,
        remote_port=None,
        remote_host="localhost",
        local_host="localhost",
    ):
        """
        Open a tunnel connecting ``local_port`` to the server's environment.

        For example, say you want to connect to a remote PostgreSQL database
        which is locked down and only accessible via the system it's running
        on. You have SSH access to this server, so you can temporarily make
        port 5432 on your local system act like port 5432 on the server::

            import psycopg2
            from fabric import Connection

            with Connection('my-db-server').forward_local(5432):
                db = psycopg2.connect(
                    host='localhost', port=5432, database='mydb'
                )
                # Do things with 'db' here

        This method is analogous to using the ``-L`` option of OpenSSH's
        ``ssh`` program.

        :param int local_port: The local port number on which to listen.

        :param int remote_port:
            The remote port number. Defaults to the same value as
            ``local_port``.

        :param str local_host:
            The local hostname/interface on which to listen. Default:
            ``localhost``.

        :param str remote_host:
            The remote hostname serving the forwarded remote port. Default:
            ``localhost`` (i.e., the host this `.Connection` is connected to.)

        :returns:
            Nothing; this method is only useful as a context manager affecting
            local operating system state.

        .. versionadded:: 2.0
        """
        if not remote_port:
            remote_port = local_port

        # TunnelManager does all of the work, sitting in the background (so we
        # can yield) and spawning threads every time somebody connects to our
        # local port.
        finished = Event()
        manager = TunnelManager(
            local_port=local_port,
            local_host=local_host,
            remote_port=remote_port,
            remote_host=remote_host,
            # TODO: not a huge fan of handing in our transport, but...?
            transport=self.transport,
            finished=finished,
        )
        manager.start()

        # Return control to caller now that things ought to be operational
        try:
            yield
        # Teardown once user exits block
        finally:
            # Signal to manager that it should close all open tunnels
            finished.set()
            # Then wait for it to do so
            manager.join()
            # Raise threading errors from within the manager, which would be
            # one of:
            # - an inner ThreadException, which was created by the manager on
            # behalf of its Tunnels; this gets directly raised.
            # - some other exception, which would thus have occurred in the
            # manager itself; we wrap this in a new ThreadException.
            # NOTE: in these cases, some of the metadata tracking in
            # ExceptionHandlingThread/ExceptionWrapper/ThreadException (which
            # is useful when dealing with multiple nearly-identical sibling IO
            # threads) is superfluous, but it doesn't feel worth breaking
            # things up further; we just ignore it for now.
            wrapper = manager.exception()
            if wrapper is not None:
                if wrapper.type is ThreadException:
                    raise wrapper.value
                else:
                    raise ThreadException([wrapper])

            # TODO: cancel port forward on transport? Does that even make sense
            # here (where we used direct-tcpip) vs the opposite method (which
            # is what uses forward-tcpip)?

    # TODO: probably push some of this down into Paramiko
    @contextmanager
    @opens
    def forward_remote(
        self,
        remote_port,
        local_port=None,
        remote_host="127.0.0.1",
        local_host="localhost",
    ):
        """
        Open a tunnel connecting ``remote_port`` to the local environment.

        For example, say you're running a daemon in development mode on your
        workstation at port 8080, and want to funnel traffic to it from a
        production or staging environment.

        In most situations this isn't possible as your office/home network
        probably blocks inbound traffic. But you have SSH access to this
        server, so you can temporarily make port 8080 on that server act like
        port 8080 on your workstation::

            from fabric import Connection

            c = Connection('my-remote-server')
            with c.forward_remote(8080):
                c.run("remote-data-writer --port 8080")
                # Assuming remote-data-writer runs until interrupted, this will
                # stay open until you Ctrl-C...

        This method is analogous to using the ``-R`` option of OpenSSH's
        ``ssh`` program.

        :param int remote_port: The remote port number on which to listen.

        :param int local_port:
            The local port number. Defaults to the same value as
            ``remote_port``.

        :param str local_host:
            The local hostname/interface the forwarded connection talks to.
            Default: ``localhost``.

        :param str remote_host:
            The remote interface address to listen on when forwarding
            connections. Default: ``127.0.0.1`` (i.e. only listen on the remote
            localhost).

        :returns:
            Nothing; this method is only useful as a context manager affecting
            local operating system state.

        .. versionadded:: 2.0
        """
        if not local_port:
            local_port = remote_port
        # Callback executes on each connection to the remote port and is given
        # a Channel hooked up to said port. (We don't actually care about the
        # source/dest host/port pairs at all; only whether the channel has data
        # to read and suchlike.)
        # We then pair that channel with a new 'outbound' socket connection to
        # the local host/port being forwarded, in a new Tunnel.
        # That Tunnel is then added to a shared data structure so we can track
        # & close them during shutdown.
        #
        # TODO: this approach is less than ideal because we have to share state
        # between ourselves & the callback handed into the transport's own
        # thread handling (which is roughly analogous to our self-controlled
        # TunnelManager for local forwarding). See if we can use more of
        # Paramiko's API (or improve it and then do so) so that isn't
        # necessary.
        tunnels = []

        def callback(channel, src_addr_tup, dst_addr_tup):
            sock = socket.socket()
            # TODO: handle connection failure such that channel, etc get closed
            sock.connect((local_host, local_port))
            # TODO: we don't actually need to generate the Events at our level,
            # do we? Just let Tunnel.__init__ do it; all we do is "press its
            # button" on shutdown...
            tunnel = Tunnel(channel=channel, sock=sock, finished=Event())
            tunnel.start()
            # Communication between ourselves & the Paramiko handling subthread
            tunnels.append(tunnel)

        # Ask Paramiko (really, the remote sshd) to call our callback whenever
        # connections are established on the remote iface/port.
        # transport.request_port_forward(remote_host, remote_port, callback)
        try:
            self.transport.request_port_forward(
                address=remote_host, port=remote_port, handler=callback
            )
            yield
        finally:
            # TODO: see above re: lack of a TunnelManager
            # TODO: and/or also refactor with TunnelManager re: shutdown logic.
            # E.g. maybe have a non-thread TunnelManager-alike with a method
            # that acts as the callback? At least then there's a tiny bit more
            # encapsulation...meh.
            for tunnel in tunnels:
                tunnel.finished.set()
                tunnel.join()
            self.transport.cancel_port_forward(
                address=remote_host, port=remote_port
            )
<EOF>
<BOF>
# TODO: this may want to move to Invoke if we can find a use for it there too?
# Or make it _more_ narrowly focused and stay here?
class NothingToDo(Exception):
    pass


class GroupException(Exception):
    """
    Lightweight exception wrapper for `.GroupResult` when one contains errors.

    .. versionadded:: 2.0
    """

    def __init__(self, result):
        #: The `.GroupResult` object which would have been returned, had there
        #: been no errors. See its docstring (and that of `.Group`) for
        #: details.
        self.result = result
<EOF>
<BOF>
# flake8: noqa
from ._version import __version_info__, __version__
from .connection import Config, Connection
from .runners import Remote, Result
from .group import Group, SerialGroup, ThreadingGroup, GroupResult
<EOF>
<BOF>
import copy
import errno
import os

from invoke.config import Config as InvokeConfig, merge_dicts
from paramiko.config import SSHConfig

from .runners import Remote
from .util import get_local_user, debug


class Config(InvokeConfig):
    """
    An `invoke.config.Config` subclass with extra Fabric-related behavior.

    This class behaves like `invoke.config.Config` in every way, with the
    following exceptions:

    - its `global_defaults` staticmethod has been extended to add/modify some
      default settings (see its documentation, below, for details);
    - it triggers loading of Fabric-specific env vars (e.g.
      ``FABRIC_RUN_HIDE=true`` instead of ``INVOKE_RUN_HIDE=true``) and
      filenames (e.g. ``/etc/fabric.yaml`` instead of ``/etc/invoke.yaml``).
    - it extends the API to account for loading ``ssh_config`` files (which are
      stored as additional attributes and have no direct relation to the
      regular config data/hierarchy.)

    Intended for use with `.Connection`, as using vanilla
    `invoke.config.Config` objects would require users to manually define
    ``port``, ``user`` and so forth.

    .. seealso:: :doc:`/concepts/configuration`, :ref:`ssh-config`

    .. versionadded:: 2.0
    """

    prefix = "fabric"

    def __init__(self, *args, **kwargs):
        """
        Creates a new Fabric-specific config object.

        For most API details, see `invoke.config.Config.__init__`. Parameters
        new to this subclass are listed below.

        :param ssh_config:
            Custom/explicit `paramiko.config.SSHConfig` object. If given,
            prevents loading of any SSH config files. Default: ``None``.

        :param str runtime_ssh_path:
            Runtime SSH config path to load. Prevents loading of system/user
            files if given. Default: ``None``.

        :param str system_ssh_path:
            Location of the system-level SSH config file. Default:
            ``/etc/ssh/ssh_config``.

        :param str user_ssh_path:
            Location of the user-level SSH config file. Default:
            ``~/.ssh/config``.

        :param bool lazy:
            Has the same meaning as the parent class' ``lazy``, but additionall
            controls whether SSH config file loading is deferred (requires
            manually calling `load_ssh_config` sometime.) For example, one may
            need to wait for user input before calling `set_runtime_ssh_path`,
            which will inform exactly what `load_ssh_config` does.
        """
        # Tease out our own kwargs.
        # TODO: consider moving more stuff out of __init__ and into methods so
        # there's less of this sort of splat-args + pop thing? Eh.
        ssh_config = kwargs.pop("ssh_config", None)
        lazy = kwargs.get("lazy", False)
        self.set_runtime_ssh_path(kwargs.pop("runtime_ssh_path", None))
        system_path = kwargs.pop("system_ssh_path", "/etc/ssh/ssh_config")
        self._set(_system_ssh_path=system_path)
        self._set(_user_ssh_path=kwargs.pop("user_ssh_path", "~/.ssh/config"))

        # Record whether we were given an explicit object (so other steps know
        # whether to bother loading from disk or not)
        # This needs doing before super __init__ as that calls our post_init
        explicit = ssh_config is not None
        self._set(_given_explicit_object=explicit)

        # Arrive at some non-None SSHConfig object (upon which to run .parse()
        # later, in _load_ssh_file())
        if ssh_config is None:
            ssh_config = SSHConfig()
        self._set(base_ssh_config=ssh_config)

        # Now that our own attributes have been prepared & kwargs yanked, we
        # can fall up into parent __init__()
        super(Config, self).__init__(*args, **kwargs)

        # And finally perform convenience non-lazy bits if needed
        if not lazy:
            self.load_ssh_config()

    def set_runtime_ssh_path(self, path):
        """
        Configure a runtime-level SSH config file path.

        If set, this will cause `load_ssh_config` to skip system and user
        files, as OpenSSH does.

        .. versionadded:: 2.0
        """
        self._set(_runtime_ssh_path=path)

    def load_ssh_config(self):
        """
        Load SSH config file(s) from disk.

        Also (beforehand) ensures that Invoke-level config re: runtime SSH
        config file paths, is accounted for.

        .. versionadded:: 2.0
        """
        # Update the runtime SSH config path (assumes enough regular config
        # levels have been loaded that anyone wanting to transmit this info
        # from a 'vanilla' Invoke config, has gotten it set.)
        if self.ssh_config_path:
            self._runtime_ssh_path = self.ssh_config_path
        # Load files from disk if we weren't given an explicit SSHConfig in
        # __init__
        if not self._given_explicit_object:
            self._load_ssh_files()

    def clone(self, *args, **kwargs):
        # TODO: clone() at this point kinda-sorta feels like it's retreading
        # __reduce__ and the related (un)pickling stuff...
        # Get cloned obj.
        # NOTE: Because we also extend .init_kwargs, the actual core SSHConfig
        # data is passed in at init time (ensuring no files get loaded a 2nd,
        # etc time) and will already be present, so we don't need to set
        # .base_ssh_config ourselves. Similarly, there's no need to worry about
        # how the SSH config paths may be inaccurate until below; nothing will
        # be referencing them.
        new = super(Config, self).clone(*args, **kwargs)
        # Copy over our custom attributes, so that the clone still resembles us
        # re: recording where the data originally came from (in case anything
        # re-runs ._load_ssh_files(), for example).
        for attr in (
            "_runtime_ssh_path",
            "_system_ssh_path",
            "_user_ssh_path",
        ):
            setattr(new, attr, getattr(self, attr))
        # Load SSH configs, in case they weren't prior to now (e.g. a vanilla
        # Invoke clone(into), instead of a us-to-us clone.)
        self.load_ssh_config()
        # All done
        return new

    def _clone_init_kwargs(self, *args, **kw):
        # Parent kwargs
        kwargs = super(Config, self)._clone_init_kwargs(*args, **kw)
        # Transmit our internal SSHConfig via explicit-obj kwarg, thus
        # bypassing any file loading. (Our extension of clone() above copies
        # over other attributes as well so that the end result looks consistent
        # with reality.)
        new_config = SSHConfig()
        # TODO: as with other spots, this implies SSHConfig needs a cleaner
        # public API re: creating and updating its core data.
        new_config._config = copy.deepcopy(self.base_ssh_config._config)
        return dict(kwargs, ssh_config=new_config)

    def _load_ssh_files(self):
        """
        Trigger loading of configured SSH config file paths.

        Expects that ``base_ssh_config`` has already been set to an
        `~paramiko.config.SSHConfig` object.

        :returns: ``None``.
        """
        # TODO: does this want to more closely ape the behavior of
        # InvokeConfig.load_files? re: having a _found attribute for each that
        # determines whether to load or skip
        if self._runtime_ssh_path is not None:
            path = self._runtime_ssh_path
            # Manually blow up like open() (_load_ssh_file normally doesn't)
            if not os.path.exists(path):
                msg = "No such file or directory: {!r}".format(path)
                raise IOError(errno.ENOENT, msg)
            self._load_ssh_file(os.path.expanduser(path))
        elif self.load_ssh_configs:
            for path in (self._user_ssh_path, self._system_ssh_path):
                self._load_ssh_file(os.path.expanduser(path))

    def _load_ssh_file(self, path):
        """
        Attempt to open and parse an SSH config file at ``path``.

        Does nothing if ``path`` is not a path to a valid file.

        :returns: ``None``.
        """
        if os.path.isfile(path):
            old_rules = len(self.base_ssh_config._config)
            with open(path) as fd:
                self.base_ssh_config.parse(fd)
            new_rules = len(self.base_ssh_config._config)
            msg = "Loaded {} new ssh_config rules from {!r}"
            debug(msg.format(new_rules - old_rules, path))
        else:
            debug("File not found, skipping")

    @staticmethod
    def global_defaults():
        """
        Default configuration values and behavior toggles.

        Fabric only extends this method in order to make minor adjustments and
        additions to Invoke's `~invoke.config.Config.global_defaults`; see its
        documentation for the base values, such as the config subtrees
        controlling behavior of ``run`` or how ``tasks`` behave.

        For Fabric-specific modifications and additions to the Invoke-level
        defaults, see our own config docs at :ref:`default-values`.

        .. versionadded:: 2.0
        """
        # TODO: hrm should the run-related things actually be derived from the
        # runner_class? E.g. Local defines local stuff, Remote defines remote
        # stuff? Doesn't help with the final config tree tho...
        # TODO: as to that, this is a core problem, Fabric wants split
        # local/remote stuff, eg replace_env wants to be False for local and
        # True remotely; shell wants to differ depending on target (and either
        # way, does not want to use local interrogation for remote)
        # TODO: is it worth moving all of our 'new' settings to a discrete
        # namespace for cleanliness' sake? e.g. ssh.port, ssh.user etc.
        # It wouldn't actually simplify this code any, but it would make it
        # easier for users to determine what came from which library/repo.
        defaults = InvokeConfig.global_defaults()
        ours = {
            # New settings
            "connect_kwargs": {},
            "forward_agent": False,
            "gateway": None,
            "load_ssh_configs": True,
            "port": 22,
            "run": {"replace_env": True},
            "runners": {"remote": Remote},
            "ssh_config_path": None,
            "tasks": {"collection_name": "fabfile"},
            # TODO: this becomes an override/extend once Invoke grows execution
            # timeouts (which should be timeouts.execute)
            "timeouts": {"connect": None},
            "user": get_local_user(),
        }
        merge_dicts(defaults, ours)
        return defaults
<EOF>
<BOF>
import logging
import sys


# Ape the half-assed logging junk from Invoke, but ensuring the logger reflects
# our name, not theirs. (Assume most contexts will rely on Invoke itself to
# literally enable/disable logging, for now.)
log = logging.getLogger("fabric")
for x in ("debug",):
    globals()[x] = getattr(log, x)


win32 = sys.platform == "win32"


def get_local_user():
    """
    Return the local executing username, or ``None`` if one can't be found.

    .. versionadded:: 2.0
    """
    # TODO: I don't understand why these lines were added outside the
    # try/except, since presumably it means the attempt at catching ImportError
    # wouldn't work. However, that's how the contributing user committed it.
    # Need an older Windows box to test it out, most likely.
    import getpass

    username = None
    # All Unix and most Windows systems support the getpass module.
    try:
        username = getpass.getuser()
    # Some SaaS platforms raise KeyError, implying there is no real user
    # involved. They get the default value of None.
    except KeyError:
        pass
    # Older (?) Windows systems don't support getpass well; they should
    # have the `win32` module instead.
    except ImportError:  # pragma: nocover
        if win32:
            import win32api
            import win32security  # noqa
            import win32profile  # noqa

            username = win32api.GetUserName()
    return username
<EOF>
<BOF>
__version_info__ = (2, 0, 5)
__version__ = ".".join(map(str, __version_info__))
<EOF>
<BOF>
try:
    from invoke.vendor.six.moves.queue import Queue
except ImportError:
    from six.moves.queue import Queue

from invoke.util import ExceptionHandlingThread

from .connection import Connection
from .exceptions import GroupException


class Group(list):
    """
    A collection of `.Connection` objects whose API operates on its contents.

    .. warning::
        **This is a partially abstract class**; you need to use one of its
        concrete subclasses (such as `.SerialGroup` or `.ThreadingGroup`) or
        you'll get ``NotImplementedError`` on most of the methods.

    Most methods in this class mirror those of `.Connection`, taking the same
    arguments; however their return values and exception-raising behavior
    differs:

    - Return values are dict-like objects (`.GroupResult`) mapping
      `.Connection` objects to the return value for the respective connections:
      `.Group.run` returns a map of `.Connection` to `.runners.Result`,
      `.Group.get` returns a map of `.Connection` to `.transfer.Result`, etc.
    - If any connections encountered exceptions, a `.GroupException` is raised,
      which is a thin wrapper around what would otherwise have been the
      `.GroupResult` returned; within that wrapped `.GroupResult`, the
      excepting connections map to the exception that was raised, in place of a
      ``Result`` (as no ``Result`` was obtained.) Any non-excepting connections
      will have a ``Result`` value, as normal.

    For example, when no exceptions occur, a session might look like this::

        >>> group = SerialGroup('host1', 'host2')
        >>> group.run("this is fine")
        {
            <Connection host='host1'>: <Result cmd='this is fine' exited=0>,
            <Connection host='host2'>: <Result cmd='this is fine' exited=0>,
        }

    With exceptions (anywhere from 1 to "all of them"), it looks like so; note
    the different exception classes, e.g. `~invoke.exceptions.UnexpectedExit`
    for a completed session whose command exited poorly, versus
    `socket.gaierror` for a host that had DNS problems::

        >>> group = SerialGroup('host1', 'host2', 'notahost')
        >>> group.run("will it blend?")
        {
            <Connection host='host1'>: <Result cmd='will it blend?' exited=0>,
            <Connection host='host2'>: <UnexpectedExit: cmd='...' exited=1>,
            <Connection host='notahost'>: gaierror(...),
        }

    .. versionadded:: 2.0
    """

    def __init__(self, *hosts):
        """
        Create a group of connections from one or more shorthand strings.

        See `.Connection` for details on the format of these strings - they
        will be used as the first positional argument of `.Connection`
        constructors.
        """
        # TODO: #563, #388 (could be here or higher up in Program area)
        self.extend(map(Connection, hosts))

    @classmethod
    def from_connections(cls, connections):
        """
        Alternate constructor accepting `.Connection` objects.

        .. versionadded:: 2.0
        """
        # TODO: *args here too; or maybe just fold into __init__ and type
        # check?
        group = cls()
        group.extend(connections)
        return group

    def run(self, *args, **kwargs):
        """
        Executes `.Connection.run` on all member `Connections <.Connection>`.

        :returns: a `.GroupResult`.

        .. versionadded:: 2.0
        """
        # TODO: probably best to suck it up & match actual run() sig?
        # TODO: how to change method of execution across contents? subclass,
        # kwargs, additional methods, inject an executor? Doing subclass for
        # now, but not 100% sure it's the best route.
        # TODO: also need way to deal with duplicate connections (see THOUGHTS)
        # TODO: and errors - probably FailureSet? How to handle other,
        # regular, non Failure, exceptions though? Still need an aggregate
        # exception type either way, whether it is FailureSet or what...
        # TODO: OTOH, users may well want to be able to operate on the hosts
        # that did not fail (esp if failure % is low) so we really _do_ want
        # something like a result object mixing success and failure, or maybe a
        # golang style two-tuple of successes and failures?
        # TODO: or keep going w/ a "return or except", but the object is
        # largely similar (if not identical) in both situations, with the
        # exception just being the signal that Shit Broke?
        raise NotImplementedError

    # TODO: how to handle sudo? Probably just an inner worker method that takes
    # the method name to actually call (run, sudo, etc)?

    # TODO: this all needs to mesh well with similar strategies applied to
    # entire tasks - so that may still end up factored out into Executors or
    # something lower level than both those and these?

    # TODO: local? Invoke wants ability to do that on its own though, which
    # would be distinct from Group. (May want to switch Group to use that,
    # though, whatever it ends up being?)

    # TODO: mirror Connection's close()?

    def get(self, *args, **kwargs):
        """
        Executes `.Connection.get` on all member `Connections <.Connection>`.

        :returns: a `.GroupResult`.

        .. versionadded:: 2.0
        """
        # TODO: probably best to suck it up & match actual get() sig?
        # TODO: actually implement on subclasses
        raise NotImplementedError


class SerialGroup(Group):
    """
    Subclass of `.Group` which executes in simple, serial fashion.

    .. versionadded:: 2.0
    """

    def run(self, *args, **kwargs):
        results = GroupResult()
        excepted = False
        for cxn in self:
            try:
                results[cxn] = cxn.run(*args, **kwargs)
            except Exception as e:
                results[cxn] = e
                excepted = True
        if excepted:
            raise GroupException(results)
        return results


def thread_worker(cxn, queue, args, kwargs):
    result = cxn.run(*args, **kwargs)
    # TODO: namedtuple or attrs object?
    queue.put((cxn, result))


class ThreadingGroup(Group):
    """
    Subclass of `.Group` which uses threading to execute concurrently.

    .. versionadded:: 2.0
    """

    def run(self, *args, **kwargs):
        results = GroupResult()
        queue = Queue()
        threads = []
        for cxn in self:
            my_kwargs = dict(cxn=cxn, queue=queue, args=args, kwargs=kwargs)
            thread = ExceptionHandlingThread(
                target=thread_worker, kwargs=my_kwargs
            )
            threads.append(thread)
        for thread in threads:
            thread.start()
        for thread in threads:
            # TODO: configurable join timeout
            # TODO: (in sudo's version) configurability around interactive
            # prompting resulting in an exception instead, as in v1
            thread.join()
        # Get non-exception results from queue
        while not queue.empty():
            # TODO: io-sleep? shouldn't matter if all threads are now joined
            cxn, result = queue.get(block=False)
            # TODO: outstanding musings about how exactly aggregate results
            # ought to ideally operate...heterogenous obj like this, multiple
            # objs, ??
            results[cxn] = result
        # Get exceptions from the threads themselves.
        # TODO: in a non-thread setup, this would differ, e.g.:
        # - a queue if using multiprocessing
        # - some other state-passing mechanism if using e.g. coroutines
        # - ???
        excepted = False
        for thread in threads:
            wrapper = thread.exception()
            if wrapper is not None:
                # Outer kwargs is Thread instantiation kwargs, inner is kwargs
                # passed to thread target/body.
                cxn = wrapper.kwargs["kwargs"]["cxn"]
                results[cxn] = wrapper.value
                excepted = True
        if excepted:
            raise GroupException(results)
        return results


class GroupResult(dict):
    """
    Collection of results and/or exceptions arising from `.Group` methods.

    Acts like a dict, but adds a couple convenience methods, to wit:

    - Keys are the individual `.Connection` objects from within the `.Group`.
    - Values are either return values / results from the called method (e.g.
      `.runners.Result` objects), *or* an exception object, if one prevented
      the method from returning.
    - Subclasses `dict`, so has all dict methods.
    - Has `.succeeded` and `.failed` attributes containing sub-dicts limited to
      just those key/value pairs that succeeded or encountered exceptions,
      respectively.

      - Of note, these attributes allow high level logic, e.g. ``if
        mygroup.run('command').failed`` and so forth.

    .. versionadded:: 2.0
    """

    def __init__(self, *args, **kwargs):
        super(dict, self).__init__(*args, **kwargs)
        self._successes = {}
        self._failures = {}

    def _bifurcate(self):
        # Short-circuit to avoid reprocessing every access.
        if self._successes or self._failures:
            return
        # TODO: if we ever expect .succeeded/.failed to be useful before a
        # GroupResult is fully initialized, this needs to become smarter.
        for key, value in self.items():
            if isinstance(value, BaseException):
                self._failures[key] = value
            else:
                self._successes[key] = value

    @property
    def succeeded(self):
        """
        A sub-dict containing only successful results.

        .. versionadded:: 2.0
        """
        self._bifurcate()
        return self._successes

    @property
    def failed(self):
        """
        A sub-dict containing only failed results.

        .. versionadded:: 2.0
        """
        self._bifurcate()
        return self._failures
<EOF>
<BOF>
from invoke import Call, Executor, Task

from . import Connection
from .exceptions import NothingToDo
from .util import debug


# TODO: come up w/ a better name heh
class FabExecutor(Executor):
    def expand_calls(self, calls, apply_hosts=True):
        # Generate new call list with per-host variants & Connections inserted
        ret = []
        # TODO: mesh well with Invoke list-type args helper (inv #132)
        hosts = []
        host_str = self.core[0].args.hosts.value
        if apply_hosts and host_str:
            hosts = host_str.split(",")
        for call in calls:
            if isinstance(call, Task):
                call = Call(task=call)
            # TODO: expand this to allow multiple types of execution plans,
            # pending outcome of invoke#461 (which, if flexible enough to
            # handle intersect of dependencies+parameterization, just becomes
            # 'honor that new feature of Invoke')
            # TODO: roles, other non-runtime host parameterizations, etc
            # Pre-tasks get added only once, not once per host.
            ret.extend(self.expand_calls(call.pre, apply_hosts=False))
            # Main task, per host
            for host in hosts:
                ret.append(self.parameterize(call, host))
            # Deal with lack of hosts arg (acts same as `inv` in that case)
            # TODO: no tests for this branch?
            if not hosts:
                ret.append(call)
            # Post-tasks added once, not once per host.
            ret.extend(self.expand_calls(call.post, apply_hosts=False))
        # Add remainder as anonymous task
        if self.core.remainder:
            # TODO: this will need to change once there are more options for
            # setting host lists besides "-H or 100% within-task"
            if not hosts:
                raise NothingToDo(
                    "Was told to run a command, but not given any hosts to run it on!"  # noqa
                )

            def anonymous(c):
                # TODO: how to make all our tests configure in_stream=False?
                c.run(self.core.remainder, in_stream=False)

            anon = Call(Task(body=anonymous))
            # TODO: see above TODOs about non-parameterized setups, roles etc
            # TODO: will likely need to refactor that logic some more so it can
            # be used both there and here.
            for host in hosts:
                ret.append(self.parameterize(anon, host))
        return ret

    def parameterize(self, call, host):
        """
        Parameterize a Call with its Context set to a per-host Config.
        """
        debug("Parameterizing {!r} for host {!r}".format(call, host))
        # Generate a custom ConnectionCall that knows how to yield a Connection
        # in its make_context(), specifically one to the host requested here.
        clone = call.clone(into=ConnectionCall)
        # TODO: using bag-of-attrs is mildly gross but whatever, I'll take it.
        clone.host = host
        return clone

    def dedupe(self, tasks):
        # Don't perform deduping, we will often have "duplicate" tasks w/
        # distinct host values/etc.
        # TODO: might want some deduplication later on though - falls under
        # "how to mesh parameterization with pre/post/etc deduping".
        return tasks


class ConnectionCall(Call):
    """
    Subclass of `invoke.tasks.Call` that generates `Connections <.Connection>`.
    """

    def make_context(self, config):
        return Connection(host=self.host, config=config)
<EOF>
<BOF>
"""
CLI entrypoint & parser configuration.

Builds on top of Invoke's core functionality for same.
"""

import getpass

from invoke import Argument, Collection, Program
from invoke import __version__ as invoke
from paramiko import __version__ as paramiko

from . import __version__ as fabric
from . import Config
from .executor import FabExecutor


class Fab(Program):
    def print_version(self):
        super(Fab, self).print_version()
        print("Paramiko {}".format(paramiko))
        print("Invoke {}".format(invoke))

    def core_args(self):
        core_args = super(Fab, self).core_args()
        my_args = [
            Argument(
                names=("S", "ssh-config"),
                help="Path to runtime SSH config file.",
            ),
            Argument(
                names=("H", "hosts"),
                help="Comma-separated host name(s) to execute tasks against.",
            ),
            Argument(
                names=("i", "identity"),
                kind=list,  # Same as OpenSSH, can give >1 key
                # TODO: automatically add hint about iterable-ness to Invoke
                # help display machinery?
                help="Path to runtime SSH identity (key) file. May be given multiple times.",  # noqa
            ),
            # TODO: worth having short flags for these prompt args?
            Argument(
                names=("prompt-for-login-password",),
                kind=bool,
                help="Request an upfront SSH-auth password prompt.",
            ),
            Argument(
                names=("prompt-for-passphrase",),
                kind=bool,
                help="Request an upfront SSH key passphrase prompt.",
            ),
        ]
        return core_args + my_args

    @property
    def _remainder_only(self):
        # No 'unparsed' (i.e. tokens intended for task contexts), and remainder
        # (text after a double-dash) implies a contextless/taskless remainder
        # execution of the style 'fab -H host -- command'.
        # NOTE: must ALSO check to ensure the double dash isn't being used for
        # tab completion machinery...
        return (
            not self.core.unparsed
            and self.core.remainder
            and not self.args.complete.value
        )

    def load_collection(self):
        # Stick in a dummy Collection if it looks like we were invoked w/o any
        # tasks, and with a remainder.
        # This isn't super ideal, but Invoke proper has no obvious "just run my
        # remainder" use case, so having it be capable of running w/o any task
        # module, makes no sense. But we want that capability for testing &
        # things like 'fab -H x,y,z -- mycommand'.
        if self._remainder_only:
            # TODO: hm we're probably not honoring project-specific configs in
            # this branch; is it worth having it assume CWD==project, since
            # that's often what users expect? Even tho no task collection to
            # honor the real "lives by task coll"?
            self.collection = Collection()
        else:
            super(Fab, self).load_collection()

    def no_tasks_given(self):
        # As above, neuter the usual "hey you didn't give me any tasks, let me
        # print help for you" behavior, if necessary.
        if not self._remainder_only:
            super(Fab, self).no_tasks_given()

    def create_config(self):
        # Create config, as parent does, but with lazy=True to avoid our own
        # SSH config autoload. (Otherwise, we can't correctly load _just_ the
        # runtime file if one's being given later.)
        self.config = self.config_class(lazy=True)
        # However, we don't really want the parent class' lazy behavior (which
        # skips loading system/global invoke-type conf files) so we manually do
        # that here to match upstream behavior.
        self.config.load_base_conf_files()
        # And merge again so that data is available.
        # TODO: really need to either A) stop giving fucks about calling
        # merge() "too many times", or B) make merge() itself determine whether
        # it needs to run and/or just merge stuff that's changed, so log spam
        # isn't as bad.
        self.config.merge()

    def update_config(self):
        # Note runtime SSH path, if given, and load SSH configurations.
        # NOTE: must do parent before our work, in case users want to disable
        # SSH config loading within a runtime-level conf file/flag.
        super(Fab, self).update_config(merge=False)
        self.config.set_runtime_ssh_path(self.args["ssh-config"].value)
        self.config.load_ssh_config()
        # Load -i identity file, if given, into connect_kwargs, at overrides
        # level.
        # TODO: this feels a little gross, but since the parent has already
        # called load_overrides, this is best we can do for now w/o losing
        # data. Still feels correct; just might be cleaner to have even more
        # Config API members around this sort of thing. Shrug.
        connect_kwargs = {}
        path = self.args["identity"].value
        if path:
            connect_kwargs["key_filename"] = path
        # Secrets prompts that want to happen at handoff time instead of
        # later/at user-time.
        # TODO: should this become part of Invoke proper in case other
        # downstreams have need of it? E.g. a prompt Argument 'type'? We're
        # already doing a similar thing there for sudo password...
        if self.args["prompt-for-login-password"].value:
            prompt = "Enter login password for use with SSH auth: "
            connect_kwargs["password"] = getpass.getpass(prompt)
        if self.args["prompt-for-passphrase"].value:
            prompt = "Enter passphrase for use unlocking SSH keys: "
            connect_kwargs["passphrase"] = getpass.getpass(prompt)
        self.config._overrides["connect_kwargs"] = connect_kwargs
        # Since we gave merge=False above, we must do it ourselves here. (Also
        # allows us to 'compile' our overrides manipulation.)
        self.config.merge()


# Mostly a concession to testing.
def make_program():
    return Fab(
        name="Fabric",
        version=fabric,
        executor_class=FabExecutor,
        config_class=Config,
    )


program = make_program()
<EOF>
<BOF>
"""
Tunnel and connection forwarding internals.

If you're looking for simple, end-user-focused connection forwarding, please
see `.Connection`, e.g. `.Connection.forward_local`.
"""

import errno
import select
import socket
import time
from threading import Event

from invoke.exceptions import ThreadException
from invoke.util import ExceptionHandlingThread


class TunnelManager(ExceptionHandlingThread):
    """
    Thread subclass for tunnelling connections over SSH between two endpoints.

    Specifically, one instance of this class is sufficient to sit around
    forwarding any number of individual connections made to one end of the
    tunnel or the other. If you need to forward connections between more than
    one set of ports, you'll end up instantiating multiple TunnelManagers.

    Wraps a `~paramiko.transport.Transport`, which should already be connected
    to the remote server.

    .. versionadded:: 2.0
    """

    def __init__(
        self,
        local_host,
        local_port,
        remote_host,
        remote_port,
        transport,
        finished,
    ):
        super(TunnelManager, self).__init__()
        self.local_address = (local_host, local_port)
        self.remote_address = (remote_host, remote_port)
        self.transport = transport
        self.finished = finished

    def _run(self):
        # Track each tunnel that gets opened during our lifetime
        tunnels = []

        # Set up OS-level listener socket on forwarded port
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # TODO: why do we want REUSEADDR exactly? and is it portable?
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        # NOTE: choosing to deal with nonblocking semantics and a fast loop,
        # versus an older approach which blocks & expects outer scope to cause
        # a socket exception by close()ing the socket.
        sock.setblocking(0)
        sock.bind(self.local_address)
        sock.listen(1)

        while not self.finished.is_set():
            # Main loop-wait: accept connections on the local listener
            # NOTE: EAGAIN means "you're nonblocking and nobody happened to
            # connect at this point in time"
            try:
                tun_sock, local_addr = sock.accept()
                # Set TCP_NODELAY to match OpenSSH's forwarding socket behavior
                tun_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
            except socket.error as e:
                if e.errno is errno.EAGAIN:
                    # TODO: make configurable
                    time.sleep(0.01)
                    continue
                raise

            # Set up direct-tcpip channel on server end
            # TODO: refactor w/ what's used for gateways
            channel = self.transport.open_channel(
                "direct-tcpip", self.remote_address, local_addr
            )

            # Set up 'worker' thread for this specific connection to our
            # tunnel, plus its dedicated signal event (which will appear as a
            # public attr, no need to track both independently).
            finished = Event()
            tunnel = Tunnel(channel=channel, sock=tun_sock, finished=finished)
            tunnel.start()
            tunnels.append(tunnel)

        exceptions = []
        # Propogate shutdown signal to all tunnels & wait for closure
        # TODO: would be nice to have some output or at least logging here,
        # especially for "sets up a handful of tunnels" use cases like
        # forwarding nontrivial HTTP traffic.
        for tunnel in tunnels:
            tunnel.finished.set()
            tunnel.join()
            wrapper = tunnel.exception()
            if wrapper:
                exceptions.append(wrapper)
        # Handle exceptions
        if exceptions:
            raise ThreadException(exceptions)

        # All we have left to close is our own sock.
        # TODO: use try/finally?
        sock.close()


class Tunnel(ExceptionHandlingThread):
    """
    Bidirectionally forward data between an SSH channel and local socket.

    .. versionadded:: 2.0
    """

    def __init__(self, channel, sock, finished):
        self.channel = channel
        self.sock = sock
        self.finished = finished
        self.socket_chunk_size = 1024
        self.channel_chunk_size = 1024
        super(Tunnel, self).__init__()

    def _run(self):
        try:
            empty_sock, empty_chan = None, None
            while not self.finished.is_set():
                r, w, x = select.select([self.sock, self.channel], [], [], 1)
                if self.sock in r:
                    empty_sock = self.read_and_write(
                        self.sock, self.channel, self.socket_chunk_size
                    )
                if self.channel in r:
                    empty_chan = self.read_and_write(
                        self.channel, self.sock, self.channel_chunk_size
                    )
                if empty_sock or empty_chan:
                    break
        finally:
            self.channel.close()
            self.sock.close()

    def read_and_write(self, reader, writer, chunk_size):
        """
        Read ``chunk_size`` from ``reader``, writing result to ``writer``.

        Returns ``None`` if successful, or ``True`` if the read was empty.

        .. versionadded:: 2.0
        """
        data = reader.recv(chunk_size)
        if len(data) == 0:
            return True
        writer.sendall(data)
<EOF>
<BOF>
"""
File transfer via SFTP and/or SCP.
"""

import os
import posixpath
import stat

from .util import debug  # TODO: actual logging! LOL

# TODO: figure out best way to direct folks seeking rsync, to patchwork's rsync
# call (which needs updating to use invoke.run() & fab 2 connection methods,
# but is otherwise suitable).
# UNLESS we want to try and shoehorn it into this module after all? Delegate
# any recursive get/put to it? Requires users to have rsync available of
# course.


class Transfer(object):
    """
    `.Connection`-wrapping class responsible for managing file upload/download.

    .. versionadded:: 2.0
    """

    # TODO: SFTP clear default, but how to do SCP? subclass? init kwarg?

    def __init__(self, connection):
        self.connection = connection

    @property
    def sftp(self):
        return self.connection.sftp()

    def is_remote_dir(self, path):
        try:
            return stat.S_ISDIR(self.sftp.stat(path).st_mode)
        except IOError:
            return False

    def get(self, remote, local=None, preserve_mode=True):
        """
        Download a file from the current connection to the local filesystem.

        :param str remote:
            Remote file to download.

            May be absolute, or relative to the remote working directory.

            .. note::
                Most SFTP servers set the remote working directory to the
                connecting user's home directory, and (unlike most shells) do
                *not* expand tildes (``~``).

                For example, instead of saying ``get("~/tmp/archive.tgz")``,
                say ``get("tmp/archive.tgz")``.

        :param local:
            Local path to store downloaded file in, or a file-like object.

            **If None or another 'falsey'/empty value is given** (the default),
            the remote file is downloaded to the current working directory (as
            seen by `os.getcwd`) using its remote filename.

            **If a string is given**, it should be a path to a local directory
            or file and is subject to similar behavior as that seen by common
            Unix utilities or OpenSSH's ``sftp`` or ``scp`` tools.

            For example, if the local path is a directory, the remote path's
            base filename will be added onto it (so ``get('foo/bar/file.txt',
            '/tmp/')`` would result in creation or overwriting of
            ``/tmp/file.txt``).

            .. note::
                When dealing with nonexistent file paths, normal Python file
                handling concerns come into play - for example, a ``local``
                path containing non-leaf directories which do not exist, will
                typically result in an `OSError`.

            **If a file-like object is given**, the contents of the remote file
            are simply written into it.

        :param bool preserve_mode:
            Whether to `os.chmod` the local file so it matches the remote
            file's mode (default: ``True``).

        :returns: A `.Result` object.

        .. versionadded:: 2.0
        """
        # TODO: how does this API change if we want to implement
        # remote-to-remote file transfer? (Is that even realistic?)
        # TODO: handle v1's string interpolation bits, especially the default
        # one, or at least think about how that would work re: split between
        # single and multiple server targets.
        # TODO: callback support
        # TODO: how best to allow changing the behavior/semantics of
        # remote/local (e.g. users might want 'safer' behavior that complains
        # instead of overwriting existing files) - this likely ties into the
        # "how to handle recursive/rsync" and "how to handle scp" questions

        # Massage remote path
        if not remote:
            raise ValueError("Remote path must not be empty!")
        orig_remote = remote
        remote = posixpath.join(
            self.sftp.getcwd() or self.sftp.normalize("."), remote
        )

        # Massage local path:
        # - handle file-ness
        # - if path, fill with remote name if empty, & make absolute
        orig_local = local
        is_file_like = hasattr(local, "write") and callable(local.write)
        if not local:
            local = posixpath.basename(remote)
        if not is_file_like:
            local = os.path.abspath(local)

        # Run Paramiko-level .get() (side-effects only. womp.)
        # TODO: push some of the path handling into Paramiko; it should be
        # responsible for dealing with path cleaning etc.
        # TODO: probably preserve warning message from v1 when overwriting
        # existing files. Use logging for that obviously.
        #
        # If local appears to be a file-like object, use sftp.getfo, not get
        if is_file_like:
            self.sftp.getfo(remotepath=remote, fl=local)
        else:
            self.sftp.get(remotepath=remote, localpath=local)
            # Set mode to same as remote end
            # TODO: Push this down into SFTPClient sometime (requires backwards
            # incompat release.)
            if preserve_mode:
                remote_mode = self.sftp.stat(remote).st_mode
                mode = stat.S_IMODE(remote_mode)
                os.chmod(local, mode)
        # Return something useful
        return Result(
            orig_remote=orig_remote,
            remote=remote,
            orig_local=orig_local,
            local=local,
            connection=self.connection,
        )

    def put(self, local, remote=None, preserve_mode=True):
        """
        Upload a file from the local filesystem to the current connection.

        :param local:
            Local path of file to upload, or a file-like object.

            **If a string is given**, it should be a path to a local (regular)
            file (not a directory).

            .. note::
                When dealing with nonexistent file paths, normal Python file
                handling concerns come into play - for example, trying to
                upload a nonexistent ``local`` path will typically result in an
                `OSError`.

            **If a file-like object is given**, its contents are written to the
            remote file path.

        :param str remote:
            Remote path to which the local file will be written.

            .. note::
                Most SFTP servers set the remote working directory to the
                connecting user's home directory, and (unlike most shells) do
                *not* expand tildes (``~``).

                For example, instead of saying ``put("archive.tgz",
                "~/tmp/")``, say ``put("archive.tgz", "tmp/")``.

                In addition, this means that 'falsey'/empty values (such as the
                default value, ``None``) are allowed and result in uploading to
                the remote home directory.

            .. note::
                When ``local`` is a file-like object, ``remote`` is required
                and must refer to a valid file path (not a directory).

        :param bool preserve_mode:
            Whether to ``chmod`` the remote file so it matches the local file's
            mode (default: ``True``).

        :returns: A `.Result` object.

        .. versionadded:: 2.0
        """
        if not local:
            raise ValueError("Local path must not be empty!")

        is_file_like = hasattr(local, "write") and callable(local.write)

        # Massage remote path
        orig_remote = remote
        if is_file_like:
            local_base = getattr(local, "name", None)
        else:
            local_base = os.path.basename(local)
        if not remote:
            if is_file_like:
                raise ValueError(
                    "Must give non-empty remote path when local is a file-like object!"  # noqa
                )
            else:
                remote = local_base
                debug("Massaged empty remote path into {!r}".format(remote))
        elif self.is_remote_dir(remote):
            # non-empty local_base implies a) text file path or b) FLO which
            # had a non-empty .name attribute. huzzah!
            if local_base:
                remote = posixpath.join(remote, local_base)
            else:
                if is_file_like:
                    raise ValueError(
                        "Can't put a file-like-object into a directory unless it has a non-empty .name attribute!"  # noqa
                    )
                else:
                    # TODO: can we ever really end up here? implies we want to
                    # reorganize all this logic so it has fewer potential holes
                    raise ValueError(
                        "Somehow got an empty local file basename ({!r}) when uploading to a directory ({!r})!".format(  # noqa
                            local_base, remote
                        )
                    )

        prejoined_remote = remote
        remote = posixpath.join(
            self.sftp.getcwd() or self.sftp.normalize("."), remote
        )
        if remote != prejoined_remote:
            msg = "Massaged relative remote path {!r} into {!r}"
            debug(msg.format(prejoined_remote, remote))

        # Massage local path
        orig_local = local
        if not is_file_like:
            local = os.path.abspath(local)
            if local != orig_local:
                debug(
                    "Massaged relative local path {!r} into {!r}".format(
                        orig_local, local
                    )
                )  # noqa

        # Run Paramiko-level .put() (side-effects only. womp.)
        # TODO: push some of the path handling into Paramiko; it should be
        # responsible for dealing with path cleaning etc.
        # TODO: probably preserve warning message from v1 when overwriting
        # existing files. Use logging for that obviously.
        #
        # If local appears to be a file-like object, use sftp.putfo, not put
        if is_file_like:
            msg = "Uploading file-like object {!r} to {!r}"
            debug(msg.format(local, remote))
            pointer = local.tell()
            try:
                local.seek(0)
                self.sftp.putfo(fl=local, remotepath=remote)
            finally:
                local.seek(pointer)
        else:
            debug("Uploading {!r} to {!r}".format(local, remote))
            self.sftp.put(localpath=local, remotepath=remote)
            # Set mode to same as local end
            # TODO: Push this down into SFTPClient sometime (requires backwards
            # incompat release.)
            if preserve_mode:
                local_mode = os.stat(local).st_mode
                mode = stat.S_IMODE(local_mode)
                self.sftp.chmod(remote, mode)
        # Return something useful
        return Result(
            orig_remote=orig_remote,
            remote=remote,
            orig_local=orig_local,
            local=local,
            connection=self.connection,
        )


class Result(object):
    """
    A container for information about the result of a file transfer.

    See individual attribute/method documentation below for details.

    .. note::
        Unlike similar classes such as `invoke.runners.Result` or
        `fabric.runners.Result` (which have a concept of "warn and return
        anyways on failure") this class has no useful truthiness behavior. If a
        file transfer fails, some exception will be raised, either an `OSError`
        or an error from within Paramiko.

    .. versionadded:: 2.0
    """

    # TODO: how does this differ from put vs get? field stating which? (feels
    # meh) distinct classes differing, for now, solely by name? (also meh)
    def __init__(self, local, orig_local, remote, orig_remote, connection):
        #: The local path the file was saved as, or the object it was saved
        #: into if a file-like object was given instead.
        #:
        #: If a string path, this value is massaged to be absolute; see
        #: `.orig_local` for the original argument value.
        self.local = local
        #: The original value given as the returning method's ``local``
        #: argument.
        self.orig_local = orig_local
        #: The remote path downloaded from. Massaged to be absolute; see
        #: `.orig_remote` for the original argument value.
        self.remote = remote
        #: The original argument value given as the returning method's
        #: ``remote`` argument.
        self.orig_remote = orig_remote
        #: The `.Connection` object this result was obtained from.
        self.connection = connection

    # TODO: ensure str/repr makes it easily differentiable from run() or
    # local() result objects (and vice versa).
<EOF>
