<BOF>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import codecs
import os
import re
import sys

import pkg_resources
from setuptools import find_packages
from setuptools import setup


def read(*parts):
    path = os.path.join(os.path.dirname(__file__), *parts)
    with codecs.open(path, encoding='utf-8') as fobj:
        return fobj.read()


def find_version(*file_paths):
    version_file = read(*file_paths)
    version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]",
                              version_file, re.M)
    if version_match:
        return version_match.group(1)
    raise RuntimeError("Unable to find version string.")


install_requires = [
    'cached-property >= 1.2.0, < 2',
    'docopt >= 0.6.1, < 0.7',
    'PyYAML >= 3.10, < 4',
    'requests >= 2.6.1, != 2.11.0, != 2.12.2, != 2.18.0, < 2.21',
    'texttable >= 0.9.0, < 0.10',
    'websocket-client >= 0.32.0, < 1.0',
    'docker >= 3.5.0, < 4.0',
    'dockerpty >= 0.4.1, < 0.5',
    'six >= 1.3.0, < 2',
    'jsonschema >= 2.5.1, < 3',
]


tests_require = [
    'pytest',
]


if sys.version_info[:2] < (3, 4):
    tests_require.append('mock >= 1.0.1')

extras_require = {
    ':python_version < "3.4"': ['enum34 >= 1.0.4, < 2'],
    ':python_version < "3.5"': ['backports.ssl_match_hostname >= 3.5'],
    ':python_version < "3.3"': ['ipaddress >= 1.0.16'],
    ':sys_platform == "win32"': ['colorama >= 0.4, < 0.5'],
    'socks': ['PySocks >= 1.5.6, != 1.5.7, < 2'],
}


try:
    if 'bdist_wheel' not in sys.argv:
        for key, value in extras_require.items():
            if key.startswith(':') and pkg_resources.evaluate_marker(key[1:]):
                install_requires.extend(value)
except Exception as e:
    print("Failed to compute platform dependencies: {}. ".format(e) +
          "All dependencies will be installed as a result.", file=sys.stderr)
    for key, value in extras_require.items():
        if key.startswith(':'):
            install_requires.extend(value)


setup(
    name='docker-compose',
    version=find_version("compose", "__init__.py"),
    description='Multi-container orchestration for Docker',
    url='https://www.docker.com/',
    author='Docker, Inc.',
    license='Apache License 2.0',
    packages=find_packages(exclude=['tests.*', 'tests']),
    include_package_data=True,
    test_suite='nose.collector',
    install_requires=install_requires,
    extras_require=extras_require,
    tests_require=tests_require,
    entry_points="""
    [console_scripts]
    docker-compose=compose.cli.main:main
    """,
    classifiers=[
        'Development Status :: 5 - Production/Stable',
        'Environment :: Console',
        'Intended Audience :: Developers',
        'License :: OSI Approved :: Apache Software License',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.6',
        'Programming Language :: Python :: 3.7',
    ],
)
<EOF>
<BOF>
#!/usr/bin/env python
"""
Migrate a Compose file from the V1 format in Compose 1.5 to the V2 format
supported by Compose 1.6+
"""
from __future__ import absolute_import
from __future__ import unicode_literals

import argparse
import logging
import sys

import ruamel.yaml

from compose.config.types import VolumeSpec


log = logging.getLogger('migrate')


def migrate(content):
    data = ruamel.yaml.load(content, ruamel.yaml.RoundTripLoader)

    service_names = data.keys()

    for name, service in data.items():
        warn_for_links(name, service)
        warn_for_external_links(name, service)
        rewrite_net(service, service_names)
        rewrite_build(service)
        rewrite_logging(service)
        rewrite_volumes_from(service, service_names)

    services = {name: data.pop(name) for name in data.keys()}

    data['version'] = "2"
    data['services'] = services
    create_volumes_section(data)

    return data


def warn_for_links(name, service):
    links = service.get('links')
    if links:
        example_service = links[0].partition(':')[0]
        log.warn(
            "Service {name} has links, which no longer create environment "
            "variables such as {example_service_upper}_PORT. "
            "If you are using those in your application code, you should "
            "instead connect directly to the hostname, e.g. "
            "'{example_service}'."
            .format(name=name, example_service=example_service,
                    example_service_upper=example_service.upper()))


def warn_for_external_links(name, service):
    external_links = service.get('external_links')
    if external_links:
        log.warn(
            "Service {name} has external_links: {ext}, which now work "
            "slightly differently. In particular, two containers must be "
            "connected to at least one network in common in order to "
            "communicate, even if explicitly linked together.\n\n"
            "Either connect the external container to your app's default "
            "network, or connect both the external container and your "
            "service's containers to a pre-existing network. See "
            "https://docs.docker.com/compose/networking/ "
            "for more on how to do this."
            .format(name=name, ext=external_links))


def rewrite_net(service, service_names):
    if 'net' in service:
        network_mode = service.pop('net')

        # "container:<service name>" is now "service:<service name>"
        if network_mode.startswith('container:'):
            name = network_mode.partition(':')[2]
            if name in service_names:
                network_mode = 'service:{}'.format(name)

        service['network_mode'] = network_mode


def rewrite_build(service):
    if 'dockerfile' in service:
        service['build'] = {
            'context': service.pop('build'),
            'dockerfile': service.pop('dockerfile'),
        }


def rewrite_logging(service):
    if 'log_driver' in service:
        service['logging'] = {'driver': service.pop('log_driver')}
        if 'log_opt' in service:
            service['logging']['options'] = service.pop('log_opt')


def rewrite_volumes_from(service, service_names):
    for idx, volume_from in enumerate(service.get('volumes_from', [])):
        if volume_from.split(':', 1)[0] not in service_names:
            service['volumes_from'][idx] = 'container:%s' % volume_from


def create_volumes_section(data):
    named_volumes = get_named_volumes(data['services'])
    if named_volumes:
        log.warn(
            "Named volumes ({names}) must be explicitly declared. Creating a "
            "'volumes' section with declarations.\n\n"
            "For backwards-compatibility, they've been declared as external. "
            "If you don't mind the volume names being prefixed with the "
            "project name, you can remove the 'external' option from each one."
            .format(names=', '.join(list(named_volumes))))

        data['volumes'] = named_volumes


def get_named_volumes(services):
    volume_specs = [
        VolumeSpec.parse(volume)
        for service in services.values()
        for volume in service.get('volumes', [])
    ]
    names = {
        spec.external
        for spec in volume_specs
        if spec.is_named_volume
    }
    return {name: {'external': True} for name in names}


def write(stream, new_format, indent, width):
    ruamel.yaml.dump(
        new_format,
        stream,
        Dumper=ruamel.yaml.RoundTripDumper,
        indent=indent,
        width=width)


def parse_opts(args):
    parser = argparse.ArgumentParser()
    parser.add_argument("filename", help="Compose file filename.")
    parser.add_argument("-i", "--in-place", action='store_true')
    parser.add_argument(
        "--indent", type=int, default=2,
        help="Number of spaces used to indent the output yaml.")
    parser.add_argument(
        "--width", type=int, default=80,
        help="Number of spaces used as the output width.")
    return parser.parse_args()


def main(args):
    logging.basicConfig(format='\033[33m%(levelname)s:\033[37m %(message)s\033[0m\n')

    opts = parse_opts(args)

    with open(opts.filename, 'r') as fh:
        new_format = migrate(fh.read())

    if opts.in_place:
        output = open(opts.filename, 'w')
    else:
        output = sys.stdout
    write(output, new_format, opts.indent, opts.width)


if __name__ == "__main__":
    main(sys.argv)
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import codecs
import hashlib
import json
import json.decoder
import logging
import ntpath
import random

import six
from docker.errors import DockerException
from docker.utils import parse_bytes as sdk_parse_bytes

from .errors import StreamParseError
from .timeparse import MULTIPLIERS
from .timeparse import timeparse


json_decoder = json.JSONDecoder()
log = logging.getLogger(__name__)


def get_output_stream(stream):
    if six.PY3:
        return stream
    return codecs.getwriter('utf-8')(stream)


def stream_as_text(stream):
    """Given a stream of bytes or text, if any of the items in the stream
    are bytes convert them to text.

    This function can be removed once docker-py returns text streams instead
    of byte streams.
    """
    for data in stream:
        if not isinstance(data, six.text_type):
            data = data.decode('utf-8', 'replace')
        yield data


def line_splitter(buffer, separator=u'\n'):
    index = buffer.find(six.text_type(separator))
    if index == -1:
        return None
    return buffer[:index + 1], buffer[index + 1:]


def split_buffer(stream, splitter=None, decoder=lambda a: a):
    """Given a generator which yields strings and a splitter function,
    joins all input, splits on the separator and yields each chunk.

    Unlike string.split(), each chunk includes the trailing
    separator, except for the last one if none was found on the end
    of the input.
    """
    splitter = splitter or line_splitter
    buffered = six.text_type('')

    for data in stream_as_text(stream):
        buffered += data
        while True:
            buffer_split = splitter(buffered)
            if buffer_split is None:
                break

            item, buffered = buffer_split
            yield item

    if buffered:
        try:
            yield decoder(buffered)
        except Exception as e:
            log.error(
                'Compose tried decoding the following data chunk, but failed:'
                '\n%s' % repr(buffered)
            )
            raise StreamParseError(e)


def json_splitter(buffer):
    """Attempt to parse a json object from a buffer. If there is at least one
    object, return it and the rest of the buffer, otherwise return None.
    """
    buffer = buffer.strip()
    try:
        obj, index = json_decoder.raw_decode(buffer)
        rest = buffer[json.decoder.WHITESPACE.match(buffer, index).end():]
        return obj, rest
    except ValueError:
        return None


def json_stream(stream):
    """Given a stream of text, return a stream of json objects.
    This handles streams which are inconsistently buffered (some entries may
    be newline delimited, and others are not).
    """
    return split_buffer(stream, json_splitter, json_decoder.decode)


def json_hash(obj):
    dump = json.dumps(obj, sort_keys=True, separators=(',', ':'), default=lambda x: x.repr())
    h = hashlib.sha256()
    h.update(dump.encode('utf8'))
    return h.hexdigest()


def microseconds_from_time_nano(time_nano):
    return int(time_nano % 1000000000 / 1000)


def nanoseconds_from_time_seconds(time_seconds):
    return int(time_seconds / MULTIPLIERS['nano'])


def parse_seconds_float(value):
    return timeparse(value or '')


def parse_nanoseconds_int(value):
    parsed = timeparse(value or '')
    if parsed is None:
        return None
    return nanoseconds_from_time_seconds(parsed)


def build_string_dict(source_dict):
    return dict((k, str(v if v is not None else '')) for k, v in source_dict.items())


def splitdrive(path):
    if len(path) == 0:
        return ('', '')
    if path[0] in ['.', '\\', '/', '~']:
        return ('', path)
    return ntpath.splitdrive(path)


def parse_bytes(n):
    try:
        return sdk_parse_bytes(n)
    except DockerException:
        return None


def unquote_path(s):
    if not s:
        return s
    if s[0] == '"' and s[-1] == '"':
        return s[1:-1]
    return s


def generate_random_id():
    while True:
        val = hex(random.getrandbits(32 * 8))[2:-1]
        try:
            int(truncate_id(val))
            continue
        except ValueError:
            return val


def truncate_id(value):
    if ':' in value:
        value = value[value.index(':') + 1:]
    if len(value) > 12:
        return value[:12]
    return value


def unique_everseen(iterable, key=lambda x: x):
    "List unique elements, preserving order. Remember all elements ever seen."
    seen = set()
    for element in iterable:
        unique_key = key(element)
        if unique_key not in seen:
            seen.add(unique_key)
            yield element


def truncate_string(s, max_chars=35):
    if len(s) > max_chars:
        return s[:max_chars - 2] + '...'
    return s
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from distutils.version import LooseVersion


class ComposeVersion(LooseVersion):
    """ A hashable version object """
    def __hash__(self):
        return hash(self.vstring)
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import datetime
import logging
import operator
import re
from functools import reduce

import enum
import six
from docker.errors import APIError

from . import parallel
from .config import ConfigurationError
from .config.config import V1
from .config.sort_services import get_container_name_from_network_mode
from .config.sort_services import get_service_name_from_network_mode
from .const import IMAGE_EVENTS
from .const import LABEL_ONE_OFF
from .const import LABEL_PROJECT
from .const import LABEL_SERVICE
from .container import Container
from .network import build_networks
from .network import get_networks
from .network import ProjectNetworks
from .service import BuildAction
from .service import ContainerNetworkMode
from .service import ContainerPidMode
from .service import ConvergenceStrategy
from .service import NetworkMode
from .service import PidMode
from .service import Service
from .service import ServiceNetworkMode
from .service import ServicePidMode
from .utils import microseconds_from_time_nano
from .utils import truncate_string
from .volume import ProjectVolumes


log = logging.getLogger(__name__)


@enum.unique
class OneOffFilter(enum.Enum):
    include = 0
    exclude = 1
    only = 2

    @classmethod
    def update_labels(cls, value, labels):
        if value == cls.only:
            labels.append('{0}={1}'.format(LABEL_ONE_OFF, "True"))
        elif value == cls.exclude:
            labels.append('{0}={1}'.format(LABEL_ONE_OFF, "False"))
        elif value == cls.include:
            pass
        else:
            raise ValueError("Invalid value for one_off: {}".format(repr(value)))


class Project(object):
    """
    A collection of services.
    """
    def __init__(self, name, services, client, networks=None, volumes=None, config_version=None):
        self.name = name
        self.services = services
        self.client = client
        self.volumes = volumes or ProjectVolumes({})
        self.networks = networks or ProjectNetworks({}, False)
        self.config_version = config_version

    def labels(self, one_off=OneOffFilter.exclude, legacy=False):
        name = self.name
        if legacy:
            name = re.sub(r'[_-]', '', name)
        labels = ['{0}={1}'.format(LABEL_PROJECT, name)]

        OneOffFilter.update_labels(one_off, labels)
        return labels

    @classmethod
    def from_config(cls, name, config_data, client, default_platform=None):
        """
        Construct a Project from a config.Config object.
        """
        use_networking = (config_data.version and config_data.version != V1)
        networks = build_networks(name, config_data, client)
        project_networks = ProjectNetworks.from_services(
            config_data.services,
            networks,
            use_networking)
        volumes = ProjectVolumes.from_config(name, config_data, client)
        project = cls(name, [], client, project_networks, volumes, config_data.version)

        for service_dict in config_data.services:
            service_dict = dict(service_dict)
            if use_networking:
                service_networks = get_networks(service_dict, networks)
            else:
                service_networks = {}

            service_dict.pop('networks', None)
            links = project.get_links(service_dict)
            network_mode = project.get_network_mode(
                service_dict, list(service_networks.keys())
            )
            pid_mode = project.get_pid_mode(service_dict)
            volumes_from = get_volumes_from(project, service_dict)

            if config_data.version != V1:
                service_dict['volumes'] = [
                    volumes.namespace_spec(volume_spec)
                    for volume_spec in service_dict.get('volumes', [])
                ]

            secrets = get_secrets(
                service_dict['name'],
                service_dict.pop('secrets', None) or [],
                config_data.secrets)

            project.services.append(
                Service(
                    service_dict.pop('name'),
                    client=client,
                    project=name,
                    use_networking=use_networking,
                    networks=service_networks,
                    links=links,
                    network_mode=network_mode,
                    volumes_from=volumes_from,
                    secrets=secrets,
                    pid_mode=pid_mode,
                    platform=service_dict.pop('platform', None),
                    default_platform=default_platform,
                    **service_dict)
            )

        return project

    @property
    def service_names(self):
        return [service.name for service in self.services]

    def get_service(self, name):
        """
        Retrieve a service by name. Raises NoSuchService
        if the named service does not exist.
        """
        for service in self.services:
            if service.name == name:
                return service

        raise NoSuchService(name)

    def validate_service_names(self, service_names):
        """
        Validate that the given list of service names only contains valid
        services. Raises NoSuchService if one of the names is invalid.
        """
        valid_names = self.service_names
        for name in service_names:
            if name not in valid_names:
                raise NoSuchService(name)

    def get_services(self, service_names=None, include_deps=False):
        """
        Returns a list of this project's services filtered
        by the provided list of names, or all services if service_names is None
        or [].

        If include_deps is specified, returns a list including the dependencies for
        service_names, in order of dependency.

        Preserves the original order of self.services where possible,
        reordering as needed to resolve dependencies.

        Raises NoSuchService if any of the named services do not exist.
        """
        if service_names is None or len(service_names) == 0:
            service_names = self.service_names

        unsorted = [self.get_service(name) for name in service_names]
        services = [s for s in self.services if s in unsorted]

        if include_deps:
            services = reduce(self._inject_deps, services, [])

        uniques = []
        [uniques.append(s) for s in services if s not in uniques]

        return uniques

    def get_services_without_duplicate(self, service_names=None, include_deps=False):
        services = self.get_services(service_names, include_deps)
        for service in services:
            service.remove_duplicate_containers()
        return services

    def get_links(self, service_dict):
        links = []
        if 'links' in service_dict:
            for link in service_dict.get('links', []):
                if ':' in link:
                    service_name, link_name = link.split(':', 1)
                else:
                    service_name, link_name = link, None
                try:
                    links.append((self.get_service(service_name), link_name))
                except NoSuchService:
                    raise ConfigurationError(
                        'Service "%s" has a link to service "%s" which does not '
                        'exist.' % (service_dict['name'], service_name))
            del service_dict['links']
        return links

    def get_network_mode(self, service_dict, networks):
        network_mode = service_dict.pop('network_mode', None)
        if not network_mode:
            if self.networks.use_networking:
                return NetworkMode(networks[0]) if networks else NetworkMode('none')
            return NetworkMode(None)

        service_name = get_service_name_from_network_mode(network_mode)
        if service_name:
            return ServiceNetworkMode(self.get_service(service_name))

        container_name = get_container_name_from_network_mode(network_mode)
        if container_name:
            try:
                return ContainerNetworkMode(Container.from_id(self.client, container_name))
            except APIError:
                raise ConfigurationError(
                    "Service '{name}' uses the network stack of container '{dep}' which "
                    "does not exist.".format(name=service_dict['name'], dep=container_name))

        return NetworkMode(network_mode)

    def get_pid_mode(self, service_dict):
        pid_mode = service_dict.pop('pid', None)
        if not pid_mode:
            return PidMode(None)

        service_name = get_service_name_from_network_mode(pid_mode)
        if service_name:
            return ServicePidMode(self.get_service(service_name))

        container_name = get_container_name_from_network_mode(pid_mode)
        if container_name:
            try:
                return ContainerPidMode(Container.from_id(self.client, container_name))
            except APIError:
                raise ConfigurationError(
                    "Service '{name}' uses the PID namespace of container '{dep}' which "
                    "does not exist.".format(name=service_dict['name'], dep=container_name)
                )

        return PidMode(pid_mode)

    def start(self, service_names=None, **options):
        containers = []

        def start_service(service):
            service_containers = service.start(quiet=True, **options)
            containers.extend(service_containers)

        services = self.get_services(service_names)

        def get_deps(service):
            return {
                (self.get_service(dep), config)
                for dep, config in service.get_dependency_configs().items()
            }

        parallel.parallel_execute(
            services,
            start_service,
            operator.attrgetter('name'),
            'Starting',
            get_deps,
        )

        return containers

    def stop(self, service_names=None, one_off=OneOffFilter.exclude, **options):
        containers = self.containers(service_names, one_off=one_off)

        def get_deps(container):
            # actually returning inversed dependencies
            return {(other, None) for other in containers
                    if container.service in
                    self.get_service(other.service).get_dependency_names()}

        parallel.parallel_execute(
            containers,
            self.build_container_operation_with_timeout_func('stop', options),
            operator.attrgetter('name'),
            'Stopping',
            get_deps,
        )

    def pause(self, service_names=None, **options):
        containers = self.containers(service_names)
        parallel.parallel_pause(reversed(containers), options)
        return containers

    def unpause(self, service_names=None, **options):
        containers = self.containers(service_names)
        parallel.parallel_unpause(containers, options)
        return containers

    def kill(self, service_names=None, **options):
        parallel.parallel_kill(self.containers(service_names), options)

    def remove_stopped(self, service_names=None, one_off=OneOffFilter.exclude, **options):
        parallel.parallel_remove(self.containers(
            service_names, stopped=True, one_off=one_off
        ), options)

    def down(
            self,
            remove_image_type,
            include_volumes,
            remove_orphans=False,
            timeout=None,
            ignore_orphans=False):
        self.stop(one_off=OneOffFilter.include, timeout=timeout)
        if not ignore_orphans:
            self.find_orphan_containers(remove_orphans)
        self.remove_stopped(v=include_volumes, one_off=OneOffFilter.include)

        self.networks.remove()

        if include_volumes:
            self.volumes.remove()

        self.remove_images(remove_image_type)

    def remove_images(self, remove_image_type):
        for service in self.get_services():
            service.remove_image(remove_image_type)

    def restart(self, service_names=None, **options):
        containers = self.containers(service_names, stopped=True)

        parallel.parallel_execute(
            containers,
            self.build_container_operation_with_timeout_func('restart', options),
            operator.attrgetter('name'),
            'Restarting',
        )
        return containers

    def build(self, service_names=None, no_cache=False, pull=False, force_rm=False, memory=None,
              build_args=None, gzip=False, parallel_build=False):

        services = []
        for service in self.get_services(service_names):
            if service.can_be_built():
                services.append(service)
            else:
                log.info('%s uses an image, skipping' % service.name)

        def build_service(service):
            service.build(no_cache, pull, force_rm, memory, build_args, gzip)

        if parallel_build:
            _, errors = parallel.parallel_execute(
                services,
                build_service,
                operator.attrgetter('name'),
                'Building',
                limit=5,
            )
            if len(errors):
                combined_errors = '\n'.join([
                    e.decode('utf-8') if isinstance(e, six.binary_type) else e for e in errors.values()
                ])
                raise ProjectError(combined_errors)

        else:
            for service in services:
                build_service(service)

    def create(
        self,
        service_names=None,
        strategy=ConvergenceStrategy.changed,
        do_build=BuildAction.none,
    ):
        services = self.get_services_without_duplicate(service_names, include_deps=True)

        for svc in services:
            svc.ensure_image_exists(do_build=do_build)
        plans = self._get_convergence_plans(services, strategy)

        for service in services:
            service.execute_convergence_plan(
                plans[service.name],
                detached=True,
                start=False)

    def events(self, service_names=None):
        def build_container_event(event, container):
            time = datetime.datetime.fromtimestamp(event['time'])
            time = time.replace(
                microsecond=microseconds_from_time_nano(event['timeNano']))
            return {
                'time': time,
                'type': 'container',
                'action': event['status'],
                'id': container.id,
                'service': container.service,
                'attributes': {
                    'name': container.name,
                    'image': event['from'],
                },
                'container': container,
            }

        service_names = set(service_names or self.service_names)
        for event in self.client.events(
            filters={'label': self.labels()},
            decode=True
        ):
            # The first part of this condition is a guard against some events
            # broadcasted by swarm that don't have a status field.
            # See https://github.com/docker/compose/issues/3316
            if 'status' not in event or event['status'] in IMAGE_EVENTS:
                # We don't receive any image events because labels aren't applied
                # to images
                continue

            # TODO: get labels from the API v1.22 , see github issue 2618
            try:
                # this can fail if the container has been removed
                container = Container.from_id(self.client, event['id'])
            except APIError:
                continue
            if container.service not in service_names:
                continue
            yield build_container_event(event, container)

    def up(self,
           service_names=None,
           start_deps=True,
           strategy=ConvergenceStrategy.changed,
           do_build=BuildAction.none,
           timeout=None,
           detached=False,
           remove_orphans=False,
           ignore_orphans=False,
           scale_override=None,
           rescale=True,
           start=True,
           always_recreate_deps=False,
           reset_container_image=False,
           renew_anonymous_volumes=False,
           silent=False,
           ):

        self.initialize()
        if not ignore_orphans:
            self.find_orphan_containers(remove_orphans)

        if scale_override is None:
            scale_override = {}

        services = self.get_services_without_duplicate(
            service_names,
            include_deps=start_deps)

        for svc in services:
            svc.ensure_image_exists(do_build=do_build, silent=silent)
        plans = self._get_convergence_plans(
            services, strategy, always_recreate_deps=always_recreate_deps)

        def do(service):

            return service.execute_convergence_plan(
                plans[service.name],
                timeout=timeout,
                detached=detached,
                scale_override=scale_override.get(service.name),
                rescale=rescale,
                start=start,
                reset_container_image=reset_container_image,
                renew_anonymous_volumes=renew_anonymous_volumes,
            )

        def get_deps(service):
            return {
                (self.get_service(dep), config)
                for dep, config in service.get_dependency_configs().items()
            }

        results, errors = parallel.parallel_execute(
            services,
            do,
            operator.attrgetter('name'),
            None,
            get_deps,
        )
        if errors:
            raise ProjectError(
                'Encountered errors while bringing up the project.'
            )

        return [
            container
            for svc_containers in results
            if svc_containers is not None
            for container in svc_containers
        ]

    def initialize(self):
        self.networks.initialize()
        self.volumes.initialize()

    def _get_convergence_plans(self, services, strategy, always_recreate_deps=False):
        plans = {}

        for service in services:
            updated_dependencies = [
                name
                for name in service.get_dependency_names()
                if name in plans and
                plans[name].action in ('recreate', 'create')
            ]

            if updated_dependencies and strategy.allows_recreate:
                log.debug('%s has upstream changes (%s)',
                          service.name,
                          ", ".join(updated_dependencies))
                containers_stopped = any(
                    service.containers(stopped=True, filters={'status': ['created', 'exited']}))
                has_links = any(c.get('HostConfig.Links') for c in service.containers())
                if always_recreate_deps or containers_stopped or not has_links:
                    plan = service.convergence_plan(ConvergenceStrategy.always)
                else:
                    plan = service.convergence_plan(strategy)
            else:
                plan = service.convergence_plan(strategy)

            plans[service.name] = plan

        return plans

    def pull(self, service_names=None, ignore_pull_failures=False, parallel_pull=False, silent=False,
             include_deps=False):
        services = self.get_services(service_names, include_deps)
        msg = not silent and 'Pulling' or None

        if parallel_pull:
            def pull_service(service):
                strm = service.pull(ignore_pull_failures, True, stream=True)
                if strm is None:  # Attempting to pull service with no `image` key is a no-op
                    return

                writer = parallel.get_stream_writer()

                for event in strm:
                    if 'status' not in event:
                        continue
                    status = event['status'].lower()
                    if 'progressDetail' in event:
                        detail = event['progressDetail']
                        if 'current' in detail and 'total' in detail:
                            percentage = float(detail['current']) / float(detail['total'])
                            status = '{} ({:.1%})'.format(status, percentage)

                    writer.write(
                        msg, service.name, truncate_string(status), lambda s: s
                    )

            _, errors = parallel.parallel_execute(
                services,
                pull_service,
                operator.attrgetter('name'),
                msg,
                limit=5,
            )
            if len(errors):
                combined_errors = '\n'.join([
                    e.decode('utf-8') if isinstance(e, six.binary_type) else e for e in errors.values()
                ])
                raise ProjectError(combined_errors)

        else:
            for service in services:
                service.pull(ignore_pull_failures, silent=silent)

    def push(self, service_names=None, ignore_push_failures=False):
        for service in self.get_services(service_names, include_deps=False):
            service.push(ignore_push_failures)

    def _labeled_containers(self, stopped=False, one_off=OneOffFilter.exclude):
        ctnrs = list(filter(None, [
            Container.from_ps(self.client, container)
            for container in self.client.containers(
                all=stopped,
                filters={'label': self.labels(one_off=one_off)})])
        )
        if ctnrs:
            return ctnrs

        return list(filter(lambda c: c.has_legacy_proj_name(self.name), filter(None, [
            Container.from_ps(self.client, container)
            for container in self.client.containers(
                all=stopped,
                filters={'label': self.labels(one_off=one_off, legacy=True)})])
        ))

    def containers(self, service_names=None, stopped=False, one_off=OneOffFilter.exclude):
        if service_names:
            self.validate_service_names(service_names)
        else:
            service_names = self.service_names

        containers = self._labeled_containers(stopped, one_off)

        def matches_service_names(container):
            return container.labels.get(LABEL_SERVICE) in service_names

        return [c for c in containers if matches_service_names(c)]

    def find_orphan_containers(self, remove_orphans):
        def _find():
            containers = self._labeled_containers()
            for ctnr in containers:
                service_name = ctnr.labels.get(LABEL_SERVICE)
                if service_name not in self.service_names:
                    yield ctnr
        orphans = list(_find())
        if not orphans:
            return
        if remove_orphans:
            for ctnr in orphans:
                log.info('Removing orphan container "{0}"'.format(ctnr.name))
                ctnr.kill()
                ctnr.remove(force=True)
        else:
            log.warning(
                'Found orphan containers ({0}) for this project. If '
                'you removed or renamed this service in your compose '
                'file, you can run this command with the '
                '--remove-orphans flag to clean it up.'.format(
                    ', '.join(["{}".format(ctnr.name) for ctnr in orphans])
                )
            )

    def _inject_deps(self, acc, service):
        dep_names = service.get_dependency_names()

        if len(dep_names) > 0:
            dep_services = self.get_services(
                service_names=list(set(dep_names)),
                include_deps=True
            )
        else:
            dep_services = []

        dep_services.append(service)
        return acc + dep_services

    def build_container_operation_with_timeout_func(self, operation, options):
        def container_operation_with_timeout(container):
            if options.get('timeout') is None:
                service = self.get_service(container.service)
                options['timeout'] = service.stop_timeout(None)
            return getattr(container, operation)(**options)
        return container_operation_with_timeout


def get_volumes_from(project, service_dict):
    volumes_from = service_dict.pop('volumes_from', None)
    if not volumes_from:
        return []

    def build_volume_from(spec):
        if spec.type == 'service':
            try:
                return spec._replace(source=project.get_service(spec.source))
            except NoSuchService:
                pass

        if spec.type == 'container':
            try:
                container = Container.from_id(project.client, spec.source)
                return spec._replace(source=container)
            except APIError:
                pass

        raise ConfigurationError(
            "Service \"{}\" mounts volumes from \"{}\", which is not the name "
            "of a service or container.".format(
                service_dict['name'],
                spec.source))

    return [build_volume_from(vf) for vf in volumes_from]


def get_secrets(service, service_secrets, secret_defs):
    secrets = []

    for secret in service_secrets:
        secret_def = secret_defs.get(secret.source)
        if not secret_def:
            raise ConfigurationError(
                "Service \"{service}\" uses an undefined secret \"{secret}\" "
                .format(service=service, secret=secret.source))

        if secret_def.get('external'):
            log.warn("Service \"{service}\" uses secret \"{secret}\" which is external. "
                     "External secrets are not available to containers created by "
                     "docker-compose.".format(service=service, secret=secret.source))
            continue

        if secret.uid or secret.gid or secret.mode:
            log.warn(
                "Service \"{service}\" uses secret \"{secret}\" with uid, "
                "gid, or mode. These fields are not supported by this "
                "implementation of the Compose file".format(
                    service=service, secret=secret.source
                )
            )

        secrets.append({'secret': secret, 'file': secret_def.get('file')})

    return secrets


class NoSuchService(Exception):
    def __init__(self, name):
        if isinstance(name, six.binary_type):
            name = name.decode('utf-8')
        self.name = name
        self.msg = "No such service: %s" % self.name

    def __str__(self):
        return self.msg


class ProjectError(Exception):
    def __init__(self, msg):
        self.msg = msg
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import re
from collections import OrderedDict

from docker.errors import NotFound
from docker.types import IPAMConfig
from docker.types import IPAMPool
from docker.utils import version_gte
from docker.utils import version_lt

from . import __version__
from .config import ConfigurationError
from .const import LABEL_NETWORK
from .const import LABEL_PROJECT
from .const import LABEL_VERSION


log = logging.getLogger(__name__)

OPTS_EXCEPTIONS = [
    'com.docker.network.driver.overlay.vxlanid_list',
    'com.docker.network.windowsshim.hnsid',
    'com.docker.network.windowsshim.networkname'
]


class Network(object):
    def __init__(self, client, project, name, driver=None, driver_opts=None,
                 ipam=None, external=False, internal=False, enable_ipv6=False,
                 labels=None, custom_name=False):
        self.client = client
        self.project = project
        self.name = name
        self.driver = driver
        self.driver_opts = driver_opts
        self.ipam = create_ipam_config_from_dict(ipam)
        self.external = external
        self.internal = internal
        self.enable_ipv6 = enable_ipv6
        self.labels = labels
        self.custom_name = custom_name
        self.legacy = None

    def ensure(self):
        if self.external:
            if self.driver == 'overlay':
                # Swarm nodes do not register overlay networks that were
                # created on a different node unless they're in use.
                # See docker/compose#4399
                return
            try:
                self.inspect()
                log.debug(
                    'Network {0} declared as external. No new '
                    'network will be created.'.format(self.name)
                )
            except NotFound:
                raise ConfigurationError(
                    'Network {name} declared as external, but could'
                    ' not be found. Please create the network manually'
                    ' using `{command} {name}` and try again.'.format(
                        name=self.full_name,
                        command='docker network create'
                    )
                )
            return

        self._set_legacy_flag()
        try:
            data = self.inspect(legacy=self.legacy)
            check_remote_network_config(data, self)
        except NotFound:
            driver_name = 'the default driver'
            if self.driver:
                driver_name = 'driver "{}"'.format(self.driver)

            log.info(
                'Creating network "{}" with {}'.format(self.full_name, driver_name)
            )

            self.client.create_network(
                name=self.full_name,
                driver=self.driver,
                options=self.driver_opts,
                ipam=self.ipam,
                internal=self.internal,
                enable_ipv6=self.enable_ipv6,
                labels=self._labels,
                attachable=version_gte(self.client._version, '1.24') or None,
                check_duplicate=True,
            )

    def remove(self):
        if self.external:
            log.info("Network %s is external, skipping", self.true_name)
            return

        log.info("Removing network {}".format(self.true_name))
        self.client.remove_network(self.true_name)

    def inspect(self, legacy=False):
        if legacy:
            return self.client.inspect_network(self.legacy_full_name)
        return self.client.inspect_network(self.full_name)

    @property
    def legacy_full_name(self):
        if self.custom_name:
            return self.name
        return '{0}_{1}'.format(
            re.sub(r'[_-]', '', self.project), self.name
        )

    @property
    def full_name(self):
        if self.custom_name:
            return self.name
        return '{0}_{1}'.format(self.project, self.name)

    @property
    def true_name(self):
        self._set_legacy_flag()
        if self.legacy:
            return self.legacy_full_name
        return self.full_name

    @property
    def _labels(self):
        if version_lt(self.client._version, '1.23'):
            return None
        labels = self.labels.copy() if self.labels else {}
        labels.update({
            LABEL_PROJECT: self.project,
            LABEL_NETWORK: self.name,
            LABEL_VERSION: __version__,
        })
        return labels

    def _set_legacy_flag(self):
        if self.legacy is not None:
            return
        try:
            data = self.inspect(legacy=True)
            self.legacy = data is not None
        except NotFound:
            self.legacy = False


def create_ipam_config_from_dict(ipam_dict):
    if not ipam_dict:
        return None

    return IPAMConfig(
        driver=ipam_dict.get('driver') or 'default',
        pool_configs=[
            IPAMPool(
                subnet=config.get('subnet'),
                iprange=config.get('ip_range'),
                gateway=config.get('gateway'),
                aux_addresses=config.get('aux_addresses'),
            )
            for config in ipam_dict.get('config', [])
        ],
        options=ipam_dict.get('options')
    )


class NetworkConfigChangedError(ConfigurationError):
    def __init__(self, net_name, property_name):
        super(NetworkConfigChangedError, self).__init__(
            'Network "{}" needs to be recreated - {} has changed'.format(
                net_name, property_name
            )
        )


def check_remote_ipam_config(remote, local):
    remote_ipam = remote.get('IPAM')
    ipam_dict = create_ipam_config_from_dict(local.ipam)
    if local.ipam.get('driver') and local.ipam.get('driver') != remote_ipam.get('Driver'):
        raise NetworkConfigChangedError(local.true_name, 'IPAM driver')
    if len(ipam_dict['Config']) != 0:
        if len(ipam_dict['Config']) != len(remote_ipam['Config']):
            raise NetworkConfigChangedError(local.true_name, 'IPAM configs')
        remote_configs = sorted(remote_ipam['Config'], key='Subnet')
        local_configs = sorted(ipam_dict['Config'], key='Subnet')
        while local_configs:
            lc = local_configs.pop()
            rc = remote_configs.pop()
            if lc.get('Subnet') != rc.get('Subnet'):
                raise NetworkConfigChangedError(local.true_name, 'IPAM config subnet')
            if lc.get('Gateway') is not None and lc.get('Gateway') != rc.get('Gateway'):
                raise NetworkConfigChangedError(local.true_name, 'IPAM config gateway')
            if lc.get('IPRange') != rc.get('IPRange'):
                raise NetworkConfigChangedError(local.true_name, 'IPAM config ip_range')
            if sorted(lc.get('AuxiliaryAddresses')) != sorted(rc.get('AuxiliaryAddresses')):
                raise NetworkConfigChangedError(local.true_name, 'IPAM config aux_addresses')

    remote_opts = remote_ipam.get('Options') or {}
    local_opts = local.ipam.get('Options') or {}
    for k in set.union(set(remote_opts.keys()), set(local_opts.keys())):
        if remote_opts.get(k) != local_opts.get(k):
            raise NetworkConfigChangedError(local.true_name, 'IPAM option "{}"'.format(k))


def check_remote_network_config(remote, local):
    if local.driver and remote.get('Driver') != local.driver:
        raise NetworkConfigChangedError(local.true_name, 'driver')
    local_opts = local.driver_opts or {}
    remote_opts = remote.get('Options') or {}
    for k in set.union(set(remote_opts.keys()), set(local_opts.keys())):
        if k in OPTS_EXCEPTIONS:
            continue
        if remote_opts.get(k) != local_opts.get(k):
            raise NetworkConfigChangedError(local.true_name, 'option "{}"'.format(k))

    if local.ipam is not None:
        check_remote_ipam_config(remote, local)

    if local.internal is not None and local.internal != remote.get('Internal', False):
        raise NetworkConfigChangedError(local.true_name, 'internal')
    if local.enable_ipv6 is not None and local.enable_ipv6 != remote.get('EnableIPv6', False):
        raise NetworkConfigChangedError(local.true_name, 'enable_ipv6')

    local_labels = local.labels or {}
    remote_labels = remote.get('Labels', {})
    for k in set.union(set(remote_labels.keys()), set(local_labels.keys())):
        if k.startswith('com.docker.'):  # We are only interested in user-specified labels
            continue
        if remote_labels.get(k) != local_labels.get(k):
            log.warn(
                'Network {}: label "{}" has changed. It may need to be'
                ' recreated.'.format(local.true_name, k)
            )


def build_networks(name, config_data, client):
    network_config = config_data.networks or {}
    networks = {
        network_name: Network(
            client=client, project=name,
            name=data.get('name', network_name),
            driver=data.get('driver'),
            driver_opts=data.get('driver_opts'),
            ipam=data.get('ipam'),
            external=bool(data.get('external', False)),
            internal=data.get('internal'),
            enable_ipv6=data.get('enable_ipv6'),
            labels=data.get('labels'),
            custom_name=data.get('name') is not None,
        )
        for network_name, data in network_config.items()
    }

    if 'default' not in networks:
        networks['default'] = Network(client, name, 'default')

    return networks


class ProjectNetworks(object):

    def __init__(self, networks, use_networking):
        self.networks = networks or {}
        self.use_networking = use_networking

    @classmethod
    def from_services(cls, services, networks, use_networking):
        service_networks = {
            network: networks.get(network)
            for service in services
            for network in get_network_names_for_service(service)
        }
        unused = set(networks) - set(service_networks) - {'default'}
        if unused:
            log.warn(
                "Some networks were defined but are not used by any service: "
                "{}".format(", ".join(unused)))
        return cls(service_networks, use_networking)

    def remove(self):
        if not self.use_networking:
            return
        for network in self.networks.values():
            try:
                network.remove()
            except NotFound:
                log.warn("Network %s not found.", network.true_name)

    def initialize(self):
        if not self.use_networking:
            return

        for network in self.networks.values():
            network.ensure()


def get_network_defs_for_service(service_dict):
    if 'network_mode' in service_dict:
        return {}
    networks = service_dict.get('networks', {'default': None})
    return dict(
        (net, (config or {}))
        for net, config in networks.items()
    )


def get_network_names_for_service(service_dict):
    return get_network_defs_for_service(service_dict).keys()


def get_networks(service_dict, network_definitions):
    networks = {}
    for name, netdef in get_network_defs_for_service(service_dict).items():
        network = network_definitions.get(name)
        if network:
            networks[network.true_name] = netdef
        else:
            raise ConfigurationError(
                'Service "{}" uses an undefined network "{}"'
                .format(service_dict['name'], name))

    if any([v.get('priority') for v in networks.values()]):
        return OrderedDict(sorted(
            networks.items(),
            key=lambda t: t[1].get('priority') or 0, reverse=True
        ))
    else:
        # Ensure Compose will pick a consistent primary network if no
        # priority is set
        return OrderedDict(sorted(networks.items(), key=lambda t: t[0]))
<EOF>
<BOF>
#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
timeparse.py
(c) Will Roberts <wildwilhelm@gmail.com>  1 February, 2014

This is a vendored and modified copy of:
github.com/wroberts/pytimeparse @ cc0550d

It has been modified to mimic the behaviour of
https://golang.org/pkg/time/#ParseDuration
'''
# MIT LICENSE
#
# Permission is hereby granted, free of charge, to any person
# obtaining a copy of this software and associated documentation files
# (the "Software"), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge,
# publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
#
# The above copyright notice and this permission notice shall be
# included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
from __future__ import absolute_import
from __future__ import unicode_literals

import re

HOURS = r'(?P<hours>[\d.]+)h'
MINS = r'(?P<mins>[\d.]+)m'
SECS = r'(?P<secs>[\d.]+)s'
MILLI = r'(?P<milli>[\d.]+)ms'
MICRO = r'(?P<micro>[\d.]+)(?:us|s)'
NANO = r'(?P<nano>[\d.]+)ns'


def opt(x):
    return r'(?:{x})?'.format(x=x)


TIMEFORMAT = r'{HOURS}{MINS}{SECS}{MILLI}{MICRO}{NANO}'.format(
    HOURS=opt(HOURS),
    MINS=opt(MINS),
    SECS=opt(SECS),
    MILLI=opt(MILLI),
    MICRO=opt(MICRO),
    NANO=opt(NANO),
)

MULTIPLIERS = dict([
    ('hours',   60 * 60),
    ('mins',    60),
    ('secs',    1),
    ('milli',   1.0 / 1000),
    ('micro',   1.0 / 1000.0 / 1000),
    ('nano',    1.0 / 1000.0 / 1000.0 / 1000.0),
])


def timeparse(sval):
    """Parse a time expression, returning it as a number of seconds.  If
    possible, the return value will be an `int`; if this is not
    possible, the return will be a `float`.  Returns `None` if a time
    expression cannot be parsed from the given string.

    Arguments:
    - `sval`: the string value to parse

    >>> timeparse('1m24s')
    84
    >>> timeparse('1.2 minutes')
    72
    >>> timeparse('1.2 seconds')
    1.2
    """
    match = re.match(r'\s*' + TIMEFORMAT + r'\s*$', sval, re.I)
    if not match or not match.group(0).strip():
        return

    mdict = match.groupdict()
    return sum(
        MULTIPLIERS[k] * cast(v) for (k, v) in mdict.items() if v is not None)


def cast(value):
    return int(value, 10) if value.isdigit() else float(value)
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals


class OperationFailedError(Exception):
    def __init__(self, reason):
        self.msg = reason


class StreamParseError(RuntimeError):
    def __init__(self, reason):
        self.msg = reason


class HealthCheckException(Exception):
    def __init__(self, reason):
        self.msg = reason


class HealthCheckFailed(HealthCheckException):
    def __init__(self, container_id):
        super(HealthCheckFailed, self).__init__(
            'Container "{}" is unhealthy.'.format(container_id)
        )


class NoHealthCheckConfigured(HealthCheckException):
    def __init__(self, service_name):
        super(NoHealthCheckConfigured, self).__init__(
            'Service "{}" is missing a healthcheck configuration'.format(
                service_name
            )
        )
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import re

from docker.errors import NotFound
from docker.utils import version_lt

from . import __version__
from .config import ConfigurationError
from .config.types import VolumeSpec
from .const import LABEL_PROJECT
from .const import LABEL_VERSION
from .const import LABEL_VOLUME


log = logging.getLogger(__name__)


class Volume(object):
    def __init__(self, client, project, name, driver=None, driver_opts=None,
                 external=False, labels=None, custom_name=False):
        self.client = client
        self.project = project
        self.name = name
        self.driver = driver
        self.driver_opts = driver_opts
        self.external = external
        self.labels = labels
        self.custom_name = custom_name
        self.legacy = None

    def create(self):
        return self.client.create_volume(
            self.full_name, self.driver, self.driver_opts, labels=self._labels
        )

    def remove(self):
        if self.external:
            log.info("Volume %s is external, skipping", self.true_name)
            return
        log.info("Removing volume %s", self.true_name)
        return self.client.remove_volume(self.true_name)

    def inspect(self, legacy=None):
        if legacy:
            return self.client.inspect_volume(self.legacy_full_name)
        return self.client.inspect_volume(self.full_name)

    def exists(self):
        self._set_legacy_flag()
        try:
            self.inspect(legacy=self.legacy)
        except NotFound:
            return False
        return True

    @property
    def full_name(self):
        if self.custom_name:
            return self.name
        return '{0}_{1}'.format(self.project.lstrip('-_'), self.name)

    @property
    def legacy_full_name(self):
        if self.custom_name:
            return self.name
        return '{0}_{1}'.format(
            re.sub(r'[_-]', '', self.project), self.name
        )

    @property
    def true_name(self):
        self._set_legacy_flag()
        if self.legacy:
            return self.legacy_full_name
        return self.full_name

    @property
    def _labels(self):
        if version_lt(self.client._version, '1.23'):
            return None
        labels = self.labels.copy() if self.labels else {}
        labels.update({
            LABEL_PROJECT: self.project,
            LABEL_VOLUME: self.name,
            LABEL_VERSION: __version__,
        })
        return labels

    def _set_legacy_flag(self):
        if self.legacy is not None:
            return
        try:
            data = self.inspect(legacy=True)
            self.legacy = data is not None
        except NotFound:
            self.legacy = False


class ProjectVolumes(object):

    def __init__(self, volumes):
        self.volumes = volumes

    @classmethod
    def from_config(cls, name, config_data, client):
        config_volumes = config_data.volumes or {}
        volumes = {
            vol_name: Volume(
                client=client,
                project=name,
                name=data.get('name', vol_name),
                driver=data.get('driver'),
                driver_opts=data.get('driver_opts'),
                custom_name=data.get('name') is not None,
                labels=data.get('labels'),
                external=bool(data.get('external', False))
            )
            for vol_name, data in config_volumes.items()
        }
        return cls(volumes)

    def remove(self):
        for volume in self.volumes.values():
            try:
                volume.remove()
            except NotFound:
                log.warn("Volume %s not found.", volume.true_name)

    def initialize(self):
        try:
            for volume in self.volumes.values():
                volume_exists = volume.exists()
                if volume.external:
                    log.debug(
                        'Volume {0} declared as external. No new '
                        'volume will be created.'.format(volume.name)
                    )
                    if not volume_exists:
                        raise ConfigurationError(
                            'Volume {name} declared as external, but could'
                            ' not be found. Please create the volume manually'
                            ' using `{command}{name}` and try again.'.format(
                                name=volume.full_name,
                                command='docker volume create --name='
                            )
                        )
                    continue

                if not volume_exists:
                    log.info(
                        'Creating volume "{0}" with {1} driver'.format(
                            volume.full_name, volume.driver or 'default'
                        )
                    )
                    volume.create()
                else:
                    check_remote_volume_config(volume.inspect(legacy=volume.legacy), volume)
        except NotFound:
            raise ConfigurationError(
                'Volume %s specifies nonexistent driver %s' % (volume.name, volume.driver)
            )

    def namespace_spec(self, volume_spec):
        if not volume_spec.is_named_volume:
            return volume_spec

        if isinstance(volume_spec, VolumeSpec):
            volume = self.volumes[volume_spec.external]
            return volume_spec._replace(external=volume.true_name)
        else:
            volume_spec.source = self.volumes[volume_spec.source].true_name
            return volume_spec


class VolumeConfigChangedError(ConfigurationError):
    def __init__(self, local, property_name, local_value, remote_value):
        super(VolumeConfigChangedError, self).__init__(
            'Configuration for volume {vol_name} specifies {property_name} '
            '{local_value}, but a volume with the same name uses a different '
            '{property_name} ({remote_value}). If you wish to use the new '
            'configuration, please remove the existing volume "{full_name}" '
            'first:\n$ docker volume rm {full_name}'.format(
                vol_name=local.name, property_name=property_name,
                local_value=local_value, remote_value=remote_value,
                full_name=local.true_name
            )
        )


def check_remote_volume_config(remote, local):
    if local.driver and remote.get('Driver') != local.driver:
        raise VolumeConfigChangedError(local, 'driver', local.driver, remote.get('Driver'))
    local_opts = local.driver_opts or {}
    remote_opts = remote.get('Options') or {}
    for k in set.union(set(remote_opts.keys()), set(local_opts.keys())):
        if k.startswith('com.docker.'):  # These options are set internally
            continue
        if remote_opts.get(k) != local_opts.get(k):
            raise VolumeConfigChangedError(
                local, '"{}" driver_opt'.format(k), local_opts.get(k), remote_opts.get(k),
            )

    local_labels = local.labels or {}
    remote_labels = remote.get('Labels') or {}
    for k in set.union(set(remote_labels.keys()), set(local_labels.keys())):
        if k.startswith('com.docker.'):  # We are only interested in user-specified labels
            continue
        if remote_labels.get(k) != local_labels.get(k):
            log.warn(
                'Volume {}: label "{}" has changed. It may need to be'
                ' recreated.'.format(local.name, k)
            )
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from compose import utils


class StreamOutputError(Exception):
    pass


def write_to_stream(s, stream):
    try:
        stream.write(s)
    except UnicodeEncodeError:
        encoding = getattr(stream, 'encoding', 'ascii')
        stream.write(s.encode(encoding, errors='replace').decode(encoding))


def stream_output(output, stream):
    is_terminal = hasattr(stream, 'isatty') and stream.isatty()
    stream = utils.get_output_stream(stream)
    lines = {}
    diff = 0

    for event in utils.json_stream(output):
        yield event
        is_progress_event = 'progress' in event or 'progressDetail' in event

        if not is_progress_event:
            print_output_event(event, stream, is_terminal)
            stream.flush()
            continue

        if not is_terminal:
            continue

        # if it's a progress event and we have a terminal, then display the progress bars
        image_id = event.get('id')
        if not image_id:
            continue

        if image_id not in lines:
            lines[image_id] = len(lines)
            write_to_stream("\n", stream)

        diff = len(lines) - lines[image_id]

        # move cursor up `diff` rows
        write_to_stream("%c[%dA" % (27, diff), stream)

        print_output_event(event, stream, is_terminal)

        if 'id' in event:
            # move cursor back down
            write_to_stream("%c[%dB" % (27, diff), stream)

        stream.flush()


def print_output_event(event, stream, is_terminal):
    if 'errorDetail' in event:
        raise StreamOutputError(event['errorDetail']['message'])

    terminator = ''

    if is_terminal and 'stream' not in event:
        # erase current line
        write_to_stream("%c[2K\r" % 27, stream)
        terminator = "\r"
    elif 'progressDetail' in event:
        return

    if 'time' in event:
        write_to_stream("[%s] " % event['time'], stream)

    if 'id' in event:
        write_to_stream("%s: " % event['id'], stream)

    if 'from' in event:
        write_to_stream("(from %s) " % event['from'], stream)

    status = event.get('status', '')

    if 'progress' in event:
        write_to_stream("%s %s%s" % (status, event['progress'], terminator), stream)
    elif 'progressDetail' in event:
        detail = event['progressDetail']
        total = detail.get('total')
        if 'current' in detail and total:
            percentage = float(detail['current']) / float(total) * 100
            write_to_stream('%s (%.1f%%)%s' % (status, percentage, terminator), stream)
        else:
            write_to_stream('%s%s' % (status, terminator), stream)
    elif 'stream' in event:
        write_to_stream("%s%s" % (event['stream'], terminator), stream)
    else:
        write_to_stream("%s%s\n" % (status, terminator), stream)


def get_digest_from_pull(events):
    for event in events:
        status = event.get('status')
        if not status or 'Digest' not in status:
            continue

        _, digest = status.split(':', 1)
        return digest.strip()
    return None


def get_digest_from_push(events):
    for event in events:
        digest = event.get('aux', {}).get('Digest')
        if digest:
            return digest
    return None
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

__version__ = '1.24.0dev'
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from compose.cli.main import main

main()
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import operator
import sys
from threading import Lock
from threading import Semaphore
from threading import Thread

from docker.errors import APIError
from docker.errors import ImageNotFound
from six.moves import _thread as thread
from six.moves.queue import Empty
from six.moves.queue import Queue

from compose.cli.colors import green
from compose.cli.colors import red
from compose.cli.signals import ShutdownException
from compose.const import PARALLEL_LIMIT
from compose.errors import HealthCheckFailed
from compose.errors import NoHealthCheckConfigured
from compose.errors import OperationFailedError
from compose.utils import get_output_stream


log = logging.getLogger(__name__)

STOP = object()


class GlobalLimit(object):
    """Simple class to hold a global semaphore limiter for a project. This class
    should be treated as a singleton that is instantiated when the project is.
    """

    global_limiter = Semaphore(PARALLEL_LIMIT)

    @classmethod
    def set_global_limit(cls, value):
        if value is None:
            value = PARALLEL_LIMIT
        cls.global_limiter = Semaphore(value)


def parallel_execute_watch(events, writer, errors, results, msg, get_name):
    """ Watch events from a parallel execution, update status and fill errors and results.
        Returns exception to re-raise.
    """
    error_to_reraise = None
    for obj, result, exception in events:
        if exception is None:
            writer.write(msg, get_name(obj), 'done', green)
            results.append(result)
        elif isinstance(exception, ImageNotFound):
            # This is to bubble up ImageNotFound exceptions to the client so we
            # can prompt the user if they want to rebuild.
            errors[get_name(obj)] = exception.explanation
            writer.write(msg, get_name(obj), 'error', red)
            error_to_reraise = exception
        elif isinstance(exception, APIError):
            errors[get_name(obj)] = exception.explanation
            writer.write(msg, get_name(obj), 'error', red)
        elif isinstance(exception, (OperationFailedError, HealthCheckFailed, NoHealthCheckConfigured)):
            errors[get_name(obj)] = exception.msg
            writer.write(msg, get_name(obj), 'error', red)
        elif isinstance(exception, UpstreamError):
            writer.write(msg, get_name(obj), 'error', red)
        else:
            errors[get_name(obj)] = exception
            error_to_reraise = exception
    return error_to_reraise


def parallel_execute(objects, func, get_name, msg, get_deps=None, limit=None):
    """Runs func on objects in parallel while ensuring that func is
    ran on object only after it is ran on all its dependencies.

    get_deps called on object must return a collection with its dependencies.
    get_name called on object must return its name.
    """
    objects = list(objects)
    stream = get_output_stream(sys.stderr)

    if ParallelStreamWriter.instance:
        writer = ParallelStreamWriter.instance
    else:
        writer = ParallelStreamWriter(stream)

    for obj in objects:
        writer.add_object(msg, get_name(obj))
    for obj in objects:
        writer.write_initial(msg, get_name(obj))

    events = parallel_execute_iter(objects, func, get_deps, limit)

    errors = {}
    results = []
    error_to_reraise = parallel_execute_watch(events, writer, errors, results, msg, get_name)

    for obj_name, error in errors.items():
        stream.write("\nERROR: for {}  {}\n".format(obj_name, error))

    if error_to_reraise:
        raise error_to_reraise

    return results, errors


def _no_deps(x):
    return []


class State(object):
    """
    Holds the state of a partially-complete parallel operation.

    state.started:   objects being processed
    state.finished:  objects which have been processed
    state.failed:    objects which either failed or whose dependencies failed
    """
    def __init__(self, objects):
        self.objects = objects

        self.started = set()
        self.finished = set()
        self.failed = set()

    def is_done(self):
        return len(self.finished) + len(self.failed) >= len(self.objects)

    def pending(self):
        return set(self.objects) - self.started - self.finished - self.failed


class NoLimit(object):
    def __enter__(self):
        pass

    def __exit__(self, *ex):
        pass


def parallel_execute_iter(objects, func, get_deps, limit):
    """
    Runs func on objects in parallel while ensuring that func is
    ran on object only after it is ran on all its dependencies.

    Returns an iterator of tuples which look like:

    # if func returned normally when run on object
    (object, result, None)

    # if func raised an exception when run on object
    (object, None, exception)

    # if func raised an exception when run on one of object's dependencies
    (object, None, UpstreamError())
    """
    if get_deps is None:
        get_deps = _no_deps

    if limit is None:
        limiter = NoLimit()
    else:
        limiter = Semaphore(limit)

    results = Queue()
    state = State(objects)

    while True:
        feed_queue(objects, func, get_deps, results, state, limiter)

        try:
            event = results.get(timeout=0.1)
        except Empty:
            continue
        # See https://github.com/docker/compose/issues/189
        except thread.error:
            raise ShutdownException()

        if event is STOP:
            break

        obj, _, exception = event
        if exception is None:
            log.debug('Finished processing: {}'.format(obj))
            state.finished.add(obj)
        else:
            log.debug('Failed: {}'.format(obj))
            state.failed.add(obj)

        yield event


def producer(obj, func, results, limiter):
    """
    The entry point for a producer thread which runs func on a single object.
    Places a tuple on the results queue once func has either returned or raised.
    """
    with limiter, GlobalLimit.global_limiter:
        try:
            result = func(obj)
            results.put((obj, result, None))
        except Exception as e:
            results.put((obj, None, e))


def feed_queue(objects, func, get_deps, results, state, limiter):
    """
    Starts producer threads for any objects which are ready to be processed
    (i.e. they have no dependencies which haven't been successfully processed).

    Shortcuts any objects whose dependencies have failed and places an
    (object, None, UpstreamError()) tuple on the results queue.
    """
    pending = state.pending()
    log.debug('Pending: {}'.format(pending))

    for obj in pending:
        deps = get_deps(obj)
        try:
            if any(dep[0] in state.failed for dep in deps):
                log.debug('{} has upstream errors - not processing'.format(obj))
                results.put((obj, None, UpstreamError()))
                state.failed.add(obj)
            elif all(
                dep not in objects or (
                    dep in state.finished and (not ready_check or ready_check(dep))
                ) for dep, ready_check in deps
            ):
                log.debug('Starting producer thread for {}'.format(obj))
                t = Thread(target=producer, args=(obj, func, results, limiter))
                t.daemon = True
                t.start()
                state.started.add(obj)
        except (HealthCheckFailed, NoHealthCheckConfigured) as e:
            log.debug(
                'Healthcheck for service(s) upstream of {} failed - '
                'not processing'.format(obj)
            )
            results.put((obj, None, e))

    if state.is_done():
        results.put(STOP)


class UpstreamError(Exception):
    pass


class ParallelStreamWriter(object):
    """Write out messages for operations happening in parallel.

    Each operation has its own line, and ANSI code characters are used
    to jump to the correct line, and write over the line.
    """

    noansi = False
    lock = Lock()
    instance = None

    @classmethod
    def set_noansi(cls, value=True):
        cls.noansi = value

    def __init__(self, stream):
        self.stream = stream
        self.lines = []
        self.width = 0
        ParallelStreamWriter.instance = self

    def add_object(self, msg, obj_index):
        if msg is None:
            return
        self.lines.append(msg + obj_index)
        self.width = max(self.width, len(msg + ' ' + obj_index))

    def write_initial(self, msg, obj_index):
        if msg is None:
            return
        return self._write_noansi(msg, obj_index, '')

    def _write_ansi(self, msg, obj_index, status):
        self.lock.acquire()
        position = self.lines.index(msg + obj_index)
        diff = len(self.lines) - position
        # move up
        self.stream.write("%c[%dA" % (27, diff))
        # erase
        self.stream.write("%c[2K\r" % 27)
        self.stream.write("{:<{width}} ... {}\r".format(msg + ' ' + obj_index,
                          status, width=self.width))
        # move back down
        self.stream.write("%c[%dB" % (27, diff))
        self.stream.flush()
        self.lock.release()

    def _write_noansi(self, msg, obj_index, status):
        self.stream.write(
            "{:<{width}} ... {}\r\n".format(
                msg + ' ' + obj_index, status, width=self.width
            )
        )
        self.stream.flush()

    def write(self, msg, obj_index, status, color_func):
        if msg is None:
            return
        if self.noansi:
            self._write_noansi(msg, obj_index, status)
        else:
            self._write_ansi(msg, obj_index, color_func(status))


def get_stream_writer():
    instance = ParallelStreamWriter.instance
    if instance is None:
        raise RuntimeError('ParallelStreamWriter has not yet been instantiated')
    return instance


def parallel_operation(containers, operation, options, message):
    parallel_execute(
        containers,
        operator.methodcaller(operation, **options),
        operator.attrgetter('name'),
        message,
    )


def parallel_remove(containers, options):
    stopped_containers = [c for c in containers if not c.is_running]
    parallel_operation(stopped_containers, 'remove', options, 'Removing')


def parallel_pause(containers, options):
    parallel_operation(containers, 'pause', options, 'Pausing')


def parallel_unpause(containers, options):
    parallel_operation(containers, 'unpause', options, 'Unpausing')


def parallel_kill(containers, options):
    parallel_operation(containers, 'kill', options, 'Killing')
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import json
import logging

import six
from docker.utils import split_command
from docker.utils.ports import split_port

from .cli.errors import UserError
from .config.serialize import denormalize_config
from .network import get_network_defs_for_service
from .service import format_environment
from .service import NoSuchImageError
from .service import parse_repository_tag


log = logging.getLogger(__name__)


SERVICE_KEYS = {
    'working_dir': 'WorkingDir',
    'user': 'User',
    'labels': 'Labels',
}

IGNORED_KEYS = {'build'}

SUPPORTED_KEYS = {
    'image',
    'ports',
    'expose',
    'networks',
    'command',
    'environment',
    'entrypoint',
} | set(SERVICE_KEYS)

VERSION = '0.1'


class NeedsPush(Exception):
    def __init__(self, image_name):
        self.image_name = image_name


class NeedsPull(Exception):
    def __init__(self, image_name, service_name):
        self.image_name = image_name
        self.service_name = service_name


class MissingDigests(Exception):
    def __init__(self, needs_push, needs_pull):
        self.needs_push = needs_push
        self.needs_pull = needs_pull


def serialize_bundle(config, image_digests):
    return json.dumps(to_bundle(config, image_digests), indent=2, sort_keys=True)


def get_image_digests(project, allow_push=False):
    digests = {}
    needs_push = set()
    needs_pull = set()

    for service in project.services:
        try:
            digests[service.name] = get_image_digest(
                service,
                allow_push=allow_push,
            )
        except NeedsPush as e:
            needs_push.add(e.image_name)
        except NeedsPull as e:
            needs_pull.add(e.service_name)

    if needs_push or needs_pull:
        raise MissingDigests(needs_push, needs_pull)

    return digests


def get_image_digest(service, allow_push=False):
    if 'image' not in service.options:
        raise UserError(
            "Service '{s.name}' doesn't define an image tag. An image name is "
            "required to generate a proper image digest for the bundle. Specify "
            "an image repo and tag with the 'image' option.".format(s=service))

    _, _, separator = parse_repository_tag(service.options['image'])
    # Compose file already uses a digest, no lookup required
    if separator == '@':
        return service.options['image']

    try:
        image = service.image()
    except NoSuchImageError:
        action = 'build' if 'build' in service.options else 'pull'
        raise UserError(
            "Image not found for service '{service}'. "
            "You might need to run `docker-compose {action} {service}`."
            .format(service=service.name, action=action))

    if image['RepoDigests']:
        # TODO: pick a digest based on the image tag if there are multiple
        # digests
        return image['RepoDigests'][0]

    if 'build' not in service.options:
        raise NeedsPull(service.image_name, service.name)

    if not allow_push:
        raise NeedsPush(service.image_name)

    return push_image(service)


def push_image(service):
    try:
        digest = service.push()
    except Exception:
        log.error(
            "Failed to push image for service '{s.name}'. Please use an "
            "image tag that can be pushed to a Docker "
            "registry.".format(s=service))
        raise

    if not digest:
        raise ValueError("Failed to get digest for %s" % service.name)

    repo, _, _ = parse_repository_tag(service.options['image'])
    identifier = '{repo}@{digest}'.format(repo=repo, digest=digest)

    # only do this if RepoDigests isn't already populated
    image = service.image()
    if not image['RepoDigests']:
        # Pull by digest so that image['RepoDigests'] is populated for next time
        # and we don't have to pull/push again
        service.client.pull(identifier)
        log.info("Stored digest for {}".format(service.image_name))

    return identifier


def to_bundle(config, image_digests):
    if config.networks:
        log.warn("Unsupported top level key 'networks' - ignoring")

    if config.volumes:
        log.warn("Unsupported top level key 'volumes' - ignoring")

    config = denormalize_config(config)

    return {
        'Version': VERSION,
        'Services': {
            name: convert_service_to_bundle(
                name,
                service_dict,
                image_digests[name],
            )
            for name, service_dict in config['services'].items()
        },
    }


def convert_service_to_bundle(name, service_dict, image_digest):
    container_config = {'Image': image_digest}

    for key, value in service_dict.items():
        if key in IGNORED_KEYS:
            continue

        if key not in SUPPORTED_KEYS:
            log.warn("Unsupported key '{}' in services.{} - ignoring".format(key, name))
            continue

        if key == 'environment':
            container_config['Env'] = format_environment({
                envkey: envvalue for envkey, envvalue in value.items()
                if envvalue
            })
            continue

        if key in SERVICE_KEYS:
            container_config[SERVICE_KEYS[key]] = value
            continue

    set_command_and_args(
        container_config,
        service_dict.get('entrypoint', []),
        service_dict.get('command', []))
    container_config['Networks'] = make_service_networks(name, service_dict)

    ports = make_port_specs(service_dict)
    if ports:
        container_config['Ports'] = ports

    return container_config


# See https://github.com/docker/swarmkit/blob/agent/exec/container/container.go#L95
def set_command_and_args(config, entrypoint, command):
    if isinstance(entrypoint, six.string_types):
        entrypoint = split_command(entrypoint)
    if isinstance(command, six.string_types):
        command = split_command(command)

    if entrypoint:
        config['Command'] = entrypoint + command
        return

    if command:
        config['Args'] = command


def make_service_networks(name, service_dict):
    networks = []

    for network_name, network_def in get_network_defs_for_service(service_dict).items():
        for key in network_def.keys():
            log.warn(
                "Unsupported key '{}' in services.{}.networks.{} - ignoring"
                .format(key, name, network_name))

        networks.append(network_name)

    return networks


def make_port_specs(service_dict):
    ports = []

    internal_ports = [
        internal_port
        for port_def in service_dict.get('ports', [])
        for internal_port in split_port(port_def)[0]
    ]

    internal_ports += service_dict.get('expose', [])

    for internal_port in internal_ports:
        spec = make_port_spec(internal_port)
        if spec not in ports:
            ports.append(spec)

    return ports


def make_port_spec(value):
    components = six.text_type(value).partition('/')
    return {
        'Protocol': components[2] or 'tcp',
        'Port': int(components[0]),
    }
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import itertools
import logging
import os
import re
import sys
from collections import namedtuple
from collections import OrderedDict
from operator import attrgetter

import enum
import six
from docker.errors import APIError
from docker.errors import ImageNotFound
from docker.errors import NotFound
from docker.types import LogConfig
from docker.types import Mount
from docker.utils import version_gte
from docker.utils import version_lt
from docker.utils.ports import build_port_bindings
from docker.utils.ports import split_port
from docker.utils.utils import convert_tmpfs_mounts

from . import __version__
from . import const
from . import progress_stream
from .config import DOCKER_CONFIG_KEYS
from .config import is_url
from .config import merge_environment
from .config import merge_labels
from .config.errors import DependencyError
from .config.types import MountSpec
from .config.types import ServicePort
from .config.types import VolumeSpec
from .const import DEFAULT_TIMEOUT
from .const import IS_WINDOWS_PLATFORM
from .const import LABEL_CONFIG_HASH
from .const import LABEL_CONTAINER_NUMBER
from .const import LABEL_ONE_OFF
from .const import LABEL_PROJECT
from .const import LABEL_SERVICE
from .const import LABEL_SLUG
from .const import LABEL_VERSION
from .const import NANOCPUS_SCALE
from .const import WINDOWS_LONGPATH_PREFIX
from .container import Container
from .errors import HealthCheckFailed
from .errors import NoHealthCheckConfigured
from .errors import OperationFailedError
from .parallel import parallel_execute
from .progress_stream import stream_output
from .progress_stream import StreamOutputError
from .utils import generate_random_id
from .utils import json_hash
from .utils import parse_bytes
from .utils import parse_seconds_float
from .utils import truncate_id
from .utils import unique_everseen


log = logging.getLogger(__name__)


HOST_CONFIG_KEYS = [
    'cap_add',
    'cap_drop',
    'cgroup_parent',
    'cpu_count',
    'cpu_percent',
    'cpu_period',
    'cpu_quota',
    'cpu_rt_period',
    'cpu_rt_runtime',
    'cpu_shares',
    'cpus',
    'cpuset',
    'device_cgroup_rules',
    'devices',
    'dns',
    'dns_search',
    'dns_opt',
    'env_file',
    'extra_hosts',
    'group_add',
    'init',
    'ipc',
    'isolation',
    'read_only',
    'log_driver',
    'log_opt',
    'mem_limit',
    'mem_reservation',
    'memswap_limit',
    'mem_swappiness',
    'oom_kill_disable',
    'oom_score_adj',
    'pid',
    'pids_limit',
    'privileged',
    'restart',
    'runtime',
    'security_opt',
    'shm_size',
    'storage_opt',
    'sysctls',
    'userns_mode',
    'volumes_from',
    'volume_driver',
]

CONDITION_STARTED = 'service_started'
CONDITION_HEALTHY = 'service_healthy'


class BuildError(Exception):
    def __init__(self, service, reason):
        self.service = service
        self.reason = reason


class NeedsBuildError(Exception):
    def __init__(self, service):
        self.service = service


class NoSuchImageError(Exception):
    pass


ServiceName = namedtuple('ServiceName', 'project service number slug')


ConvergencePlan = namedtuple('ConvergencePlan', 'action containers')


@enum.unique
class ConvergenceStrategy(enum.Enum):
    """Enumeration for all possible convergence strategies. Values refer to
    when containers should be recreated.
    """
    changed = 1
    always = 2
    never = 3

    @property
    def allows_recreate(self):
        return self is not type(self).never


@enum.unique
class ImageType(enum.Enum):
    """Enumeration for the types of images known to compose."""
    none = 0
    local = 1
    all = 2


@enum.unique
class BuildAction(enum.Enum):
    """Enumeration for the possible build actions."""
    none = 0
    force = 1
    skip = 2


class Service(object):
    def __init__(
        self,
        name,
        client=None,
        project='default',
        use_networking=False,
        links=None,
        volumes_from=None,
        network_mode=None,
        networks=None,
        secrets=None,
        scale=None,
        pid_mode=None,
        default_platform=None,
        **options
    ):
        self.name = name
        self.client = client
        self.project = project
        self.use_networking = use_networking
        self.links = links or []
        self.volumes_from = volumes_from or []
        self.network_mode = network_mode or NetworkMode(None)
        self.pid_mode = pid_mode or PidMode(None)
        self.networks = networks or {}
        self.secrets = secrets or []
        self.scale_num = scale or 1
        self.default_platform = default_platform
        self.options = options

    def __repr__(self):
        return '<Service: {}>'.format(self.name)

    def containers(self, stopped=False, one_off=False, filters=None, labels=None):
        if filters is None:
            filters = {}
        filters.update({'label': self.labels(one_off=one_off) + (labels or [])})

        result = list(filter(None, [
            Container.from_ps(self.client, container)
            for container in self.client.containers(
                all=stopped,
                filters=filters)])
        )
        if result:
            return result

        filters.update({'label': self.labels(one_off=one_off, legacy=True) + (labels or [])})
        return list(
            filter(
                lambda c: c.has_legacy_proj_name(self.project), filter(None, [
                    Container.from_ps(self.client, container)
                    for container in self.client.containers(
                        all=stopped,
                        filters=filters)])
            )
        )

    def get_container(self, number=1):
        """Return a :class:`compose.container.Container` for this service. The
        container must be active, and match `number`.
        """
        for container in self.containers(labels=['{0}={1}'.format(LABEL_CONTAINER_NUMBER, number)]):
            return container

        raise ValueError("No container found for %s_%s" % (self.name, number))

    def start(self, **options):
        containers = self.containers(stopped=True)
        for c in containers:
            self.start_container_if_stopped(c, **options)
        return containers

    def show_scale_warnings(self, desired_num):
        if self.custom_container_name and desired_num > 1:
            log.warn('The "%s" service is using the custom container name "%s". '
                     'Docker requires each container to have a unique name. '
                     'Remove the custom name to scale the service.'
                     % (self.name, self.custom_container_name))

        if self.specifies_host_port() and desired_num > 1:
            log.warn('The "%s" service specifies a port on the host. If multiple containers '
                     'for this service are created on a single host, the port will clash.'
                     % self.name)

    def scale(self, desired_num, timeout=None):
        """
        Adjusts the number of containers to the specified number and ensures
        they are running.

        - creates containers until there are at least `desired_num`
        - stops containers until there are at most `desired_num` running
        - starts containers until there are at least `desired_num` running
        - removes all stopped containers
        """

        self.show_scale_warnings(desired_num)

        running_containers = self.containers(stopped=False)
        num_running = len(running_containers)
        for c in running_containers:
            if not c.has_legacy_proj_name(self.project):
                continue
            log.info('Recreating container with legacy name %s' % c.name)
            self.recreate_container(c, timeout, start_new_container=False)

        if desired_num == num_running:
            # do nothing as we already have the desired number
            log.info('Desired container number already achieved')
            return

        if desired_num > num_running:
            all_containers = self.containers(stopped=True)

            if num_running != len(all_containers):
                # we have some stopped containers, check for divergences
                stopped_containers = [
                    c for c in all_containers if not c.is_running
                ]

                # Remove containers that have diverged
                divergent_containers = [
                    c for c in stopped_containers if self._containers_have_diverged([c])
                ]
                for c in divergent_containers:
                        c.remove()

                all_containers = list(set(all_containers) - set(divergent_containers))

            sorted_containers = sorted(all_containers, key=attrgetter('number'))
            self._execute_convergence_start(
                sorted_containers, desired_num, timeout, True, True
            )

        if desired_num < num_running:
            num_to_stop = num_running - desired_num

            sorted_running_containers = sorted(
                running_containers,
                key=attrgetter('number'))

            self._downscale(sorted_running_containers[-num_to_stop:], timeout)

    def create_container(self,
                         one_off=False,
                         previous_container=None,
                         number=None,
                         quiet=False,
                         **override_options):
        """
        Create a container for this service. If the image doesn't exist, attempt to pull
        it.
        """
        # This is only necessary for `scale` and `volumes_from`
        # auto-creating containers to satisfy the dependency.
        self.ensure_image_exists()

        container_options = self._get_container_create_options(
            override_options,
            number or self._next_container_number(one_off=one_off),
            one_off=one_off,
            previous_container=previous_container,
        )

        if 'name' in container_options and not quiet:
            log.info("Creating %s" % container_options['name'])

        try:
            return Container.create(self.client, **container_options)
        except APIError as ex:
            raise OperationFailedError("Cannot create container for service %s: %s" %
                                       (self.name, ex.explanation))

    def ensure_image_exists(self, do_build=BuildAction.none, silent=False):
        if self.can_be_built() and do_build == BuildAction.force:
            self.build()
            return

        try:
            self.image()
            return
        except NoSuchImageError:
            pass

        if not self.can_be_built():
            self.pull(silent=silent)
            return

        if do_build == BuildAction.skip:
            raise NeedsBuildError(self)

        self.build()
        log.warn(
            "Image for service {} was built because it did not already exist. To "
            "rebuild this image you must use `docker-compose build` or "
            "`docker-compose up --build`.".format(self.name))

    def image(self):
        try:
            return self.client.inspect_image(self.image_name)
        except ImageNotFound:
            raise NoSuchImageError("Image '{}' not found".format(self.image_name))

    @property
    def image_name(self):
        return self.options.get('image', '{project}_{s.name}'.format(
            s=self, project=self.project.lstrip('_-')
        ))

    @property
    def platform(self):
        platform = self.options.get('platform')
        if not platform and version_gte(self.client.api_version, '1.35'):
            platform = self.default_platform
        return platform

    def convergence_plan(self, strategy=ConvergenceStrategy.changed):
        containers = self.containers(stopped=True)

        if not containers:
            return ConvergencePlan('create', [])

        if strategy is ConvergenceStrategy.never:
            return ConvergencePlan('start', containers)

        if (
            strategy is ConvergenceStrategy.always or
            self._containers_have_diverged(containers)
        ):
            return ConvergencePlan('recreate', containers)

        stopped = [c for c in containers if not c.is_running]

        if stopped:
            return ConvergencePlan('start', stopped)

        return ConvergencePlan('noop', containers)

    def _containers_have_diverged(self, containers):
        config_hash = None

        try:
            config_hash = self.config_hash
        except NoSuchImageError as e:
            log.debug(
                'Service %s has diverged: %s',
                self.name, six.text_type(e),
            )
            return True

        has_diverged = False

        for c in containers:
            if c.has_legacy_proj_name(self.project):
                log.debug('%s has diverged: Legacy project name' % c.name)
                has_diverged = True
                continue
            container_config_hash = c.labels.get(LABEL_CONFIG_HASH, None)
            if container_config_hash != config_hash:
                log.debug(
                    '%s has diverged: %s != %s',
                    c.name, container_config_hash, config_hash,
                )
                has_diverged = True

        return has_diverged

    def _execute_convergence_create(self, scale, detached, start):

        i = self._next_container_number()

        def create_and_start(service, n):
            container = service.create_container(number=n, quiet=True)
            if not detached:
                container.attach_log_stream()
            if start:
                self.start_container(container)
            return container

        containers, errors = parallel_execute(
            [
                ServiceName(self.project, self.name, index, generate_random_id())
                for index in range(i, i + scale)
            ],
            lambda service_name: create_and_start(self, service_name.number),
            lambda service_name: self.get_container_name(
                service_name.service, service_name.number, service_name.slug
            ),
            "Creating"
        )
        for error in errors.values():
            raise OperationFailedError(error)

        return containers

    def _execute_convergence_recreate(self, containers, scale, timeout, detached, start,
                                      renew_anonymous_volumes):
            if scale is not None and len(containers) > scale:
                self._downscale(containers[scale:], timeout)
                containers = containers[:scale]

            def recreate(container):
                return self.recreate_container(
                    container, timeout=timeout, attach_logs=not detached,
                    start_new_container=start, renew_anonymous_volumes=renew_anonymous_volumes
                )
            containers, errors = parallel_execute(
                containers,
                recreate,
                lambda c: c.name,
                "Recreating",
            )
            for error in errors.values():
                raise OperationFailedError(error)

            if scale is not None and len(containers) < scale:
                containers.extend(self._execute_convergence_create(
                    scale - len(containers), detached, start
                ))
            return containers

    def _execute_convergence_start(self, containers, scale, timeout, detached, start):
            if scale is not None and len(containers) > scale:
                self._downscale(containers[scale:], timeout)
                containers = containers[:scale]
            if start:
                _, errors = parallel_execute(
                    containers,
                    lambda c: self.start_container_if_stopped(c, attach_logs=not detached, quiet=True),
                    lambda c: c.name,
                    "Starting",
                )

                for error in errors.values():
                    raise OperationFailedError(error)

            if scale is not None and len(containers) < scale:
                containers.extend(self._execute_convergence_create(
                    scale - len(containers), detached, start
                ))
            return containers

    def _downscale(self, containers, timeout=None):
        def stop_and_remove(container):
            container.stop(timeout=self.stop_timeout(timeout))
            container.remove()

        parallel_execute(
            containers,
            stop_and_remove,
            lambda c: c.name,
            "Stopping and removing",
        )

    def execute_convergence_plan(self, plan, timeout=None, detached=False,
                                 start=True, scale_override=None,
                                 rescale=True, reset_container_image=False,
                                 renew_anonymous_volumes=False):
        (action, containers) = plan
        scale = scale_override if scale_override is not None else self.scale_num
        containers = sorted(containers, key=attrgetter('number'))

        self.show_scale_warnings(scale)

        if action == 'create':
            return self._execute_convergence_create(
                scale, detached, start
            )

        # The create action needs always needs an initial scale, but otherwise,
        # we set scale to none in no-rescale scenarios (`run` dependencies)
        if not rescale:
            scale = None

        if action == 'recreate':
            if reset_container_image:
                # Updating the image ID on the container object lets us recover old volumes if
                # the new image uses them as well
                img_id = self.image()['Id']
                for c in containers:
                    c.reset_image(img_id)
            return self._execute_convergence_recreate(
                containers, scale, timeout, detached, start,
                renew_anonymous_volumes,
            )

        if action == 'start':
            return self._execute_convergence_start(
                containers, scale, timeout, detached, start
            )

        if action == 'noop':
            if scale != len(containers):
                return self._execute_convergence_start(
                    containers, scale, timeout, detached, start
                )
            for c in containers:
                log.info("%s is up-to-date" % c.name)

            return containers

        raise Exception("Invalid action: {}".format(action))

    def recreate_container(self, container, timeout=None, attach_logs=False, start_new_container=True,
                           renew_anonymous_volumes=False):
        """Recreate a container.

        The original container is renamed to a temporary name so that data
        volumes can be copied to the new container, before the original
        container is removed.
        """

        container.stop(timeout=self.stop_timeout(timeout))
        container.rename_to_tmp_name()
        new_container = self.create_container(
            previous_container=container if not renew_anonymous_volumes else None,
            number=container.number,
            quiet=True,
        )
        if attach_logs:
            new_container.attach_log_stream()
        if start_new_container:
            self.start_container(new_container)
        container.remove()
        return new_container

    def stop_timeout(self, timeout):
        if timeout is not None:
            return timeout
        timeout = parse_seconds_float(self.options.get('stop_grace_period'))
        if timeout is not None:
            return timeout
        return DEFAULT_TIMEOUT

    def start_container_if_stopped(self, container, attach_logs=False, quiet=False):
        if not container.is_running:
            if not quiet:
                log.info("Starting %s" % container.name)
            if attach_logs:
                container.attach_log_stream()
            return self.start_container(container)

    def start_container(self, container, use_network_aliases=True):
        self.connect_container_to_networks(container, use_network_aliases)
        try:
            container.start()
        except APIError as ex:
            raise OperationFailedError("Cannot start service %s: %s" % (self.name, ex.explanation))
        return container

    @property
    def prioritized_networks(self):
        return OrderedDict(
            sorted(
                self.networks.items(),
                key=lambda t: t[1].get('priority') or 0, reverse=True
            )
        )

    def connect_container_to_networks(self, container, use_network_aliases=True):
        connected_networks = container.get('NetworkSettings.Networks')

        for network, netdefs in self.prioritized_networks.items():
            if network in connected_networks:
                if short_id_alias_exists(container, network):
                    continue
                self.client.disconnect_container_from_network(container.id, network)

            aliases = self._get_aliases(netdefs, container) if use_network_aliases else []

            self.client.connect_container_to_network(
                container.id, network,
                aliases=aliases,
                ipv4_address=netdefs.get('ipv4_address', None),
                ipv6_address=netdefs.get('ipv6_address', None),
                links=self._get_links(False),
                link_local_ips=netdefs.get('link_local_ips', None),
            )

    def remove_duplicate_containers(self, timeout=None):
        for c in self.duplicate_containers():
            log.info('Removing %s' % c.name)
            c.stop(timeout=self.stop_timeout(timeout))
            c.remove()

    def duplicate_containers(self):
        containers = sorted(
            self.containers(stopped=True),
            key=lambda c: c.get('Created'),
        )

        numbers = set()

        for c in containers:
            if c.number in numbers:
                yield c
            else:
                numbers.add(c.number)

    @property
    def config_hash(self):
        return json_hash(self.config_dict())

    def config_dict(self):
        def image_id():
            try:
                return self.image()['Id']
            except NoSuchImageError:
                return None

        return {
            'options': self.options,
            'image_id': image_id(),
            'links': self.get_link_names(),
            'net': self.network_mode.id,
            'networks': self.networks,
            'volumes_from': [
                (v.source.name, v.mode)
                for v in self.volumes_from if isinstance(v.source, Service)
            ],
        }

    def get_dependency_names(self):
        net_name = self.network_mode.service_name
        pid_namespace = self.pid_mode.service_name
        return (
            self.get_linked_service_names() +
            self.get_volumes_from_names() +
            ([net_name] if net_name else []) +
            ([pid_namespace] if pid_namespace else []) +
            list(self.options.get('depends_on', {}).keys())
        )

    def get_dependency_configs(self):
        net_name = self.network_mode.service_name
        pid_namespace = self.pid_mode.service_name

        configs = dict(
            [(name, None) for name in self.get_linked_service_names()]
        )
        configs.update(dict(
            [(name, None) for name in self.get_volumes_from_names()]
        ))
        configs.update({net_name: None} if net_name else {})
        configs.update({pid_namespace: None} if pid_namespace else {})
        configs.update(self.options.get('depends_on', {}))
        for svc, config in self.options.get('depends_on', {}).items():
            if config['condition'] == CONDITION_STARTED:
                configs[svc] = lambda s: True
            elif config['condition'] == CONDITION_HEALTHY:
                configs[svc] = lambda s: s.is_healthy()
            else:
                # The config schema already prevents this, but it might be
                # bypassed if Compose is called programmatically.
                raise ValueError(
                    'depends_on condition "{}" is invalid.'.format(
                        config['condition']
                    )
                )

        return configs

    def get_linked_service_names(self):
        return [service.name for (service, _) in self.links]

    def get_link_names(self):
        return [(service.name, alias) for service, alias in self.links]

    def get_volumes_from_names(self):
        return [s.source.name for s in self.volumes_from if isinstance(s.source, Service)]

    def _next_container_number(self, one_off=False):
        containers = itertools.chain(
            self._fetch_containers(
                all=True,
                filters={'label': self.labels(one_off=one_off)}
            ), self._fetch_containers(
                all=True,
                filters={'label': self.labels(one_off=one_off, legacy=True)}
            )
        )
        numbers = [c.number for c in containers]
        return 1 if not numbers else max(numbers) + 1

    def _fetch_containers(self, **fetch_options):
        # Account for containers that might have been removed since we fetched
        # the list.
        def soft_inspect(container):
            try:
                return Container.from_id(self.client, container['Id'])
            except NotFound:
                return None

        return filter(None, [
            soft_inspect(container)
            for container in self.client.containers(**fetch_options)
        ])

    def _get_aliases(self, network, container=None):
        return list(
            {self.name} |
            ({container.short_id} if container else set()) |
            set(network.get('aliases', ()))
        )

    def build_default_networking_config(self):
        if not self.networks:
            return {}

        network = self.networks[self.network_mode.id]
        endpoint = {
            'Aliases': self._get_aliases(network),
            'IPAMConfig': {},
        }

        if network.get('ipv4_address'):
            endpoint['IPAMConfig']['IPv4Address'] = network.get('ipv4_address')
        if network.get('ipv6_address'):
            endpoint['IPAMConfig']['IPv6Address'] = network.get('ipv6_address')

        return {"EndpointsConfig": {self.network_mode.id: endpoint}}

    def _get_links(self, link_to_self):
        links = {}

        for service, link_name in self.links:
            for container in service.containers():
                links[link_name or service.name] = container.name
                links[container.name] = container.name
                links[container.name_without_project] = container.name

        if link_to_self:
            for container in self.containers():
                links[self.name] = container.name
                links[container.name] = container.name
                links[container.name_without_project] = container.name

        for external_link in self.options.get('external_links') or []:
            if ':' not in external_link:
                link_name = external_link
            else:
                external_link, link_name = external_link.split(':')
            links[link_name] = external_link

        return [
            (alias, container_name)
            for (container_name, alias) in links.items()
        ]

    def _get_volumes_from(self):
        return [build_volume_from(spec) for spec in self.volumes_from]

    def _get_container_create_options(
            self,
            override_options,
            number,
            one_off=False,
            previous_container=None):
        add_config_hash = (not one_off and not override_options)
        slug = generate_random_id() if previous_container is None else previous_container.full_slug

        container_options = dict(
            (k, self.options[k])
            for k in DOCKER_CONFIG_KEYS if k in self.options)
        override_volumes = override_options.pop('volumes', [])
        container_options.update(override_options)

        if not container_options.get('name'):
            container_options['name'] = self.get_container_name(self.name, number, slug, one_off)

        container_options.setdefault('detach', True)

        # If a qualified hostname was given, split it into an
        # unqualified hostname and a domainname unless domainname
        # was also given explicitly. This matches behavior
        # until Docker Engine 1.11.0 - Docker API 1.23.
        if (version_lt(self.client.api_version, '1.23') and
                'hostname' in container_options and
                'domainname' not in container_options and
                '.' in container_options['hostname']):
            parts = container_options['hostname'].partition('.')
            container_options['hostname'] = parts[0]
            container_options['domainname'] = parts[2]

        if (version_gte(self.client.api_version, '1.25') and
                'stop_grace_period' in self.options):
            container_options['stop_timeout'] = self.stop_timeout(None)

        if 'ports' in container_options or 'expose' in self.options:
            container_options['ports'] = build_container_ports(
                formatted_ports(container_options.get('ports', [])),
                self.options)

        if 'volumes' in container_options or override_volumes:
            container_options['volumes'] = list(set(
                container_options.get('volumes', []) + override_volumes
            ))

        container_options['environment'] = merge_environment(
            self._parse_proxy_config(),
            merge_environment(
                self.options.get('environment'),
                override_options.get('environment')
            )
        )

        container_options['labels'] = merge_labels(
            self.options.get('labels'),
            override_options.get('labels'))

        container_options, override_options = self._build_container_volume_options(
            previous_container, container_options, override_options
        )

        container_options['image'] = self.image_name

        container_options['labels'] = build_container_labels(
            container_options.get('labels', {}),
            self.labels(one_off=one_off),
            number,
            self.config_hash if add_config_hash else None,
            slug
        )

        # Delete options which are only used in HostConfig
        for key in HOST_CONFIG_KEYS:
            container_options.pop(key, None)

        container_options['host_config'] = self._get_container_host_config(
            override_options,
            one_off=one_off)

        networking_config = self.build_default_networking_config()
        if networking_config:
            container_options['networking_config'] = networking_config

        container_options['environment'] = format_environment(
            container_options['environment'])
        return container_options

    def _build_container_volume_options(self, previous_container, container_options, override_options):
        container_volumes = []
        container_mounts = []
        if 'volumes' in container_options:
            container_volumes = [
                v for v in container_options.get('volumes') if isinstance(v, VolumeSpec)
            ]
            container_mounts = [v for v in container_options.get('volumes') if isinstance(v, MountSpec)]

        binds, affinity = merge_volume_bindings(
            container_volumes, self.options.get('tmpfs') or [], previous_container,
            container_mounts
        )
        container_options['environment'].update(affinity)

        container_options['volumes'] = dict((v.internal, {}) for v in container_volumes or {})
        if version_gte(self.client.api_version, '1.30'):
            override_options['mounts'] = [build_mount(v) for v in container_mounts] or None
        else:
            # Workaround for 3.2 format
            override_options['tmpfs'] = self.options.get('tmpfs') or []
            for m in container_mounts:
                if m.is_tmpfs:
                    override_options['tmpfs'].append(m.target)
                else:
                    binds.append(m.legacy_repr())
                    container_options['volumes'][m.target] = {}

        secret_volumes = self.get_secret_volumes()
        if secret_volumes:
            if version_lt(self.client.api_version, '1.30'):
                binds.extend(v.legacy_repr() for v in secret_volumes)
                container_options['volumes'].update(
                    (v.target, {}) for v in secret_volumes
                )
            else:
                override_options['mounts'] = override_options.get('mounts') or []
                override_options['mounts'].extend([build_mount(v) for v in secret_volumes])

        # Remove possible duplicates (see e.g. https://github.com/docker/compose/issues/5885).
        # unique_everseen preserves order. (see https://github.com/docker/compose/issues/6091).
        override_options['binds'] = list(unique_everseen(binds))
        return container_options, override_options

    def _get_container_host_config(self, override_options, one_off=False):
        options = dict(self.options, **override_options)

        logging_dict = options.get('logging', None)
        blkio_config = convert_blkio_config(options.get('blkio_config', None))
        log_config = get_log_config(logging_dict)
        init_path = None
        if isinstance(options.get('init'), six.string_types):
            init_path = options.get('init')
            options['init'] = True

        security_opt = [
            o.value for o in options.get('security_opt')
        ] if options.get('security_opt') else None

        nano_cpus = None
        if 'cpus' in options:
            nano_cpus = int(options.get('cpus') * NANOCPUS_SCALE)

        return self.client.create_host_config(
            links=self._get_links(link_to_self=one_off),
            port_bindings=build_port_bindings(
                formatted_ports(options.get('ports', []))
            ),
            binds=options.get('binds'),
            volumes_from=self._get_volumes_from(),
            privileged=options.get('privileged', False),
            network_mode=self.network_mode.mode,
            devices=options.get('devices'),
            dns=options.get('dns'),
            dns_opt=options.get('dns_opt'),
            dns_search=options.get('dns_search'),
            restart_policy=options.get('restart'),
            runtime=options.get('runtime'),
            cap_add=options.get('cap_add'),
            cap_drop=options.get('cap_drop'),
            mem_limit=options.get('mem_limit'),
            mem_reservation=options.get('mem_reservation'),
            memswap_limit=options.get('memswap_limit'),
            ulimits=build_ulimits(options.get('ulimits')),
            log_config=log_config,
            extra_hosts=options.get('extra_hosts'),
            read_only=options.get('read_only'),
            pid_mode=self.pid_mode.mode,
            security_opt=security_opt,
            ipc_mode=options.get('ipc'),
            cgroup_parent=options.get('cgroup_parent'),
            cpu_quota=options.get('cpu_quota'),
            shm_size=options.get('shm_size'),
            sysctls=options.get('sysctls'),
            pids_limit=options.get('pids_limit'),
            tmpfs=options.get('tmpfs'),
            oom_kill_disable=options.get('oom_kill_disable'),
            oom_score_adj=options.get('oom_score_adj'),
            mem_swappiness=options.get('mem_swappiness'),
            group_add=options.get('group_add'),
            userns_mode=options.get('userns_mode'),
            init=options.get('init', None),
            init_path=init_path,
            isolation=options.get('isolation'),
            cpu_count=options.get('cpu_count'),
            cpu_percent=options.get('cpu_percent'),
            nano_cpus=nano_cpus,
            volume_driver=options.get('volume_driver'),
            cpuset_cpus=options.get('cpuset'),
            cpu_shares=options.get('cpu_shares'),
            storage_opt=options.get('storage_opt'),
            blkio_weight=blkio_config.get('weight'),
            blkio_weight_device=blkio_config.get('weight_device'),
            device_read_bps=blkio_config.get('device_read_bps'),
            device_read_iops=blkio_config.get('device_read_iops'),
            device_write_bps=blkio_config.get('device_write_bps'),
            device_write_iops=blkio_config.get('device_write_iops'),
            mounts=options.get('mounts'),
            device_cgroup_rules=options.get('device_cgroup_rules'),
            cpu_period=options.get('cpu_period'),
            cpu_rt_period=options.get('cpu_rt_period'),
            cpu_rt_runtime=options.get('cpu_rt_runtime'),
        )

    def get_secret_volumes(self):
        def build_spec(secret):
            target = secret['secret'].target
            if target is None:
                target = '{}/{}'.format(const.SECRETS_PATH, secret['secret'].source)
            elif not os.path.isabs(target):
                target = '{}/{}'.format(const.SECRETS_PATH, target)

            return MountSpec('bind', secret['file'], target, read_only=True)

        return [build_spec(secret) for secret in self.secrets]

    def build(self, no_cache=False, pull=False, force_rm=False, memory=None, build_args_override=None,
              gzip=False):
        log.info('Building %s' % self.name)

        build_opts = self.options.get('build', {})

        build_args = build_opts.get('args', {}).copy()
        if build_args_override:
            build_args.update(build_args_override)

        for k, v in self._parse_proxy_config().items():
            build_args.setdefault(k, v)

        path = rewrite_build_path(build_opts.get('context'))
        if self.platform and version_lt(self.client.api_version, '1.35'):
            raise OperationFailedError(
                'Impossible to perform platform-targeted builds for API version < 1.35'
            )

        build_output = self.client.build(
            path=path,
            tag=self.image_name,
            rm=True,
            forcerm=force_rm,
            pull=pull,
            nocache=no_cache,
            dockerfile=build_opts.get('dockerfile', None),
            cache_from=build_opts.get('cache_from', None),
            labels=build_opts.get('labels', None),
            buildargs=build_args,
            network_mode=build_opts.get('network', None),
            target=build_opts.get('target', None),
            shmsize=parse_bytes(build_opts.get('shm_size')) if build_opts.get('shm_size') else None,
            extra_hosts=build_opts.get('extra_hosts', None),
            container_limits={
                'memory': parse_bytes(memory) if memory else None
            },
            gzip=gzip,
            isolation=build_opts.get('isolation', self.options.get('isolation', None)),
            platform=self.platform,
        )

        try:
            all_events = list(stream_output(build_output, sys.stdout))
        except StreamOutputError as e:
            raise BuildError(self, six.text_type(e))

        # Ensure the HTTP connection is not reused for another
        # streaming command, as the Docker daemon can sometimes
        # complain about it
        self.client.close()

        image_id = None

        for event in all_events:
            if 'stream' in event:
                match = re.search(r'Successfully built ([0-9a-f]+)', event.get('stream', ''))
                if match:
                    image_id = match.group(1)

        if image_id is None:
            raise BuildError(self, event if all_events else 'Unknown')

        return image_id

    def can_be_built(self):
        return 'build' in self.options

    def labels(self, one_off=False, legacy=False):
        proj_name = self.project if not legacy else re.sub(r'[_-]', '', self.project)
        return [
            '{0}={1}'.format(LABEL_PROJECT, proj_name),
            '{0}={1}'.format(LABEL_SERVICE, self.name),
            '{0}={1}'.format(LABEL_ONE_OFF, "True" if one_off else "False"),
        ]

    @property
    def custom_container_name(self):
        return self.options.get('container_name')

    def get_container_name(self, service_name, number, slug, one_off=False):
        if self.custom_container_name and not one_off:
            return self.custom_container_name

        container_name = build_container_name(
            self.project, service_name, number, slug, one_off,
        )
        ext_links_origins = [l.split(':')[0] for l in self.options.get('external_links', [])]
        if container_name in ext_links_origins:
            raise DependencyError(
                'Service {0} has a self-referential external link: {1}'.format(
                    self.name, container_name
                )
            )
        return container_name

    def remove_image(self, image_type):
        if not image_type or image_type == ImageType.none:
            return False
        if image_type == ImageType.local and self.options.get('image'):
            return False

        log.info("Removing image %s", self.image_name)
        try:
            self.client.remove_image(self.image_name)
            return True
        except APIError as e:
            log.error("Failed to remove image for service %s: %s", self.name, e)
            return False

    def specifies_host_port(self):
        def has_host_port(binding):
            if isinstance(binding, dict):
                external_bindings = binding.get('published')
            else:
                _, external_bindings = split_port(binding)

            # there are no external bindings
            if external_bindings is None:
                return False

            # we only need to check the first binding from the range
            external_binding = external_bindings[0]

            # non-tuple binding means there is a host port specified
            if not isinstance(external_binding, tuple):
                return True

            # extract actual host port from tuple of (host_ip, host_port)
            _, host_port = external_binding
            if host_port is not None:
                return True

            return False

        return any(has_host_port(binding) for binding in self.options.get('ports', []))

    def _do_pull(self, repo, pull_kwargs, silent, ignore_pull_failures):
        try:
            output = self.client.pull(repo, **pull_kwargs)
            if silent:
                with open(os.devnull, 'w') as devnull:
                    for event in stream_output(output, devnull):
                        yield event
            else:
                for event in stream_output(output, sys.stdout):
                    yield event
        except (StreamOutputError, NotFound) as e:
            if not ignore_pull_failures:
                raise
            else:
                log.error(six.text_type(e))

    def pull(self, ignore_pull_failures=False, silent=False, stream=False):
        if 'image' not in self.options:
            return

        repo, tag, separator = parse_repository_tag(self.options['image'])
        kwargs = {
            'tag': tag or 'latest',
            'stream': True,
            'platform': self.platform,
        }
        if not silent:
            log.info('Pulling %s (%s%s%s)...' % (self.name, repo, separator, tag))

        if kwargs['platform'] and version_lt(self.client.api_version, '1.35'):
            raise OperationFailedError(
                'Impossible to perform platform-targeted pulls for API version < 1.35'
            )

        event_stream = self._do_pull(repo, kwargs, silent, ignore_pull_failures)
        if stream:
            return event_stream
        return progress_stream.get_digest_from_pull(event_stream)

    def push(self, ignore_push_failures=False):
        if 'image' not in self.options or 'build' not in self.options:
            return

        repo, tag, separator = parse_repository_tag(self.options['image'])
        tag = tag or 'latest'
        log.info('Pushing %s (%s%s%s)...' % (self.name, repo, separator, tag))
        output = self.client.push(repo, tag=tag, stream=True)

        try:
            return progress_stream.get_digest_from_push(
                stream_output(output, sys.stdout))
        except StreamOutputError as e:
            if not ignore_push_failures:
                raise
            else:
                log.error(six.text_type(e))

    def is_healthy(self):
        """ Check that all containers for this service report healthy.
            Returns false if at least one healthcheck is pending.
            If an unhealthy container is detected, raise a HealthCheckFailed
            exception.
        """
        result = True
        for ctnr in self.containers():
            ctnr.inspect()
            status = ctnr.get('State.Health.Status')
            if status is None:
                raise NoHealthCheckConfigured(self.name)
            elif status == 'starting':
                result = False
            elif status == 'unhealthy':
                raise HealthCheckFailed(ctnr.short_id)
        return result

    def _parse_proxy_config(self):
        client = self.client
        if 'proxies' not in client._general_configs:
            return {}
        docker_host = getattr(client, '_original_base_url', client.base_url)
        proxy_config = client._general_configs['proxies'].get(
            docker_host, client._general_configs['proxies'].get('default')
        ) or {}

        permitted = {
            'ftpProxy': 'FTP_PROXY',
            'httpProxy': 'HTTP_PROXY',
            'httpsProxy': 'HTTPS_PROXY',
            'noProxy': 'NO_PROXY',
        }

        result = {}

        for k, v in proxy_config.items():
            if k not in permitted:
                continue
            result[permitted[k]] = result[permitted[k].lower()] = v

        return result


def short_id_alias_exists(container, network):
    aliases = container.get(
        'NetworkSettings.Networks.{net}.Aliases'.format(net=network)) or ()
    return container.short_id in aliases


class PidMode(object):
    def __init__(self, mode):
        self._mode = mode

    @property
    def mode(self):
        return self._mode

    @property
    def service_name(self):
        return None


class ServicePidMode(PidMode):
    def __init__(self, service):
        self.service = service

    @property
    def service_name(self):
        return self.service.name

    @property
    def mode(self):
        containers = self.service.containers()
        if containers:
            return 'container:' + containers[0].id

        log.warn(
            "Service %s is trying to use reuse the PID namespace "
            "of another service that is not running." % (self.service_name)
        )
        return None


class ContainerPidMode(PidMode):
    def __init__(self, container):
        self.container = container
        self._mode = 'container:{}'.format(container.id)


class NetworkMode(object):
    """A `standard` network mode (ex: host, bridge)"""

    service_name = None

    def __init__(self, network_mode):
        self.network_mode = network_mode

    @property
    def id(self):
        return self.network_mode

    mode = id


class ContainerNetworkMode(object):
    """A network mode that uses a container's network stack."""

    service_name = None

    def __init__(self, container):
        self.container = container

    @property
    def id(self):
        return self.container.id

    @property
    def mode(self):
        return 'container:' + self.container.id


class ServiceNetworkMode(object):
    """A network mode that uses a service's network stack."""

    def __init__(self, service):
        self.service = service

    @property
    def id(self):
        return self.service.name

    service_name = id

    @property
    def mode(self):
        containers = self.service.containers()
        if containers:
            return 'container:' + containers[0].id

        log.warn("Service %s is trying to use reuse the network stack "
                 "of another service that is not running." % (self.id))
        return None


# Names


def build_container_name(project, service, number, slug, one_off=False):
    bits = [project.lstrip('-_'), service]
    if one_off:
        bits.append('run')
    return '_'.join(
        bits + ([str(number), truncate_id(slug)] if slug else [str(number)])
    )


# Images

def parse_repository_tag(repo_path):
    """Splits image identification into base image path, tag/digest
    and it's separator.

    Example:

    >>> parse_repository_tag('user/repo@sha256:digest')
    ('user/repo', 'sha256:digest', '@')
    >>> parse_repository_tag('user/repo:v1')
    ('user/repo', 'v1', ':')
    """
    tag_separator = ":"
    digest_separator = "@"

    if digest_separator in repo_path:
        repo, tag = repo_path.rsplit(digest_separator, 1)
        return repo, tag, digest_separator

    repo, tag = repo_path, ""
    if tag_separator in repo_path:
        repo, tag = repo_path.rsplit(tag_separator, 1)
        if "/" in tag:
            repo, tag = repo_path, ""

    return repo, tag, tag_separator


# Volumes


def merge_volume_bindings(volumes, tmpfs, previous_container, mounts):
    """
        Return a list of volume bindings for a container. Container data volumes
        are replaced by those from the previous container.
        Anonymous mounts are updated in place.
    """
    affinity = {}

    volume_bindings = OrderedDict(
        build_volume_binding(volume)
        for volume in volumes
        if volume.external
    )

    if previous_container:
        old_volumes, old_mounts = get_container_data_volumes(
            previous_container, volumes, tmpfs, mounts
        )
        warn_on_masked_volume(volumes, old_volumes, previous_container.service)
        volume_bindings.update(
            build_volume_binding(volume) for volume in old_volumes
        )

        if old_volumes or old_mounts:
            affinity = {'affinity:container': '=' + previous_container.id}

    return list(volume_bindings.values()), affinity


def get_container_data_volumes(container, volumes_option, tmpfs_option, mounts_option):
    """
        Find the container data volumes that are in `volumes_option`, and return
        a mapping of volume bindings for those volumes.
        Anonymous volume mounts are updated in place instead.
    """
    volumes = []
    volumes_option = volumes_option or []

    container_mounts = dict(
        (mount['Destination'], mount)
        for mount in container.get('Mounts') or {}
    )

    image_volumes = [
        VolumeSpec.parse(volume)
        for volume in
        container.image_config['ContainerConfig'].get('Volumes') or {}
    ]

    for volume in set(volumes_option + image_volumes):
        # No need to preserve host volumes
        if volume.external:
            continue

        # Attempting to rebind tmpfs volumes breaks: https://github.com/docker/compose/issues/4751
        if volume.internal in convert_tmpfs_mounts(tmpfs_option).keys():
            continue

        mount = container_mounts.get(volume.internal)

        # New volume, doesn't exist in the old container
        if not mount:
            continue

        # Volume was previously a host volume, now it's a container volume
        if not mount.get('Name'):
            continue

        # Volume (probably an image volume) is overridden by a mount in the service's config
        # and would cause a duplicate mountpoint error
        if volume.internal in [m.target for m in mounts_option]:
            continue

        # Copy existing volume from old container
        volume = volume._replace(external=mount['Name'])
        volumes.append(volume)

    updated_mounts = False
    for mount in mounts_option:
        if mount.type != 'volume':
            continue

        ctnr_mount = container_mounts.get(mount.target)
        if not ctnr_mount or not ctnr_mount.get('Name'):
            continue

        mount.source = ctnr_mount['Name']
        updated_mounts = True

    return volumes, updated_mounts


def warn_on_masked_volume(volumes_option, container_volumes, service):
    container_volumes = dict(
        (volume.internal, volume.external)
        for volume in container_volumes)

    for volume in volumes_option:
        if (
            volume.external and
            volume.internal in container_volumes and
            container_volumes.get(volume.internal) != volume.external
        ):
            log.warn((
                "Service \"{service}\" is using volume \"{volume}\" from the "
                "previous container. Host mapping \"{host_path}\" has no effect. "
                "Remove the existing containers (with `docker-compose rm {service}`) "
                "to use the host volume mapping."
            ).format(
                service=service,
                volume=volume.internal,
                host_path=volume.external))


def build_volume_binding(volume_spec):
    return volume_spec.internal, volume_spec.repr()


def build_volume_from(volume_from_spec):
    """
    volume_from can be either a service or a container. We want to return the
    container.id and format it into a string complete with the mode.
    """
    if isinstance(volume_from_spec.source, Service):
        containers = volume_from_spec.source.containers(stopped=True)
        if not containers:
            return "{}:{}".format(
                volume_from_spec.source.create_container().id,
                volume_from_spec.mode)

        container = containers[0]
        return "{}:{}".format(container.id, volume_from_spec.mode)
    elif isinstance(volume_from_spec.source, Container):
        return "{}:{}".format(volume_from_spec.source.id, volume_from_spec.mode)


def build_mount(mount_spec):
    kwargs = {}
    if mount_spec.options:
        for option, sdk_name in mount_spec.options_map[mount_spec.type].items():
            if option in mount_spec.options:
                kwargs[sdk_name] = mount_spec.options[option]

    return Mount(
        type=mount_spec.type, target=mount_spec.target, source=mount_spec.source,
        read_only=mount_spec.read_only, consistency=mount_spec.consistency, **kwargs
    )

# Labels


def build_container_labels(label_options, service_labels, number, config_hash, slug):
    labels = dict(label_options or {})
    labels.update(label.split('=', 1) for label in service_labels)
    labels[LABEL_CONTAINER_NUMBER] = str(number)
    labels[LABEL_SLUG] = slug
    labels[LABEL_VERSION] = __version__

    if config_hash:
        log.debug("Added config hash: %s" % config_hash)
        labels[LABEL_CONFIG_HASH] = config_hash

    return labels


# Ulimits


def build_ulimits(ulimit_config):
    if not ulimit_config:
        return None
    ulimits = []
    for limit_name, soft_hard_values in six.iteritems(ulimit_config):
        if isinstance(soft_hard_values, six.integer_types):
            ulimits.append({'name': limit_name, 'soft': soft_hard_values, 'hard': soft_hard_values})
        elif isinstance(soft_hard_values, dict):
            ulimit_dict = {'name': limit_name}
            ulimit_dict.update(soft_hard_values)
            ulimits.append(ulimit_dict)

    return ulimits


def get_log_config(logging_dict):
    log_driver = logging_dict.get('driver', "") if logging_dict else ""
    log_options = logging_dict.get('options', None) if logging_dict else None
    return LogConfig(
        type=log_driver,
        config=log_options
    )


# TODO: remove once fix is available in docker-py
def format_environment(environment):
    def format_env(key, value):
        if value is None:
            return key
        if isinstance(value, six.binary_type):
            value = value.decode('utf-8')
        return '{key}={value}'.format(key=key, value=value)
    return [format_env(*item) for item in environment.items()]


# Ports
def formatted_ports(ports):
    result = []
    for port in ports:
        if isinstance(port, ServicePort):
            result.append(port.legacy_repr())
        else:
            result.append(port)
    return result


def build_container_ports(container_ports, options):
    ports = []
    all_ports = container_ports + options.get('expose', [])
    for port_range in all_ports:
        internal_range, _ = split_port(port_range)
        for port in internal_range:
            port = str(port)
            if '/' in port:
                port = tuple(port.split('/'))
            ports.append(port)
    return ports


def convert_blkio_config(blkio_config):
    result = {}
    if blkio_config is None:
        return result

    result['weight'] = blkio_config.get('weight')
    for field in [
        "device_read_bps", "device_read_iops", "device_write_bps",
        "device_write_iops", "weight_device",
    ]:
        if field not in blkio_config:
            continue
        arr = []
        for item in blkio_config[field]:
            arr.append(dict([(k.capitalize(), v) for k, v in item.items()]))
        result[field] = arr
    return result


def rewrite_build_path(path):
    # python2 os.stat() doesn't support unicode on some UNIX, so we
    # encode it to a bytestring to be safe
    if not six.PY3 and not IS_WINDOWS_PLATFORM:
        path = path.encode('utf8')

    if IS_WINDOWS_PLATFORM and not is_url(path) and not path.startswith(WINDOWS_LONGPATH_PREFIX):
        path = WINDOWS_LONGPATH_PREFIX + os.path.normpath(path)

    return path
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import sys

from .version import ComposeVersion

DEFAULT_TIMEOUT = 10
HTTP_TIMEOUT = 60
IMAGE_EVENTS = ['delete', 'import', 'load', 'pull', 'push', 'save', 'tag', 'untag']
IS_WINDOWS_PLATFORM = (sys.platform == "win32")
LABEL_CONTAINER_NUMBER = 'com.docker.compose.container-number'
LABEL_ONE_OFF = 'com.docker.compose.oneoff'
LABEL_PROJECT = 'com.docker.compose.project'
LABEL_SERVICE = 'com.docker.compose.service'
LABEL_NETWORK = 'com.docker.compose.network'
LABEL_VERSION = 'com.docker.compose.version'
LABEL_SLUG = 'com.docker.compose.slug'
LABEL_VOLUME = 'com.docker.compose.volume'
LABEL_CONFIG_HASH = 'com.docker.compose.config-hash'
NANOCPUS_SCALE = 1000000000
PARALLEL_LIMIT = 64

SECRETS_PATH = '/run/secrets'
WINDOWS_LONGPATH_PREFIX = '\\\\?\\'

COMPOSEFILE_V1 = ComposeVersion('1')
COMPOSEFILE_V2_0 = ComposeVersion('2.0')
COMPOSEFILE_V2_1 = ComposeVersion('2.1')
COMPOSEFILE_V2_2 = ComposeVersion('2.2')
COMPOSEFILE_V2_3 = ComposeVersion('2.3')
COMPOSEFILE_V2_4 = ComposeVersion('2.4')

COMPOSEFILE_V3_0 = ComposeVersion('3.0')
COMPOSEFILE_V3_1 = ComposeVersion('3.1')
COMPOSEFILE_V3_2 = ComposeVersion('3.2')
COMPOSEFILE_V3_3 = ComposeVersion('3.3')
COMPOSEFILE_V3_4 = ComposeVersion('3.4')
COMPOSEFILE_V3_5 = ComposeVersion('3.5')
COMPOSEFILE_V3_6 = ComposeVersion('3.6')
COMPOSEFILE_V3_7 = ComposeVersion('3.7')

API_VERSIONS = {
    COMPOSEFILE_V1: '1.21',
    COMPOSEFILE_V2_0: '1.22',
    COMPOSEFILE_V2_1: '1.24',
    COMPOSEFILE_V2_2: '1.25',
    COMPOSEFILE_V2_3: '1.30',
    COMPOSEFILE_V2_4: '1.35',
    COMPOSEFILE_V3_0: '1.25',
    COMPOSEFILE_V3_1: '1.25',
    COMPOSEFILE_V3_2: '1.25',
    COMPOSEFILE_V3_3: '1.30',
    COMPOSEFILE_V3_4: '1.30',
    COMPOSEFILE_V3_5: '1.30',
    COMPOSEFILE_V3_6: '1.36',
    COMPOSEFILE_V3_7: '1.38',
}

API_VERSION_TO_ENGINE_VERSION = {
    API_VERSIONS[COMPOSEFILE_V1]: '1.9.0',
    API_VERSIONS[COMPOSEFILE_V2_0]: '1.10.0',
    API_VERSIONS[COMPOSEFILE_V2_1]: '1.12.0',
    API_VERSIONS[COMPOSEFILE_V2_2]: '1.13.0',
    API_VERSIONS[COMPOSEFILE_V2_3]: '17.06.0',
    API_VERSIONS[COMPOSEFILE_V2_4]: '17.12.0',
    API_VERSIONS[COMPOSEFILE_V3_0]: '1.13.0',
    API_VERSIONS[COMPOSEFILE_V3_1]: '1.13.0',
    API_VERSIONS[COMPOSEFILE_V3_2]: '1.13.0',
    API_VERSIONS[COMPOSEFILE_V3_3]: '17.06.0',
    API_VERSIONS[COMPOSEFILE_V3_4]: '17.06.0',
    API_VERSIONS[COMPOSEFILE_V3_5]: '17.06.0',
    API_VERSIONS[COMPOSEFILE_V3_6]: '18.02.0',
    API_VERSIONS[COMPOSEFILE_V3_7]: '18.06.0',
}
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from functools import reduce

import six
from docker.errors import ImageNotFound

from .const import LABEL_CONTAINER_NUMBER
from .const import LABEL_PROJECT
from .const import LABEL_SERVICE
from .const import LABEL_SLUG
from .const import LABEL_VERSION
from .utils import truncate_id
from .version import ComposeVersion


class Container(object):
    """
    Represents a Docker container, constructed from the output of
    GET /containers/:id:/json.
    """
    def __init__(self, client, dictionary, has_been_inspected=False):
        self.client = client
        self.dictionary = dictionary
        self.has_been_inspected = has_been_inspected
        self.log_stream = None

    @classmethod
    def from_ps(cls, client, dictionary, **kwargs):
        """
        Construct a container object from the output of GET /containers/json.
        """
        name = get_container_name(dictionary)
        if name is None:
            return None

        new_dictionary = {
            'Id': dictionary['Id'],
            'Image': dictionary['Image'],
            'Name': '/' + name,
        }
        return cls(client, new_dictionary, **kwargs)

    @classmethod
    def from_id(cls, client, id):
        return cls(client, client.inspect_container(id), has_been_inspected=True)

    @classmethod
    def create(cls, client, **options):
        response = client.create_container(**options)
        return cls.from_id(client, response['Id'])

    @property
    def id(self):
        return self.dictionary['Id']

    @property
    def image(self):
        return self.dictionary['Image']

    @property
    def image_config(self):
        return self.client.inspect_image(self.image)

    @property
    def short_id(self):
        return self.id[:12]

    @property
    def name(self):
        return self.dictionary['Name'][1:]

    @property
    def project(self):
        return self.labels.get(LABEL_PROJECT)

    @property
    def service(self):
        return self.labels.get(LABEL_SERVICE)

    @property
    def name_without_project(self):
        if self.name.startswith('{0}_{1}'.format(self.project, self.service)):
            return '{0}_{1}{2}'.format(self.service, self.number, '_' + self.slug if self.slug else '')
        else:
            return self.name

    @property
    def number(self):
        number = self.labels.get(LABEL_CONTAINER_NUMBER)
        if not number:
            raise ValueError("Container {0} does not have a {1} label".format(
                self.short_id, LABEL_CONTAINER_NUMBER))
        return int(number)

    @property
    def slug(self):
        if not self.full_slug:
            return None
        return truncate_id(self.full_slug)

    @property
    def full_slug(self):
        return self.labels.get(LABEL_SLUG)

    @property
    def ports(self):
        self.inspect_if_not_inspected()
        return self.get('NetworkSettings.Ports') or {}

    @property
    def human_readable_ports(self):
        def format_port(private, public):
            if not public:
                return [private]
            return [
                '{HostIp}:{HostPort}->{private}'.format(private=private, **pub)
                for pub in public
            ]

        return ', '.join(
            ','.join(format_port(*item))
            for item in sorted(six.iteritems(self.ports))
        )

    @property
    def labels(self):
        return self.get('Config.Labels') or {}

    @property
    def stop_signal(self):
        return self.get('Config.StopSignal')

    @property
    def log_config(self):
        return self.get('HostConfig.LogConfig') or None

    @property
    def human_readable_state(self):
        if self.is_paused:
            return 'Paused'
        if self.is_restarting:
            return 'Restarting'
        if self.is_running:
            return 'Ghost' if self.get('State.Ghost') else self.human_readable_health_status
        else:
            return 'Exit %s' % self.get('State.ExitCode')

    @property
    def human_readable_command(self):
        entrypoint = self.get('Config.Entrypoint') or []
        cmd = self.get('Config.Cmd') or []
        return ' '.join(entrypoint + cmd)

    @property
    def environment(self):
        def parse_env(var):
            if '=' in var:
                return var.split("=", 1)
            return var, None
        return dict(parse_env(var) for var in self.get('Config.Env') or [])

    @property
    def exit_code(self):
        return self.get('State.ExitCode')

    @property
    def is_running(self):
        return self.get('State.Running')

    @property
    def is_restarting(self):
        return self.get('State.Restarting')

    @property
    def is_paused(self):
        return self.get('State.Paused')

    @property
    def log_driver(self):
        return self.get('HostConfig.LogConfig.Type')

    @property
    def has_api_logs(self):
        log_type = self.log_driver
        return not log_type or log_type in ('json-file', 'journald')

    @property
    def human_readable_health_status(self):
        """ Generate UP status string with up time and health
        """
        status_string = 'Up'
        container_status = self.get('State.Health.Status')
        if container_status == 'starting':
            status_string += ' (health: starting)'
        elif container_status is not None:
            status_string += ' (%s)' % container_status
        return status_string

    def attach_log_stream(self):
        """A log stream can only be attached if the container uses a json-file
        log driver.
        """
        if self.has_api_logs:
            self.log_stream = self.attach(stdout=True, stderr=True, stream=True)

    def get(self, key):
        """Return a value from the container or None if the value is not set.

        :param key: a string using dotted notation for nested dictionary
                    lookups
        """
        self.inspect_if_not_inspected()

        def get_value(dictionary, key):
            return (dictionary or {}).get(key)

        return reduce(get_value, key.split('.'), self.dictionary)

    def get_local_port(self, port, protocol='tcp'):
        port = self.ports.get("%s/%s" % (port, protocol))
        return "{HostIp}:{HostPort}".format(**port[0]) if port else None

    def get_mount(self, mount_dest):
        for mount in self.get('Mounts'):
            if mount['Destination'] == mount_dest:
                return mount
        return None

    def start(self, **options):
        return self.client.start(self.id, **options)

    def stop(self, **options):
        return self.client.stop(self.id, **options)

    def pause(self, **options):
        return self.client.pause(self.id, **options)

    def unpause(self, **options):
        return self.client.unpause(self.id, **options)

    def kill(self, **options):
        return self.client.kill(self.id, **options)

    def restart(self, **options):
        return self.client.restart(self.id, **options)

    def remove(self, **options):
        return self.client.remove_container(self.id, **options)

    def create_exec(self, command, **options):
        return self.client.exec_create(self.id, command, **options)

    def start_exec(self, exec_id, **options):
        return self.client.exec_start(exec_id, **options)

    def rename_to_tmp_name(self):
        """Rename the container to a hopefully unique temporary container name
        by prepending the short id.
        """
        if not self.name.startswith(self.short_id):
            self.client.rename(
                self.id, '{0}_{1}'.format(self.short_id, self.name)
            )

    def inspect_if_not_inspected(self):
        if not self.has_been_inspected:
            self.inspect()

    def wait(self):
        return self.client.wait(self.id).get('StatusCode', 127)

    def logs(self, *args, **kwargs):
        return self.client.logs(self.id, *args, **kwargs)

    def inspect(self):
        self.dictionary = self.client.inspect_container(self.id)
        self.has_been_inspected = True
        return self.dictionary

    def image_exists(self):
        try:
            self.client.inspect_image(self.image)
        except ImageNotFound:
            return False

        return True

    def reset_image(self, img_id):
        """ If this container's image has been removed, temporarily replace the old image ID
            with `img_id`.
        """
        if not self.image_exists():
            self.dictionary['Image'] = img_id

    def attach(self, *args, **kwargs):
        return self.client.attach(self.id, *args, **kwargs)

    def has_legacy_proj_name(self, project_name):
        return (
            ComposeVersion(self.labels.get(LABEL_VERSION)) < ComposeVersion('1.21.0') and
            self.project != project_name
        )

    def __repr__(self):
        return '<Container: %s (%s)>' % (self.name, self.id[:6])

    def __eq__(self, other):
        if type(self) != type(other):
            return False
        return self.id == other.id

    def __hash__(self):
        return self.id.__hash__()


def get_container_name(container):
    if not container.get('Name') and not container.get('Names'):
        return None
    # inspect
    if 'Name' in container:
        return container['Name']
    # ps
    shortest_name = min(container['Names'], key=lambda n: len(n.split('/')))
    return shortest_name.split('/')[-1]
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import division
from __future__ import unicode_literals

import math
import os
import platform
import ssl
import subprocess
import sys

import docker
import six

import compose
from ..const import IS_WINDOWS_PLATFORM

# WindowsError is not defined on non-win32 platforms. Avoid runtime errors by
# defining it as OSError (its parent class) if missing.
try:
    WindowsError
except NameError:
    WindowsError = OSError


def yesno(prompt, default=None):
    """
    Prompt the user for a yes or no.

    Can optionally specify a default value, which will only be
    used if they enter a blank line.

    Unrecognised input (anything other than "y", "n", "yes",
    "no" or "") will return None.
    """
    answer = input(prompt).strip().lower()

    if answer == "y" or answer == "yes":
        return True
    elif answer == "n" or answer == "no":
        return False
    elif answer == "":
        return default
    else:
        return None


def input(prompt):
    """
    Version of input (raw_input in Python 2) which forces a flush of sys.stdout
    to avoid problems where the prompt fails to appear due to line buffering
    """
    sys.stdout.write(prompt)
    sys.stdout.flush()
    return sys.stdin.readline().rstrip('\n')


def call_silently(*args, **kwargs):
    """
    Like subprocess.call(), but redirects stdout and stderr to /dev/null.
    """
    with open(os.devnull, 'w') as shutup:
        try:
            return subprocess.call(*args, stdout=shutup, stderr=shutup, **kwargs)
        except WindowsError:
            # On Windows, subprocess.call() can still raise exceptions. Normalize
            # to POSIXy behaviour by returning a nonzero exit code.
            return 1


def is_mac():
    return platform.system() == 'Darwin'


def is_ubuntu():
    return platform.system() == 'Linux' and platform.linux_distribution()[0] == 'Ubuntu'


def is_windows():
    return IS_WINDOWS_PLATFORM


def get_version_info(scope):
    versioninfo = 'docker-compose version {}, build {}'.format(
        compose.__version__,
        get_build_version())

    if scope == 'compose':
        return versioninfo
    if scope == 'full':
        return (
            "{}\n"
            "docker-py version: {}\n"
            "{} version: {}\n"
            "OpenSSL version: {}"
        ).format(
            versioninfo,
            docker.version,
            platform.python_implementation(),
            platform.python_version(),
            ssl.OPENSSL_VERSION)

    raise ValueError("{} is not a valid version scope".format(scope))


def get_build_version():
    filename = os.path.join(os.path.dirname(compose.__file__), 'GITSHA')
    if not os.path.exists(filename):
        return 'unknown'

    with open(filename) as fh:
        return fh.read().strip()


def is_docker_for_mac_installed():
    return is_mac() and os.path.isdir('/Applications/Docker.app')


def generate_user_agent():
    parts = [
        "docker-compose/{}".format(compose.__version__),
        "docker-py/{}".format(docker.__version__),
    ]
    try:
        p_system = platform.system()
        p_release = platform.release()
    except IOError:
        pass
    else:
        parts.append("{}/{}".format(p_system, p_release))
    return " ".join(parts)


def human_readable_file_size(size):
    suffixes = ['B', 'kB', 'MB', 'GB', 'TB', 'PB', 'EB', ]
    order = int(math.log(size, 2) / 10) if size else 0
    if order >= len(suffixes):
        order = len(suffixes) - 1

    return '{0:.3g} {1}'.format(
        size / float(1 << (order * 10)),
        suffixes[order]
    )


def binarystr_to_unicode(s):
    if not isinstance(s, six.binary_type):
        return s

    if IS_WINDOWS_PLATFORM:
        try:
            return s.decode('windows-1250')
        except UnicodeDecodeError:
            pass
    return s.decode('utf-8', 'replace')
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import functools
import logging
import pprint
from itertools import chain

import six


def format_call(args, kwargs):
    args = (repr(a) for a in args)
    kwargs = ("{0!s}={1!r}".format(*item) for item in six.iteritems(kwargs))
    return "({0})".format(", ".join(chain(args, kwargs)))


def format_return(result, max_lines):
    if isinstance(result, (list, tuple, set)):
        return "({0} with {1} items)".format(type(result).__name__, len(result))

    if result:
        lines = pprint.pformat(result).split('\n')
        extra = '\n...' if len(lines) > max_lines else ''
        return '\n'.join(lines[:max_lines]) + extra

    return result


class VerboseProxy(object):
    """Proxy all function calls to another class and log method name, arguments
    and return values for each call.
    """

    def __init__(self, obj_name, obj, log_name=None, max_lines=10):
        self.obj_name = obj_name
        self.obj = obj
        self.max_lines = max_lines
        self.log = logging.getLogger(log_name or __name__)

    def __getattr__(self, name):
        attr = getattr(self.obj, name)

        if not six.callable(attr):
            return attr

        return functools.partial(self.proxy_callable, name)

    def proxy_callable(self, call_name, *args, **kwargs):
        self.log.info("%s %s <- %s",
                      self.obj_name,
                      call_name,
                      format_call(args, kwargs))

        result = getattr(self.obj, call_name)(*args, **kwargs)
        self.log.info("%s %s -> %s",
                      self.obj_name,
                      call_name,
                      format_return(result, self.max_lines))
        return result
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import os

import six
import texttable

from compose.cli import colors


def get_tty_width():
    tty_size = os.popen('stty size 2> /dev/null', 'r').read().split()
    if len(tty_size) != 2:
        return 0
    _, width = tty_size
    return int(width)


class Formatter(object):
    """Format tabular data for printing."""
    def table(self, headers, rows):
        table = texttable.Texttable(max_width=get_tty_width())
        table.set_cols_dtype(['t' for h in headers])
        table.add_rows([headers] + rows)
        table.set_deco(table.HEADER)
        table.set_chars(['-', '|', '+', '-'])

        return table.draw()


class ConsoleWarningFormatter(logging.Formatter):
    """A logging.Formatter which prints WARNING and ERROR messages with
    a prefix of the log level colored appropriate for the log level.
    """

    def get_level_message(self, record):
        separator = ': '
        if record.levelno == logging.WARNING:
            return colors.yellow(record.levelname) + separator
        if record.levelno == logging.ERROR:
            return colors.red(record.levelname) + separator

        return ''

    def format(self, record):
        if isinstance(record.msg, six.binary_type):
            record.msg = record.msg.decode('utf-8')
        message = super(ConsoleWarningFormatter, self).format(record)
        return '{0}{1}'.format(self.get_level_message(record), message)
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import os
import re

import six

from . import errors
from . import verbose_proxy
from .. import config
from .. import parallel
from ..config.environment import Environment
from ..const import API_VERSIONS
from ..project import Project
from .docker_client import docker_client
from .docker_client import get_tls_version
from .docker_client import tls_config_from_options
from .utils import get_version_info

log = logging.getLogger(__name__)


def project_from_options(project_dir, options):
    override_dir = options.get('--project-directory')
    environment = Environment.from_env_file(override_dir or project_dir)
    set_parallel_limit(environment)

    host = options.get('--host')
    if host is not None:
        host = host.lstrip('=')
    return get_project(
        project_dir,
        get_config_path_from_options(project_dir, options, environment),
        project_name=options.get('--project-name'),
        verbose=options.get('--verbose'),
        host=host,
        tls_config=tls_config_from_options(options, environment),
        environment=environment,
        override_dir=override_dir,
        compatibility=options.get('--compatibility'),
    )


def set_parallel_limit(environment):
    parallel_limit = environment.get('COMPOSE_PARALLEL_LIMIT')
    if parallel_limit:
        try:
            parallel_limit = int(parallel_limit)
        except ValueError:
            raise errors.UserError(
                'COMPOSE_PARALLEL_LIMIT must be an integer (found: "{}")'.format(
                    environment.get('COMPOSE_PARALLEL_LIMIT')
                )
            )
        if parallel_limit <= 1:
            raise errors.UserError('COMPOSE_PARALLEL_LIMIT can not be less than 2')
        parallel.GlobalLimit.set_global_limit(parallel_limit)


def get_config_from_options(base_dir, options):
    override_dir = options.get('--project-directory')
    environment = Environment.from_env_file(override_dir or base_dir)
    config_path = get_config_path_from_options(
        base_dir, options, environment
    )
    return config.load(
        config.find(base_dir, config_path, environment, override_dir),
        options.get('--compatibility')
    )


def get_config_path_from_options(base_dir, options, environment):
    def unicode_paths(paths):
        return [p.decode('utf-8') if isinstance(p, six.binary_type) else p for p in paths]

    file_option = options.get('--file')
    if file_option:
        return unicode_paths(file_option)

    config_files = environment.get('COMPOSE_FILE')
    if config_files:
        pathsep = environment.get('COMPOSE_PATH_SEPARATOR', os.pathsep)
        return unicode_paths(config_files.split(pathsep))
    return None


def get_client(environment, verbose=False, version=None, tls_config=None, host=None,
               tls_version=None):

    client = docker_client(
        version=version, tls_config=tls_config, host=host,
        environment=environment, tls_version=get_tls_version(environment)
    )
    if verbose:
        version_info = six.iteritems(client.version())
        log.info(get_version_info('full'))
        log.info("Docker base_url: %s", client.base_url)
        log.info("Docker version: %s",
                 ", ".join("%s=%s" % item for item in version_info))
        return verbose_proxy.VerboseProxy('docker', client)
    return client


def get_project(project_dir, config_path=None, project_name=None, verbose=False,
                host=None, tls_config=None, environment=None, override_dir=None,
                compatibility=False):
    if not environment:
        environment = Environment.from_env_file(project_dir)
    config_details = config.find(project_dir, config_path, environment, override_dir)
    project_name = get_project_name(
        config_details.working_dir, project_name, environment
    )
    config_data = config.load(config_details, compatibility)

    api_version = environment.get(
        'COMPOSE_API_VERSION',
        API_VERSIONS[config_data.version])

    client = get_client(
        verbose=verbose, version=api_version, tls_config=tls_config,
        host=host, environment=environment
    )

    with errors.handle_connection_errors(client):
        return Project.from_config(
            project_name, config_data, client, environment.get('DOCKER_DEFAULT_PLATFORM')
        )


def get_project_name(working_dir, project_name=None, environment=None):
    def normalize_name(name):
        return re.sub(r'[^-_a-z0-9]', '', name.lower())

    if not environment:
        environment = Environment.from_env_file(working_dir)
    project_name = project_name or environment.get('COMPOSE_PROJECT_NAME')
    if project_name:
        return normalize_name(project_name)

    project = os.path.basename(os.path.abspath(working_dir))
    if project:
        return normalize_name(project)

    return 'default'
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import contextlib
import logging
import socket
from distutils.spawn import find_executable
from textwrap import dedent

from docker.errors import APIError
from requests.exceptions import ConnectionError as RequestsConnectionError
from requests.exceptions import ReadTimeout
from requests.exceptions import SSLError
from requests.packages.urllib3.exceptions import ReadTimeoutError

from ..const import API_VERSION_TO_ENGINE_VERSION
from .utils import binarystr_to_unicode
from .utils import is_docker_for_mac_installed
from .utils import is_mac
from .utils import is_ubuntu
from .utils import is_windows


log = logging.getLogger(__name__)


class UserError(Exception):

    def __init__(self, msg):
        self.msg = dedent(msg).strip()

    def __unicode__(self):
        return self.msg

    __str__ = __unicode__


class ConnectionError(Exception):
    pass


@contextlib.contextmanager
def handle_connection_errors(client):
    try:
        yield
    except SSLError as e:
        log.error('SSL error: %s' % e)
        raise ConnectionError()
    except RequestsConnectionError as e:
        if e.args and isinstance(e.args[0], ReadTimeoutError):
            log_timeout_error(client.timeout)
            raise ConnectionError()
        exit_with_error(get_conn_error_message(client.base_url))
    except APIError as e:
        log_api_error(e, client.api_version)
        raise ConnectionError()
    except (ReadTimeout, socket.timeout):
        log_timeout_error(client.timeout)
        raise ConnectionError()
    except Exception as e:
        if is_windows():
            import pywintypes
            if isinstance(e, pywintypes.error):
                log_windows_pipe_error(e)
                raise ConnectionError()
        raise


def log_windows_pipe_error(exc):
    if exc.winerror == 2:
        log.error("Couldn't connect to Docker daemon. You might need to start Docker for Windows.")
    elif exc.winerror == 232:  # https://github.com/docker/compose/issues/5005
        log.error(
            "The current Compose file version is not compatible with your engine version. "
            "Please upgrade your Compose file to a more recent version, or set "
            "a COMPOSE_API_VERSION in your environment."
        )
    else:
        log.error(
            "Windows named pipe error: {} (code: {})".format(
                binarystr_to_unicode(exc.strerror), exc.winerror
            )
        )


def log_timeout_error(timeout):
    log.error(
        "An HTTP request took too long to complete. Retry with --verbose to "
        "obtain debug information.\n"
        "If you encounter this issue regularly because of slow network "
        "conditions, consider setting COMPOSE_HTTP_TIMEOUT to a higher "
        "value (current value: %s)." % timeout)


def log_api_error(e, client_version):
    explanation = binarystr_to_unicode(e.explanation)

    if 'client is newer than server' not in explanation:
        log.error(explanation)
        return

    version = API_VERSION_TO_ENGINE_VERSION.get(client_version)
    if not version:
        # They've set a custom API version
        log.error(explanation)
        return

    log.error(
        "The Docker Engine version is less than the minimum required by "
        "Compose. Your current project requires a Docker Engine of "
        "version {version} or greater.".format(version=version)
    )


def exit_with_error(msg):
    log.error(dedent(msg).strip())
    raise ConnectionError()


def get_conn_error_message(url):
    try:
        if find_executable('docker') is None:
            return docker_not_found_msg("Couldn't connect to Docker daemon.")
        if is_docker_for_mac_installed():
            return conn_error_docker_for_mac
        if find_executable('docker-machine') is not None:
            return conn_error_docker_machine
    except UnicodeDecodeError:
        # https://github.com/docker/compose/issues/5442
        # Ignore the error and print the generic message instead.
        pass
    return conn_error_generic.format(url=url)


def docker_not_found_msg(problem):
    return "{} You might need to install Docker:\n\n{}".format(
        problem, docker_install_url())


def docker_install_url():
    if is_mac():
        return docker_install_url_mac
    elif is_ubuntu():
        return docker_install_url_ubuntu
    elif is_windows():
        return docker_install_url_windows
    else:
        return docker_install_url_generic


docker_install_url_mac = "https://docs.docker.com/engine/installation/mac/"
docker_install_url_ubuntu = "https://docs.docker.com/engine/installation/ubuntulinux/"
docker_install_url_windows = "https://docs.docker.com/engine/installation/windows/"
docker_install_url_generic = "https://docs.docker.com/engine/installation/"


conn_error_docker_machine = """
    Couldn't connect to Docker daemon - you might need to run `docker-machine start default`.
"""

conn_error_docker_for_mac = """
    Couldn't connect to Docker daemon. You might need to start Docker for Mac.
"""


conn_error_generic = """
    Couldn't connect to Docker daemon at {url} - is it running?

    If it's at a non-standard location, specify the URL with the DOCKER_HOST environment variable.
"""
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import os.path
import ssl

from docker import APIClient
from docker.errors import TLSParameterError
from docker.tls import TLSConfig
from docker.utils import kwargs_from_env
from docker.utils.config import home_dir

from ..config.environment import Environment
from ..const import HTTP_TIMEOUT
from ..utils import unquote_path
from .errors import UserError
from .utils import generate_user_agent

log = logging.getLogger(__name__)


def default_cert_path():
    return os.path.join(home_dir(), '.docker')


def get_tls_version(environment):
    compose_tls_version = environment.get('COMPOSE_TLS_VERSION', None)
    if not compose_tls_version:
        return None

    tls_attr_name = "PROTOCOL_{}".format(compose_tls_version)
    if not hasattr(ssl, tls_attr_name):
        log.warn(
            'The "{}" protocol is unavailable. You may need to update your '
            'version of Python or OpenSSL. Falling back to TLSv1 (default).'
            .format(compose_tls_version)
        )
        return None

    return getattr(ssl, tls_attr_name)


def tls_config_from_options(options, environment=None):
    environment = environment or Environment()
    cert_path = environment.get('DOCKER_CERT_PATH') or None

    tls = options.get('--tls', False)
    ca_cert = unquote_path(options.get('--tlscacert'))
    cert = unquote_path(options.get('--tlscert'))
    key = unquote_path(options.get('--tlskey'))
    # verify is a special case - with docopt `--tlsverify` = False means it
    # wasn't used, so we set it if either the environment or the flag is True
    # see https://github.com/docker/compose/issues/5632
    verify = options.get('--tlsverify') or environment.get_boolean('DOCKER_TLS_VERIFY')

    skip_hostname_check = options.get('--skip-hostname-check', False)
    if cert_path is not None and not any((ca_cert, cert, key)):
        # FIXME: Modify TLSConfig to take a cert_path argument and do this internally
        cert = os.path.join(cert_path, 'cert.pem')
        key = os.path.join(cert_path, 'key.pem')
        ca_cert = os.path.join(cert_path, 'ca.pem')

    if verify and not any((ca_cert, cert, key)):
        # Default location for cert files is ~/.docker
        ca_cert = os.path.join(default_cert_path(), 'ca.pem')
        cert = os.path.join(default_cert_path(), 'cert.pem')
        key = os.path.join(default_cert_path(), 'key.pem')

    tls_version = get_tls_version(environment)

    advanced_opts = any([ca_cert, cert, key, verify, tls_version])

    if tls is True and not advanced_opts:
        return True
    elif advanced_opts:  # --tls is a noop
        client_cert = None
        if cert or key:
            client_cert = (cert, key)

        return TLSConfig(
            client_cert=client_cert, verify=verify, ca_cert=ca_cert,
            assert_hostname=False if skip_hostname_check else None,
            ssl_version=tls_version
        )

    return None


def docker_client(environment, version=None, tls_config=None, host=None,
                  tls_version=None):
    """
    Returns a docker-py client configured using environment variables
    according to the same logic as the official Docker client.
    """
    try:
        kwargs = kwargs_from_env(environment=environment, ssl_version=tls_version)
    except TLSParameterError:
        raise UserError(
            "TLS configuration is invalid - make sure your DOCKER_TLS_VERIFY "
            "and DOCKER_CERT_PATH are set correctly.\n"
            "You might need to run `eval \"$(docker-machine env default)\"`")

    if host:
        kwargs['base_url'] = host
    if tls_config:
        kwargs['tls'] = tls_config

    if version:
        kwargs['version'] = version

    timeout = environment.get('COMPOSE_HTTP_TIMEOUT')
    if timeout:
        kwargs['timeout'] = int(timeout)
    else:
        kwargs['timeout'] = HTTP_TIMEOUT

    kwargs['user_agent'] = generate_user_agent()

    # Workaround for
    # https://pyinstaller.readthedocs.io/en/v3.3.1/runtime-information.html#ld-library-path-libpath-considerations
    if 'LD_LIBRARY_PATH_ORIG' in environment:
        kwargs['credstore_env'] = {
            'LD_LIBRARY_PATH': environment.get('LD_LIBRARY_PATH_ORIG'),
        }

    client = APIClient(**kwargs)
    client._original_base_url = kwargs.get('base_url')

    return client
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import contextlib
import functools
import json
import logging
import pipes
import re
import subprocess
import sys
from distutils.spawn import find_executable
from inspect import getdoc
from operator import attrgetter

import docker

from . import errors
from . import signals
from .. import __version__
from ..bundle import get_image_digests
from ..bundle import MissingDigests
from ..bundle import serialize_bundle
from ..config import ConfigurationError
from ..config import parse_environment
from ..config import parse_labels
from ..config import resolve_build_args
from ..config.environment import Environment
from ..config.serialize import serialize_config
from ..config.types import VolumeSpec
from ..const import COMPOSEFILE_V2_2 as V2_2
from ..const import IS_WINDOWS_PLATFORM
from ..errors import StreamParseError
from ..progress_stream import StreamOutputError
from ..project import NoSuchService
from ..project import OneOffFilter
from ..project import ProjectError
from ..service import BuildAction
from ..service import BuildError
from ..service import ConvergenceStrategy
from ..service import ImageType
from ..service import NeedsBuildError
from ..service import OperationFailedError
from .command import get_config_from_options
from .command import project_from_options
from .docopt_command import DocoptDispatcher
from .docopt_command import get_handler
from .docopt_command import NoSuchCommand
from .errors import UserError
from .formatter import ConsoleWarningFormatter
from .formatter import Formatter
from .log_printer import build_log_presenters
from .log_printer import LogPrinter
from .utils import get_version_info
from .utils import human_readable_file_size
from .utils import yesno


if not IS_WINDOWS_PLATFORM:
    from dockerpty.pty import PseudoTerminal, RunOperation, ExecOperation

log = logging.getLogger(__name__)
console_handler = logging.StreamHandler(sys.stderr)


def main():
    signals.ignore_sigpipe()
    try:
        command = dispatch()
        command()
    except (KeyboardInterrupt, signals.ShutdownException):
        log.error("Aborting.")
        sys.exit(1)
    except (UserError, NoSuchService, ConfigurationError,
            ProjectError, OperationFailedError) as e:
        log.error(e.msg)
        sys.exit(1)
    except BuildError as e:
        log.error("Service '%s' failed to build: %s" % (e.service.name, e.reason))
        sys.exit(1)
    except StreamOutputError as e:
        log.error(e)
        sys.exit(1)
    except NeedsBuildError as e:
        log.error("Service '%s' needs to be built, but --no-build was passed." % e.service.name)
        sys.exit(1)
    except NoSuchCommand as e:
        commands = "\n".join(parse_doc_section("commands:", getdoc(e.supercommand)))
        log.error("No such command: %s\n\n%s", e.command, commands)
        sys.exit(1)
    except (errors.ConnectionError, StreamParseError):
        sys.exit(1)


def dispatch():
    setup_logging()
    dispatcher = DocoptDispatcher(
        TopLevelCommand,
        {'options_first': True, 'version': get_version_info('compose')})

    options, handler, command_options = dispatcher.parse(sys.argv[1:])
    setup_console_handler(console_handler,
                          options.get('--verbose'),
                          options.get('--no-ansi'),
                          options.get("--log-level"))
    setup_parallel_logger(options.get('--no-ansi'))
    if options.get('--no-ansi'):
        command_options['--no-color'] = True
    return functools.partial(perform_command, options, handler, command_options)


def perform_command(options, handler, command_options):
    if options['COMMAND'] in ('help', 'version'):
        # Skip looking up the compose file.
        handler(command_options)
        return

    if options['COMMAND'] == 'config':
        command = TopLevelCommand(None, options=options)
        handler(command, command_options)
        return

    project = project_from_options('.', options)
    command = TopLevelCommand(project, options=options)
    with errors.handle_connection_errors(project.client):
        handler(command, command_options)


def setup_logging():
    root_logger = logging.getLogger()
    root_logger.addHandler(console_handler)
    root_logger.setLevel(logging.DEBUG)

    # Disable requests logging
    logging.getLogger("requests").propagate = False


def setup_parallel_logger(noansi):
    if noansi:
        import compose.parallel
        compose.parallel.ParallelStreamWriter.set_noansi()


def setup_console_handler(handler, verbose, noansi=False, level=None):
    if handler.stream.isatty() and noansi is False:
        format_class = ConsoleWarningFormatter
    else:
        format_class = logging.Formatter

    if verbose:
        handler.setFormatter(format_class('%(name)s.%(funcName)s: %(message)s'))
        loglevel = logging.DEBUG
    else:
        handler.setFormatter(format_class())
        loglevel = logging.INFO

    if level is not None:
        levels = {
            'DEBUG': logging.DEBUG,
            'INFO': logging.INFO,
            'WARNING': logging.WARNING,
            'ERROR': logging.ERROR,
            'CRITICAL': logging.CRITICAL,
        }
        loglevel = levels.get(level.upper())
        if loglevel is None:
            raise UserError(
                'Invalid value for --log-level. Expected one of DEBUG, INFO, WARNING, ERROR, CRITICAL.'
            )

    handler.setLevel(loglevel)


# stolen from docopt master
def parse_doc_section(name, source):
    pattern = re.compile('^([^\n]*' + name + '[^\n]*\n?(?:[ \t].*?(?:\n|$))*)',
                         re.IGNORECASE | re.MULTILINE)
    return [s.strip() for s in pattern.findall(source)]


class TopLevelCommand(object):
    """Define and run multi-container applications with Docker.

    Usage:
      docker-compose [-f <arg>...] [options] [COMMAND] [ARGS...]
      docker-compose -h|--help

    Options:
      -f, --file FILE             Specify an alternate compose file
                                  (default: docker-compose.yml)
      -p, --project-name NAME     Specify an alternate project name
                                  (default: directory name)
      --verbose                   Show more output
      --log-level LEVEL           Set log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
      --no-ansi                   Do not print ANSI control characters
      -v, --version               Print version and exit
      -H, --host HOST             Daemon socket to connect to

      --tls                       Use TLS; implied by --tlsverify
      --tlscacert CA_PATH         Trust certs signed only by this CA
      --tlscert CLIENT_CERT_PATH  Path to TLS certificate file
      --tlskey TLS_KEY_PATH       Path to TLS key file
      --tlsverify                 Use TLS and verify the remote
      --skip-hostname-check       Don't check the daemon's hostname against the
                                  name specified in the client certificate
      --project-directory PATH    Specify an alternate working directory
                                  (default: the path of the Compose file)
      --compatibility             If set, Compose will attempt to convert deploy
                                  keys in v3 files to their non-Swarm equivalent

    Commands:
      build              Build or rebuild services
      bundle             Generate a Docker bundle from the Compose file
      config             Validate and view the Compose file
      create             Create services
      down               Stop and remove containers, networks, images, and volumes
      events             Receive real time events from containers
      exec               Execute a command in a running container
      help               Get help on a command
      images             List images
      kill               Kill containers
      logs               View output from containers
      pause              Pause services
      port               Print the public port for a port binding
      ps                 List containers
      pull               Pull service images
      push               Push service images
      restart            Restart services
      rm                 Remove stopped containers
      run                Run a one-off command
      scale              Set number of containers for a service
      start              Start services
      stop               Stop services
      top                Display the running processes
      unpause            Unpause services
      up                 Create and start containers
      version            Show the Docker-Compose version information
    """

    def __init__(self, project, options=None):
        self.project = project
        self.toplevel_options = options or {}

    @property
    def project_dir(self):
        return self.toplevel_options.get('--project-directory') or '.'

    def build(self, options):
        """
        Build or rebuild services.

        Services are built once and then tagged as `project_service`,
        e.g. `composetest_db`. If you change a service's `Dockerfile` or the
        contents of its build directory, you can run `docker-compose build` to rebuild it.

        Usage: build [options] [--build-arg key=val...] [SERVICE...]

        Options:
            --compress              Compress the build context using gzip.
            --force-rm              Always remove intermediate containers.
            --no-cache              Do not use cache when building the image.
            --pull                  Always attempt to pull a newer version of the image.
            -m, --memory MEM        Sets memory limit for the build container.
            --build-arg key=val     Set build-time variables for services.
            --parallel              Build images in parallel.
        """
        service_names = options['SERVICE']
        build_args = options.get('--build-arg', None)
        if build_args:
            if not service_names and docker.utils.version_lt(self.project.client.api_version, '1.25'):
                raise UserError(
                    '--build-arg is only supported when services are specified for API version < 1.25.'
                    ' Please use a Compose file version > 2.2 or specify which services to build.'
                )
            environment = Environment.from_env_file(self.project_dir)
            build_args = resolve_build_args(build_args, environment)

        self.project.build(
            service_names=options['SERVICE'],
            no_cache=bool(options.get('--no-cache', False)),
            pull=bool(options.get('--pull', False)),
            force_rm=bool(options.get('--force-rm', False)),
            memory=options.get('--memory'),
            build_args=build_args,
            gzip=options.get('--compress', False),
            parallel_build=options.get('--parallel', False),
        )

    def bundle(self, options):
        """
        Generate a Distributed Application Bundle (DAB) from the Compose file.

        Images must have digests stored, which requires interaction with a
        Docker registry. If digests aren't stored for all images, you can fetch
        them with `docker-compose pull` or `docker-compose push`. To push images
        automatically when bundling, pass `--push-images`. Only services with
        a `build` option specified will have their images pushed.

        Usage: bundle [options]

        Options:
            --push-images              Automatically push images for any services
                                       which have a `build` option specified.

            -o, --output PATH          Path to write the bundle file to.
                                       Defaults to "<project name>.dab".
        """
        compose_config = get_config_from_options('.', self.toplevel_options)

        output = options["--output"]
        if not output:
            output = "{}.dab".format(self.project.name)

        image_digests = image_digests_for_project(self.project, options['--push-images'])

        with open(output, 'w') as f:
            f.write(serialize_bundle(compose_config, image_digests))

        log.info("Wrote bundle to {}".format(output))

    def config(self, options):
        """
        Validate and view the Compose file.

        Usage: config [options]

        Options:
            --resolve-image-digests  Pin image tags to digests.
            -q, --quiet              Only validate the configuration, don't print
                                     anything.
            --services               Print the service names, one per line.
            --volumes                Print the volume names, one per line.
            --hash="*"               Print the service config hash, one per line.
                                     Set "service1,service2" for a list of specified services
                                     or use the wildcard symbol to display all services
        """

        compose_config = get_config_from_options('.', self.toplevel_options)
        image_digests = None

        if options['--resolve-image-digests']:
            self.project = project_from_options('.', self.toplevel_options)
            with errors.handle_connection_errors(self.project.client):
                image_digests = image_digests_for_project(self.project)

        if options['--quiet']:
            return

        if options['--services']:
            print('\n'.join(service['name'] for service in compose_config.services))
            return

        if options['--volumes']:
            print('\n'.join(volume for volume in compose_config.volumes))
            return

        if options['--hash'] is not None:
            h = options['--hash']
            self.project = project_from_options('.', self.toplevel_options)
            services = [svc for svc in options['--hash'].split(',')] if h != '*' else None
            with errors.handle_connection_errors(self.project.client):
                for service in self.project.get_services(services):
                    print('{} {}'.format(service.name, service.config_hash))
            return

        print(serialize_config(compose_config, image_digests))

    def create(self, options):
        """
        Creates containers for a service.
        This command is deprecated. Use the `up` command with `--no-start` instead.

        Usage: create [options] [SERVICE...]

        Options:
            --force-recreate       Recreate containers even if their configuration and
                                   image haven't changed. Incompatible with --no-recreate.
            --no-recreate          If containers already exist, don't recreate them.
                                   Incompatible with --force-recreate.
            --no-build             Don't build an image, even if it's missing.
            --build                Build images before creating containers.
        """
        service_names = options['SERVICE']

        log.warn(
            'The create command is deprecated. '
            'Use the up command with the --no-start flag instead.'
        )

        self.project.create(
            service_names=service_names,
            strategy=convergence_strategy_from_opts(options),
            do_build=build_action_from_opts(options),
        )

    def down(self, options):
        """
        Stops containers and removes containers, networks, volumes, and images
        created by `up`.

        By default, the only things removed are:

        - Containers for services defined in the Compose file
        - Networks defined in the `networks` section of the Compose file
        - The default network, if one is used

        Networks and volumes defined as `external` are never removed.

        Usage: down [options]

        Options:
            --rmi type              Remove images. Type must be one of:
                                      'all': Remove all images used by any service.
                                      'local': Remove only images that don't have a
                                      custom tag set by the `image` field.
            -v, --volumes           Remove named volumes declared in the `volumes`
                                    section of the Compose file and anonymous volumes
                                    attached to containers.
            --remove-orphans        Remove containers for services not defined in the
                                    Compose file
            -t, --timeout TIMEOUT   Specify a shutdown timeout in seconds.
                                    (default: 10)
        """
        environment = Environment.from_env_file(self.project_dir)
        ignore_orphans = environment.get_boolean('COMPOSE_IGNORE_ORPHANS')

        if ignore_orphans and options['--remove-orphans']:
            raise UserError("COMPOSE_IGNORE_ORPHANS and --remove-orphans cannot be combined.")

        image_type = image_type_from_opt('--rmi', options['--rmi'])
        timeout = timeout_from_opts(options)
        self.project.down(
            image_type,
            options['--volumes'],
            options['--remove-orphans'],
            timeout=timeout,
            ignore_orphans=ignore_orphans)

    def events(self, options):
        """
        Receive real time events from containers.

        Usage: events [options] [SERVICE...]

        Options:
            --json      Output events as a stream of json objects
        """
        def format_event(event):
            attributes = ["%s=%s" % item for item in event['attributes'].items()]
            return ("{time} {type} {action} {id} ({attrs})").format(
                attrs=", ".join(sorted(attributes)),
                **event)

        def json_format_event(event):
            event['time'] = event['time'].isoformat()
            event.pop('container')
            return json.dumps(event)

        for event in self.project.events():
            formatter = json_format_event if options['--json'] else format_event
            print(formatter(event))
            sys.stdout.flush()

    def exec_command(self, options):
        """
        Execute a command in a running container

        Usage: exec [options] [-e KEY=VAL...] SERVICE COMMAND [ARGS...]

        Options:
            -d, --detach      Detached mode: Run command in the background.
            --privileged      Give extended privileges to the process.
            -u, --user USER   Run the command as this user.
            -T                Disable pseudo-tty allocation. By default `docker-compose exec`
                              allocates a TTY.
            --index=index     index of the container if there are multiple
                              instances of a service [default: 1]
            -e, --env KEY=VAL Set environment variables (can be used multiple times,
                              not supported in API < 1.25)
            -w, --workdir DIR Path to workdir directory for this command.
        """
        environment = Environment.from_env_file(self.project_dir)
        use_cli = not environment.get_boolean('COMPOSE_INTERACTIVE_NO_CLI')
        index = int(options.get('--index'))
        service = self.project.get_service(options['SERVICE'])
        detach = options.get('--detach')

        if options['--env'] and docker.utils.version_lt(self.project.client.api_version, '1.25'):
            raise UserError("Setting environment for exec is not supported in API < 1.25 (%s)"
                            % self.project.client.api_version)

        if options['--workdir'] and docker.utils.version_lt(self.project.client.api_version, '1.35'):
            raise UserError("Setting workdir for exec is not supported in API < 1.35 (%s)"
                            % self.project.client.api_version)

        try:
            container = service.get_container(number=index)
        except ValueError as e:
            raise UserError(str(e))
        command = [options['COMMAND']] + options['ARGS']
        tty = not options["-T"]

        if IS_WINDOWS_PLATFORM or use_cli and not detach:
            sys.exit(call_docker(
                build_exec_command(options, container.id, command),
                self.toplevel_options)
            )

        create_exec_options = {
            "privileged": options["--privileged"],
            "user": options["--user"],
            "tty": tty,
            "stdin": True,
            "workdir": options["--workdir"],
        }

        if docker.utils.version_gte(self.project.client.api_version, '1.25'):
            create_exec_options["environment"] = options["--env"]

        exec_id = container.create_exec(command, **create_exec_options)

        if detach:
            container.start_exec(exec_id, tty=tty, stream=True)
            return

        signals.set_signal_handler_to_shutdown()
        try:
            operation = ExecOperation(
                self.project.client,
                exec_id,
                interactive=tty,
            )
            pty = PseudoTerminal(self.project.client, operation)
            pty.start()
        except signals.ShutdownException:
            log.info("received shutdown exception: closing")
        exit_code = self.project.client.exec_inspect(exec_id).get("ExitCode")
        sys.exit(exit_code)

    @classmethod
    def help(cls, options):
        """
        Get help on a command.

        Usage: help [COMMAND]
        """
        if options['COMMAND']:
            subject = get_handler(cls, options['COMMAND'])
        else:
            subject = cls

        print(getdoc(subject))

    def images(self, options):
        """
        List images used by the created containers.
        Usage: images [options] [SERVICE...]

        Options:
            -q, --quiet  Only display IDs
        """
        containers = sorted(
            self.project.containers(service_names=options['SERVICE'], stopped=True) +
            self.project.containers(service_names=options['SERVICE'], one_off=OneOffFilter.only),
            key=attrgetter('name'))

        if options['--quiet']:
            for image in set(c.image for c in containers):
                print(image.split(':')[1])
            return

        def add_default_tag(img_name):
            if ':' not in img_name.split('/')[-1]:
                return '{}:latest'.format(img_name)
            return img_name

        headers = [
            'Container',
            'Repository',
            'Tag',
            'Image Id',
            'Size'
        ]
        rows = []
        for container in containers:
            image_config = container.image_config
            service = self.project.get_service(container.service)
            index = 0
            img_name = add_default_tag(service.image_name)
            if img_name in image_config['RepoTags']:
                index = image_config['RepoTags'].index(img_name)
            repo_tags = (
                image_config['RepoTags'][index].rsplit(':', 1) if image_config['RepoTags']
                else ('<none>', '<none>')
            )

            image_id = image_config['Id'].split(':')[1][:12]
            size = human_readable_file_size(image_config['Size'])
            rows.append([
                container.name,
                repo_tags[0],
                repo_tags[1],
                image_id,
                size
            ])
        print(Formatter().table(headers, rows))

    def kill(self, options):
        """
        Force stop service containers.

        Usage: kill [options] [SERVICE...]

        Options:
            -s SIGNAL         SIGNAL to send to the container.
                              Default signal is SIGKILL.
        """
        signal = options.get('-s', 'SIGKILL')

        self.project.kill(service_names=options['SERVICE'], signal=signal)

    def logs(self, options):
        """
        View output from containers.

        Usage: logs [options] [SERVICE...]

        Options:
            --no-color          Produce monochrome output.
            -f, --follow        Follow log output.
            -t, --timestamps    Show timestamps.
            --tail="all"        Number of lines to show from the end of the logs
                                for each container.
        """
        containers = self.project.containers(service_names=options['SERVICE'], stopped=True)

        tail = options['--tail']
        if tail is not None:
            if tail.isdigit():
                tail = int(tail)
            elif tail != 'all':
                raise UserError("tail flag must be all or a number")
        log_args = {
            'follow': options['--follow'],
            'tail': tail,
            'timestamps': options['--timestamps']
        }
        print("Attaching to", list_containers(containers))
        log_printer_from_project(
            self.project,
            containers,
            options['--no-color'],
            log_args,
            event_stream=self.project.events(service_names=options['SERVICE'])).run()

    def pause(self, options):
        """
        Pause services.

        Usage: pause [SERVICE...]
        """
        containers = self.project.pause(service_names=options['SERVICE'])
        exit_if(not containers, 'No containers to pause', 1)

    def port(self, options):
        """
        Print the public port for a port binding.

        Usage: port [options] SERVICE PRIVATE_PORT

        Options:
            --protocol=proto  tcp or udp [default: tcp]
            --index=index     index of the container if there are multiple
                              instances of a service [default: 1]
        """
        index = int(options.get('--index'))
        service = self.project.get_service(options['SERVICE'])
        try:
            container = service.get_container(number=index)
        except ValueError as e:
            raise UserError(str(e))
        print(container.get_local_port(
            options['PRIVATE_PORT'],
            protocol=options.get('--protocol') or 'tcp') or '')

    def ps(self, options):
        """
        List containers.

        Usage: ps [options] [SERVICE...]

        Options:
            -q, --quiet          Only display IDs
            --services           Display services
            --filter KEY=VAL     Filter services by a property
            -a, --all            Show all stopped containers (including those created by the run command)
        """
        if options['--quiet'] and options['--services']:
            raise UserError('--quiet and --services cannot be combined')

        if options['--services']:
            filt = build_filter(options.get('--filter'))
            services = self.project.services
            if filt:
                services = filter_services(filt, services, self.project)
            print('\n'.join(service.name for service in services))
            return

        if options['--all']:
            containers = sorted(self.project.containers(service_names=options['SERVICE'],
                                                        one_off=OneOffFilter.include, stopped=True))
        else:
            containers = sorted(
                self.project.containers(service_names=options['SERVICE'], stopped=True) +
                self.project.containers(service_names=options['SERVICE'], one_off=OneOffFilter.only),
                key=attrgetter('name'))

        if options['--quiet']:
            for container in containers:
                print(container.id)
        else:
            headers = [
                'Name',
                'Command',
                'State',
                'Ports',
            ]
            rows = []
            for container in containers:
                command = container.human_readable_command
                if len(command) > 30:
                    command = '%s ...' % command[:26]
                rows.append([
                    container.name,
                    command,
                    container.human_readable_state,
                    container.human_readable_ports,
                ])
            print(Formatter().table(headers, rows))

    def pull(self, options):
        """
        Pulls images for services defined in a Compose file, but does not start the containers.

        Usage: pull [options] [SERVICE...]

        Options:
            --ignore-pull-failures  Pull what it can and ignores images with pull failures.
            --parallel              Deprecated, pull multiple images in parallel (enabled by default).
            --no-parallel           Disable parallel pulling.
            -q, --quiet             Pull without printing progress information
            --include-deps          Also pull services declared as dependencies
        """
        if options.get('--parallel'):
            log.warn('--parallel option is deprecated and will be removed in future versions.')
        self.project.pull(
            service_names=options['SERVICE'],
            ignore_pull_failures=options.get('--ignore-pull-failures'),
            parallel_pull=not options.get('--no-parallel'),
            silent=options.get('--quiet'),
            include_deps=options.get('--include-deps'),
        )

    def push(self, options):
        """
        Pushes images for services.

        Usage: push [options] [SERVICE...]

        Options:
            --ignore-push-failures  Push what it can and ignores images with push failures.
        """
        self.project.push(
            service_names=options['SERVICE'],
            ignore_push_failures=options.get('--ignore-push-failures')
        )

    def rm(self, options):
        """
        Removes stopped service containers.

        By default, anonymous volumes attached to containers will not be removed. You
        can override this with `-v`. To list all volumes, use `docker volume ls`.

        Any data which is not in a volume will be lost.

        Usage: rm [options] [SERVICE...]

        Options:
            -f, --force   Don't ask to confirm removal
            -s, --stop    Stop the containers, if required, before removing
            -v            Remove any anonymous volumes attached to containers
            -a, --all     Deprecated - no effect.
        """
        if options.get('--all'):
            log.warn(
                '--all flag is obsolete. This is now the default behavior '
                'of `docker-compose rm`'
            )
        one_off = OneOffFilter.include

        if options.get('--stop'):
            self.project.stop(service_names=options['SERVICE'], one_off=one_off)

        all_containers = self.project.containers(
            service_names=options['SERVICE'], stopped=True, one_off=one_off
        )
        stopped_containers = [c for c in all_containers if not c.is_running]

        if len(stopped_containers) > 0:
            print("Going to remove", list_containers(stopped_containers))
            if options.get('--force') \
                    or yesno("Are you sure? [yN] ", default=False):
                self.project.remove_stopped(
                    service_names=options['SERVICE'],
                    v=options.get('-v', False),
                    one_off=one_off
                )
        else:
            print("No stopped containers")

    def run(self, options):
        """
        Run a one-off command on a service.

        For example:

            $ docker-compose run web python manage.py shell

        By default, linked services will be started, unless they are already
        running. If you do not want to start linked services, use
        `docker-compose run --no-deps SERVICE COMMAND [ARGS...]`.

        Usage:
            run [options] [-v VOLUME...] [-p PORT...] [-e KEY=VAL...] [-l KEY=VALUE...]
                SERVICE [COMMAND] [ARGS...]

        Options:
            -d, --detach          Detached mode: Run container in the background, print
                                  new container name.
            --name NAME           Assign a name to the container
            --entrypoint CMD      Override the entrypoint of the image.
            -e KEY=VAL            Set an environment variable (can be used multiple times)
            -l, --label KEY=VAL   Add or override a label (can be used multiple times)
            -u, --user=""         Run as specified username or uid
            --no-deps             Don't start linked services.
            --rm                  Remove container after run. Ignored in detached mode.
            -p, --publish=[]      Publish a container's port(s) to the host
            --service-ports       Run command with the service's ports enabled and mapped
                                  to the host.
            --use-aliases         Use the service's network aliases in the network(s) the
                                  container connects to.
            -v, --volume=[]       Bind mount a volume (default [])
            -T                    Disable pseudo-tty allocation. By default `docker-compose run`
                                  allocates a TTY.
            -w, --workdir=""      Working directory inside the container
        """
        service = self.project.get_service(options['SERVICE'])
        detach = options.get('--detach')

        if options['--publish'] and options['--service-ports']:
            raise UserError(
                'Service port mapping and manual port mapping '
                'can not be used together'
            )

        if options['COMMAND'] is not None:
            command = [options['COMMAND']] + options['ARGS']
        elif options['--entrypoint'] is not None:
            command = []
        else:
            command = service.options.get('command')

        container_options = build_one_off_container_options(options, detach, command)
        run_one_off_container(
            container_options, self.project, service, options,
            self.toplevel_options, self.project_dir
        )

    def scale(self, options):
        """
        Set number of containers to run for a service.

        Numbers are specified in the form `service=num` as arguments.
        For example:

            $ docker-compose scale web=2 worker=3

        This command is deprecated. Use the up command with the `--scale` flag
        instead.

        Usage: scale [options] [SERVICE=NUM...]

        Options:
          -t, --timeout TIMEOUT      Specify a shutdown timeout in seconds.
                                     (default: 10)
        """
        timeout = timeout_from_opts(options)

        if self.project.config_version == V2_2:
            raise UserError(
                'The scale command is incompatible with the v2.2 format. '
                'Use the up command with the --scale flag instead.'
            )
        else:
            log.warn(
                'The scale command is deprecated. '
                'Use the up command with the --scale flag instead.'
            )

        for service_name, num in parse_scale_args(options['SERVICE=NUM']).items():
            self.project.get_service(service_name).scale(num, timeout=timeout)

    def start(self, options):
        """
        Start existing containers.

        Usage: start [SERVICE...]
        """
        containers = self.project.start(service_names=options['SERVICE'])
        exit_if(not containers, 'No containers to start', 1)

    def stop(self, options):
        """
        Stop running containers without removing them.

        They can be started again with `docker-compose start`.

        Usage: stop [options] [SERVICE...]

        Options:
          -t, --timeout TIMEOUT      Specify a shutdown timeout in seconds.
                                     (default: 10)
        """
        timeout = timeout_from_opts(options)
        self.project.stop(service_names=options['SERVICE'], timeout=timeout)

    def restart(self, options):
        """
        Restart running containers.

        Usage: restart [options] [SERVICE...]

        Options:
          -t, --timeout TIMEOUT      Specify a shutdown timeout in seconds.
                                     (default: 10)
        """
        timeout = timeout_from_opts(options)
        containers = self.project.restart(service_names=options['SERVICE'], timeout=timeout)
        exit_if(not containers, 'No containers to restart', 1)

    def top(self, options):
        """
        Display the running processes

        Usage: top [SERVICE...]

        """
        containers = sorted(
            self.project.containers(service_names=options['SERVICE'], stopped=False) +
            self.project.containers(service_names=options['SERVICE'], one_off=OneOffFilter.only),
            key=attrgetter('name')
        )

        for idx, container in enumerate(containers):
            if idx > 0:
                print()

            top_data = self.project.client.top(container.name)
            headers = top_data.get("Titles")
            rows = []

            for process in top_data.get("Processes", []):
                rows.append(process)

            print(container.name)
            print(Formatter().table(headers, rows))

    def unpause(self, options):
        """
        Unpause services.

        Usage: unpause [SERVICE...]
        """
        containers = self.project.unpause(service_names=options['SERVICE'])
        exit_if(not containers, 'No containers to unpause', 1)

    def up(self, options):
        """
        Builds, (re)creates, starts, and attaches to containers for a service.

        Unless they are already running, this command also starts any linked services.

        The `docker-compose up` command aggregates the output of each container. When
        the command exits, all containers are stopped. Running `docker-compose up -d`
        starts the containers in the background and leaves them running.

        If there are existing containers for a service, and the service's configuration
        or image was changed after the container's creation, `docker-compose up` picks
        up the changes by stopping and recreating the containers (preserving mounted
        volumes). To prevent Compose from picking up changes, use the `--no-recreate`
        flag.

        If you want to force Compose to stop and recreate all containers, use the
        `--force-recreate` flag.

        Usage: up [options] [--scale SERVICE=NUM...] [SERVICE...]

        Options:
            -d, --detach               Detached mode: Run containers in the background,
                                       print new container names. Incompatible with
                                       --abort-on-container-exit.
            --no-color                 Produce monochrome output.
            --quiet-pull               Pull without printing progress information
            --no-deps                  Don't start linked services.
            --force-recreate           Recreate containers even if their configuration
                                       and image haven't changed.
            --always-recreate-deps     Recreate dependent containers.
                                       Incompatible with --no-recreate.
            --no-recreate              If containers already exist, don't recreate
                                       them. Incompatible with --force-recreate and -V.
            --no-build                 Don't build an image, even if it's missing.
            --no-start                 Don't start the services after creating them.
            --build                    Build images before starting containers.
            --abort-on-container-exit  Stops all containers if any container was
                                       stopped. Incompatible with -d.
            -t, --timeout TIMEOUT      Use this timeout in seconds for container
                                       shutdown when attached or when containers are
                                       already running. (default: 10)
            -V, --renew-anon-volumes   Recreate anonymous volumes instead of retrieving
                                       data from the previous containers.
            --remove-orphans           Remove containers for services not defined
                                       in the Compose file.
            --exit-code-from SERVICE   Return the exit code of the selected service
                                       container. Implies --abort-on-container-exit.
            --scale SERVICE=NUM        Scale SERVICE to NUM instances. Overrides the
                                       `scale` setting in the Compose file if present.
        """
        start_deps = not options['--no-deps']
        always_recreate_deps = options['--always-recreate-deps']
        exit_value_from = exitval_from_opts(options, self.project)
        cascade_stop = options['--abort-on-container-exit']
        service_names = options['SERVICE']
        timeout = timeout_from_opts(options)
        remove_orphans = options['--remove-orphans']
        detached = options.get('--detach')
        no_start = options.get('--no-start')

        if detached and (cascade_stop or exit_value_from):
            raise UserError("--abort-on-container-exit and -d cannot be combined.")

        environment = Environment.from_env_file(self.project_dir)
        ignore_orphans = environment.get_boolean('COMPOSE_IGNORE_ORPHANS')

        if ignore_orphans and remove_orphans:
            raise UserError("COMPOSE_IGNORE_ORPHANS and --remove-orphans cannot be combined.")

        opts = ['--detach', '--abort-on-container-exit', '--exit-code-from']
        for excluded in [x for x in opts if options.get(x) and no_start]:
            raise UserError('--no-start and {} cannot be combined.'.format(excluded))

        with up_shutdown_context(self.project, service_names, timeout, detached):
            warn_for_swarm_mode(self.project.client)

            def up(rebuild):
                return self.project.up(
                    service_names=service_names,
                    start_deps=start_deps,
                    strategy=convergence_strategy_from_opts(options),
                    do_build=build_action_from_opts(options),
                    timeout=timeout,
                    detached=detached,
                    remove_orphans=remove_orphans,
                    ignore_orphans=ignore_orphans,
                    scale_override=parse_scale_args(options['--scale']),
                    start=not no_start,
                    always_recreate_deps=always_recreate_deps,
                    reset_container_image=rebuild,
                    renew_anonymous_volumes=options.get('--renew-anon-volumes'),
                    silent=options.get('--quiet-pull'),
                )

            try:
                to_attach = up(False)
            except docker.errors.ImageNotFound as e:
                log.error(
                    "The image for the service you're trying to recreate has been removed. "
                    "If you continue, volume data could be lost. Consider backing up your data "
                    "before continuing.\n".format(e.explanation)
                )
                res = yesno("Continue with the new image? [yN]", False)
                if res is None or not res:
                    raise e

                to_attach = up(True)

            if detached or no_start:
                return

            attached_containers = filter_containers_to_service_names(to_attach, service_names)

            log_printer = log_printer_from_project(
                self.project,
                attached_containers,
                options['--no-color'],
                {'follow': True},
                cascade_stop,
                event_stream=self.project.events(service_names=service_names))
            print("Attaching to", list_containers(log_printer.containers))
            cascade_starter = log_printer.run()

            if cascade_stop:
                print("Aborting on container exit...")
                all_containers = self.project.containers(service_names=options['SERVICE'], stopped=True)
                exit_code = compute_exit_code(
                    exit_value_from, attached_containers, cascade_starter, all_containers
                )

                self.project.stop(service_names=service_names, timeout=timeout)
                if exit_value_from:
                    exit_code = compute_service_exit_code(exit_value_from, attached_containers)

                sys.exit(exit_code)

    @classmethod
    def version(cls, options):
        """
        Show version information

        Usage: version [--short]

        Options:
            --short     Shows only Compose's version number.
        """
        if options['--short']:
            print(__version__)
        else:
            print(get_version_info('full'))


def compute_service_exit_code(exit_value_from, attached_containers):
    candidates = list(filter(
        lambda c: c.service == exit_value_from,
        attached_containers))
    if not candidates:
        log.error(
            'No containers matching the spec "{0}" '
            'were run.'.format(exit_value_from)
        )
        return 2
    if len(candidates) > 1:
        exit_values = filter(
            lambda e: e != 0,
            [c.inspect()['State']['ExitCode'] for c in candidates]
        )

        return exit_values[0]
    return candidates[0].inspect()['State']['ExitCode']


def compute_exit_code(exit_value_from, attached_containers, cascade_starter, all_containers):
    exit_code = 0
    for e in all_containers:
        if (not e.is_running and cascade_starter == e.name):
            if not e.exit_code == 0:
                exit_code = e.exit_code
                break

    return exit_code


def convergence_strategy_from_opts(options):
    no_recreate = options['--no-recreate']
    force_recreate = options['--force-recreate']
    renew_anonymous_volumes = options.get('--renew-anon-volumes')
    if force_recreate and no_recreate:
        raise UserError("--force-recreate and --no-recreate cannot be combined.")

    if no_recreate and renew_anonymous_volumes:
        raise UserError('--no-recreate and --renew-anon-volumes cannot be combined.')

    if force_recreate or renew_anonymous_volumes:
        return ConvergenceStrategy.always

    if no_recreate:
        return ConvergenceStrategy.never

    return ConvergenceStrategy.changed


def timeout_from_opts(options):
    timeout = options.get('--timeout')
    return None if timeout is None else int(timeout)


def image_digests_for_project(project, allow_push=False):
    try:
        return get_image_digests(
            project,
            allow_push=allow_push
        )
    except MissingDigests as e:
        def list_images(images):
            return "\n".join("    {}".format(name) for name in sorted(images))

        paras = ["Some images are missing digests."]

        if e.needs_push:
            command_hint = (
                "Use `docker-compose push {}` to push them. "
                .format(" ".join(sorted(e.needs_push)))
            )
            paras += [
                "The following images can be pushed:",
                list_images(e.needs_push),
                command_hint,
            ]

        if e.needs_pull:
            command_hint = (
                "Use `docker-compose pull {}` to pull them. "
                .format(" ".join(sorted(e.needs_pull)))
            )

            paras += [
                "The following images need to be pulled:",
                list_images(e.needs_pull),
                command_hint,
            ]

        raise UserError("\n\n".join(paras))


def exitval_from_opts(options, project):
    exit_value_from = options.get('--exit-code-from')
    if exit_value_from:
        if not options.get('--abort-on-container-exit'):
            log.warn('using --exit-code-from implies --abort-on-container-exit')
            options['--abort-on-container-exit'] = True
        if exit_value_from not in [s.name for s in project.get_services()]:
            log.error('No service named "%s" was found in your compose file.',
                      exit_value_from)
            sys.exit(2)
    return exit_value_from


def image_type_from_opt(flag, value):
    if not value:
        return ImageType.none
    try:
        return ImageType[value]
    except KeyError:
        raise UserError("%s flag must be one of: all, local" % flag)


def build_action_from_opts(options):
    if options['--build'] and options['--no-build']:
        raise UserError("--build and --no-build can not be combined.")

    if options['--build']:
        return BuildAction.force

    if options['--no-build']:
        return BuildAction.skip

    return BuildAction.none


def build_one_off_container_options(options, detach, command):
    container_options = {
        'command': command,
        'tty': not (detach or options['-T'] or not sys.stdin.isatty()),
        'stdin_open': not detach,
        'detach': detach,
    }

    if options['-e']:
        container_options['environment'] = Environment.from_command_line(
            parse_environment(options['-e'])
        )

    if options['--label']:
        container_options['labels'] = parse_labels(options['--label'])

    if options.get('--entrypoint') is not None:
        container_options['entrypoint'] = (
            [""] if options['--entrypoint'] == '' else options['--entrypoint']
        )

    # Ensure that run command remains one-off (issue #6302)
    container_options['restart'] = None

    if options['--user']:
        container_options['user'] = options.get('--user')

    if not options['--service-ports']:
        container_options['ports'] = []

    if options['--publish']:
        container_options['ports'] = options.get('--publish')

    if options['--name']:
        container_options['name'] = options['--name']

    if options['--workdir']:
        container_options['working_dir'] = options['--workdir']

    if options['--volume']:
        volumes = [VolumeSpec.parse(i) for i in options['--volume']]
        container_options['volumes'] = volumes

    return container_options


def run_one_off_container(container_options, project, service, options, toplevel_options,
                          project_dir='.'):
    if not options['--no-deps']:
        deps = service.get_dependency_names()
        if deps:
            project.up(
                service_names=deps,
                start_deps=True,
                strategy=ConvergenceStrategy.never,
                rescale=False
            )

    project.initialize()

    container = service.create_container(
        quiet=True,
        one_off=True,
        **container_options)

    use_network_aliases = options['--use-aliases']

    if options.get('--detach'):
        service.start_container(container, use_network_aliases)
        print(container.name)
        return

    def remove_container(force=False):
        if options['--rm']:
            project.client.remove_container(container.id, force=True, v=True)

    environment = Environment.from_env_file(project_dir)
    use_cli = not environment.get_boolean('COMPOSE_INTERACTIVE_NO_CLI')

    signals.set_signal_handler_to_shutdown()
    signals.set_signal_handler_to_hang_up()
    try:
        try:
            if IS_WINDOWS_PLATFORM or use_cli:
                service.connect_container_to_networks(container, use_network_aliases)
                exit_code = call_docker(
                    ["start", "--attach", "--interactive", container.id],
                    toplevel_options
                )
            else:
                operation = RunOperation(
                    project.client,
                    container.id,
                    interactive=not options['-T'],
                    logs=False,
                )
                pty = PseudoTerminal(project.client, operation)
                sockets = pty.sockets()
                service.start_container(container, use_network_aliases)
                pty.start(sockets)
                exit_code = container.wait()
        except (signals.ShutdownException):
            project.client.stop(container.id)
            exit_code = 1
    except (signals.ShutdownException, signals.HangUpException):
        project.client.kill(container.id)
        remove_container(force=True)
        sys.exit(2)

    remove_container()
    sys.exit(exit_code)


def log_printer_from_project(
    project,
    containers,
    monochrome,
    log_args,
    cascade_stop=False,
    event_stream=None,
):
    return LogPrinter(
        containers,
        build_log_presenters(project.service_names, monochrome),
        event_stream or project.events(),
        cascade_stop=cascade_stop,
        log_args=log_args)


def filter_containers_to_service_names(containers, service_names):
    if not service_names:
        return containers

    return [
        container
        for container in containers if container.service in service_names
    ]


@contextlib.contextmanager
def up_shutdown_context(project, service_names, timeout, detached):
    if detached:
        yield
        return

    signals.set_signal_handler_to_shutdown()
    try:
        try:
            yield
        except signals.ShutdownException:
            print("Gracefully stopping... (press Ctrl+C again to force)")
            project.stop(service_names=service_names, timeout=timeout)
    except signals.ShutdownException:
        project.kill(service_names=service_names)
        sys.exit(2)


def list_containers(containers):
    return ", ".join(c.name for c in containers)


def exit_if(condition, message, exit_code):
    if condition:
        log.error(message)
        raise SystemExit(exit_code)


def call_docker(args, dockeropts):
    executable_path = find_executable('docker')
    if not executable_path:
        raise UserError(errors.docker_not_found_msg("Couldn't find `docker` binary."))

    tls = dockeropts.get('--tls', False)
    ca_cert = dockeropts.get('--tlscacert')
    cert = dockeropts.get('--tlscert')
    key = dockeropts.get('--tlskey')
    verify = dockeropts.get('--tlsverify')
    host = dockeropts.get('--host')
    tls_options = []
    if tls:
        tls_options.append('--tls')
    if ca_cert:
        tls_options.extend(['--tlscacert', ca_cert])
    if cert:
        tls_options.extend(['--tlscert', cert])
    if key:
        tls_options.extend(['--tlskey', key])
    if verify:
        tls_options.append('--tlsverify')
    if host:
        tls_options.extend(
            ['--host', re.sub(r'^https?://', 'tcp://', host.lstrip('='))]
        )

    args = [executable_path] + tls_options + args
    log.debug(" ".join(map(pipes.quote, args)))

    return subprocess.call(args)


def parse_scale_args(options):
    res = {}
    for s in options:
        if '=' not in s:
            raise UserError('Arguments to scale should be in the form service=num')
        service_name, num = s.split('=', 1)
        try:
            num = int(num)
        except ValueError:
            raise UserError(
                'Number of containers for service "%s" is not a number' % service_name
            )
        res[service_name] = num
    return res


def build_exec_command(options, container_id, command):
    args = ["exec"]

    if options["--detach"]:
        args += ["--detach"]
    else:
        args += ["--interactive"]

    if not options["-T"]:
        args += ["--tty"]

    if options["--privileged"]:
        args += ["--privileged"]

    if options["--user"]:
        args += ["--user", options["--user"]]

    if options["--env"]:
        for env_variable in options["--env"]:
            args += ["--env", env_variable]

    if options["--workdir"]:
        args += ["--workdir", options["--workdir"]]

    args += [container_id]
    args += command
    return args


def has_container_with_state(containers, state):
    states = {
        'running': lambda c: c.is_running,
        'stopped': lambda c: not c.is_running,
        'paused': lambda c: c.is_paused,
        'restarting': lambda c: c.is_restarting,
    }
    for container in containers:
        if state not in states:
            raise UserError("Invalid state: %s" % state)
        if states[state](container):
            return True


def filter_services(filt, services, project):
    def should_include(service):
        for f in filt:
            if f == 'status':
                state = filt[f]
                containers = project.containers([service.name], stopped=True)
                if not has_container_with_state(containers, state):
                    return False
            elif f == 'source':
                source = filt[f]
                if source == 'image' or source == 'build':
                    if source not in service.options:
                        return False
                else:
                    raise UserError("Invalid value for source filter: %s" % source)
            else:
                raise UserError("Invalid filter: %s" % f)
        return True

    return filter(should_include, services)


def build_filter(arg):
    filt = {}
    if arg is not None:
        if '=' not in arg:
            raise UserError("Arguments to --filter should be in form KEY=VAL")
        key, val = arg.split('=', 1)
        filt[key] = val
    return filt


def warn_for_swarm_mode(client):
    info = client.info()
    if info.get('Swarm', {}).get('LocalNodeState') == 'active':
        if info.get('ServerVersion', '').startswith('ucp'):
            # UCP does multi-node scheduling with traditional Compose files.
            return

        log.warn(
            "The Docker Engine you're using is running in swarm mode.\n\n"
            "Compose does not use swarm mode to deploy services to multiple nodes in a swarm. "
            "All containers will be scheduled on the current node.\n\n"
            "To deploy your application across the swarm, "
            "use `docker stack deploy`.\n"
        )
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from inspect import getdoc

from docopt import docopt
from docopt import DocoptExit


def docopt_full_help(docstring, *args, **kwargs):
    try:
        return docopt(docstring, *args, **kwargs)
    except DocoptExit:
        raise SystemExit(docstring)


class DocoptDispatcher(object):

    def __init__(self, command_class, options):
        self.command_class = command_class
        self.options = options

    def parse(self, argv):
        command_help = getdoc(self.command_class)
        options = docopt_full_help(command_help, argv, **self.options)
        command = options['COMMAND']

        if command is None:
            raise SystemExit(command_help)

        handler = get_handler(self.command_class, command)
        docstring = getdoc(handler)

        if docstring is None:
            raise NoSuchCommand(command, self)

        command_options = docopt_full_help(docstring, options['ARGS'], options_first=True)
        return options, handler, command_options


def get_handler(command_class, command):
    command = command.replace('-', '_')
    # we certainly want to have "exec" command, since that's what docker client has
    # but in python exec is a keyword
    if command == "exec":
        command = "exec_command"

    if not hasattr(command_class, command):
        raise NoSuchCommand(command, command_class)

    return getattr(command_class, command)


class NoSuchCommand(Exception):
    def __init__(self, command, supercommand):
        super(NoSuchCommand, self).__init__("No such command: %s" % command)

        self.command = command
        self.supercommand = supercommand
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import signal

from ..const import IS_WINDOWS_PLATFORM


class ShutdownException(Exception):
    pass


class HangUpException(Exception):
    pass


def shutdown(signal, frame):
    raise ShutdownException()


def set_signal_handler(handler):
    signal.signal(signal.SIGINT, handler)
    signal.signal(signal.SIGTERM, handler)


def set_signal_handler_to_shutdown():
    set_signal_handler(shutdown)


def hang_up(signal, frame):
    raise HangUpException()


def set_signal_handler_to_hang_up():
    # on Windows a ValueError will be raised if trying to set signal handler for SIGHUP
    if not IS_WINDOWS_PLATFORM:
        signal.signal(signal.SIGHUP, hang_up)


def ignore_sigpipe():
    # Restore default behavior for SIGPIPE instead of raising
    # an exception when encountered.
    if not IS_WINDOWS_PLATFORM:
        signal.signal(signal.SIGPIPE, signal.SIG_DFL)
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from ..const import IS_WINDOWS_PLATFORM

NAMES = [
    'grey',
    'red',
    'green',
    'yellow',
    'blue',
    'magenta',
    'cyan',
    'white'
]


def get_pairs():
    for i, name in enumerate(NAMES):
        yield(name, str(30 + i))
        yield('intense_' + name, str(30 + i) + ';1')


def ansi(code):
    return '\033[{0}m'.format(code)


def ansi_color(code, s):
    return '{0}{1}{2}'.format(ansi(code), s, ansi(0))


def make_color_fn(code):
    return lambda s: ansi_color(code, s)


if IS_WINDOWS_PLATFORM:
    import colorama
    colorama.init(strip=False)
for (name, code) in get_pairs():
    globals()[name] = make_color_fn(code)


def rainbow():
    cs = ['cyan', 'yellow', 'green', 'magenta', 'red', 'blue',
          'intense_cyan', 'intense_yellow', 'intense_green',
          'intense_magenta', 'intense_red', 'intense_blue']

    for c in cs:
        yield globals()[c]
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import sys
from collections import namedtuple
from itertools import cycle
from threading import Thread

from docker.errors import APIError
from six.moves import _thread as thread
from six.moves.queue import Empty
from six.moves.queue import Queue

from . import colors
from compose import utils
from compose.cli.signals import ShutdownException
from compose.utils import split_buffer


class LogPresenter(object):

    def __init__(self, prefix_width, color_func):
        self.prefix_width = prefix_width
        self.color_func = color_func

    def present(self, container, line):
        prefix = container.name_without_project.ljust(self.prefix_width)
        return '{prefix} {line}'.format(
            prefix=self.color_func(prefix + ' |'),
            line=line)


def build_log_presenters(service_names, monochrome):
    """Return an iterable of functions.

    Each function can be used to format the logs output of a container.
    """
    prefix_width = max_name_width(service_names)

    def no_color(text):
        return text

    for color_func in cycle([no_color] if monochrome else colors.rainbow()):
        yield LogPresenter(prefix_width, color_func)


def max_name_width(service_names, max_index_width=3):
    """Calculate the maximum width of container names so we can make the log
    prefixes line up like so:

    db_1  | Listening
    web_1 | Listening
    """
    return max(len(name) for name in service_names) + max_index_width


class LogPrinter(object):
    """Print logs from many containers to a single output stream."""

    def __init__(self,
                 containers,
                 presenters,
                 event_stream,
                 output=sys.stdout,
                 cascade_stop=False,
                 log_args=None):
        self.containers = containers
        self.presenters = presenters
        self.event_stream = event_stream
        self.output = utils.get_output_stream(output)
        self.cascade_stop = cascade_stop
        self.log_args = log_args or {}

    def run(self):
        if not self.containers:
            return

        queue = Queue()
        thread_args = queue, self.log_args
        thread_map = build_thread_map(self.containers, self.presenters, thread_args)
        start_producer_thread((
            thread_map,
            self.event_stream,
            self.presenters,
            thread_args))

        for line in consume_queue(queue, self.cascade_stop):
            remove_stopped_threads(thread_map)

            if self.cascade_stop:
                matching_container = [cont.name for cont in self.containers if cont.name == line]
                if line in matching_container:
                    # Returning the name of the container that started the
                    # the cascade_stop so we can return the correct exit code
                    return line

            if not line:
                if not thread_map:
                    # There are no running containers left to tail, so exit
                    return
                # We got an empty line because of a timeout, but there are still
                # active containers to tail, so continue
                continue

            self.write(line)

    def write(self, line):
        try:
            self.output.write(line)
        except UnicodeEncodeError:
            # This may happen if the user's locale settings don't support UTF-8
            # and UTF-8 characters are present in the log line. The following
            # will output a "degraded" log with unsupported characters
            # replaced by `?`
            self.output.write(line.encode('ascii', 'replace').decode())
        self.output.flush()


def remove_stopped_threads(thread_map):
    for container_id, tailer_thread in list(thread_map.items()):
        if not tailer_thread.is_alive():
            thread_map.pop(container_id, None)


def build_thread(container, presenter, queue, log_args):
    tailer = Thread(
        target=tail_container_logs,
        args=(container, presenter, queue, log_args))
    tailer.daemon = True
    tailer.start()
    return tailer


def build_thread_map(initial_containers, presenters, thread_args):
    return {
        container.id: build_thread(container, next(presenters), *thread_args)
        for container in initial_containers
    }


class QueueItem(namedtuple('_QueueItem', 'item is_stop exc')):

    @classmethod
    def new(cls, item):
        return cls(item, None, None)

    @classmethod
    def exception(cls, exc):
        return cls(None, None, exc)

    @classmethod
    def stop(cls, item=None):
        return cls(item, True, None)


def tail_container_logs(container, presenter, queue, log_args):
    generator = get_log_generator(container)

    try:
        for item in generator(container, log_args):
            queue.put(QueueItem.new(presenter.present(container, item)))
    except Exception as e:
        queue.put(QueueItem.exception(e))
        return
    if log_args.get('follow'):
        queue.put(QueueItem.new(presenter.color_func(wait_on_exit(container))))
    queue.put(QueueItem.stop(container.name))


def get_log_generator(container):
    if container.has_api_logs:
        return build_log_generator
    return build_no_log_generator


def build_no_log_generator(container, log_args):
    """Return a generator that prints a warning about logs and waits for
    container to exit.
    """
    yield "WARNING: no logs are available with the '{}' log driver\n".format(
        container.log_driver)


def build_log_generator(container, log_args):
    # if the container doesn't have a log_stream we need to attach to container
    # before log printer starts running
    if container.log_stream is None:
        stream = container.logs(stdout=True, stderr=True, stream=True, **log_args)
    else:
        stream = container.log_stream

    return split_buffer(stream)


def wait_on_exit(container):
    try:
        exit_code = container.wait()
        return "%s exited with code %s\n" % (container.name, exit_code)
    except APIError as e:
        return "Unexpected API error for %s (HTTP code %s)\nResponse body:\n%s\n" % (
            container.name, e.response.status_code,
            e.response.text or '[empty]'
        )


def start_producer_thread(thread_args):
    producer = Thread(target=watch_events, args=thread_args)
    producer.daemon = True
    producer.start()


def watch_events(thread_map, event_stream, presenters, thread_args):
    crashed_containers = set()
    for event in event_stream:
        if event['action'] == 'stop':
            thread_map.pop(event['id'], None)

        if event['action'] == 'die':
            thread_map.pop(event['id'], None)
            crashed_containers.add(event['id'])

        if event['action'] != 'start':
            continue

        if event['id'] in thread_map:
            if thread_map[event['id']].is_alive():
                continue
            # Container was stopped and started, we need a new thread
            thread_map.pop(event['id'], None)

        # Container crashed so we should reattach to it
        if event['id'] in crashed_containers:
            event['container'].attach_log_stream()
            crashed_containers.remove(event['id'])

        thread_map[event['id']] = build_thread(
            event['container'],
            next(presenters),
            *thread_args)


def consume_queue(queue, cascade_stop):
    """Consume the queue by reading lines off of it and yielding them."""
    while True:
        try:
            item = queue.get(timeout=0.1)
        except Empty:
            yield None
            continue
        # See https://github.com/docker/compose/issues/189
        except thread.error:
            raise ShutdownException()

        if item.exc:
            raise item.exc

        if item.is_stop and not cascade_stop:
            continue

        yield item.item
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals


VERSION_EXPLANATION = (
    'You might be seeing this error because you\'re using the wrong Compose file version. '
    'Either specify a supported version (e.g "2.2" or "3.3") and place '
    'your service definitions under the `services` key, or omit the `version` key '
    'and place your service definitions at the root of the file to use '
    'version 1.\nFor more on the Compose file format versions, see '
    'https://docs.docker.com/compose/compose-file/')


class ConfigurationError(Exception):
    def __init__(self, msg):
        self.msg = msg

    def __str__(self):
        return self.msg


class DependencyError(ConfigurationError):
    pass


class CircularReference(ConfigurationError):
    def __init__(self, trail):
        self.trail = trail

    @property
    def msg(self):
        lines = [
            "{} in {}".format(service_name, filename)
            for (filename, service_name) in self.trail
        ]
        return "Circular reference:\n  {}".format("\n  extends ".join(lines))


class ComposeFileNotFound(ConfigurationError):
    def __init__(self, supported_filenames):
        super(ComposeFileNotFound, self).__init__("""
        Can't find a suitable configuration file in this directory or any
        parent. Are you in the right directory?

        Supported filenames: %s
        """ % ", ".join(supported_filenames))


class DuplicateOverrideFileFound(ConfigurationError):
    def __init__(self, override_filenames):
        self.override_filenames = override_filenames
        super(DuplicateOverrideFileFound, self).__init__(
            "Multiple override files found: {}. You may only use a single "
            "override file.".format(", ".join(override_filenames))
        )
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging
import re
from string import Template

import six

from .errors import ConfigurationError
from compose.const import COMPOSEFILE_V2_0 as V2_0
from compose.utils import parse_bytes
from compose.utils import parse_nanoseconds_int


log = logging.getLogger(__name__)


class Interpolator(object):

    def __init__(self, templater, mapping):
        self.templater = templater
        self.mapping = mapping

    def interpolate(self, string):
        try:
            return self.templater(string).substitute(self.mapping)
        except ValueError:
            raise InvalidInterpolation(string)


def interpolate_environment_variables(version, config, section, environment):
    if version <= V2_0:
        interpolator = Interpolator(Template, environment)
    else:
        interpolator = Interpolator(TemplateWithDefaults, environment)

    def process_item(name, config_dict):
        return dict(
            (key, interpolate_value(name, key, val, section, interpolator))
            for key, val in (config_dict or {}).items()
        )

    return dict(
        (name, process_item(name, config_dict or {}))
        for name, config_dict in config.items()
    )


def get_config_path(config_key, section, name):
    return '{}/{}/{}'.format(section, name, config_key)


def interpolate_value(name, config_key, value, section, interpolator):
    try:
        return recursive_interpolate(value, interpolator, get_config_path(config_key, section, name))
    except InvalidInterpolation as e:
        raise ConfigurationError(
            'Invalid interpolation format for "{config_key}" option '
            'in {section} "{name}": "{string}"'.format(
                config_key=config_key,
                name=name,
                section=section,
                string=e.string))
    except UnsetRequiredSubstitution as e:
        raise ConfigurationError(
            'Missing mandatory value for "{config_key}" option in {section} "{name}": {err}'.format(
                config_key=config_key,
                name=name,
                section=section,
                err=e.err
            )
        )


def recursive_interpolate(obj, interpolator, config_path):
    def append(config_path, key):
        return '{}/{}'.format(config_path, key)

    if isinstance(obj, six.string_types):
        return converter.convert(config_path, interpolator.interpolate(obj))
    if isinstance(obj, dict):
        return dict(
            (key, recursive_interpolate(val, interpolator, append(config_path, key)))
            for (key, val) in obj.items()
        )
    if isinstance(obj, list):
        return [recursive_interpolate(val, interpolator, config_path) for val in obj]
    return converter.convert(config_path, obj)


class TemplateWithDefaults(Template):
    pattern = r"""
        %(delim)s(?:
            (?P<escaped>%(delim)s) |
            (?P<named>%(id)s)      |
            {(?P<braced>%(bid)s)}  |
            (?P<invalid>)
        )
        """ % {
        'delim': re.escape('$'),
        'id': r'[_a-z][_a-z0-9]*',
        'bid': r'[_a-z][_a-z0-9]*(?:(?P<sep>:?[-?])[^}]*)?',
    }

    @staticmethod
    def process_braced_group(braced, sep, mapping):
        if ':-' == sep:
            var, _, default = braced.partition(':-')
            return mapping.get(var) or default
        elif '-' == sep:
            var, _, default = braced.partition('-')
            return mapping.get(var, default)

        elif ':?' == sep:
            var, _, err = braced.partition(':?')
            result = mapping.get(var)
            if not result:
                raise UnsetRequiredSubstitution(err)
            return result
        elif '?' == sep:
            var, _, err = braced.partition('?')
            if var in mapping:
                return mapping.get(var)
            raise UnsetRequiredSubstitution(err)

    # Modified from python2.7/string.py
    def substitute(self, mapping):
        # Helper function for .sub()

        def convert(mo):
            named = mo.group('named') or mo.group('braced')
            braced = mo.group('braced')
            if braced is not None:
                sep = mo.group('sep')
                if sep:
                    return self.process_braced_group(braced, sep, mapping)

            if named is not None:
                val = mapping[named]
                if isinstance(val, six.binary_type):
                    val = val.decode('utf-8')
                return '%s' % (val,)
            if mo.group('escaped') is not None:
                return self.delimiter
            if mo.group('invalid') is not None:
                self._invalid(mo)
            raise ValueError('Unrecognized named group in pattern',
                             self.pattern)
        return self.pattern.sub(convert, self.template)


class InvalidInterpolation(Exception):
    def __init__(self, string):
        self.string = string


class UnsetRequiredSubstitution(Exception):
    def __init__(self, custom_err_msg):
        self.err = custom_err_msg


PATH_JOKER = '[^/]+'
FULL_JOKER = '.+'


def re_path(*args):
    return re.compile('^{}$'.format('/'.join(args)))


def re_path_basic(section, name):
    return re_path(section, PATH_JOKER, name)


def service_path(*args):
    return re_path('service', PATH_JOKER, *args)


def to_boolean(s):
    if not isinstance(s, six.string_types):
        return s
    s = s.lower()
    if s in ['y', 'yes', 'true', 'on']:
        return True
    elif s in ['n', 'no', 'false', 'off']:
        return False
    raise ValueError('"{}" is not a valid boolean value'.format(s))


def to_int(s):
    if not isinstance(s, six.string_types):
        return s

    # We must be able to handle octal representation for `mode` values notably
    if six.PY3 and re.match('^0[0-9]+$', s.strip()):
        s = '0o' + s[1:]
    try:
        return int(s, base=0)
    except ValueError:
        raise ValueError('"{}" is not a valid integer'.format(s))


def to_float(s):
    if not isinstance(s, six.string_types):
        return s

    try:
        return float(s)
    except ValueError:
        raise ValueError('"{}" is not a valid float'.format(s))


def to_str(o):
    if isinstance(o, (bool, float, int)):
        return '{}'.format(o)
    return o


def bytes_to_int(s):
    v = parse_bytes(s)
    if v is None:
        raise ValueError('"{}" is not a valid byte value'.format(s))
    return v


def to_microseconds(v):
    if not isinstance(v, six.string_types):
        return v
    return int(parse_nanoseconds_int(v) / 1000)


class ConversionMap(object):
    map = {
        service_path('blkio_config', 'weight'): to_int,
        service_path('blkio_config', 'weight_device', 'weight'): to_int,
        service_path('build', 'labels', FULL_JOKER): to_str,
        service_path('cpus'): to_float,
        service_path('cpu_count'): to_int,
        service_path('cpu_quota'): to_microseconds,
        service_path('cpu_period'): to_microseconds,
        service_path('cpu_rt_period'): to_microseconds,
        service_path('cpu_rt_runtime'): to_microseconds,
        service_path('configs', 'mode'): to_int,
        service_path('secrets', 'mode'): to_int,
        service_path('healthcheck', 'retries'): to_int,
        service_path('healthcheck', 'disable'): to_boolean,
        service_path('deploy', 'labels', PATH_JOKER): to_str,
        service_path('deploy', 'replicas'): to_int,
        service_path('deploy', 'update_config', 'parallelism'): to_int,
        service_path('deploy', 'update_config', 'max_failure_ratio'): to_float,
        service_path('deploy', 'rollback_config', 'parallelism'): to_int,
        service_path('deploy', 'rollback_config', 'max_failure_ratio'): to_float,
        service_path('deploy', 'restart_policy', 'max_attempts'): to_int,
        service_path('mem_swappiness'): to_int,
        service_path('labels', FULL_JOKER): to_str,
        service_path('oom_kill_disable'): to_boolean,
        service_path('oom_score_adj'): to_int,
        service_path('ports', 'target'): to_int,
        service_path('ports', 'published'): to_int,
        service_path('scale'): to_int,
        service_path('ulimits', PATH_JOKER): to_int,
        service_path('ulimits', PATH_JOKER, 'soft'): to_int,
        service_path('ulimits', PATH_JOKER, 'hard'): to_int,
        service_path('privileged'): to_boolean,
        service_path('read_only'): to_boolean,
        service_path('stdin_open'): to_boolean,
        service_path('tty'): to_boolean,
        service_path('volumes', 'read_only'): to_boolean,
        service_path('volumes', 'volume', 'nocopy'): to_boolean,
        service_path('volumes', 'tmpfs', 'size'): bytes_to_int,
        re_path_basic('network', 'attachable'): to_boolean,
        re_path_basic('network', 'external'): to_boolean,
        re_path_basic('network', 'internal'): to_boolean,
        re_path('network', PATH_JOKER, 'labels', FULL_JOKER): to_str,
        re_path_basic('volume', 'external'): to_boolean,
        re_path('volume', PATH_JOKER, 'labels', FULL_JOKER): to_str,
        re_path_basic('secret', 'external'): to_boolean,
        re_path('secret', PATH_JOKER, 'labels', FULL_JOKER): to_str,
        re_path_basic('config', 'external'): to_boolean,
        re_path('config', PATH_JOKER, 'labels', FULL_JOKER): to_str,
    }

    def convert(self, path, value):
        for rexp in self.map.keys():
            if rexp.match(path):
                try:
                    return self.map[rexp](value)
                except ValueError as e:
                    raise ConfigurationError(
                        'Error while attempting to convert {} to appropriate type: {}'.format(
                            path.replace('/', '.'), e
                        )
                    )
        return value


converter = ConversionMap()
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import six
import yaml

from compose.config import types
from compose.const import COMPOSEFILE_V1 as V1
from compose.const import COMPOSEFILE_V2_1 as V2_1
from compose.const import COMPOSEFILE_V2_3 as V2_3
from compose.const import COMPOSEFILE_V3_0 as V3_0
from compose.const import COMPOSEFILE_V3_2 as V3_2
from compose.const import COMPOSEFILE_V3_4 as V3_4
from compose.const import COMPOSEFILE_V3_5 as V3_5


def serialize_config_type(dumper, data):
    representer = dumper.represent_str if six.PY3 else dumper.represent_unicode
    return representer(data.repr())


def serialize_dict_type(dumper, data):
    return dumper.represent_dict(data.repr())


def serialize_string(dumper, data):
    """ Ensure boolean-like strings are quoted in the output and escape $ characters """
    representer = dumper.represent_str if six.PY3 else dumper.represent_unicode

    if isinstance(data, six.binary_type):
        data = data.decode('utf-8')

    data = data.replace('$', '$$')

    if data.lower() in ('y', 'n', 'yes', 'no', 'on', 'off', 'true', 'false'):
        # Empirically only y/n appears to be an issue, but this might change
        # depending on which PyYaml version is being used. Err on safe side.
        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='"')
    return representer(data)


yaml.SafeDumper.add_representer(types.MountSpec, serialize_dict_type)
yaml.SafeDumper.add_representer(types.VolumeFromSpec, serialize_config_type)
yaml.SafeDumper.add_representer(types.VolumeSpec, serialize_config_type)
yaml.SafeDumper.add_representer(types.SecurityOpt, serialize_config_type)
yaml.SafeDumper.add_representer(types.ServiceSecret, serialize_dict_type)
yaml.SafeDumper.add_representer(types.ServiceConfig, serialize_dict_type)
yaml.SafeDumper.add_representer(types.ServicePort, serialize_dict_type)
yaml.SafeDumper.add_representer(str, serialize_string)
yaml.SafeDumper.add_representer(six.text_type, serialize_string)


def denormalize_config(config, image_digests=None):
    result = {'version': str(V2_1) if config.version == V1 else str(config.version)}
    denormalized_services = [
        denormalize_service_dict(
            service_dict,
            config.version,
            image_digests[service_dict['name']] if image_digests else None)
        for service_dict in config.services
    ]
    result['services'] = {
        service_dict.pop('name'): service_dict
        for service_dict in denormalized_services
    }

    for key in ('networks', 'volumes', 'secrets', 'configs'):
        config_dict = getattr(config, key)
        if not config_dict:
            continue
        result[key] = config_dict.copy()
        for name, conf in result[key].items():
            if 'external_name' in conf:
                del conf['external_name']

            if 'name' in conf:
                if config.version < V2_1 or (
                        config.version >= V3_0 and config.version < v3_introduced_name_key(key)):
                    del conf['name']
                elif 'external' in conf:
                    conf['external'] = bool(conf['external'])

            if 'attachable' in conf and config.version < V3_2:
                # For compatibility mode, this option is invalid in v2
                del conf['attachable']

    return result


def v3_introduced_name_key(key):
    if key == 'volumes':
        return V3_4
    return V3_5


def serialize_config(config, image_digests=None):
    return yaml.safe_dump(
        denormalize_config(config, image_digests),
        default_flow_style=False,
        indent=2,
        width=80,
        allow_unicode=True
    )


def serialize_ns_time_value(value):
    result = (value, 'ns')
    table = [
        (1000., 'us'),
        (1000., 'ms'),
        (1000., 's'),
        (60., 'm'),
        (60., 'h')
    ]
    for stage in table:
        tmp = value / stage[0]
        if tmp == int(value / stage[0]):
            value = tmp
            result = (int(value), stage[1])
        else:
            break
    return '{0}{1}'.format(*result)


def denormalize_service_dict(service_dict, version, image_digest=None):
    service_dict = service_dict.copy()

    if image_digest:
        service_dict['image'] = image_digest

    if 'restart' in service_dict:
        service_dict['restart'] = types.serialize_restart_spec(
            service_dict['restart']
        )

    if version == V1 and 'network_mode' not in service_dict:
        service_dict['network_mode'] = 'bridge'

    if 'depends_on' in service_dict and (version < V2_1 or version >= V3_0):
        service_dict['depends_on'] = sorted([
            svc for svc in service_dict['depends_on'].keys()
        ])

    if 'healthcheck' in service_dict:
        if 'interval' in service_dict['healthcheck']:
            service_dict['healthcheck']['interval'] = serialize_ns_time_value(
                service_dict['healthcheck']['interval']
            )
        if 'timeout' in service_dict['healthcheck']:
            service_dict['healthcheck']['timeout'] = serialize_ns_time_value(
                service_dict['healthcheck']['timeout']
            )

        if 'start_period' in service_dict['healthcheck']:
            service_dict['healthcheck']['start_period'] = serialize_ns_time_value(
                service_dict['healthcheck']['start_period']
            )

    if 'ports' in service_dict:
        service_dict['ports'] = [
            p.legacy_repr() if p.external_ip or version < V3_2 else p
            for p in service_dict['ports']
        ]
    if 'volumes' in service_dict and (version < V2_3 or (version > V3_0 and version < V3_2)):
        service_dict['volumes'] = [
            v.legacy_repr() if isinstance(v, types.MountSpec) else v for v in service_dict['volumes']
        ]

    return service_dict
<EOF>
<BOF>
# flake8: noqa
from __future__ import absolute_import
from __future__ import unicode_literals

from . import environment
from .config import ConfigurationError
from .config import DOCKER_CONFIG_KEYS
from .config import find
from .config import is_url
from .config import load
from .config import merge_environment
from .config import merge_labels
from .config import parse_environment
from .config import parse_labels
from .config import resolve_build_args
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import functools
import io
import logging
import os
import string
import sys
from collections import namedtuple

import six
import yaml
from cached_property import cached_property

from . import types
from .. import const
from ..const import COMPOSEFILE_V1 as V1
from ..const import COMPOSEFILE_V2_1 as V2_1
from ..const import COMPOSEFILE_V2_3 as V2_3
from ..const import COMPOSEFILE_V3_0 as V3_0
from ..const import COMPOSEFILE_V3_4 as V3_4
from ..utils import build_string_dict
from ..utils import json_hash
from ..utils import parse_bytes
from ..utils import parse_nanoseconds_int
from ..utils import splitdrive
from ..version import ComposeVersion
from .environment import env_vars_from_file
from .environment import Environment
from .environment import split_env
from .errors import CircularReference
from .errors import ComposeFileNotFound
from .errors import ConfigurationError
from .errors import DuplicateOverrideFileFound
from .errors import VERSION_EXPLANATION
from .interpolation import interpolate_environment_variables
from .sort_services import get_container_name_from_network_mode
from .sort_services import get_service_name_from_network_mode
from .sort_services import sort_service_dicts
from .types import MountSpec
from .types import parse_extra_hosts
from .types import parse_restart_spec
from .types import SecurityOpt
from .types import ServiceLink
from .types import ServicePort
from .types import VolumeFromSpec
from .types import VolumeSpec
from .validation import match_named_volumes
from .validation import validate_against_config_schema
from .validation import validate_config_section
from .validation import validate_cpu
from .validation import validate_depends_on
from .validation import validate_extends_file_path
from .validation import validate_healthcheck
from .validation import validate_links
from .validation import validate_network_mode
from .validation import validate_pid_mode
from .validation import validate_service_constraints
from .validation import validate_top_level_object
from .validation import validate_ulimits


DOCKER_CONFIG_KEYS = [
    'cap_add',
    'cap_drop',
    'cgroup_parent',
    'command',
    'cpu_count',
    'cpu_percent',
    'cpu_period',
    'cpu_quota',
    'cpu_rt_period',
    'cpu_rt_runtime',
    'cpu_shares',
    'cpus',
    'cpuset',
    'detach',
    'device_cgroup_rules',
    'devices',
    'dns',
    'dns_search',
    'dns_opt',
    'domainname',
    'entrypoint',
    'env_file',
    'environment',
    'extra_hosts',
    'group_add',
    'hostname',
    'healthcheck',
    'image',
    'ipc',
    'isolation',
    'labels',
    'links',
    'mac_address',
    'mem_limit',
    'mem_reservation',
    'memswap_limit',
    'mem_swappiness',
    'net',
    'oom_score_adj',
    'oom_kill_disable',
    'pid',
    'ports',
    'privileged',
    'read_only',
    'restart',
    'runtime',
    'secrets',
    'security_opt',
    'shm_size',
    'pids_limit',
    'stdin_open',
    'stop_signal',
    'sysctls',
    'tty',
    'user',
    'userns_mode',
    'volume_driver',
    'volumes',
    'volumes_from',
    'working_dir',
]

ALLOWED_KEYS = DOCKER_CONFIG_KEYS + [
    'blkio_config',
    'build',
    'container_name',
    'credential_spec',
    'dockerfile',
    'init',
    'log_driver',
    'log_opt',
    'logging',
    'network_mode',
    'platform',
    'scale',
    'stop_grace_period',
]

DOCKER_VALID_URL_PREFIXES = (
    'http://',
    'https://',
    'git://',
    'github.com/',
    'git@',
)

SUPPORTED_FILENAMES = [
    'docker-compose.yml',
    'docker-compose.yaml',
]

DEFAULT_OVERRIDE_FILENAMES = ('docker-compose.override.yml', 'docker-compose.override.yaml')


log = logging.getLogger(__name__)


class ConfigDetails(namedtuple('_ConfigDetails', 'working_dir config_files environment')):
    """
    :param working_dir: the directory to use for relative paths in the config
    :type  working_dir: string
    :param config_files: list of configuration files to load
    :type  config_files: list of :class:`ConfigFile`
    :param environment: computed environment values for this project
    :type  environment: :class:`environment.Environment`
     """
    def __new__(cls, working_dir, config_files, environment=None):
        if environment is None:
            environment = Environment.from_env_file(working_dir)
        return super(ConfigDetails, cls).__new__(
            cls, working_dir, config_files, environment
        )


class ConfigFile(namedtuple('_ConfigFile', 'filename config')):
    """
    :param filename: filename of the config file
    :type  filename: string
    :param config: contents of the config file
    :type  config: :class:`dict`
    """

    @classmethod
    def from_filename(cls, filename):
        return cls(filename, load_yaml(filename))

    @cached_property
    def version(self):
        if 'version' not in self.config:
            return V1

        version = self.config['version']

        if isinstance(version, dict):
            log.warn('Unexpected type for "version" key in "{}". Assuming '
                     '"version" is the name of a service, and defaulting to '
                     'Compose file version 1.'.format(self.filename))
            return V1

        if not isinstance(version, six.string_types):
            raise ConfigurationError(
                'Version in "{}" is invalid - it should be a string.'
                .format(self.filename))

        if version == '1':
            raise ConfigurationError(
                'Version in "{}" is invalid. {}'
                .format(self.filename, VERSION_EXPLANATION)
            )

        if version == '2':
            return const.COMPOSEFILE_V2_0

        if version == '3':
            return const.COMPOSEFILE_V3_0

        return ComposeVersion(version)

    def get_service(self, name):
        return self.get_service_dicts()[name]

    def get_service_dicts(self):
        return self.config if self.version == V1 else self.config.get('services', {})

    def get_volumes(self):
        return {} if self.version == V1 else self.config.get('volumes', {})

    def get_networks(self):
        return {} if self.version == V1 else self.config.get('networks', {})

    def get_secrets(self):
        return {} if self.version < const.COMPOSEFILE_V3_1 else self.config.get('secrets', {})

    def get_configs(self):
        return {} if self.version < const.COMPOSEFILE_V3_3 else self.config.get('configs', {})


class Config(namedtuple('_Config', 'version services volumes networks secrets configs')):
    """
    :param version: configuration version
    :type  version: int
    :param services: List of service description dictionaries
    :type  services: :class:`list`
    :param volumes: Dictionary mapping volume names to description dictionaries
    :type  volumes: :class:`dict`
    :param networks: Dictionary mapping network names to description dictionaries
    :type  networks: :class:`dict`
    :param secrets: Dictionary mapping secret names to description dictionaries
    :type secrets: :class:`dict`
    :param configs: Dictionary mapping config names to description dictionaries
    :type configs: :class:`dict`
    """


class ServiceConfig(namedtuple('_ServiceConfig', 'working_dir filename name config')):

    @classmethod
    def with_abs_paths(cls, working_dir, filename, name, config):
        if not working_dir:
            raise ValueError("No working_dir for ServiceConfig.")

        return cls(
            os.path.abspath(working_dir),
            os.path.abspath(filename) if filename else filename,
            name,
            config)


def find(base_dir, filenames, environment, override_dir=None):
    if filenames == ['-']:
        return ConfigDetails(
            os.path.abspath(override_dir) if override_dir else os.getcwd(),
            [ConfigFile(None, yaml.safe_load(sys.stdin))],
            environment
        )

    if filenames:
        filenames = [os.path.join(base_dir, f) for f in filenames]
    else:
        filenames = get_default_config_files(base_dir)

    log.debug("Using configuration files: {}".format(",".join(filenames)))
    return ConfigDetails(
        override_dir if override_dir else os.path.dirname(filenames[0]),
        [ConfigFile.from_filename(f) for f in filenames],
        environment
    )


def validate_config_version(config_files):
    main_file = config_files[0]
    validate_top_level_object(main_file)
    for next_file in config_files[1:]:
        validate_top_level_object(next_file)

        if main_file.version != next_file.version:
            raise ConfigurationError(
                "Version mismatch: file {0} specifies version {1} but "
                "extension file {2} uses version {3}".format(
                    main_file.filename,
                    main_file.version,
                    next_file.filename,
                    next_file.version))


def get_default_config_files(base_dir):
    (candidates, path) = find_candidates_in_parent_dirs(SUPPORTED_FILENAMES, base_dir)

    if not candidates:
        raise ComposeFileNotFound(SUPPORTED_FILENAMES)

    winner = candidates[0]

    if len(candidates) > 1:
        log.warn("Found multiple config files with supported names: %s", ", ".join(candidates))
        log.warn("Using %s\n", winner)

    return [os.path.join(path, winner)] + get_default_override_file(path)


def get_default_override_file(path):
    override_files_in_path = [os.path.join(path, override_filename) for override_filename
                              in DEFAULT_OVERRIDE_FILENAMES
                              if os.path.exists(os.path.join(path, override_filename))]
    if len(override_files_in_path) > 1:
        raise DuplicateOverrideFileFound(override_files_in_path)
    return override_files_in_path


def find_candidates_in_parent_dirs(filenames, path):
    """
    Given a directory path to start, looks for filenames in the
    directory, and then each parent directory successively,
    until found.

    Returns tuple (candidates, path).
    """
    candidates = [filename for filename in filenames
                  if os.path.exists(os.path.join(path, filename))]

    if not candidates:
        parent_dir = os.path.join(path, '..')
        if os.path.abspath(parent_dir) != os.path.abspath(path):
            return find_candidates_in_parent_dirs(filenames, parent_dir)

    return (candidates, path)


def check_swarm_only_config(service_dicts, compatibility=False):
    warning_template = (
        "Some services ({services}) use the '{key}' key, which will be ignored. "
        "Compose does not support '{key}' configuration - use "
        "`docker stack deploy` to deploy to a swarm."
    )

    def check_swarm_only_key(service_dicts, key):
        services = [s for s in service_dicts if s.get(key)]
        if services:
            log.warn(
                warning_template.format(
                    services=", ".join(sorted(s['name'] for s in services)),
                    key=key
                )
            )
    if not compatibility:
        check_swarm_only_key(service_dicts, 'deploy')
    check_swarm_only_key(service_dicts, 'credential_spec')
    check_swarm_only_key(service_dicts, 'configs')


def load(config_details, compatibility=False):
    """Load the configuration from a working directory and a list of
    configuration files.  Files are loaded in order, and merged on top
    of each other to create the final configuration.

    Return a fully interpolated, extended and validated configuration.
    """
    validate_config_version(config_details.config_files)

    processed_files = [
        process_config_file(config_file, config_details.environment)
        for config_file in config_details.config_files
    ]
    config_details = config_details._replace(config_files=processed_files)

    main_file = config_details.config_files[0]
    volumes = load_mapping(
        config_details.config_files, 'get_volumes', 'Volume'
    )
    networks = load_mapping(
        config_details.config_files, 'get_networks', 'Network'
    )
    secrets = load_mapping(
        config_details.config_files, 'get_secrets', 'Secret', config_details.working_dir
    )
    configs = load_mapping(
        config_details.config_files, 'get_configs', 'Config', config_details.working_dir
    )
    service_dicts = load_services(config_details, main_file, compatibility)

    if main_file.version != V1:
        for service_dict in service_dicts:
            match_named_volumes(service_dict, volumes)

    check_swarm_only_config(service_dicts, compatibility)

    version = V2_3 if compatibility and main_file.version >= V3_0 else main_file.version

    return Config(version, service_dicts, volumes, networks, secrets, configs)


def load_mapping(config_files, get_func, entity_type, working_dir=None):
    mapping = {}

    for config_file in config_files:
        for name, config in getattr(config_file, get_func)().items():
            mapping[name] = config or {}
            if not config:
                continue

            external = config.get('external')
            if external:
                validate_external(entity_type, name, config, config_file.version)
                if isinstance(external, dict):
                    config['name'] = external.get('name')
                elif not config.get('name'):
                    config['name'] = name

            if 'driver_opts' in config:
                config['driver_opts'] = build_string_dict(
                    config['driver_opts']
                )

            if 'labels' in config:
                config['labels'] = parse_labels(config['labels'])

            if 'file' in config:
                config['file'] = expand_path(working_dir, config['file'])

    return mapping


def validate_external(entity_type, name, config, version):
    if (version < V2_1 or (version >= V3_0 and version < V3_4)) and len(config.keys()) > 1:
        raise ConfigurationError(
            "{} {} declared as external but specifies additional attributes "
            "({}).".format(
                entity_type, name, ', '.join(k for k in config if k != 'external')))


def load_services(config_details, config_file, compatibility=False):
    def build_service(service_name, service_dict, service_names):
        service_config = ServiceConfig.with_abs_paths(
            config_details.working_dir,
            config_file.filename,
            service_name,
            service_dict)
        resolver = ServiceExtendsResolver(
            service_config, config_file, environment=config_details.environment
        )
        service_dict = process_service(resolver.run())

        service_config = service_config._replace(config=service_dict)
        validate_service(service_config, service_names, config_file)
        service_dict = finalize_service(
            service_config,
            service_names,
            config_file.version,
            config_details.environment,
            compatibility
        )
        return service_dict

    def build_services(service_config):
        service_names = service_config.keys()
        return sort_service_dicts([
            build_service(name, service_dict, service_names)
            for name, service_dict in service_config.items()
        ])

    def merge_services(base, override):
        all_service_names = set(base) | set(override)
        return {
            name: merge_service_dicts_from_files(
                base.get(name, {}),
                override.get(name, {}),
                config_file.version)
            for name in all_service_names
        }

    service_configs = [
        file.get_service_dicts() for file in config_details.config_files
    ]

    service_config = service_configs[0]
    for next_config in service_configs[1:]:
        service_config = merge_services(service_config, next_config)

    return build_services(service_config)


def interpolate_config_section(config_file, config, section, environment):
    validate_config_section(config_file.filename, config, section)
    return interpolate_environment_variables(
        config_file.version,
        config,
        section,
        environment
    )


def process_config_file(config_file, environment, service_name=None):
    services = interpolate_config_section(
        config_file,
        config_file.get_service_dicts(),
        'service',
        environment)

    if config_file.version > V1:
        processed_config = dict(config_file.config)
        processed_config['services'] = services
        processed_config['volumes'] = interpolate_config_section(
            config_file,
            config_file.get_volumes(),
            'volume',
            environment)
        processed_config['networks'] = interpolate_config_section(
            config_file,
            config_file.get_networks(),
            'network',
            environment)
        if config_file.version >= const.COMPOSEFILE_V3_1:
            processed_config['secrets'] = interpolate_config_section(
                config_file,
                config_file.get_secrets(),
                'secret',
                environment)
        if config_file.version >= const.COMPOSEFILE_V3_3:
            processed_config['configs'] = interpolate_config_section(
                config_file,
                config_file.get_configs(),
                'config',
                environment
            )
    else:
        processed_config = services

    config_file = config_file._replace(config=processed_config)
    validate_against_config_schema(config_file)

    if service_name and service_name not in services:
        raise ConfigurationError(
            "Cannot extend service '{}' in {}: Service not found".format(
                service_name, config_file.filename))

    return config_file


class ServiceExtendsResolver(object):
    def __init__(self, service_config, config_file, environment, already_seen=None):
        self.service_config = service_config
        self.working_dir = service_config.working_dir
        self.already_seen = already_seen or []
        self.config_file = config_file
        self.environment = environment

    @property
    def signature(self):
        return self.service_config.filename, self.service_config.name

    def detect_cycle(self):
        if self.signature in self.already_seen:
            raise CircularReference(self.already_seen + [self.signature])

    def run(self):
        self.detect_cycle()

        if 'extends' in self.service_config.config:
            service_dict = self.resolve_extends(*self.validate_and_construct_extends())
            return self.service_config._replace(config=service_dict)

        return self.service_config

    def validate_and_construct_extends(self):
        extends = self.service_config.config['extends']
        if not isinstance(extends, dict):
            extends = {'service': extends}

        config_path = self.get_extended_config_path(extends)
        service_name = extends['service']

        if config_path == self.config_file.filename:
            try:
                service_config = self.config_file.get_service(service_name)
            except KeyError:
                raise ConfigurationError(
                    "Cannot extend service '{}' in {}: Service not found".format(
                        service_name, config_path)
                )
        else:
            extends_file = ConfigFile.from_filename(config_path)
            validate_config_version([self.config_file, extends_file])
            extended_file = process_config_file(
                extends_file, self.environment, service_name=service_name
            )
            service_config = extended_file.get_service(service_name)

        return config_path, service_config, service_name

    def resolve_extends(self, extended_config_path, service_dict, service_name):
        resolver = ServiceExtendsResolver(
            ServiceConfig.with_abs_paths(
                os.path.dirname(extended_config_path),
                extended_config_path,
                service_name,
                service_dict),
            self.config_file,
            already_seen=self.already_seen + [self.signature],
            environment=self.environment
        )

        service_config = resolver.run()
        other_service_dict = process_service(service_config)
        validate_extended_service_dict(
            other_service_dict,
            extended_config_path,
            service_name)

        return merge_service_dicts(
            other_service_dict,
            self.service_config.config,
            self.config_file.version)

    def get_extended_config_path(self, extends_options):
        """Service we are extending either has a value for 'file' set, which we
        need to obtain a full path too or we are extending from a service
        defined in our own file.
        """
        filename = self.service_config.filename
        validate_extends_file_path(
            self.service_config.name,
            extends_options,
            filename)
        if 'file' in extends_options:
            return expand_path(self.working_dir, extends_options['file'])
        return filename


def resolve_environment(service_dict, environment=None):
    """Unpack any environment variables from an env_file, if set.
    Interpolate environment values if set.
    """
    env = {}
    for env_file in service_dict.get('env_file', []):
        env.update(env_vars_from_file(env_file))

    env.update(parse_environment(service_dict.get('environment')))
    return dict(resolve_env_var(k, v, environment) for k, v in six.iteritems(env))


def resolve_build_args(buildargs, environment):
    args = parse_build_arguments(buildargs)
    return dict(resolve_env_var(k, v, environment) for k, v in six.iteritems(args))


def validate_extended_service_dict(service_dict, filename, service):
    error_prefix = "Cannot extend service '%s' in %s:" % (service, filename)

    if 'links' in service_dict:
        raise ConfigurationError(
            "%s services with 'links' cannot be extended" % error_prefix)

    if 'volumes_from' in service_dict:
        raise ConfigurationError(
            "%s services with 'volumes_from' cannot be extended" % error_prefix)

    if 'net' in service_dict:
        if get_container_name_from_network_mode(service_dict['net']):
            raise ConfigurationError(
                "%s services with 'net: container' cannot be extended" % error_prefix)

    if 'network_mode' in service_dict:
        if get_service_name_from_network_mode(service_dict['network_mode']):
            raise ConfigurationError(
                "%s services with 'network_mode: service' cannot be extended" % error_prefix)

    if 'depends_on' in service_dict:
        raise ConfigurationError(
            "%s services with 'depends_on' cannot be extended" % error_prefix)


def validate_service(service_config, service_names, config_file):
    service_dict, service_name = service_config.config, service_config.name
    validate_service_constraints(service_dict, service_name, config_file)
    validate_paths(service_dict)

    validate_cpu(service_config)
    validate_ulimits(service_config)
    validate_network_mode(service_config, service_names)
    validate_pid_mode(service_config, service_names)
    validate_depends_on(service_config, service_names)
    validate_links(service_config, service_names)
    validate_healthcheck(service_config)

    if not service_dict.get('image') and has_uppercase(service_name):
        raise ConfigurationError(
            "Service '{name}' contains uppercase characters which are not valid "
            "as part of an image name. Either use a lowercase service name or "
            "use the `image` field to set a custom name for the service image."
            .format(name=service_name))


def process_service(service_config):
    working_dir = service_config.working_dir
    service_dict = dict(service_config.config)

    if 'env_file' in service_dict:
        service_dict['env_file'] = [
            expand_path(working_dir, path)
            for path in to_list(service_dict['env_file'])
        ]

    if 'build' in service_dict:
        process_build_section(service_dict, working_dir)

    if 'volumes' in service_dict and service_dict.get('volume_driver') is None:
        service_dict['volumes'] = resolve_volume_paths(working_dir, service_dict)

    if 'sysctls' in service_dict:
        service_dict['sysctls'] = build_string_dict(parse_sysctls(service_dict['sysctls']))

    if 'labels' in service_dict:
        service_dict['labels'] = parse_labels(service_dict['labels'])

    service_dict = process_depends_on(service_dict)

    for field in ['dns', 'dns_search', 'tmpfs']:
        if field in service_dict:
            service_dict[field] = to_list(service_dict[field])

    service_dict = process_security_opt(process_blkio_config(process_ports(
        process_healthcheck(service_dict)
    )))

    return service_dict


def process_build_section(service_dict, working_dir):
    if isinstance(service_dict['build'], six.string_types):
        service_dict['build'] = resolve_build_path(working_dir, service_dict['build'])
    elif isinstance(service_dict['build'], dict):
        if 'context' in service_dict['build']:
            path = service_dict['build']['context']
            service_dict['build']['context'] = resolve_build_path(working_dir, path)
        if 'labels' in service_dict['build']:
            service_dict['build']['labels'] = parse_labels(service_dict['build']['labels'])


def process_ports(service_dict):
    if 'ports' not in service_dict:
        return service_dict

    ports = []
    for port_definition in service_dict['ports']:
        if isinstance(port_definition, ServicePort):
            ports.append(port_definition)
        else:
            ports.extend(ServicePort.parse(port_definition))
    service_dict['ports'] = ports
    return service_dict


def process_depends_on(service_dict):
    if 'depends_on' in service_dict and not isinstance(service_dict['depends_on'], dict):
        service_dict['depends_on'] = dict([
            (svc, {'condition': 'service_started'}) for svc in service_dict['depends_on']
        ])
    return service_dict


def process_blkio_config(service_dict):
    if not service_dict.get('blkio_config'):
        return service_dict

    for field in ['device_read_bps', 'device_write_bps']:
        if field in service_dict['blkio_config']:
            for v in service_dict['blkio_config'].get(field, []):
                rate = v.get('rate', 0)
                v['rate'] = parse_bytes(rate)
                if v['rate'] is None:
                    raise ConfigurationError('Invalid format for bytes value: "{}"'.format(rate))

    for field in ['device_read_iops', 'device_write_iops']:
        if field in service_dict['blkio_config']:
            for v in service_dict['blkio_config'].get(field, []):
                try:
                    v['rate'] = int(v.get('rate', 0))
                except ValueError:
                    raise ConfigurationError(
                        'Invalid IOPS value: "{}". Must be a positive integer.'.format(v.get('rate'))
                    )

    return service_dict


def process_healthcheck(service_dict):
    if 'healthcheck' not in service_dict:
        return service_dict

    hc = service_dict['healthcheck']

    if 'disable' in hc:
        del hc['disable']
        hc['test'] = ['NONE']

    for field in ['interval', 'timeout', 'start_period']:
        if field not in hc or isinstance(hc[field], six.integer_types):
            continue
        hc[field] = parse_nanoseconds_int(hc[field])

    return service_dict


def finalize_service_volumes(service_dict, environment):
    if 'volumes' in service_dict:
        finalized_volumes = []
        normalize = environment.get_boolean('COMPOSE_CONVERT_WINDOWS_PATHS')
        win_host = environment.get_boolean('COMPOSE_FORCE_WINDOWS_HOST')
        for v in service_dict['volumes']:
            if isinstance(v, dict):
                finalized_volumes.append(MountSpec.parse(v, normalize, win_host))
            else:
                finalized_volumes.append(VolumeSpec.parse(v, normalize, win_host))
        service_dict['volumes'] = finalized_volumes

    return service_dict


def finalize_service(service_config, service_names, version, environment, compatibility):
    service_dict = dict(service_config.config)

    if 'environment' in service_dict or 'env_file' in service_dict:
        service_dict['environment'] = resolve_environment(service_dict, environment)
        service_dict.pop('env_file', None)

    if 'volumes_from' in service_dict:
        service_dict['volumes_from'] = [
            VolumeFromSpec.parse(vf, service_names, version)
            for vf in service_dict['volumes_from']
        ]

    service_dict = finalize_service_volumes(service_dict, environment)

    if 'net' in service_dict:
        network_mode = service_dict.pop('net')
        container_name = get_container_name_from_network_mode(network_mode)
        if container_name and container_name in service_names:
            service_dict['network_mode'] = 'service:{}'.format(container_name)
        else:
            service_dict['network_mode'] = network_mode

    if 'networks' in service_dict:
        service_dict['networks'] = parse_networks(service_dict['networks'])

    if 'restart' in service_dict:
        service_dict['restart'] = parse_restart_spec(service_dict['restart'])

    if 'secrets' in service_dict:
        service_dict['secrets'] = [
            types.ServiceSecret.parse(s) for s in service_dict['secrets']
        ]

    if 'configs' in service_dict:
        service_dict['configs'] = [
            types.ServiceConfig.parse(c) for c in service_dict['configs']
        ]

    normalize_build(service_dict, service_config.working_dir, environment)

    if compatibility:
        service_dict, ignored_keys = translate_deploy_keys_to_container_config(
            service_dict
        )
        if ignored_keys:
            log.warn(
                'The following deploy sub-keys are not supported in compatibility mode and have'
                ' been ignored: {}'.format(', '.join(ignored_keys))
            )

    service_dict['name'] = service_config.name
    return normalize_v1_service_format(service_dict)


def translate_resource_keys_to_container_config(resources_dict, service_dict):
    if 'limits' in resources_dict:
        service_dict['mem_limit'] = resources_dict['limits'].get('memory')
        if 'cpus' in resources_dict['limits']:
            service_dict['cpus'] = float(resources_dict['limits']['cpus'])
    if 'reservations' in resources_dict:
        service_dict['mem_reservation'] = resources_dict['reservations'].get('memory')
        if 'cpus' in resources_dict['reservations']:
            return ['resources.reservations.cpus']
    return []


def convert_restart_policy(name):
    try:
        return {
            'any': 'always',
            'none': 'no',
            'on-failure': 'on-failure'
        }[name]
    except KeyError:
        raise ConfigurationError('Invalid restart policy "{}"'.format(name))


def translate_deploy_keys_to_container_config(service_dict):
    if 'credential_spec' in service_dict:
        del service_dict['credential_spec']
    if 'configs' in service_dict:
        del service_dict['configs']

    if 'deploy' not in service_dict:
        return service_dict, []

    deploy_dict = service_dict['deploy']
    ignored_keys = [
        k for k in ['endpoint_mode', 'labels', 'update_config', 'rollback_config', 'placement']
        if k in deploy_dict
    ]

    if 'replicas' in deploy_dict and deploy_dict.get('mode', 'replicated') == 'replicated':
        service_dict['scale'] = deploy_dict['replicas']

    if 'restart_policy' in deploy_dict:
        service_dict['restart'] = {
            'Name': convert_restart_policy(deploy_dict['restart_policy'].get('condition', 'any')),
            'MaximumRetryCount': deploy_dict['restart_policy'].get('max_attempts', 0)
        }
        for k in deploy_dict['restart_policy'].keys():
            if k != 'condition' and k != 'max_attempts':
                ignored_keys.append('restart_policy.{}'.format(k))

    ignored_keys.extend(
        translate_resource_keys_to_container_config(
            deploy_dict.get('resources', {}), service_dict
        )
    )

    del service_dict['deploy']

    return service_dict, ignored_keys


def normalize_v1_service_format(service_dict):
    if 'log_driver' in service_dict or 'log_opt' in service_dict:
        if 'logging' not in service_dict:
            service_dict['logging'] = {}
        if 'log_driver' in service_dict:
            service_dict['logging']['driver'] = service_dict['log_driver']
            del service_dict['log_driver']
        if 'log_opt' in service_dict:
            service_dict['logging']['options'] = service_dict['log_opt']
            del service_dict['log_opt']

    if 'dockerfile' in service_dict:
        service_dict['build'] = service_dict.get('build', {})
        service_dict['build'].update({
            'dockerfile': service_dict.pop('dockerfile')
        })

    return service_dict


def merge_service_dicts_from_files(base, override, version):
    """When merging services from multiple files we need to merge the `extends`
    field. This is not handled by `merge_service_dicts()` which is used to
    perform the `extends`.
    """
    new_service = merge_service_dicts(base, override, version)
    if 'extends' in override:
        new_service['extends'] = override['extends']
    elif 'extends' in base:
        new_service['extends'] = base['extends']
    return new_service


class MergeDict(dict):
    """A dict-like object responsible for merging two dicts into one."""

    def __init__(self, base, override):
        self.base = base
        self.override = override

    def needs_merge(self, field):
        return field in self.base or field in self.override

    def merge_field(self, field, merge_func, default=None):
        if not self.needs_merge(field):
            return

        self[field] = merge_func(
            self.base.get(field, default),
            self.override.get(field, default))

    def merge_mapping(self, field, parse_func=None):
        if not self.needs_merge(field):
            return

        if parse_func is None:
            def parse_func(m):
                return m or {}

        self[field] = parse_func(self.base.get(field))
        self[field].update(parse_func(self.override.get(field)))

    def merge_sequence(self, field, parse_func):
        def parse_sequence_func(seq):
            return to_mapping((parse_func(item) for item in seq), 'merge_field')

        if not self.needs_merge(field):
            return

        merged = parse_sequence_func(self.base.get(field, []))
        merged.update(parse_sequence_func(self.override.get(field, [])))
        self[field] = [item.repr() for item in sorted(merged.values())]

    def merge_scalar(self, field):
        if self.needs_merge(field):
            self[field] = self.override.get(field, self.base.get(field))


def merge_service_dicts(base, override, version):
    md = MergeDict(base, override)

    md.merge_mapping('environment', parse_environment)
    md.merge_mapping('labels', parse_labels)
    md.merge_mapping('ulimits', parse_flat_dict)
    md.merge_mapping('networks', parse_networks)
    md.merge_mapping('sysctls', parse_sysctls)
    md.merge_mapping('depends_on', parse_depends_on)
    md.merge_mapping('storage_opt', parse_flat_dict)
    md.merge_sequence('links', ServiceLink.parse)
    md.merge_sequence('secrets', types.ServiceSecret.parse)
    md.merge_sequence('configs', types.ServiceConfig.parse)
    md.merge_sequence('security_opt', types.SecurityOpt.parse)
    md.merge_mapping('extra_hosts', parse_extra_hosts)

    for field in ['volumes', 'devices']:
        md.merge_field(field, merge_path_mappings)

    for field in [
        'cap_add', 'cap_drop', 'expose', 'external_links',
        'volumes_from', 'device_cgroup_rules',
    ]:
        md.merge_field(field, merge_unique_items_lists, default=[])

    for field in ['dns', 'dns_search', 'env_file', 'tmpfs']:
        md.merge_field(field, merge_list_or_string)

    md.merge_field('logging', merge_logging, default={})
    merge_ports(md, base, override)
    md.merge_field('blkio_config', merge_blkio_config, default={})
    md.merge_field('healthcheck', merge_healthchecks, default={})
    md.merge_field('deploy', merge_deploy, default={})

    for field in set(ALLOWED_KEYS) - set(md):
        md.merge_scalar(field)

    if version == V1:
        legacy_v1_merge_image_or_build(md, base, override)
    elif md.needs_merge('build'):
        md['build'] = merge_build(md, base, override)

    return dict(md)


def merge_unique_items_lists(base, override):
    override = [str(o) for o in override]
    base = [str(b) for b in base]
    return sorted(set().union(base, override))


def merge_healthchecks(base, override):
    if override.get('disabled') is True:
        return override
    result = base.copy()
    result.update(override)
    return result


def merge_ports(md, base, override):
    def parse_sequence_func(seq):
        acc = []
        for item in seq:
            acc.extend(ServicePort.parse(item))
        return to_mapping(acc, 'merge_field')

    field = 'ports'

    if not md.needs_merge(field):
        return

    merged = parse_sequence_func(md.base.get(field, []))
    merged.update(parse_sequence_func(md.override.get(field, [])))
    md[field] = [item for item in sorted(merged.values(), key=lambda x: x.target)]


def merge_build(output, base, override):
    def to_dict(service):
        build_config = service.get('build', {})
        if isinstance(build_config, six.string_types):
            return {'context': build_config}
        return build_config

    md = MergeDict(to_dict(base), to_dict(override))
    md.merge_scalar('context')
    md.merge_scalar('dockerfile')
    md.merge_scalar('network')
    md.merge_scalar('target')
    md.merge_scalar('shm_size')
    md.merge_scalar('isolation')
    md.merge_mapping('args', parse_build_arguments)
    md.merge_field('cache_from', merge_unique_items_lists, default=[])
    md.merge_mapping('labels', parse_labels)
    md.merge_mapping('extra_hosts', parse_extra_hosts)
    return dict(md)


def merge_deploy(base, override):
    md = MergeDict(base or {}, override or {})
    md.merge_scalar('mode')
    md.merge_scalar('endpoint_mode')
    md.merge_scalar('replicas')
    md.merge_mapping('labels', parse_labels)
    md.merge_mapping('update_config')
    md.merge_mapping('rollback_config')
    md.merge_mapping('restart_policy')
    if md.needs_merge('resources'):
        resources_md = MergeDict(md.base.get('resources') or {}, md.override.get('resources') or {})
        resources_md.merge_mapping('limits')
        resources_md.merge_field('reservations', merge_reservations, default={})
        md['resources'] = dict(resources_md)
    if md.needs_merge('placement'):
        placement_md = MergeDict(md.base.get('placement') or {}, md.override.get('placement') or {})
        placement_md.merge_field('constraints', merge_unique_items_lists, default=[])
        placement_md.merge_field('preferences', merge_unique_objects_lists, default=[])
        md['placement'] = dict(placement_md)

    return dict(md)


def merge_reservations(base, override):
    md = MergeDict(base, override)
    md.merge_scalar('cpus')
    md.merge_scalar('memory')
    md.merge_sequence('generic_resources', types.GenericResource.parse)
    return dict(md)


def merge_unique_objects_lists(base, override):
    result = dict((json_hash(i), i) for i in base + override)
    return [i[1] for i in sorted([(k, v) for k, v in result.items()], key=lambda x: x[0])]


def merge_blkio_config(base, override):
    md = MergeDict(base, override)
    md.merge_scalar('weight')

    def merge_blkio_limits(base, override):
        index = dict((b['path'], b) for b in base)
        for o in override:
            index[o['path']] = o

        return sorted(list(index.values()), key=lambda x: x['path'])

    for field in [
            "device_read_bps", "device_read_iops", "device_write_bps",
            "device_write_iops", "weight_device",
    ]:
        md.merge_field(field, merge_blkio_limits, default=[])

    return dict(md)


def merge_logging(base, override):
    md = MergeDict(base, override)
    md.merge_scalar('driver')
    if md.get('driver') == base.get('driver') or base.get('driver') is None:
        md.merge_mapping('options', lambda m: m or {})
    elif override.get('options'):
        md['options'] = override.get('options', {})
    return dict(md)


def legacy_v1_merge_image_or_build(output, base, override):
    output.pop('image', None)
    output.pop('build', None)
    if 'image' in override:
        output['image'] = override['image']
    elif 'build' in override:
        output['build'] = override['build']
    elif 'image' in base:
        output['image'] = base['image']
    elif 'build' in base:
        output['build'] = base['build']


def merge_environment(base, override):
    env = parse_environment(base)
    env.update(parse_environment(override))
    return env


def merge_labels(base, override):
    labels = parse_labels(base)
    labels.update(parse_labels(override))
    return labels


def split_kv(kvpair):
    if '=' in kvpair:
        return kvpair.split('=', 1)
    else:
        return kvpair, ''


def parse_dict_or_list(split_func, type_name, arguments):
    if not arguments:
        return {}

    if isinstance(arguments, list):
        return dict(split_func(e) for e in arguments)

    if isinstance(arguments, dict):
        return dict(arguments)

    raise ConfigurationError(
        "%s \"%s\" must be a list or mapping," %
        (type_name, arguments)
    )


parse_build_arguments = functools.partial(parse_dict_or_list, split_env, 'build arguments')
parse_environment = functools.partial(parse_dict_or_list, split_env, 'environment')
parse_labels = functools.partial(parse_dict_or_list, split_kv, 'labels')
parse_networks = functools.partial(parse_dict_or_list, lambda k: (k, None), 'networks')
parse_sysctls = functools.partial(parse_dict_or_list, split_kv, 'sysctls')
parse_depends_on = functools.partial(
    parse_dict_or_list, lambda k: (k, {'condition': 'service_started'}), 'depends_on'
)


def parse_flat_dict(d):
    if not d:
        return {}

    if isinstance(d, dict):
        return dict(d)

    raise ConfigurationError("Invalid type: expected mapping")


def resolve_env_var(key, val, environment):
    if val is not None:
        return key, val
    elif environment and key in environment:
        return key, environment[key]
    else:
        return key, None


def resolve_volume_paths(working_dir, service_dict):
    return [
        resolve_volume_path(working_dir, volume)
        for volume in service_dict['volumes']
    ]


def resolve_volume_path(working_dir, volume):
    if isinstance(volume, dict):
        if volume.get('source', '').startswith(('.', '~')) and volume['type'] == 'bind':
            volume['source'] = expand_path(working_dir, volume['source'])
        return volume

    mount_params = None
    container_path, mount_params = split_path_mapping(volume)

    if mount_params is not None:
        host_path, mode = mount_params
        if host_path is None:
            return container_path
        if host_path.startswith('.'):
            host_path = expand_path(working_dir, host_path)
        host_path = os.path.expanduser(host_path)
        return u"{}:{}{}".format(host_path, container_path, (':' + mode if mode else ''))

    return container_path


def normalize_build(service_dict, working_dir, environment):

    if 'build' in service_dict:
        build = {}
        # Shortcut where specifying a string is treated as the build context
        if isinstance(service_dict['build'], six.string_types):
            build['context'] = service_dict.pop('build')
        else:
            build.update(service_dict['build'])
            if 'args' in build:
                build['args'] = build_string_dict(
                    resolve_build_args(build.get('args'), environment)
                )

        service_dict['build'] = build


def resolve_build_path(working_dir, build_path):
    if is_url(build_path):
        return build_path
    return expand_path(working_dir, build_path)


def is_url(build_path):
    return build_path.startswith(DOCKER_VALID_URL_PREFIXES)


def validate_paths(service_dict):
    if 'build' in service_dict:
        build = service_dict.get('build', {})

        if isinstance(build, six.string_types):
            build_path = build
        elif isinstance(build, dict) and 'context' in build:
            build_path = build['context']
        else:
            # We have a build section but no context, so nothing to validate
            return

        if (
            not is_url(build_path) and
            (not os.path.exists(build_path) or not os.access(build_path, os.R_OK))
        ):
            raise ConfigurationError(
                "build path %s either does not exist, is not accessible, "
                "or is not a valid URL." % build_path)


def merge_path_mappings(base, override):
    d = dict_from_path_mappings(base)
    d.update(dict_from_path_mappings(override))
    return path_mappings_from_dict(d)


def dict_from_path_mappings(path_mappings):
    if path_mappings:
        return dict(split_path_mapping(v) for v in path_mappings)
    else:
        return {}


def path_mappings_from_dict(d):
    return [join_path_mapping(v) for v in sorted(d.items())]


def split_path_mapping(volume_path):
    """
    Ascertain if the volume_path contains a host path as well as a container
    path. Using splitdrive so windows absolute paths won't cause issues with
    splitting on ':'.
    """
    if isinstance(volume_path, dict):
        return (volume_path.get('target'), volume_path)
    drive, volume_config = splitdrive(volume_path)

    if ':' in volume_config:
        (host, container) = volume_config.split(':', 1)
        container_drive, container_path = splitdrive(container)
        mode = None
        if ':' in container_path:
            container_path, mode = container_path.rsplit(':', 1)

        return (container_drive + container_path, (drive + host, mode))
    else:
        return (volume_path, None)


def process_security_opt(service_dict):
    security_opts = service_dict.get('security_opt', [])
    result = []
    for value in security_opts:
        result.append(SecurityOpt.parse(value))
    if result:
        service_dict['security_opt'] = result
    return service_dict


def join_path_mapping(pair):
    (container, host) = pair
    if isinstance(host, dict):
        return host
    elif host is None:
        return container
    else:
        host, mode = host
        result = ":".join((host, container))
        if mode:
            result += ":" + mode
        return result


def expand_path(working_dir, path):
    return os.path.abspath(os.path.join(working_dir, os.path.expanduser(path)))


def merge_list_or_string(base, override):
    return to_list(base) + to_list(override)


def to_list(value):
    if value is None:
        return []
    elif isinstance(value, six.string_types):
        return [value]
    else:
        return value


def to_mapping(sequence, key_field):
    return {getattr(item, key_field): item for item in sequence}


def has_uppercase(name):
    return any(char in string.ascii_uppercase for char in name)


def load_yaml(filename, encoding=None, binary=True):
    try:
        with io.open(filename, 'rb' if binary else 'r', encoding=encoding) as fh:
            return yaml.safe_load(fh)
    except (IOError, yaml.YAMLError, UnicodeDecodeError) as e:
        if encoding is None:
            # Sometimes the user's locale sets an encoding that doesn't match
            # the YAML files. Im such cases, retry once with the "default"
            # UTF-8 encoding
            return load_yaml(filename, encoding='utf-8-sig', binary=False)
        error_name = getattr(e, '__module__', '') + '.' + e.__class__.__name__
        raise ConfigurationError(u"{}: {}".format(error_name, e))
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from compose.config.errors import DependencyError


def get_service_name_from_network_mode(network_mode):
    return get_source_name_from_network_mode(network_mode, 'service')


def get_container_name_from_network_mode(network_mode):
    return get_source_name_from_network_mode(network_mode, 'container')


def get_source_name_from_network_mode(network_mode, source_type):
    if not network_mode:
        return

    if not network_mode.startswith(source_type+':'):
        return

    _, net_name = network_mode.split(':', 1)
    return net_name


def get_service_names(links):
    return [link.split(':')[0] for link in links]


def get_service_names_from_volumes_from(volumes_from):
    return [volume_from.source for volume_from in volumes_from]


def get_service_dependents(service_dict, services):
    name = service_dict['name']
    return [
        service for service in services
        if (name in get_service_names(service.get('links', [])) or
            name in get_service_names_from_volumes_from(service.get('volumes_from', [])) or
            name == get_service_name_from_network_mode(service.get('network_mode')) or
            name == get_service_name_from_network_mode(service.get('pid')) or
            name in service.get('depends_on', []))
    ]


def sort_service_dicts(services):
    # Topological sort (Cormen/Tarjan algorithm).
    unmarked = services[:]
    temporary_marked = set()
    sorted_services = []

    def visit(n):
        if n['name'] in temporary_marked:
            if n['name'] in get_service_names(n.get('links', [])):
                raise DependencyError('A service can not link to itself: %s' % n['name'])
            if n['name'] in n.get('volumes_from', []):
                raise DependencyError('A service can not mount itself as volume: %s' % n['name'])
            if n['name'] in n.get('depends_on', []):
                raise DependencyError('A service can not depend on itself: %s' % n['name'])
            raise DependencyError('Circular dependency between %s' % ' and '.join(temporary_marked))

        if n in unmarked:
            temporary_marked.add(n['name'])
            for m in get_service_dependents(n, services):
                visit(m)
            temporary_marked.remove(n['name'])
            unmarked.remove(n)
            sorted_services.insert(0, n)

    while unmarked:
        visit(unmarked[-1])

    return sorted_services
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import json
import logging
import os
import re
import sys

import six
from docker.utils.ports import split_port
from jsonschema import Draft4Validator
from jsonschema import FormatChecker
from jsonschema import RefResolver
from jsonschema import ValidationError

from ..const import COMPOSEFILE_V1 as V1
from ..const import NANOCPUS_SCALE
from .errors import ConfigurationError
from .errors import VERSION_EXPLANATION
from .sort_services import get_service_name_from_network_mode


log = logging.getLogger(__name__)


DOCKER_CONFIG_HINTS = {
    'cpu_share': 'cpu_shares',
    'add_host': 'extra_hosts',
    'hosts': 'extra_hosts',
    'extra_host': 'extra_hosts',
    'device': 'devices',
    'link': 'links',
    'memory_swap': 'memswap_limit',
    'port': 'ports',
    'privilege': 'privileged',
    'priviliged': 'privileged',
    'privilige': 'privileged',
    'volume': 'volumes',
    'workdir': 'working_dir',
}


VALID_NAME_CHARS = r'[a-zA-Z0-9\._\-]'
VALID_EXPOSE_FORMAT = r'^\d+(\-\d+)?(\/[a-zA-Z]+)?$'

VALID_IPV4_SEG = r'(\d{1,2}|1\d{2}|2[0-4]\d|25[0-5])'
VALID_IPV4_ADDR = r"({IPV4_SEG}\.){{3}}{IPV4_SEG}".format(IPV4_SEG=VALID_IPV4_SEG)
VALID_REGEX_IPV4_CIDR = r"^{IPV4_ADDR}/(\d|[1-2]\d|3[0-2])$".format(IPV4_ADDR=VALID_IPV4_ADDR)

VALID_IPV6_SEG = r'[0-9a-fA-F]{1,4}'
VALID_REGEX_IPV6_CIDR = "".join(r"""
^
(
    (({IPV6_SEG}:){{7}}{IPV6_SEG})|
    (({IPV6_SEG}:){{1,7}}:)|
    (({IPV6_SEG}:){{1,6}}(:{IPV6_SEG}){{1,1}})|
    (({IPV6_SEG}:){{1,5}}(:{IPV6_SEG}){{1,2}})|
    (({IPV6_SEG}:){{1,4}}(:{IPV6_SEG}){{1,3}})|
    (({IPV6_SEG}:){{1,3}}(:{IPV6_SEG}){{1,4}})|
    (({IPV6_SEG}:){{1,2}}(:{IPV6_SEG}){{1,5}})|
    (({IPV6_SEG}:){{1,1}}(:{IPV6_SEG}){{1,6}})|
    (:((:{IPV6_SEG}){{1,7}}|:))|
    (fe80:(:{IPV6_SEG}){{0,4}}%[0-9a-zA-Z]{{1,}})|
    (::(ffff(:0{{1,4}}){{0,1}}:){{0,1}}{IPV4_ADDR})|
    (({IPV6_SEG}:){{1,4}}:{IPV4_ADDR})
)
/(\d|[1-9]\d|1[0-1]\d|12[0-8])
$
""".format(IPV6_SEG=VALID_IPV6_SEG, IPV4_ADDR=VALID_IPV4_ADDR).split())


@FormatChecker.cls_checks(format="ports", raises=ValidationError)
def format_ports(instance):
    try:
        split_port(instance)
    except ValueError as e:
        raise ValidationError(six.text_type(e))
    return True


@FormatChecker.cls_checks(format="expose", raises=ValidationError)
def format_expose(instance):
    if isinstance(instance, six.string_types):
        if not re.match(VALID_EXPOSE_FORMAT, instance):
            raise ValidationError(
                "should be of the format 'PORT[/PROTOCOL]'")

    return True


@FormatChecker.cls_checks("subnet_ip_address", raises=ValidationError)
def format_subnet_ip_address(instance):
    if isinstance(instance, six.string_types):
        if not re.match(VALID_REGEX_IPV4_CIDR, instance) and \
                not re.match(VALID_REGEX_IPV6_CIDR, instance):
            raise ValidationError("should use the CIDR format")

    return True


def match_named_volumes(service_dict, project_volumes):
    service_volumes = service_dict.get('volumes', [])
    for volume_spec in service_volumes:
        if volume_spec.is_named_volume and volume_spec.external not in project_volumes:
            raise ConfigurationError(
                'Named volume "{0}" is used in service "{1}" but no'
                ' declaration was found in the volumes section.'.format(
                    volume_spec.repr(), service_dict.get('name')
                )
            )


def python_type_to_yaml_type(type_):
    type_name = type(type_).__name__
    return {
        'dict': 'mapping',
        'list': 'array',
        'int': 'number',
        'float': 'number',
        'bool': 'boolean',
        'unicode': 'string',
        'str': 'string',
        'bytes': 'string',
    }.get(type_name, type_name)


def validate_config_section(filename, config, section):
    """Validate the structure of a configuration section. This must be done
    before interpolation so it's separate from schema validation.
    """
    if not isinstance(config, dict):
        raise ConfigurationError(
            "In file '{filename}', {section} must be a mapping, not "
            "{type}.".format(
                filename=filename,
                section=section,
                type=anglicize_json_type(python_type_to_yaml_type(config))))

    for key, value in config.items():
        if not isinstance(key, six.string_types):
            raise ConfigurationError(
                "In file '{filename}', the {section} name {name} must be a "
                "quoted string, i.e. '{name}'.".format(
                    filename=filename,
                    section=section,
                    name=key))

        if not isinstance(value, (dict, type(None))):
            raise ConfigurationError(
                "In file '{filename}', {section} '{name}' must be a mapping not "
                "{type}.".format(
                    filename=filename,
                    section=section,
                    name=key,
                    type=anglicize_json_type(python_type_to_yaml_type(value))))


def validate_top_level_object(config_file):
    if not isinstance(config_file.config, dict):
        raise ConfigurationError(
            "Top level object in '{}' needs to be an object not '{}'.".format(
                config_file.filename,
                type(config_file.config)))


def validate_ulimits(service_config):
    ulimit_config = service_config.config.get('ulimits', {})
    for limit_name, soft_hard_values in six.iteritems(ulimit_config):
        if isinstance(soft_hard_values, dict):
            if not soft_hard_values['soft'] <= soft_hard_values['hard']:
                raise ConfigurationError(
                    "Service '{s.name}' has invalid ulimit '{ulimit}'. "
                    "'soft' value can not be greater than 'hard' value ".format(
                        s=service_config,
                        ulimit=ulimit_config))


def validate_extends_file_path(service_name, extends_options, filename):
    """
    The service to be extended must either be defined in the config key 'file',
    or within 'filename'.
    """
    error_prefix = "Invalid 'extends' configuration for %s:" % service_name

    if 'file' not in extends_options and filename is None:
        raise ConfigurationError(
            "%s you need to specify a 'file', e.g. 'file: something.yml'" % error_prefix
        )


def validate_network_mode(service_config, service_names):
    network_mode = service_config.config.get('network_mode')
    if not network_mode:
        return

    if 'networks' in service_config.config:
        raise ConfigurationError("'network_mode' and 'networks' cannot be combined")

    dependency = get_service_name_from_network_mode(network_mode)
    if not dependency:
        return

    if dependency not in service_names:
        raise ConfigurationError(
            "Service '{s.name}' uses the network stack of service '{dep}' which "
            "is undefined.".format(s=service_config, dep=dependency))


def validate_pid_mode(service_config, service_names):
    pid_mode = service_config.config.get('pid')
    if not pid_mode:
        return

    dependency = get_service_name_from_network_mode(pid_mode)
    if not dependency:
        return
    if dependency not in service_names:
        raise ConfigurationError(
            "Service '{s.name}' uses the PID namespace of service '{dep}' which "
            "is undefined.".format(s=service_config, dep=dependency)
        )


def validate_links(service_config, service_names):
    for link in service_config.config.get('links', []):
        if link.split(':')[0] not in service_names:
            raise ConfigurationError(
                "Service '{s.name}' has a link to service '{link}' which is "
                "undefined.".format(s=service_config, link=link))


def validate_depends_on(service_config, service_names):
    deps = service_config.config.get('depends_on', {})
    for dependency in deps.keys():
        if dependency not in service_names:
            raise ConfigurationError(
                "Service '{s.name}' depends on service '{dep}' which is "
                "undefined.".format(s=service_config, dep=dependency)
            )


def get_unsupported_config_msg(path, error_key):
    msg = "Unsupported config option for {}: '{}'".format(path_string(path), error_key)
    if error_key in DOCKER_CONFIG_HINTS:
        msg += " (did you mean '{}'?)".format(DOCKER_CONFIG_HINTS[error_key])
    return msg


def anglicize_json_type(json_type):
    if json_type.startswith(('a', 'e', 'i', 'o', 'u')):
        return 'an ' + json_type
    return 'a ' + json_type


def is_service_dict_schema(schema_id):
    return schema_id in ('config_schema_v1.json', '#/properties/services')


def handle_error_for_schema_with_id(error, path):
    schema_id = error.schema['id']

    if is_service_dict_schema(schema_id) and error.validator == 'additionalProperties':
        return "Invalid service name '{}' - only {} characters are allowed".format(
            # The service_name is one of the keys in the json object
            [i for i in list(error.instance) if not i or any(filter(
                lambda c: not re.match(VALID_NAME_CHARS, c), i
            ))][0],
            VALID_NAME_CHARS
        )

    if error.validator == 'additionalProperties':
        if schema_id == '#/definitions/service':
            invalid_config_key = parse_key_from_error_msg(error)
            return get_unsupported_config_msg(path, invalid_config_key)

        if schema_id.startswith('config_schema_v'):
            invalid_config_key = parse_key_from_error_msg(error)
            return ('Invalid top-level property "{key}". Valid top-level '
                    'sections for this Compose file are: {properties}, and '
                    'extensions starting with "x-".\n\n{explanation}').format(
                key=invalid_config_key,
                properties=', '.join(error.schema['properties'].keys()),
                explanation=VERSION_EXPLANATION
            )

        if not error.path:
            return '{}\n\n{}'.format(error.message, VERSION_EXPLANATION)


def handle_generic_error(error, path):
    msg_format = None
    error_msg = error.message

    if error.validator == 'oneOf':
        msg_format = "{path} {msg}"
        config_key, error_msg = _parse_oneof_validator(error)
        if config_key:
            path.append(config_key)

    elif error.validator == 'type':
        msg_format = "{path} contains an invalid type, it should be {msg}"
        error_msg = _parse_valid_types_from_validator(error.validator_value)

    elif error.validator == 'required':
        error_msg = ", ".join(error.validator_value)
        msg_format = "{path} is invalid, {msg} is required."

    elif error.validator == 'dependencies':
        config_key = list(error.validator_value.keys())[0]
        required_keys = ",".join(error.validator_value[config_key])

        msg_format = "{path} is invalid: {msg}"
        path.append(config_key)
        error_msg = "when defining '{}' you must set '{}' as well".format(
            config_key,
            required_keys)

    elif error.cause:
        error_msg = six.text_type(error.cause)
        msg_format = "{path} is invalid: {msg}"

    elif error.path:
        msg_format = "{path} value {msg}"

    if msg_format:
        return msg_format.format(path=path_string(path), msg=error_msg)

    return error.message


def parse_key_from_error_msg(error):
    try:
        return error.message.split("'")[1]
    except IndexError:
        return error.message.split('(')[1].split(' ')[0].strip("'")


def path_string(path):
    return ".".join(c for c in path if isinstance(c, six.string_types))


def _parse_valid_types_from_validator(validator):
    """A validator value can be either an array of valid types or a string of
    a valid type. Parse the valid types and prefix with the correct article.
    """
    if not isinstance(validator, list):
        return anglicize_json_type(validator)

    if len(validator) == 1:
        return anglicize_json_type(validator[0])

    return "{}, or {}".format(
        ", ".join([anglicize_json_type(validator[0])] + validator[1:-1]),
        anglicize_json_type(validator[-1]))


def _parse_oneof_validator(error):
    """oneOf has multiple schemas, so we need to reason about which schema, sub
    schema or constraint the validation is failing on.
    Inspecting the context value of a ValidationError gives us information about
    which sub schema failed and which kind of error it is.
    """
    types = []
    for context in error.context:
        if context.validator == 'oneOf':
            _, error_msg = _parse_oneof_validator(context)
            return path_string(context.path), error_msg

        if context.validator == 'required':
            return (None, context.message)

        if context.validator == 'additionalProperties':
            invalid_config_key = parse_key_from_error_msg(context)
            return (None, "contains unsupported option: '{}'".format(invalid_config_key))

        if context.validator == 'uniqueItems':
            return (
                path_string(context.path) if context.path else None,
                "contains non-unique items, please remove duplicates from {}".format(
                    context.instance),
            )

        if context.path:
            return (
                path_string(context.path),
                "contains {}, which is an invalid type, it should be {}".format(
                    json.dumps(context.instance),
                    _parse_valid_types_from_validator(context.validator_value)),
            )

        if context.validator == 'type':
            types.append(context.validator_value)

    valid_types = _parse_valid_types_from_validator(types)
    return (None, "contains an invalid type, it should be {}".format(valid_types))


def process_service_constraint_errors(error, service_name, version):
    if version == V1:
        if 'image' in error.instance and 'build' in error.instance:
            return (
                "Service {} has both an image and build path specified. "
                "A service can either be built to image or use an existing "
                "image, not both.".format(service_name))

        if 'image' in error.instance and 'dockerfile' in error.instance:
            return (
                "Service {} has both an image and alternate Dockerfile. "
                "A service can either be built to image or use an existing "
                "image, not both.".format(service_name))

    if 'image' not in error.instance and 'build' not in error.instance:
        return (
            "Service {} has neither an image nor a build context specified. "
            "At least one must be provided.".format(service_name))


def process_config_schema_errors(error):
    path = list(error.path)

    if 'id' in error.schema:
        error_msg = handle_error_for_schema_with_id(error, path)
        if error_msg:
            return error_msg

    return handle_generic_error(error, path)


def validate_against_config_schema(config_file):
    schema = load_jsonschema(config_file)
    format_checker = FormatChecker(["ports", "expose", "subnet_ip_address"])
    validator = Draft4Validator(
        schema,
        resolver=RefResolver(get_resolver_path(), schema),
        format_checker=format_checker)
    handle_errors(
        validator.iter_errors(config_file.config),
        process_config_schema_errors,
        config_file.filename)


def validate_service_constraints(config, service_name, config_file):
    def handler(errors):
        return process_service_constraint_errors(
            errors, service_name, config_file.version)

    schema = load_jsonschema(config_file)
    validator = Draft4Validator(schema['definitions']['constraints']['service'])
    handle_errors(validator.iter_errors(config), handler, None)


def validate_cpu(service_config):
    cpus = service_config.config.get('cpus')
    if not cpus:
        return
    nano_cpus = cpus * NANOCPUS_SCALE
    if isinstance(nano_cpus, float) and not nano_cpus.is_integer():
        raise ConfigurationError(
            "cpus must have nine or less digits after decimal point")


def get_schema_path():
    return os.path.dirname(os.path.abspath(__file__))


def load_jsonschema(config_file):
    filename = os.path.join(
        get_schema_path(),
        "config_schema_v{0}.json".format(config_file.version))

    if not os.path.exists(filename):
        raise ConfigurationError(
            'Version in "{}" is unsupported. {}'
            .format(config_file.filename, VERSION_EXPLANATION))

    with open(filename, "r") as fh:
        return json.load(fh)


def get_resolver_path():
    schema_path = get_schema_path()
    if sys.platform == "win32":
        scheme = "///"
        # TODO: why is this necessary?
        schema_path = schema_path.replace('\\', '/')
    else:
        scheme = "//"
    return "file:{}{}/".format(scheme, schema_path)


def handle_errors(errors, format_error_func, filename):
    """jsonschema returns an error tree full of information to explain what has
    gone wrong. Process each error and pull out relevant information and re-write
    helpful error messages that are relevant.
    """
    errors = list(sorted(errors, key=str))
    if not errors:
        return

    error_msg = '\n'.join(format_error_func(error) for error in errors)
    raise ConfigurationError(
        "The Compose file{file_msg} is invalid because:\n{error_msg}".format(
            file_msg=" '{}'".format(filename) if filename else "",
            error_msg=error_msg))


def validate_healthcheck(service_config):
    healthcheck = service_config.config.get('healthcheck', {})

    if 'test' in healthcheck and isinstance(healthcheck['test'], list):
        if len(healthcheck['test']) == 0:
            raise ConfigurationError(
                'Service "{}" defines an invalid healthcheck: '
                '"test" is an empty list'
                .format(service_config.name))

        # when disable is true config.py::process_healthcheck adds "test: ['NONE']" to service_config
        elif healthcheck['test'][0] == 'NONE' and len(healthcheck) > 1:
            raise ConfigurationError(
                'Service "{}" defines an invalid healthcheck: '
                '"disable: true" cannot be combined with other options'
                .format(service_config.name))

        elif healthcheck['test'][0] not in ('NONE', 'CMD', 'CMD-SHELL'):
            raise ConfigurationError(
                'Service "{}" defines an invalid healthcheck: '
                'when "test" is a list the first item must be either NONE, CMD or CMD-SHELL'
                .format(service_config.name))
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import codecs
import contextlib
import logging
import os

import six

from ..const import IS_WINDOWS_PLATFORM
from .errors import ConfigurationError

log = logging.getLogger(__name__)


def split_env(env):
    if isinstance(env, six.binary_type):
        env = env.decode('utf-8', 'replace')
    if '=' in env:
        return env.split('=', 1)
    else:
        return env, None


def env_vars_from_file(filename):
    """
    Read in a line delimited file of environment variables.
    """
    if not os.path.exists(filename):
        raise ConfigurationError("Couldn't find env file: %s" % filename)
    elif not os.path.isfile(filename):
        raise ConfigurationError("%s is not a file." % (filename))
    env = {}
    with contextlib.closing(codecs.open(filename, 'r', 'utf-8-sig')) as fileobj:
        for line in fileobj:
            line = line.strip()
            if line and not line.startswith('#'):
                k, v = split_env(line)
                env[k] = v
    return env


class Environment(dict):
    def __init__(self, *args, **kwargs):
        super(Environment, self).__init__(*args, **kwargs)
        self.missing_keys = []

    @classmethod
    def from_env_file(cls, base_dir):
        def _initialize():
            result = cls()
            if base_dir is None:
                return result
            env_file_path = os.path.join(base_dir, '.env')
            try:
                return cls(env_vars_from_file(env_file_path))
            except ConfigurationError:
                pass
            return result
        instance = _initialize()
        instance.update(os.environ)
        return instance

    @classmethod
    def from_command_line(cls, parsed_env_opts):
        result = cls()
        for k, v in parsed_env_opts.items():
            # Values from the command line take priority, unless they're unset
            # in which case they take the value from the system's environment
            if v is None and k in os.environ:
                result[k] = os.environ[k]
            else:
                result[k] = v
        return result

    def __getitem__(self, key):
        try:
            return super(Environment, self).__getitem__(key)
        except KeyError:
            if IS_WINDOWS_PLATFORM:
                try:
                    return super(Environment, self).__getitem__(key.upper())
                except KeyError:
                    pass
            if key not in self.missing_keys:
                log.warn(
                    "The {} variable is not set. Defaulting to a blank string."
                    .format(key)
                )
                self.missing_keys.append(key)

            return ""

    def __contains__(self, key):
        result = super(Environment, self).__contains__(key)
        if IS_WINDOWS_PLATFORM:
            return (
                result or super(Environment, self).__contains__(key.upper())
            )
        return result

    def get(self, key, *args, **kwargs):
        if IS_WINDOWS_PLATFORM:
            return super(Environment, self).get(
                key,
                super(Environment, self).get(key.upper(), *args, **kwargs)
            )
        return super(Environment, self).get(key, *args, **kwargs)

    def get_boolean(self, key):
        # Convert a value to a boolean using "common sense" rules.
        # Unset, empty, "0" and "false" (i-case) yield False.
        # All other values yield True.
        value = self.get(key)
        if not value:
            return False
        if value.lower() in ['0', 'false']:
            return False
        return True
<EOF>
<BOF>
"""
Types for objects parsed from the configuration.
"""
from __future__ import absolute_import
from __future__ import unicode_literals

import json
import ntpath
import os
import re
from collections import namedtuple

import six
from docker.utils.ports import build_port_bindings

from ..const import COMPOSEFILE_V1 as V1
from ..utils import unquote_path
from .errors import ConfigurationError
from compose.const import IS_WINDOWS_PLATFORM
from compose.utils import splitdrive

win32_root_path_pattern = re.compile(r'^[A-Za-z]\:\\.*')


class VolumeFromSpec(namedtuple('_VolumeFromSpec', 'source mode type')):

    # TODO: drop service_names arg when v1 is removed
    @classmethod
    def parse(cls, volume_from_config, service_names, version):
        func = cls.parse_v1 if version == V1 else cls.parse_v2
        return func(service_names, volume_from_config)

    @classmethod
    def parse_v1(cls, service_names, volume_from_config):
        parts = volume_from_config.split(':')
        if len(parts) > 2:
            raise ConfigurationError(
                "volume_from {} has incorrect format, should be "
                "service[:mode]".format(volume_from_config))

        if len(parts) == 1:
            source = parts[0]
            mode = 'rw'
        else:
            source, mode = parts

        type = 'service' if source in service_names else 'container'
        return cls(source, mode, type)

    @classmethod
    def parse_v2(cls, service_names, volume_from_config):
        parts = volume_from_config.split(':')
        if len(parts) > 3:
            raise ConfigurationError(
                "volume_from {} has incorrect format, should be one of "
                "'<service name>[:<mode>]' or "
                "'container:<container name>[:<mode>]'".format(volume_from_config))

        if len(parts) == 1:
            source = parts[0]
            return cls(source, 'rw', 'service')

        if len(parts) == 2:
            if parts[0] == 'container':
                type, source = parts
                return cls(source, 'rw', type)

            source, mode = parts
            return cls(source, mode, 'service')

        if len(parts) == 3:
            type, source, mode = parts
            if type not in ('service', 'container'):
                raise ConfigurationError(
                    "Unknown volumes_from type '{}' in '{}'".format(
                        type,
                        volume_from_config))

        return cls(source, mode, type)

    def repr(self):
        return '{v.type}:{v.source}:{v.mode}'.format(v=self)


def parse_restart_spec(restart_config):
    if not restart_config:
        return None
    parts = restart_config.split(':')
    if len(parts) > 2:
        raise ConfigurationError(
            "Restart %s has incorrect format, should be "
            "mode[:max_retry]" % restart_config)
    if len(parts) == 2:
        name, max_retry_count = parts
    else:
        name, = parts
        max_retry_count = 0

    return {'Name': name, 'MaximumRetryCount': int(max_retry_count)}


def serialize_restart_spec(restart_spec):
    if not restart_spec:
        return ''
    parts = [restart_spec['Name']]
    if restart_spec['MaximumRetryCount']:
        parts.append(six.text_type(restart_spec['MaximumRetryCount']))
    return ':'.join(parts)


def parse_extra_hosts(extra_hosts_config):
    if not extra_hosts_config:
        return {}

    if isinstance(extra_hosts_config, dict):
        return dict(extra_hosts_config)

    if isinstance(extra_hosts_config, list):
        extra_hosts_dict = {}
        for extra_hosts_line in extra_hosts_config:
            # TODO: validate string contains ':' ?
            host, ip = extra_hosts_line.split(':', 1)
            extra_hosts_dict[host.strip()] = ip.strip()
        return extra_hosts_dict


def normalize_path_for_engine(path):
    """Windows paths, c:\\my\\path\\shiny, need to be changed to be compatible with
    the Engine. Volume paths are expected to be linux style /c/my/path/shiny/
    """
    drive, tail = splitdrive(path)

    if drive:
        path = '/' + drive.lower().rstrip(':') + tail

    return path.replace('\\', '/')


def normpath(path, win_host=False):
    """ Custom path normalizer that handles Compose-specific edge cases like
        UNIX paths on Windows hosts and vice-versa. """

    sysnorm = ntpath.normpath if win_host else os.path.normpath
    # If a path looks like a UNIX absolute path on Windows, it probably is;
    # we'll need to revert the backslashes to forward slashes after normalization
    flip_slashes = path.startswith('/') and IS_WINDOWS_PLATFORM
    path = sysnorm(path)
    if flip_slashes:
        path = path.replace('\\', '/')
    return path


class MountSpec(object):
    options_map = {
        'volume': {
            'nocopy': 'no_copy'
        },
        'bind': {
            'propagation': 'propagation'
        },
        'tmpfs': {
            'size': 'tmpfs_size'
        }
    }
    _fields = ['type', 'source', 'target', 'read_only', 'consistency']

    @classmethod
    def parse(cls, mount_dict, normalize=False, win_host=False):
        if mount_dict.get('source'):
            if mount_dict['type'] == 'tmpfs':
                raise ConfigurationError('tmpfs mounts can not specify a source')

            mount_dict['source'] = normpath(mount_dict['source'], win_host)
            if normalize:
                mount_dict['source'] = normalize_path_for_engine(mount_dict['source'])

        return cls(**mount_dict)

    def __init__(self, type, source=None, target=None, read_only=None, consistency=None, **kwargs):
        self.type = type
        self.source = source
        self.target = target
        self.read_only = read_only
        self.consistency = consistency
        self.options = None
        if self.type in kwargs:
            self.options = kwargs[self.type]

    def as_volume_spec(self):
        mode = 'ro' if self.read_only else 'rw'
        return VolumeSpec(external=self.source, internal=self.target, mode=mode)

    def legacy_repr(self):
        return self.as_volume_spec().repr()

    def repr(self):
        res = {}
        for field in self._fields:
            if getattr(self, field, None):
                res[field] = getattr(self, field)
        if self.options:
            res[self.type] = self.options
        return res

    @property
    def is_named_volume(self):
        return self.type == 'volume' and self.source

    @property
    def is_tmpfs(self):
        return self.type == 'tmpfs'

    @property
    def external(self):
        return self.source


class VolumeSpec(namedtuple('_VolumeSpec', 'external internal mode')):
    win32 = False

    @classmethod
    def _parse_unix(cls, volume_config):
        parts = volume_config.split(':')

        if len(parts) > 3:
            raise ConfigurationError(
                "Volume %s has incorrect format, should be "
                "external:internal[:mode]" % volume_config)

        if len(parts) == 1:
            external = None
            internal = os.path.normpath(parts[0])
        else:
            external = os.path.normpath(parts[0])
            internal = os.path.normpath(parts[1])

        mode = 'rw'
        if len(parts) == 3:
            mode = parts[2]

        return cls(external, internal, mode)

    @classmethod
    def _parse_win32(cls, volume_config, normalize):
        # relative paths in windows expand to include the drive, eg C:\
        # so we join the first 2 parts back together to count as one
        mode = 'rw'

        def separate_next_section(volume_config):
            drive, tail = splitdrive(volume_config)
            parts = tail.split(':', 1)
            if drive:
                parts[0] = drive + parts[0]
            return parts

        parts = separate_next_section(volume_config)
        if len(parts) == 1:
            internal = parts[0]
            external = None
        else:
            external = parts[0]
            parts = separate_next_section(parts[1])
            external = normpath(external, True)
            internal = parts[0]
            if len(parts) > 1:
                if ':' in parts[1]:
                    raise ConfigurationError(
                        "Volume %s has incorrect format, should be "
                        "external:internal[:mode]" % volume_config
                    )
                mode = parts[1]

        if normalize:
            external = normalize_path_for_engine(external) if external else None

        result = cls(external, internal, mode)
        result.win32 = True
        return result

    @classmethod
    def parse(cls, volume_config, normalize=False, win_host=False):
        """Parse a volume_config path and split it into external:internal[:mode]
        parts to be returned as a valid VolumeSpec.
        """
        if IS_WINDOWS_PLATFORM or win_host:
            return cls._parse_win32(volume_config, normalize)
        else:
            return cls._parse_unix(volume_config)

    def repr(self):
        external = self.external + ':' if self.external else ''
        mode = ':' + self.mode if self.external else ''
        return '{ext}{v.internal}{mode}'.format(mode=mode, ext=external, v=self)

    @property
    def is_named_volume(self):
        res = self.external and not self.external.startswith(('.', '/', '~'))
        if not self.win32:
            return res

        return (
            res and not self.external.startswith('\\') and
            not win32_root_path_pattern.match(self.external)
        )


class ServiceLink(namedtuple('_ServiceLink', 'target alias')):

    @classmethod
    def parse(cls, link_spec):
        target, _, alias = link_spec.partition(':')
        if not alias:
            alias = target
        return cls(target, alias)

    def repr(self):
        if self.target == self.alias:
            return self.target
        return '{s.target}:{s.alias}'.format(s=self)

    @property
    def merge_field(self):
        return self.alias


class ServiceConfigBase(namedtuple('_ServiceConfigBase', 'source target uid gid mode name')):
    @classmethod
    def parse(cls, spec):
        if isinstance(spec, six.string_types):
            return cls(spec, None, None, None, None, None)
        return cls(
            spec.get('source'),
            spec.get('target'),
            spec.get('uid'),
            spec.get('gid'),
            spec.get('mode'),
            spec.get('name')
        )

    @property
    def merge_field(self):
        return self.source

    def repr(self):
        return dict(
            [(k, v) for k, v in zip(self._fields, self) if v is not None]
        )


class ServiceSecret(ServiceConfigBase):
    pass


class ServiceConfig(ServiceConfigBase):
    pass


class ServicePort(namedtuple('_ServicePort', 'target published protocol mode external_ip')):
    def __new__(cls, target, published, *args, **kwargs):
        try:
            if target:
                target = int(target)
        except ValueError:
            raise ConfigurationError('Invalid target port: {}'.format(target))

        if published:
            if isinstance(published, six.string_types) and '-' in published:  # "x-y:z" format
                a, b = published.split('-', 1)
                try:
                    int(a)
                    int(b)
                except ValueError:
                    raise ConfigurationError('Invalid published port: {}'.format(published))
            else:
                try:
                    published = int(published)
                except ValueError:
                    raise ConfigurationError('Invalid published port: {}'.format(published))

        return super(ServicePort, cls).__new__(
            cls, target, published, *args, **kwargs
        )

    @classmethod
    def parse(cls, spec):
        if isinstance(spec, cls):
            # When extending a service with ports, the port definitions have already been parsed
            return [spec]

        if not isinstance(spec, dict):
            result = []
            try:
                for k, v in build_port_bindings([spec]).items():
                    if '/' in k:
                        target, proto = k.split('/', 1)
                    else:
                        target, proto = (k, None)
                    for pub in v:
                        if pub is None:
                            result.append(
                                cls(target, None, proto, None, None)
                            )
                        elif isinstance(pub, tuple):
                            result.append(
                                cls(target, pub[1], proto, None, pub[0])
                            )
                        else:
                            result.append(
                                cls(target, pub, proto, None, None)
                            )
            except ValueError as e:
                raise ConfigurationError(str(e))

            return result

        return [cls(
            spec.get('target'),
            spec.get('published'),
            spec.get('protocol'),
            spec.get('mode'),
            None
        )]

    @property
    def merge_field(self):
        return (self.target, self.published, self.external_ip, self.protocol)

    def repr(self):
        return dict(
            [(k, v) for k, v in zip(self._fields, self) if v is not None]
        )

    def legacy_repr(self):
        return normalize_port_dict(self.repr())


class GenericResource(namedtuple('_GenericResource', 'kind value')):
    @classmethod
    def parse(cls, dct):
        if 'discrete_resource_spec' not in dct:
            raise ConfigurationError(
                'generic_resource entry must include a discrete_resource_spec key'
            )
        if 'kind' not in dct['discrete_resource_spec']:
            raise ConfigurationError(
                'generic_resource entry must include a discrete_resource_spec.kind subkey'
            )
        return cls(
            dct['discrete_resource_spec']['kind'],
            dct['discrete_resource_spec'].get('value')
        )

    def repr(self):
        return {
            'discrete_resource_spec': {
                'kind': self.kind,
                'value': self.value,
            }
        }

    @property
    def merge_field(self):
        return self.kind


def normalize_port_dict(port):
    return '{external_ip}{has_ext_ip}{published}{is_pub}{target}/{protocol}'.format(
        published=port.get('published', ''),
        is_pub=(':' if port.get('published') is not None or port.get('external_ip') else ''),
        target=port.get('target'),
        protocol=port.get('protocol', 'tcp'),
        external_ip=port.get('external_ip', ''),
        has_ext_ip=(':' if port.get('external_ip') else ''),
    )


class SecurityOpt(namedtuple('_SecurityOpt', 'value src_file')):
    @classmethod
    def parse(cls, value):
        if not isinstance(value, six.string_types):
            return value
        # based on https://github.com/docker/cli/blob/9de1b162f/cli/command/container/opts.go#L673-L697
        con = value.split('=', 2)
        if len(con) == 1 and con[0] != 'no-new-privileges':
            if ':' not in value:
                raise ConfigurationError('Invalid security_opt: {}'.format(value))
            con = value.split(':', 2)

        if con[0] == 'seccomp' and con[1] != 'unconfined':
            try:
                with open(unquote_path(con[1]), 'r') as f:
                    seccomp_data = json.load(f)
            except (IOError, ValueError) as e:
                raise ConfigurationError('Error reading seccomp profile: {}'.format(e))
            return cls(
                'seccomp={}'.format(json.dumps(seccomp_data)), con[1]
            )
        return cls(value, None)

    def repr(self):
        if self.src_file is not None:
            return 'seccomp:{}'.format(self.src_file)
        return self.value

    @property
    def merge_field(self):
        return self.value
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import os

from compose.config.config import ConfigDetails
from compose.config.config import ConfigFile
from compose.config.config import load


def build_config(contents, **kwargs):
    return load(build_config_details(contents, **kwargs))


def build_config_details(contents, working_dir='working_dir', filename='filename.yml'):
    return ConfigDetails(
        working_dir,
        [ConfigFile(filename, contents)],
    )


def create_custom_host_file(client, filename, content):
    dirname = os.path.dirname(filename)
    container = client.create_container(
        'busybox:latest',
        ['sh', '-c', 'echo -n "{}" > {}'.format(content, filename)],
        volumes={dirname: {}},
        host_config=client.create_host_config(
            binds={dirname: {'bind': dirname, 'ro': False}},
            network_mode='none',
        ),
    )
    try:
        client.start(container)
        exitcode = client.wait(container)['StatusCode']

        if exitcode != 0:
            output = client.logs(container)
            raise Exception(
                "Container exited with code {}:\n{}".format(exitcode, output))

        container_info = client.inspect_container(container)
        if 'Node' in container_info:
            return container_info['Node']['Name']
    finally:
        client.remove_container(container, force=True)


def create_host_file(client, filename):
    with open(filename, 'r') as fh:
        content = fh.read()

    return create_custom_host_file(client, filename, content)
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import sys

if sys.version_info >= (2, 7):
    import unittest  # NOQA
else:
    import unittest2 as unittest  # NOQA

try:
    from unittest import mock
except ImportError:
    import mock  # NOQA
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest

from .. import mock
from .testcases import DockerClientTestCase
from compose.config.types import VolumeSpec
from compose.project import Project
from compose.service import ConvergenceStrategy


class ResilienceTest(DockerClientTestCase):
    def setUp(self):
        self.db = self.create_service(
            'db',
            volumes=[VolumeSpec.parse('/var/db')],
            command='top')
        self.project = Project('composetest', [self.db], self.client)

        container = self.db.create_container()
        self.db.start_container(container)
        self.host_path = container.get_mount('/var/db')['Source']

    def tearDown(self):
        del self.project
        del self.db
        super(ResilienceTest, self).tearDown()

    def test_successful_recreate(self):
        self.project.up(strategy=ConvergenceStrategy.always)
        container = self.db.containers()[0]
        assert container.get_mount('/var/db')['Source'] == self.host_path

    def test_create_failure(self):
        with mock.patch('compose.service.Service.create_container', crash):
            with pytest.raises(Crash):
                self.project.up(strategy=ConvergenceStrategy.always)

        self.project.up()
        container = self.db.containers()[0]
        assert container.get_mount('/var/db')['Source'] == self.host_path

    def test_start_failure(self):
        with mock.patch('compose.service.Service.start_container', crash):
            with pytest.raises(Crash):
                self.project.up(strategy=ConvergenceStrategy.always)

        self.project.up()
        container = self.db.containers()[0]
        assert container.get_mount('/var/db')['Source'] == self.host_path


class Crash(Exception):
    pass


def crash(*args, **kwargs):
    raise Crash()
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import json
import os
import random
import shutil
import tempfile

import py
import pytest
from docker.errors import APIError
from docker.errors import NotFound

from .. import mock
from ..helpers import build_config as load_config
from ..helpers import create_host_file
from .testcases import DockerClientTestCase
from .testcases import SWARM_SKIP_CONTAINERS_ALL
from compose.config import config
from compose.config import ConfigurationError
from compose.config import types
from compose.config.types import VolumeFromSpec
from compose.config.types import VolumeSpec
from compose.const import COMPOSEFILE_V2_0 as V2_0
from compose.const import COMPOSEFILE_V2_1 as V2_1
from compose.const import COMPOSEFILE_V2_2 as V2_2
from compose.const import COMPOSEFILE_V2_3 as V2_3
from compose.const import COMPOSEFILE_V3_1 as V3_1
from compose.const import LABEL_PROJECT
from compose.const import LABEL_SERVICE
from compose.container import Container
from compose.errors import HealthCheckFailed
from compose.errors import NoHealthCheckConfigured
from compose.project import Project
from compose.project import ProjectError
from compose.service import ConvergenceStrategy
from tests.integration.testcases import if_runtime_available
from tests.integration.testcases import is_cluster
from tests.integration.testcases import no_cluster
from tests.integration.testcases import v2_1_only
from tests.integration.testcases import v2_2_only
from tests.integration.testcases import v2_3_only
from tests.integration.testcases import v2_only
from tests.integration.testcases import v3_only


def build_config(**kwargs):
    return config.Config(
        version=kwargs.get('version'),
        services=kwargs.get('services'),
        volumes=kwargs.get('volumes'),
        networks=kwargs.get('networks'),
        secrets=kwargs.get('secrets'),
        configs=kwargs.get('configs'),
    )


class ProjectTest(DockerClientTestCase):

    def test_containers(self):
        web = self.create_service('web')
        db = self.create_service('db')
        project = Project('composetest', [web, db], self.client)

        project.up()

        containers = project.containers()
        assert len(containers) == 2

    @pytest.mark.skipif(SWARM_SKIP_CONTAINERS_ALL, reason='Swarm /containers/json bug')
    def test_containers_stopped(self):
        web = self.create_service('web')
        db = self.create_service('db')
        project = Project('composetest', [web, db], self.client)

        project.up()
        assert len(project.containers()) == 2
        assert len(project.containers(stopped=True)) == 2

        project.stop()
        assert len(project.containers()) == 0
        assert len(project.containers(stopped=True)) == 2

    def test_containers_with_service_names(self):
        web = self.create_service('web')
        db = self.create_service('db')
        project = Project('composetest', [web, db], self.client)

        project.up()

        containers = project.containers(['web'])
        assert len(containers) == 1
        assert containers[0].name.startswith('composetest_web_')

    def test_containers_with_extra_service(self):
        web = self.create_service('web')
        web_1 = web.create_container()

        db = self.create_service('db')
        db_1 = db.create_container()

        self.create_service('extra').create_container()

        project = Project('composetest', [web, db], self.client)
        assert set(project.containers(stopped=True)) == set([web_1, db_1])

    def test_parallel_pull_with_no_image(self):
        config_data = build_config(
            version=V2_3,
            services=[{
                'name': 'web',
                'build': {'context': '.'},
            }],
        )

        project = Project.from_config(
            name='composetest',
            config_data=config_data,
            client=self.client
        )

        project.pull(parallel_pull=True)

    def test_volumes_from_service(self):
        project = Project.from_config(
            name='composetest',
            config_data=load_config({
                'data': {
                    'image': 'busybox:latest',
                    'volumes': ['/var/data'],
                },
                'db': {
                    'image': 'busybox:latest',
                    'volumes_from': ['data'],
                },
            }),
            client=self.client,
        )
        db = project.get_service('db')
        data = project.get_service('data')
        assert db.volumes_from == [VolumeFromSpec(data, 'rw', 'service')]

    def test_volumes_from_container(self):
        data_container = Container.create(
            self.client,
            image='busybox:latest',
            volumes=['/var/data'],
            name='composetest_data_container',
            labels={LABEL_PROJECT: 'composetest'},
            host_config={},
        )
        project = Project.from_config(
            name='composetest',
            config_data=load_config({
                'db': {
                    'image': 'busybox:latest',
                    'volumes_from': ['composetest_data_container'],
                },
            }),
            client=self.client,
        )
        db = project.get_service('db')
        assert db._get_volumes_from() == [data_container.id + ':rw']

    @v2_only()
    @no_cluster('container networks not supported in Swarm')
    def test_network_mode_from_service(self):
        project = Project.from_config(
            name='composetest',
            client=self.client,
            config_data=load_config({
                'version': str(V2_0),
                'services': {
                    'net': {
                        'image': 'busybox:latest',
                        'command': ["top"]
                    },
                    'web': {
                        'image': 'busybox:latest',
                        'network_mode': 'service:net',
                        'command': ["top"]
                    },
                },
            }),
        )

        project.up()

        web = project.get_service('web')
        net = project.get_service('net')
        assert web.network_mode.mode == 'container:' + net.containers()[0].id

    @v2_only()
    @no_cluster('container networks not supported in Swarm')
    def test_network_mode_from_container(self):
        def get_project():
            return Project.from_config(
                name='composetest',
                config_data=load_config({
                    'version': str(V2_0),
                    'services': {
                        'web': {
                            'image': 'busybox:latest',
                            'network_mode': 'container:composetest_net_container'
                        },
                    },
                }),
                client=self.client,
            )

        with pytest.raises(ConfigurationError) as excinfo:
            get_project()

        assert "container 'composetest_net_container' which does not exist" in excinfo.exconly()

        net_container = Container.create(
            self.client,
            image='busybox:latest',
            name='composetest_net_container',
            command='top',
            labels={LABEL_PROJECT: 'composetest'},
            host_config={},
        )
        net_container.start()

        project = get_project()
        project.up()

        web = project.get_service('web')
        assert web.network_mode.mode == 'container:' + net_container.id

    @no_cluster('container networks not supported in Swarm')
    def test_net_from_service_v1(self):
        project = Project.from_config(
            name='composetest',
            config_data=load_config({
                'net': {
                    'image': 'busybox:latest',
                    'command': ["top"]
                },
                'web': {
                    'image': 'busybox:latest',
                    'net': 'container:net',
                    'command': ["top"]
                },
            }),
            client=self.client,
        )

        project.up()

        web = project.get_service('web')
        net = project.get_service('net')
        assert web.network_mode.mode == 'container:' + net.containers()[0].id

    @no_cluster('container networks not supported in Swarm')
    def test_net_from_container_v1(self):
        def get_project():
            return Project.from_config(
                name='composetest',
                config_data=load_config({
                    'web': {
                        'image': 'busybox:latest',
                        'net': 'container:composetest_net_container'
                    },
                }),
                client=self.client,
            )

        with pytest.raises(ConfigurationError) as excinfo:
            get_project()

        assert "container 'composetest_net_container' which does not exist" in excinfo.exconly()

        net_container = Container.create(
            self.client,
            image='busybox:latest',
            name='composetest_net_container',
            command='top',
            labels={LABEL_PROJECT: 'composetest'},
            host_config={},
        )
        net_container.start()

        project = get_project()
        project.up()

        web = project.get_service('web')
        assert web.network_mode.mode == 'container:' + net_container.id

    def test_start_pause_unpause_stop_kill_remove(self):
        web = self.create_service('web')
        db = self.create_service('db')
        project = Project('composetest', [web, db], self.client)

        project.start()

        assert len(web.containers()) == 0
        assert len(db.containers()) == 0

        web_container_1 = web.create_container()
        web_container_2 = web.create_container()
        db_container = db.create_container()

        project.start(service_names=['web'])
        assert set(c.name for c in project.containers() if c.is_running) == set(
            [web_container_1.name, web_container_2.name]
        )

        project.start()
        assert set(c.name for c in project.containers() if c.is_running) == set(
            [web_container_1.name, web_container_2.name, db_container.name]
        )

        project.pause(service_names=['web'])
        assert set([c.name for c in project.containers() if c.is_paused]) == set(
            [web_container_1.name, web_container_2.name]
        )

        project.pause()
        assert set([c.name for c in project.containers() if c.is_paused]) == set(
            [web_container_1.name, web_container_2.name, db_container.name]
        )

        project.unpause(service_names=['db'])
        assert len([c.name for c in project.containers() if c.is_paused]) == 2

        project.unpause()
        assert len([c.name for c in project.containers() if c.is_paused]) == 0

        project.stop(service_names=['web'], timeout=1)
        assert set(c.name for c in project.containers() if c.is_running) == set([db_container.name])

        project.kill(service_names=['db'])
        assert len([c for c in project.containers() if c.is_running]) == 0
        assert len(project.containers(stopped=True)) == 3

        project.remove_stopped(service_names=['web'])
        assert len(project.containers(stopped=True)) == 1

        project.remove_stopped()
        assert len(project.containers(stopped=True)) == 0

    def test_create(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        project = Project('composetest', [web, db], self.client)

        project.create(['db'])
        containers = project.containers(stopped=True)
        assert len(containers) == 1
        assert not containers[0].is_running
        db_containers = db.containers(stopped=True)
        assert len(db_containers) == 1
        assert not db_containers[0].is_running
        assert len(web.containers(stopped=True)) == 0

    def test_create_twice(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        project = Project('composetest', [web, db], self.client)

        project.create(['db', 'web'])
        project.create(['db', 'web'])
        containers = project.containers(stopped=True)
        assert len(containers) == 2
        db_containers = db.containers(stopped=True)
        assert len(db_containers) == 1
        assert not db_containers[0].is_running
        web_containers = web.containers(stopped=True)
        assert len(web_containers) == 1
        assert not web_containers[0].is_running

    def test_create_with_links(self):
        db = self.create_service('db')
        web = self.create_service('web', links=[(db, 'db')])
        project = Project('composetest', [db, web], self.client)

        project.create(['web'])
        # self.assertEqual(len(project.containers()), 0)
        assert len(project.containers(stopped=True)) == 2
        assert not [c for c in project.containers(stopped=True) if c.is_running]
        assert len(db.containers(stopped=True)) == 1
        assert len(web.containers(stopped=True)) == 1

    def test_create_strategy_always(self):
        db = self.create_service('db')
        project = Project('composetest', [db], self.client)
        project.create(['db'])
        old_id = project.containers(stopped=True)[0].id

        project.create(['db'], strategy=ConvergenceStrategy.always)
        assert len(project.containers(stopped=True)) == 1

        db_container = project.containers(stopped=True)[0]
        assert not db_container.is_running
        assert db_container.id != old_id

    def test_create_strategy_never(self):
        db = self.create_service('db')
        project = Project('composetest', [db], self.client)
        project.create(['db'])
        old_id = project.containers(stopped=True)[0].id

        project.create(['db'], strategy=ConvergenceStrategy.never)
        assert len(project.containers(stopped=True)) == 1

        db_container = project.containers(stopped=True)[0]
        assert not db_container.is_running
        assert db_container.id == old_id

    def test_project_up(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        project = Project('composetest', [web, db], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up(['db'])
        assert len(project.containers()) == 1
        assert len(db.containers()) == 1
        assert len(web.containers()) == 0

    def test_project_up_starts_uncreated_services(self):
        db = self.create_service('db')
        web = self.create_service('web', links=[(db, 'db')])
        project = Project('composetest', [db, web], self.client)
        project.up(['db'])
        assert len(project.containers()) == 1

        project.up()
        assert len(project.containers()) == 2
        assert len(db.containers()) == 1
        assert len(web.containers()) == 1

    def test_recreate_preserves_volumes(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/etc')])
        project = Project('composetest', [web, db], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up(['db'])
        assert len(project.containers()) == 1
        old_db_id = project.containers()[0].id
        db_volume_path = project.containers()[0].get('Volumes./etc')

        project.up(strategy=ConvergenceStrategy.always)
        assert len(project.containers()) == 2

        db_container = [c for c in project.containers() if c.service == 'db'][0]
        assert db_container.id != old_db_id
        assert db_container.get('Volumes./etc') == db_volume_path

    @v2_3_only()
    def test_recreate_preserves_mounts(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[types.MountSpec(type='volume', target='/etc')])
        project = Project('composetest', [web, db], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up(['db'])
        assert len(project.containers()) == 1
        old_db_id = project.containers()[0].id
        db_volume_path = project.containers()[0].get_mount('/etc')['Source']

        project.up(strategy=ConvergenceStrategy.always)
        assert len(project.containers()) == 2

        db_container = [c for c in project.containers() if c.service == 'db'][0]
        assert db_container.id != old_db_id
        assert db_container.get_mount('/etc')['Source'] == db_volume_path

    def test_project_up_with_no_recreate_running(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        project = Project('composetest', [web, db], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up(['db'])
        assert len(project.containers()) == 1
        container, = project.containers()
        old_db_id = container.id
        db_volume_path = container.get_mount('/var/db')['Source']

        project.up(strategy=ConvergenceStrategy.never)
        assert len(project.containers()) == 2

        db_container = [c for c in project.containers() if c.name == container.name][0]
        assert db_container.id == old_db_id
        assert db_container.get_mount('/var/db')['Source'] == db_volume_path

    def test_project_up_with_no_recreate_stopped(self):
        web = self.create_service('web')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        project = Project('composetest', [web, db], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up(['db'])
        project.kill()

        old_containers = project.containers(stopped=True)

        assert len(old_containers) == 1
        old_container, = old_containers
        old_db_id = old_container.id
        db_volume_path = old_container.get_mount('/var/db')['Source']

        project.up(strategy=ConvergenceStrategy.never)

        new_containers = project.containers(stopped=True)
        assert len(new_containers) == 2
        assert [c.is_running for c in new_containers] == [True, True]

        db_container = [c for c in new_containers if c.service == 'db'][0]
        assert db_container.id == old_db_id
        assert db_container.get_mount('/var/db')['Source'] == db_volume_path

    def test_project_up_without_all_services(self):
        console = self.create_service('console')
        db = self.create_service('db')
        project = Project('composetest', [console, db], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up()
        assert len(project.containers()) == 2
        assert len(db.containers()) == 1
        assert len(console.containers()) == 1

    def test_project_up_starts_links(self):
        console = self.create_service('console')
        db = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        web = self.create_service('web', links=[(db, 'db')])

        project = Project('composetest', [web, db, console], self.client)
        project.start()
        assert len(project.containers()) == 0

        project.up(['web'])
        assert len(project.containers()) == 2
        assert len(web.containers()) == 1
        assert len(db.containers()) == 1
        assert len(console.containers()) == 0

    def test_project_up_starts_depends(self):
        project = Project.from_config(
            name='composetest',
            config_data=load_config({
                'console': {
                    'image': 'busybox:latest',
                    'command': ["top"],
                },
                'data': {
                    'image': 'busybox:latest',
                    'command': ["top"]
                },
                'db': {
                    'image': 'busybox:latest',
                    'command': ["top"],
                    'volumes_from': ['data'],
                },
                'web': {
                    'image': 'busybox:latest',
                    'command': ["top"],
                    'links': ['db'],
                },
            }),
            client=self.client,
        )
        project.start()
        assert len(project.containers()) == 0

        project.up(['web'])
        assert len(project.containers()) == 3
        assert len(project.get_service('web').containers()) == 1
        assert len(project.get_service('db').containers()) == 1
        assert len(project.get_service('data').containers()) == 1
        assert len(project.get_service('console').containers()) == 0

    def test_project_up_with_no_deps(self):
        project = Project.from_config(
            name='composetest',
            config_data=load_config({
                'console': {
                    'image': 'busybox:latest',
                    'command': ["top"],
                },
                'data': {
                    'image': 'busybox:latest',
                    'command': ["top"]
                },
                'db': {
                    'image': 'busybox:latest',
                    'command': ["top"],
                    'volumes_from': ['data'],
                },
                'web': {
                    'image': 'busybox:latest',
                    'command': ["top"],
                    'links': ['db'],
                },
            }),
            client=self.client,
        )
        project.start()
        assert len(project.containers()) == 0

        project.up(['db'], start_deps=False)
        assert len(project.containers(stopped=True)) == 2
        assert len(project.get_service('web').containers()) == 0
        assert len(project.get_service('db').containers()) == 1
        assert len(project.get_service('data').containers(stopped=True)) == 1
        assert not project.get_service('data').containers(stopped=True)[0].is_running
        assert len(project.get_service('console').containers()) == 0

    def test_project_up_recreate_with_tmpfs_volume(self):
        # https://github.com/docker/compose/issues/4751
        project = Project.from_config(
            name='composetest',
            config_data=load_config({
                'version': '2.1',
                'services': {
                    'foo': {
                        'image': 'busybox:latest',
                        'tmpfs': ['/dev/shm'],
                        'volumes': ['/dev/shm']
                    }
                }
            }), client=self.client
        )
        project.up()
        project.up(strategy=ConvergenceStrategy.always)

    def test_unscale_after_restart(self):
        web = self.create_service('web')
        project = Project('composetest', [web], self.client)

        project.start()

        service = project.get_service('web')
        service.scale(1)
        assert len(service.containers()) == 1
        service.scale(3)
        assert len(service.containers()) == 3
        project.up()
        service = project.get_service('web')
        assert len(service.containers()) == 1
        service.scale(1)
        assert len(service.containers()) == 1
        project.up(scale_override={'web': 3})
        service = project.get_service('web')
        assert len(service.containers()) == 3
        # does scale=0 ,makes any sense? after recreating at least 1 container is running
        service.scale(0)
        project.up()
        service = project.get_service('web')
        assert len(service.containers()) == 1

    @v2_only()
    def test_project_up_networks(self):
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top',
                'networks': {
                    'foo': None,
                    'bar': None,
                    'baz': {'aliases': ['extra']},
                },
            }],
            networks={
                'foo': {'driver': 'bridge'},
                'bar': {'driver': None},
                'baz': {},
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up()

        containers = project.containers()
        assert len(containers) == 1
        container, = containers

        for net_name in ['foo', 'bar', 'baz']:
            full_net_name = 'composetest_{}'.format(net_name)
            network_data = self.client.inspect_network(full_net_name)
            assert network_data['Name'] == full_net_name

        aliases_key = 'NetworkSettings.Networks.{net}.Aliases'
        assert 'web' in container.get(aliases_key.format(net='composetest_foo'))
        assert 'web' in container.get(aliases_key.format(net='composetest_baz'))
        assert 'extra' in container.get(aliases_key.format(net='composetest_baz'))

        foo_data = self.client.inspect_network('composetest_foo')
        assert foo_data['Driver'] == 'bridge'

    @v2_only()
    def test_up_with_ipam_config(self):
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'networks': {'front': None},
            }],
            networks={
                'front': {
                    'driver': 'bridge',
                    'driver_opts': {
                        "com.docker.network.bridge.enable_icc": "false",
                    },
                    'ipam': {
                        'driver': 'default',
                        'config': [{
                            "subnet": "172.28.0.0/16",
                            "ip_range": "172.28.5.0/24",
                            "gateway": "172.28.5.254",
                            "aux_addresses": {
                                "a": "172.28.1.5",
                                "b": "172.28.1.6",
                                "c": "172.28.1.7",
                            },
                        }],
                    },
                },
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up()

        network = self.client.networks(names=['composetest_front'])[0]

        assert network['Options'] == {
            "com.docker.network.bridge.enable_icc": "false"
        }

        assert network['IPAM'] == {
            'Driver': 'default',
            'Options': None,
            'Config': [{
                'Subnet': "172.28.0.0/16",
                'IPRange': "172.28.5.0/24",
                'Gateway': "172.28.5.254",
                'AuxiliaryAddresses': {
                    'a': '172.28.1.5',
                    'b': '172.28.1.6',
                    'c': '172.28.1.7',
                },
            }],
        }

    @v2_only()
    def test_up_with_ipam_options(self):
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'networks': {'front': None},
            }],
            networks={
                'front': {
                    'driver': 'bridge',
                    'ipam': {
                        'driver': 'default',
                        'options': {
                            "com.docker.compose.network.test": "9-29-045"
                        }
                    },
                },
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up()

        network = self.client.networks(names=['composetest_front'])[0]

        assert network['IPAM']['Options'] == {
            "com.docker.compose.network.test": "9-29-045"
        }

    @v2_1_only()
    def test_up_with_network_static_addresses(self):
        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top',
                'networks': {
                    'static_test': {
                        'ipv4_address': '172.16.100.100',
                        'ipv6_address': 'fe80::1001:102'
                    }
                },
            }],
            networks={
                'static_test': {
                    'driver': 'bridge',
                    'driver_opts': {
                        "com.docker.network.enable_ipv6": "true",
                    },
                    'ipam': {
                        'driver': 'default',
                        'config': [
                            {"subnet": "172.16.100.0/24",
                             "gateway": "172.16.100.1"},
                            {"subnet": "fe80::/64",
                             "gateway": "fe80::1001:1"}
                        ]
                    },
                    'enable_ipv6': True,
                }
            }
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up(detached=True)

        service_container = project.get_service('web').containers()[0]

        ipam_config = (service_container.inspect().get('NetworkSettings', {}).
                       get('Networks', {}).get('composetest_static_test', {}).
                       get('IPAMConfig', {}))
        assert ipam_config.get('IPv4Address') == '172.16.100.100'
        assert ipam_config.get('IPv6Address') == 'fe80::1001:102'

    @v2_3_only()
    def test_up_with_network_priorities(self):
        mac_address = '74:6f:75:68:6f:75'

        def get_config_data(p1, p2, p3):
            return build_config(
                version=V2_3,
                services=[{
                    'name': 'web',
                    'image': 'busybox:latest',
                    'networks': {
                        'n1': {
                            'priority': p1,
                        },
                        'n2': {
                            'priority': p2,
                        },
                        'n3': {
                            'priority': p3,
                        }
                    },
                    'command': 'top',
                    'mac_address': mac_address
                }],
                networks={
                    'n1': {},
                    'n2': {},
                    'n3': {}
                }
            )

        config1 = get_config_data(1000, 1, 1)
        config2 = get_config_data(2, 3, 1)
        config3 = get_config_data(5, 40, 100)

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config1
        )
        project.up(detached=True)
        service_container = project.get_service('web').containers()[0]
        net_config = service_container.inspect()['NetworkSettings']['Networks']['composetest_n1']
        assert net_config['MacAddress'] == mac_address

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config2
        )
        project.up(detached=True)
        service_container = project.get_service('web').containers()[0]
        net_config = service_container.inspect()['NetworkSettings']['Networks']['composetest_n2']
        assert net_config['MacAddress'] == mac_address

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config3
        )
        project.up(detached=True)
        service_container = project.get_service('web').containers()[0]
        net_config = service_container.inspect()['NetworkSettings']['Networks']['composetest_n3']
        assert net_config['MacAddress'] == mac_address

    @v2_1_only()
    def test_up_with_enable_ipv6(self):
        self.require_api_version('1.23')
        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top',
                'networks': {
                    'static_test': {
                        'ipv6_address': 'fe80::1001:102'
                    }
                },
            }],
            networks={
                'static_test': {
                    'driver': 'bridge',
                    'enable_ipv6': True,
                    'ipam': {
                        'driver': 'default',
                        'config': [
                            {"subnet": "fe80::/64",
                             "gateway": "fe80::1001:1"}
                        ]
                    }
                }
            }
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up(detached=True)
        network = [n for n in self.client.networks() if 'static_test' in n['Name']][0]
        service_container = project.get_service('web').containers()[0]

        assert network['EnableIPv6'] is True
        ipam_config = (service_container.inspect().get('NetworkSettings', {}).
                       get('Networks', {}).get('composetest_static_test', {}).
                       get('IPAMConfig', {}))
        assert ipam_config.get('IPv6Address') == 'fe80::1001:102'

    @v2_only()
    def test_up_with_network_static_addresses_missing_subnet(self):
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'networks': {
                    'static_test': {
                        'ipv4_address': '172.16.100.100',
                        'ipv6_address': 'fe80::1001:101'
                    }
                },
            }],
            networks={
                'static_test': {
                    'driver': 'bridge',
                    'driver_opts': {
                        "com.docker.network.enable_ipv6": "true",
                    },
                    'ipam': {
                        'driver': 'default',
                    },
                },
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )

        with pytest.raises(ProjectError):
            project.up()

    @v2_1_only()
    def test_up_with_network_link_local_ips(self):
        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'networks': {
                    'linklocaltest': {
                        'link_local_ips': ['169.254.8.8']
                    }
                }
            }],
            networks={
                'linklocaltest': {'driver': 'bridge'}
            }
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )
        project.up(detached=True)

        service_container = project.get_service('web').containers(stopped=True)[0]
        ipam_config = service_container.inspect().get(
            'NetworkSettings', {}
        ).get(
            'Networks', {}
        ).get(
            'composetest_linklocaltest', {}
        ).get('IPAMConfig', {})
        assert 'LinkLocalIPs' in ipam_config
        assert ipam_config['LinkLocalIPs'] == ['169.254.8.8']

    @v2_1_only()
    def test_up_with_custom_name_resources(self):
        config_data = build_config(
            version=V2_2,
            services=[{
                'name': 'web',
                'volumes': [VolumeSpec.parse('foo:/container-path')],
                'networks': {'foo': {}},
                'image': 'busybox:latest'
            }],
            networks={
                'foo': {
                    'name': 'zztop',
                    'labels': {'com.docker.compose.test_value': 'sharpdressedman'}
                }
            },
            volumes={
                'foo': {
                    'name': 'acdc',
                    'labels': {'com.docker.compose.test_value': 'thefuror'}
                }
            }
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )

        project.up(detached=True)
        network = [n for n in self.client.networks() if n['Name'] == 'zztop'][0]
        volume = [v for v in self.client.volumes()['Volumes'] if v['Name'] == 'acdc'][0]

        assert network['Labels']['com.docker.compose.test_value'] == 'sharpdressedman'
        assert volume['Labels']['com.docker.compose.test_value'] == 'thefuror'

    @v2_1_only()
    def test_up_with_isolation(self):
        self.require_api_version('1.24')
        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'isolation': 'default'
            }],
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )
        project.up(detached=True)
        service_container = project.get_service('web').containers(stopped=True)[0]
        assert service_container.inspect()['HostConfig']['Isolation'] == 'default'

    @v2_1_only()
    def test_up_with_invalid_isolation(self):
        self.require_api_version('1.24')
        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'isolation': 'foobar'
            }],
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )
        with pytest.raises(ProjectError):
            project.up()

    @v2_3_only()
    @if_runtime_available('runc')
    def test_up_with_runtime(self):
        self.require_api_version('1.30')
        config_data = build_config(
            version=V2_3,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'runtime': 'runc'
            }],
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )
        project.up(detached=True)
        service_container = project.get_service('web').containers(stopped=True)[0]
        assert service_container.inspect()['HostConfig']['Runtime'] == 'runc'

    @v2_3_only()
    def test_up_with_invalid_runtime(self):
        self.require_api_version('1.30')
        config_data = build_config(
            version=V2_3,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'runtime': 'foobar'
            }],
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )
        with pytest.raises(ProjectError):
            project.up()

    @v2_3_only()
    @if_runtime_available('nvidia')
    def test_up_with_nvidia_runtime(self):
        self.require_api_version('1.30')
        config_data = build_config(
            version=V2_3,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'runtime': 'nvidia'
            }],
        )
        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )
        project.up(detached=True)
        service_container = project.get_service('web').containers(stopped=True)[0]
        assert service_container.inspect()['HostConfig']['Runtime'] == 'nvidia'

    @v2_only()
    def test_project_up_with_network_internal(self):
        self.require_api_version('1.23')
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'networks': {'internal': None},
            }],
            networks={
                'internal': {'driver': 'bridge', 'internal': True},
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up()

        network = self.client.networks(names=['composetest_internal'])[0]

        assert network['Internal'] is True

    @v2_1_only()
    def test_project_up_with_network_label(self):
        self.require_api_version('1.23')

        network_name = 'network_with_label'

        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'networks': {network_name: None}
            }],
            networks={
                network_name: {'labels': {'label_key': 'label_val'}}
            }
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data
        )

        project.up()

        networks = [
            n for n in self.client.networks()
            if n['Name'].startswith('composetest_')
        ]

        assert [n['Name'] for n in networks] == ['composetest_{}'.format(network_name)]
        assert 'label_key' in networks[0]['Labels']
        assert networks[0]['Labels']['label_key'] == 'label_val'

    @v2_only()
    def test_project_up_volumes(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={vol_name: {'driver': 'local'}},
        )

        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.up()
        assert len(project.containers()) == 1

        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'

    @v2_1_only()
    def test_project_up_with_volume_labels(self):
        self.require_api_version('1.23')

        volume_name = 'volume_with_label'

        config_data = build_config(
            version=V2_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'volumes': [VolumeSpec.parse('{}:/data'.format(volume_name))]
            }],
            volumes={
                volume_name: {
                    'labels': {
                        'label_key': 'label_val'
                    }
                }
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )

        project.up()

        volumes = [
            v for v in self.client.volumes().get('Volumes', [])
            if v['Name'].split('/')[-1].startswith('composetest_')
        ]

        assert set([v['Name'].split('/')[-1] for v in volumes]) == set(
            ['composetest_{}'.format(volume_name)]
        )

        assert 'label_key' in volumes[0]['Labels']
        assert volumes[0]['Labels']['label_key'] == 'label_val'

    @v2_only()
    def test_project_up_logging_with_multiple_files(self):
        base_file = config.ConfigFile(
            'base.yml',
            {
                'version': str(V2_0),
                'services': {
                    'simple': {'image': 'busybox:latest', 'command': 'top'},
                    'another': {
                        'image': 'busybox:latest',
                        'command': 'top',
                        'logging': {
                            'driver': "json-file",
                            'options': {
                                'max-size': "10m"
                            }
                        }
                    }
                }

            })
        override_file = config.ConfigFile(
            'override.yml',
            {
                'version': str(V2_0),
                'services': {
                    'another': {
                        'logging': {
                            'driver': "none"
                        }
                    }
                }

            })
        details = config.ConfigDetails('.', [base_file, override_file])

        tmpdir = py.test.ensuretemp('logging_test')
        self.addCleanup(tmpdir.remove)
        with tmpdir.as_cwd():
            config_data = config.load(details)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        project.up()
        containers = project.containers()
        assert len(containers) == 2

        another = project.get_service('another').containers()[0]
        log_config = another.get('HostConfig.LogConfig')
        assert log_config
        assert log_config.get('Type') == 'none'

    @v2_only()
    def test_project_up_port_mappings_with_multiple_files(self):
        base_file = config.ConfigFile(
            'base.yml',
            {
                'version': str(V2_0),
                'services': {
                    'simple': {
                        'image': 'busybox:latest',
                        'command': 'top',
                        'ports': ['1234:1234']
                    },
                },

            })
        override_file = config.ConfigFile(
            'override.yml',
            {
                'version': str(V2_0),
                'services': {
                    'simple': {
                        'ports': ['1234:1234']
                    }
                }

            })
        details = config.ConfigDetails('.', [base_file, override_file])

        config_data = config.load(details)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        project.up()
        containers = project.containers()
        assert len(containers) == 1

    @v2_2_only()
    def test_project_up_config_scale(self):
        config_data = build_config(
            version=V2_2,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top',
                'scale': 3
            }]
        )

        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        project.up()
        assert len(project.containers()) == 3

        project.up(scale_override={'web': 2})
        assert len(project.containers()) == 2

        project.up(scale_override={'web': 4})
        assert len(project.containers()) == 4

        project.stop()
        project.up()
        assert len(project.containers()) == 3

    @v2_only()
    def test_initialize_volumes(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={vol_name: {}},
        )

        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.volumes.initialize()

        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'

    @v2_only()
    def test_project_up_implicit_volume_driver(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={vol_name: {}},
        )

        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.up()

        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'

    @v3_only()
    def test_project_up_with_secrets(self):
        node = create_host_file(self.client, os.path.abspath('tests/fixtures/secrets/default'))

        config_data = build_config(
            version=V3_1,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'cat /run/secrets/special',
                'secrets': [
                    types.ServiceSecret.parse({'source': 'super', 'target': 'special'}),
                ],
                'environment': ['constraint:node=={}'.format(node if node is not None else '*')]
            }],
            secrets={
                'super': {
                    'file': os.path.abspath('tests/fixtures/secrets/default'),
                },
            },
        )

        project = Project.from_config(
            client=self.client,
            name='composetest',
            config_data=config_data,
        )
        project.up()
        project.stop()

        containers = project.containers(stopped=True)
        assert len(containers) == 1
        container, = containers

        output = container.logs()
        assert output == b"This is the secret\n"

    @v2_only()
    def test_initialize_volumes_invalid_volume_driver(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))

        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={vol_name: {'driver': 'foobar'}},
        )

        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        with pytest.raises(APIError if is_cluster(self.client) else config.ConfigurationError):
            project.volumes.initialize()

    @v2_only()
    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_initialize_volumes_updated_driver(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)

        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={vol_name: {'driver': 'local'}},
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.volumes.initialize()

        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'

        config_data = config_data._replace(
            volumes={vol_name: {'driver': 'smb'}}
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data,
            client=self.client
        )
        with pytest.raises(config.ConfigurationError) as e:
            project.volumes.initialize()
        assert 'Configuration for volume {0} specifies driver smb'.format(
            vol_name
        ) in str(e.value)

    @v2_only()
    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_initialize_volumes_updated_driver_opts(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)
        tmpdir = tempfile.mkdtemp(prefix='compose_test_')
        self.addCleanup(shutil.rmtree, tmpdir)
        driver_opts = {'o': 'bind', 'device': tmpdir, 'type': 'none'}

        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={
                vol_name: {
                    'driver': 'local',
                    'driver_opts': driver_opts
                }
            },
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.volumes.initialize()

        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'
        assert volume_data['Options'] == driver_opts

        driver_opts['device'] = '/opt/data/localdata'
        project = Project.from_config(
            name='composetest',
            config_data=config_data,
            client=self.client
        )
        with pytest.raises(config.ConfigurationError) as e:
            project.volumes.initialize()
        assert 'Configuration for volume {0} specifies "device" driver_opt {1}'.format(
            vol_name, driver_opts['device']
        ) in str(e.value)

    @v2_only()
    def test_initialize_volumes_updated_blank_driver(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)

        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={vol_name: {'driver': 'local'}},
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.volumes.initialize()

        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'

        config_data = config_data._replace(
            volumes={vol_name: {}}
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data,
            client=self.client
        )
        project.volumes.initialize()
        volume_data = self.get_volume_data(full_vol_name)
        assert volume_data['Name'].split('/')[-1] == full_vol_name
        assert volume_data['Driver'] == 'local'

    @v2_only()
    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_initialize_volumes_external_volumes(self):
        # Use composetest_ prefix so it gets garbage-collected in tearDown()
        vol_name = 'composetest_{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)
        self.client.create_volume(vol_name)
        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={
                vol_name: {'external': True, 'name': vol_name}
            },
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        project.volumes.initialize()

        with pytest.raises(NotFound):
            self.client.inspect_volume(full_vol_name)

    @v2_only()
    def test_initialize_volumes_inexistent_external_volume(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))

        config_data = build_config(
            version=V2_0,
            services=[{
                'name': 'web',
                'image': 'busybox:latest',
                'command': 'top'
            }],
            volumes={
                vol_name: {'external': True, 'name': vol_name}
            },
        )
        project = Project.from_config(
            name='composetest',
            config_data=config_data, client=self.client
        )
        with pytest.raises(config.ConfigurationError) as e:
            project.volumes.initialize()
        assert 'Volume {0} declared as external'.format(
            vol_name
        ) in str(e.value)

    @v2_only()
    def test_project_up_named_volumes_in_binds(self):
        vol_name = '{0:x}'.format(random.getrandbits(32))
        full_vol_name = 'composetest_{0}'.format(vol_name)

        base_file = config.ConfigFile(
            'base.yml',
            {
                'version': str(V2_0),
                'services': {
                    'simple': {
                        'image': 'busybox:latest',
                        'command': 'top',
                        'volumes': ['{0}:/data'.format(vol_name)]
                    },
                },
                'volumes': {
                    vol_name: {'driver': 'local'}
                }

            })
        config_details = config.ConfigDetails('.', [base_file])
        config_data = config.load(config_details)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        service = project.services[0]
        assert service.name == 'simple'
        volumes = service.options.get('volumes')
        assert len(volumes) == 1
        assert volumes[0].external == full_vol_name
        project.up()
        engine_volumes = self.client.volumes()['Volumes']
        container = service.get_container()
        assert [mount['Name'] for mount in container.get('Mounts')] == [full_vol_name]
        assert next((v for v in engine_volumes if v['Name'] == vol_name), None) is None

    def test_project_up_orphans(self):
        config_dict = {
            'service1': {
                'image': 'busybox:latest',
                'command': 'top',
            }
        }

        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        project.up()
        config_dict['service2'] = config_dict['service1']
        del config_dict['service1']

        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        with mock.patch('compose.project.log') as mock_log:
            project.up()

        mock_log.warning.assert_called_once_with(mock.ANY)

        assert len([
            ctnr for ctnr in project._labeled_containers()
            if ctnr.labels.get(LABEL_SERVICE) == 'service1'
        ]) == 1

        project.up(remove_orphans=True)

        assert len([
            ctnr for ctnr in project._labeled_containers()
            if ctnr.labels.get(LABEL_SERVICE) == 'service1'
        ]) == 0

    def test_project_up_ignore_orphans(self):
        config_dict = {
            'service1': {
                'image': 'busybox:latest',
                'command': 'top',
            }
        }

        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        project.up()
        config_dict['service2'] = config_dict['service1']
        del config_dict['service1']

        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        with mock.patch('compose.project.log') as mock_log:
            project.up(ignore_orphans=True)

        mock_log.warning.assert_not_called()

    @v2_1_only()
    def test_project_up_healthy_dependency(self):
        config_dict = {
            'version': '2.1',
            'services': {
                'svc1': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'healthcheck': {
                        'test': 'exit 0',
                        'retries': 1,
                        'timeout': '10s',
                        'interval': '1s'
                    },
                },
                'svc2': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'depends_on': {
                        'svc1': {'condition': 'service_healthy'},
                    }
                }
            }
        }
        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        project.up()
        containers = project.containers()
        assert len(containers) == 2

        svc1 = project.get_service('svc1')
        svc2 = project.get_service('svc2')
        assert 'svc1' in svc2.get_dependency_names()
        assert svc1.is_healthy()

    @v2_1_only()
    def test_project_up_unhealthy_dependency(self):
        config_dict = {
            'version': '2.1',
            'services': {
                'svc1': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'healthcheck': {
                        'test': 'exit 1',
                        'retries': 1,
                        'timeout': '10s',
                        'interval': '1s'
                    },
                },
                'svc2': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'depends_on': {
                        'svc1': {'condition': 'service_healthy'},
                    }
                }
            }
        }
        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        with pytest.raises(ProjectError):
            project.up()
        containers = project.containers()
        assert len(containers) == 1

        svc1 = project.get_service('svc1')
        svc2 = project.get_service('svc2')
        assert 'svc1' in svc2.get_dependency_names()
        with pytest.raises(HealthCheckFailed):
            svc1.is_healthy()

    @v2_1_only()
    def test_project_up_no_healthcheck_dependency(self):
        config_dict = {
            'version': '2.1',
            'services': {
                'svc1': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'healthcheck': {
                        'disable': True
                    },
                },
                'svc2': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'depends_on': {
                        'svc1': {'condition': 'service_healthy'},
                    }
                }
            }
        }
        config_data = load_config(config_dict)
        project = Project.from_config(
            name='composetest', config_data=config_data, client=self.client
        )
        with pytest.raises(ProjectError):
            project.up()
        containers = project.containers()
        assert len(containers) == 1

        svc1 = project.get_service('svc1')
        svc2 = project.get_service('svc2')
        assert 'svc1' in svc2.get_dependency_names()
        with pytest.raises(NoHealthCheckConfigured):
            svc1.is_healthy()

    def test_project_up_seccomp_profile(self):
        seccomp_data = {
            'defaultAction': 'SCMP_ACT_ALLOW',
            'syscalls': []
        }
        fd, profile_path = tempfile.mkstemp('_seccomp.json')
        self.addCleanup(os.remove, profile_path)
        with os.fdopen(fd, 'w') as f:
            json.dump(seccomp_data, f)

        config_dict = {
            'version': '2.3',
            'services': {
                'svc1': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'security_opt': ['seccomp:"{}"'.format(profile_path)]
                }
            }
        }

        config_data = load_config(config_dict)
        project = Project.from_config(name='composetest', config_data=config_data, client=self.client)
        project.up()
        containers = project.containers()
        assert len(containers) == 1

        remote_secopts = containers[0].get('HostConfig.SecurityOpt')
        assert len(remote_secopts) == 1
        assert remote_secopts[0].startswith('seccomp=')
        assert json.loads(remote_secopts[0].lstrip('seccomp=')) == seccomp_data

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_project_up_name_starts_with_illegal_char(self):
        config_dict = {
            'version': '2.3',
            'services': {
                'svc1': {
                    'image': 'busybox:latest',
                    'command': 'ls',
                    'volumes': ['foo:/foo:rw'],
                    'networks': ['bar'],
                },
            },
            'volumes': {
                'foo': {},
            },
            'networks': {
                'bar': {},
            }
        }
        config_data = load_config(config_dict)
        project = Project.from_config(
            name='_underscoretest', config_data=config_data, client=self.client
        )
        project.up()
        self.addCleanup(project.down, None, True)

        containers = project.containers(stopped=True)
        assert len(containers) == 1
        assert containers[0].name.startswith('underscoretest_svc1_')
        assert containers[0].project == '_underscoretest'

        full_vol_name = 'underscoretest_foo'
        vol_data = self.get_volume_data(full_vol_name)
        assert vol_data
        assert vol_data['Labels'][LABEL_PROJECT] == '_underscoretest'

        full_net_name = '_underscoretest_bar'
        net_data = self.client.inspect_network(full_net_name)
        assert net_data
        assert net_data['Labels'][LABEL_PROJECT] == '_underscoretest'

        project2 = Project.from_config(
            name='-dashtest', config_data=config_data, client=self.client
        )
        project2.up()
        self.addCleanup(project2.down, None, True)

        containers = project2.containers(stopped=True)
        assert len(containers) == 1
        assert containers[0].name.startswith('dashtest_svc1_')
        assert containers[0].project == '-dashtest'

        full_vol_name = 'dashtest_foo'
        vol_data = self.get_volume_data(full_vol_name)
        assert vol_data
        assert vol_data['Labels'][LABEL_PROJECT] == '-dashtest'

        full_net_name = '-dashtest_bar'
        net_data = self.client.inspect_network(full_net_name)
        assert net_data
        assert net_data['Labels'][LABEL_PROJECT] == '-dashtest'
<EOF>
<BOF>
"""
Integration tests which cover state convergence (aka smart recreate) performed
by `docker-compose up`.
"""
from __future__ import absolute_import
from __future__ import unicode_literals

import py
from docker.errors import ImageNotFound

from .testcases import DockerClientTestCase
from .testcases import get_links
from .testcases import no_cluster
from compose.config import config
from compose.project import Project
from compose.service import ConvergenceStrategy


class ProjectTestCase(DockerClientTestCase):
    def run_up(self, cfg, **kwargs):
        kwargs.setdefault('timeout', 1)
        kwargs.setdefault('detached', True)

        project = self.make_project(cfg)
        project.up(**kwargs)
        return set(project.containers(stopped=True))

    def make_project(self, cfg):
        details = config.ConfigDetails(
            'working_dir',
            [config.ConfigFile(None, cfg)])
        return Project.from_config(
            name='composetest',
            client=self.client,
            config_data=config.load(details))


class BasicProjectTest(ProjectTestCase):
    def setUp(self):
        super(BasicProjectTest, self).setUp()

        self.cfg = {
            'db': {'image': 'busybox:latest', 'command': 'top'},
            'web': {'image': 'busybox:latest', 'command': 'top'},
        }

    def test_no_change(self):
        old_containers = self.run_up(self.cfg)
        assert len(old_containers) == 2

        new_containers = self.run_up(self.cfg)
        assert len(new_containers) == 2

        assert old_containers == new_containers

    def test_partial_change(self):
        old_containers = self.run_up(self.cfg)
        old_db = [c for c in old_containers if c.name_without_project.startswith('db_')][0]
        old_web = [c for c in old_containers if c.name_without_project.startswith('web_')][0]

        self.cfg['web']['command'] = '/bin/true'

        new_containers = self.run_up(self.cfg)
        assert len(new_containers) == 2

        preserved = list(old_containers & new_containers)
        assert preserved == [old_db]

        removed = list(old_containers - new_containers)
        assert removed == [old_web]

        created = list(new_containers - old_containers)
        assert len(created) == 1
        assert created[0].name_without_project == old_web.name_without_project
        assert created[0].get('Config.Cmd') == ['/bin/true']

    def test_all_change(self):
        old_containers = self.run_up(self.cfg)
        assert len(old_containers) == 2

        self.cfg['web']['command'] = '/bin/true'
        self.cfg['db']['command'] = '/bin/true'

        new_containers = self.run_up(self.cfg)
        assert len(new_containers) == 2

        unchanged = old_containers & new_containers
        assert len(unchanged) == 0

        new = new_containers - old_containers
        assert len(new) == 2


class ProjectWithDependenciesTest(ProjectTestCase):
    def setUp(self):
        super(ProjectWithDependenciesTest, self).setUp()

        self.cfg = {
            'db': {
                'image': 'busybox:latest',
                'command': 'tail -f /dev/null',
            },
            'web': {
                'image': 'busybox:latest',
                'command': 'tail -f /dev/null',
                'links': ['db'],
            },
            'nginx': {
                'image': 'busybox:latest',
                'command': 'tail -f /dev/null',
                'links': ['web'],
            },
        }

    def test_up(self):
        containers = self.run_up(self.cfg)
        assert set(c.service for c in containers) == set(['db', 'web', 'nginx'])

    def test_change_leaf(self):
        old_containers = self.run_up(self.cfg)

        self.cfg['nginx']['environment'] = {'NEW_VAR': '1'}
        new_containers = self.run_up(self.cfg)

        assert set(c.service for c in new_containers - old_containers) == set(['nginx'])

    def test_change_middle(self):
        old_containers = self.run_up(self.cfg)

        self.cfg['web']['environment'] = {'NEW_VAR': '1'}
        new_containers = self.run_up(self.cfg)

        assert set(c.service for c in new_containers - old_containers) == set(['web'])

    def test_change_middle_always_recreate_deps(self):
        old_containers = self.run_up(self.cfg, always_recreate_deps=True)

        self.cfg['web']['environment'] = {'NEW_VAR': '1'}
        new_containers = self.run_up(self.cfg, always_recreate_deps=True)

        assert set(c.service for c in new_containers - old_containers) == {'web', 'nginx'}

    def test_change_root(self):
        old_containers = self.run_up(self.cfg)

        self.cfg['db']['environment'] = {'NEW_VAR': '1'}
        new_containers = self.run_up(self.cfg)

        assert set(c.service for c in new_containers - old_containers) == set(['db'])

    def test_change_root_always_recreate_deps(self):
        old_containers = self.run_up(self.cfg, always_recreate_deps=True)

        self.cfg['db']['environment'] = {'NEW_VAR': '1'}
        new_containers = self.run_up(self.cfg, always_recreate_deps=True)

        assert set(c.service for c in new_containers - old_containers) == {
            'db', 'web', 'nginx'
        }

    def test_change_root_no_recreate(self):
        old_containers = self.run_up(self.cfg)

        self.cfg['db']['environment'] = {'NEW_VAR': '1'}
        new_containers = self.run_up(
            self.cfg,
            strategy=ConvergenceStrategy.never)

        assert new_containers - old_containers == set()

    def test_service_removed_while_down(self):
        next_cfg = {
            'web': {
                'image': 'busybox:latest',
                'command': 'tail -f /dev/null',
            },
            'nginx': self.cfg['nginx'],
        }

        containers = self.run_up(self.cfg)
        assert len(containers) == 3

        project = self.make_project(self.cfg)
        project.stop(timeout=1)

        containers = self.run_up(next_cfg)
        assert len(containers) == 2

    def test_service_recreated_when_dependency_created(self):
        containers = self.run_up(self.cfg, service_names=['web'], start_deps=False)
        assert len(containers) == 1

        containers = self.run_up(self.cfg)
        assert len(containers) == 3

        web, = [c for c in containers if c.service == 'web']
        nginx, = [c for c in containers if c.service == 'nginx']
        db, = [c for c in containers if c.service == 'db']

        assert set(get_links(web)) == {
            'composetest_db_{}_{}'.format(db.number, db.slug),
            'db',
            'db_{}_{}'.format(db.number, db.slug)
        }
        assert set(get_links(nginx)) == {
            'composetest_web_{}_{}'.format(web.number, web.slug),
            'web',
            'web_{}_{}'.format(web.number, web.slug)
        }


class ServiceStateTest(DockerClientTestCase):
    """Test cases for Service.convergence_plan."""

    def test_trigger_create(self):
        web = self.create_service('web')
        assert ('create', []) == web.convergence_plan()

    def test_trigger_noop(self):
        web = self.create_service('web')
        container = web.create_container()
        web.start()

        web = self.create_service('web')
        assert ('noop', [container]) == web.convergence_plan()

    def test_trigger_start(self):
        options = dict(command=["top"])

        web = self.create_service('web', **options)
        web.scale(2)

        containers = web.containers(stopped=True)
        containers[0].stop()
        containers[0].inspect()

        assert [c.is_running for c in containers] == [False, True]

        assert ('start', containers[0:1]) == web.convergence_plan()

    def test_trigger_recreate_with_config_change(self):
        web = self.create_service('web', command=["top"])
        container = web.create_container()

        web = self.create_service('web', command=["top", "-d", "1"])
        assert ('recreate', [container]) == web.convergence_plan()

    def test_trigger_recreate_with_nonexistent_image_tag(self):
        web = self.create_service('web', image="busybox:latest")
        container = web.create_container()

        web = self.create_service('web', image="nonexistent-image")
        assert ('recreate', [container]) == web.convergence_plan()

    def test_trigger_recreate_with_image_change(self):
        repo = 'composetest_myimage'
        tag = 'latest'
        image = '{}:{}'.format(repo, tag)

        def safe_remove_image(image):
            try:
                self.client.remove_image(image)
            except ImageNotFound:
                pass

        image_id = self.client.images(name='busybox')[0]['Id']
        self.client.tag(image_id, repository=repo, tag=tag)
        self.addCleanup(safe_remove_image, image)

        web = self.create_service('web', image=image)
        container = web.create_container()

        # update the image
        c = self.client.create_container(image, ['touch', '/hello.txt'], host_config={})

        # In the case of a cluster, there's a chance we pick up the old image when
        # calculating the new hash. To circumvent that, untag the old image first
        # See also: https://github.com/moby/moby/issues/26852
        self.client.remove_image(image, force=True)

        self.client.commit(c, repository=repo, tag=tag)
        self.client.remove_container(c)

        web = self.create_service('web', image=image)
        assert ('recreate', [container]) == web.convergence_plan()

    @no_cluster('Can not guarantee the build will be run on the same node the service is deployed')
    def test_trigger_recreate_with_build(self):
        context = py.test.ensuretemp('test_trigger_recreate_with_build')
        self.addCleanup(context.remove)

        base_image = "FROM busybox\nLABEL com.docker.compose.test_image=true\n"
        dockerfile = context.join('Dockerfile')
        dockerfile.write(base_image)

        web = self.create_service('web', build={'context': str(context)})
        container = web.create_container()

        dockerfile.write(base_image + 'CMD echo hello world\n')
        web.build()

        web = self.create_service('web', build={'context': str(context)})
        assert ('recreate', [container]) == web.convergence_plan()

    def test_image_changed_to_build(self):
        context = py.test.ensuretemp('test_image_changed_to_build')
        self.addCleanup(context.remove)
        context.join('Dockerfile').write("""
            FROM busybox
            LABEL com.docker.compose.test_image=true
        """)

        web = self.create_service('web', image='busybox')
        container = web.create_container()

        web = self.create_service('web', build={'context': str(context)})
        plan = web.convergence_plan()
        assert ('recreate', [container]) == plan
        containers = web.execute_convergence_plan(plan)
        assert len(containers) == 1
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest

from .testcases import DockerClientTestCase
from compose.config.errors import ConfigurationError
from compose.const import LABEL_NETWORK
from compose.const import LABEL_PROJECT
from compose.network import Network


class NetworkTest(DockerClientTestCase):
    def test_network_default_labels(self):
        net = Network(self.client, 'composetest', 'foonet')
        net.ensure()
        net_data = net.inspect()
        labels = net_data['Labels']
        assert labels[LABEL_NETWORK] == net.name
        assert labels[LABEL_PROJECT] == net.project

    def test_network_external_default_ensure(self):
        net = Network(
            self.client, 'composetest', 'foonet',
            external=True
        )

        with pytest.raises(ConfigurationError):
            net.ensure()

    def test_network_external_overlay_ensure(self):
        net = Network(
            self.client, 'composetest', 'foonet',
            driver='overlay', external=True
        )

        assert net.ensure() is None
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import functools
import os

import pytest
from docker.errors import APIError
from docker.utils import version_lt

from .. import unittest
from compose.cli.docker_client import docker_client
from compose.config.config import resolve_environment
from compose.config.environment import Environment
from compose.const import API_VERSIONS
from compose.const import COMPOSEFILE_V1 as V1
from compose.const import COMPOSEFILE_V2_0 as V2_0
from compose.const import COMPOSEFILE_V2_0 as V2_1
from compose.const import COMPOSEFILE_V2_2 as V2_2
from compose.const import COMPOSEFILE_V2_3 as V2_3
from compose.const import COMPOSEFILE_V3_0 as V3_0
from compose.const import COMPOSEFILE_V3_2 as V3_2
from compose.const import COMPOSEFILE_V3_5 as V3_5
from compose.const import LABEL_PROJECT
from compose.progress_stream import stream_output
from compose.service import Service

SWARM_SKIP_CONTAINERS_ALL = os.environ.get('SWARM_SKIP_CONTAINERS_ALL', '0') != '0'
SWARM_SKIP_CPU_SHARES = os.environ.get('SWARM_SKIP_CPU_SHARES', '0') != '0'
SWARM_SKIP_RM_VOLUMES = os.environ.get('SWARM_SKIP_RM_VOLUMES', '0') != '0'
SWARM_ASSUME_MULTINODE = os.environ.get('SWARM_ASSUME_MULTINODE', '0') != '0'


def pull_busybox(client):
    client.pull('busybox:latest', stream=False)


def get_links(container):
    links = container.get('HostConfig.Links') or []

    def format_link(link):
        _, alias = link.split(':')
        return alias.split('/')[-1]

    return [format_link(link) for link in links]


def engine_max_version():
    if 'DOCKER_VERSION' not in os.environ:
        return V3_5
    version = os.environ['DOCKER_VERSION'].partition('-')[0]
    if version_lt(version, '1.10'):
        return V1
    if version_lt(version, '1.12'):
        return V2_0
    if version_lt(version, '1.13'):
        return V2_1
    if version_lt(version, '17.06'):
        return V3_2
    return V3_5


def min_version_skip(version):
    return pytest.mark.skipif(
        engine_max_version() < version,
        reason="Engine version %s is too low" % version
    )


def v2_only():
    return min_version_skip(V2_0)


def v2_1_only():
    return min_version_skip(V2_1)


def v2_2_only():
    return min_version_skip(V2_2)


def v2_3_only():
    return min_version_skip(V2_3)


def v3_only():
    return min_version_skip(V3_0)


class DockerClientTestCase(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        version = API_VERSIONS[engine_max_version()]
        cls.client = docker_client(Environment(), version)

    @classmethod
    def tearDownClass(cls):
        del cls.client

    def tearDown(self):
        for c in self.client.containers(
                all=True,
                filters={'label': '%s=composetest' % LABEL_PROJECT}):
            self.client.remove_container(c['Id'], force=True)

        for i in self.client.images(
                filters={'label': 'com.docker.compose.test_image'}):
            try:
                self.client.remove_image(i, force=True)
            except APIError as e:
                if e.is_server_error():
                    pass

        volumes = self.client.volumes().get('Volumes') or []
        for v in volumes:
            if 'composetest_' in v['Name']:
                self.client.remove_volume(v['Name'])

        networks = self.client.networks()
        for n in networks:
            if 'composetest_' in n['Name']:
                self.client.remove_network(n['Name'])

    def create_service(self, name, **kwargs):
        if 'image' not in kwargs and 'build' not in kwargs:
            kwargs['image'] = 'busybox:latest'

        if 'command' not in kwargs:
            kwargs['command'] = ["top"]

        kwargs['environment'] = resolve_environment(
            kwargs, Environment.from_env_file(None)
        )
        labels = dict(kwargs.setdefault('labels', {}))
        labels['com.docker.compose.test-name'] = self.id()

        return Service(name, client=self.client, project='composetest', **kwargs)

    def check_build(self, *args, **kwargs):
        kwargs.setdefault('rm', True)
        build_output = self.client.build(*args, **kwargs)
        with open(os.devnull, 'w') as devnull:
            for event in stream_output(build_output, devnull):
                pass

    def require_api_version(self, minimum):
        api_version = self.client.version()['ApiVersion']
        if version_lt(api_version, minimum):
            pytest.skip("API version is too low ({} < {})".format(api_version, minimum))

    def get_volume_data(self, volume_name):
        if not is_cluster(self.client):
            return self.client.inspect_volume(volume_name)

        volumes = self.client.volumes(filters={'name': volume_name})['Volumes']
        assert len(volumes) > 0
        return self.client.inspect_volume(volumes[0]['Name'])


def if_runtime_available(runtime):
    def decorator(f):
        @functools.wraps(f)
        def wrapper(self, *args, **kwargs):
            if runtime not in self.client.info().get('Runtimes', {}):
                return pytest.skip("This daemon does not support the '{}'' runtime".format(runtime))
            return f(self, *args, **kwargs)
        return wrapper

    return decorator


def is_cluster(client):
    if SWARM_ASSUME_MULTINODE:
        return True

    def get_nodes_number():
        try:
            return len(client.nodes())
        except APIError:
            # If the Engine is not part of a Swarm, the SDK will raise
            # an APIError
            return 0

    if not hasattr(is_cluster, 'nodes') or is_cluster.nodes is None:
        # Only make the API call if the value hasn't been cached yet
        is_cluster.nodes = get_nodes_number()

    return is_cluster.nodes > 1


def no_cluster(reason):
    def decorator(f):
        @functools.wraps(f)
        def wrapper(self, *args, **kwargs):
            if is_cluster(self.client):
                pytest.skip("Test will not be run in cluster mode: %s" % reason)
                return
            return f(self, *args, **kwargs)
        return wrapper

    return decorator
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import os
import re
import shutil
import tempfile
from distutils.spawn import find_executable
from os import path

import pytest
from docker.errors import APIError
from docker.errors import ImageNotFound
from six import StringIO
from six import text_type

from .. import mock
from .testcases import docker_client
from .testcases import DockerClientTestCase
from .testcases import get_links
from .testcases import pull_busybox
from .testcases import SWARM_SKIP_CONTAINERS_ALL
from .testcases import SWARM_SKIP_CPU_SHARES
from compose import __version__
from compose.config.types import MountSpec
from compose.config.types import SecurityOpt
from compose.config.types import VolumeFromSpec
from compose.config.types import VolumeSpec
from compose.const import IS_WINDOWS_PLATFORM
from compose.const import LABEL_CONFIG_HASH
from compose.const import LABEL_CONTAINER_NUMBER
from compose.const import LABEL_ONE_OFF
from compose.const import LABEL_PROJECT
from compose.const import LABEL_SERVICE
from compose.const import LABEL_SLUG
from compose.const import LABEL_VERSION
from compose.container import Container
from compose.errors import OperationFailedError
from compose.parallel import ParallelStreamWriter
from compose.project import OneOffFilter
from compose.service import ConvergencePlan
from compose.service import ConvergenceStrategy
from compose.service import NetworkMode
from compose.service import PidMode
from compose.service import Service
from compose.utils import parse_nanoseconds_int
from tests.helpers import create_custom_host_file
from tests.integration.testcases import is_cluster
from tests.integration.testcases import no_cluster
from tests.integration.testcases import v2_1_only
from tests.integration.testcases import v2_2_only
from tests.integration.testcases import v2_3_only
from tests.integration.testcases import v2_only
from tests.integration.testcases import v3_only


def create_and_start_container(service, **override_options):
    container = service.create_container(**override_options)
    return service.start_container(container)


class ServiceTest(DockerClientTestCase):

    def test_containers(self):
        foo = self.create_service('foo')
        bar = self.create_service('bar')

        create_and_start_container(foo)

        assert len(foo.containers()) == 1
        assert foo.containers()[0].name.startswith('composetest_foo_')
        assert len(bar.containers()) == 0

        create_and_start_container(bar)
        create_and_start_container(bar)

        assert len(foo.containers()) == 1
        assert len(bar.containers()) == 2

        names = [c.name for c in bar.containers()]
        assert len(names) == 2
        assert all(name.startswith('composetest_bar_') for name in names)

    def test_containers_one_off(self):
        db = self.create_service('db')
        container = db.create_container(one_off=True)
        assert db.containers(stopped=True) == []
        assert db.containers(one_off=OneOffFilter.only, stopped=True) == [container]

    def test_project_is_added_to_container_name(self):
        service = self.create_service('web')
        create_and_start_container(service)
        assert service.containers()[0].name.startswith('composetest_web_')

    def test_create_container_with_one_off(self):
        db = self.create_service('db')
        container = db.create_container(one_off=True)
        assert container.name.startswith('composetest_db_run_')

    def test_create_container_with_one_off_when_existing_container_is_running(self):
        db = self.create_service('db')
        db.start()
        container = db.create_container(one_off=True)
        assert container.name.startswith('composetest_db_run_')

    def test_create_container_with_unspecified_volume(self):
        service = self.create_service('db', volumes=[VolumeSpec.parse('/var/db')])
        container = service.create_container()
        service.start_container(container)
        assert container.get_mount('/var/db')

    def test_create_container_with_volume_driver(self):
        service = self.create_service('db', volume_driver='foodriver')
        container = service.create_container()
        service.start_container(container)
        assert 'foodriver' == container.get('HostConfig.VolumeDriver')

    @pytest.mark.skipif(SWARM_SKIP_CPU_SHARES, reason='Swarm --cpu-shares bug')
    def test_create_container_with_cpu_shares(self):
        service = self.create_service('db', cpu_shares=73)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.CpuShares') == 73

    def test_create_container_with_cpu_quota(self):
        service = self.create_service('db', cpu_quota=40000, cpu_period=150000)
        container = service.create_container()
        container.start()
        assert container.get('HostConfig.CpuQuota') == 40000
        assert container.get('HostConfig.CpuPeriod') == 150000

    @pytest.mark.xfail(raises=OperationFailedError, reason='not supported by kernel')
    def test_create_container_with_cpu_rt(self):
        service = self.create_service('db', cpu_rt_runtime=40000, cpu_rt_period=150000)
        container = service.create_container()
        container.start()
        assert container.get('HostConfig.CpuRealtimeRuntime') == 40000
        assert container.get('HostConfig.CpuRealtimePeriod') == 150000

    @v2_2_only()
    def test_create_container_with_cpu_count(self):
        self.require_api_version('1.25')
        service = self.create_service('db', cpu_count=2)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.CpuCount') == 2

    @v2_2_only()
    @pytest.mark.skipif(not IS_WINDOWS_PLATFORM, reason='cpu_percent is not supported for Linux')
    def test_create_container_with_cpu_percent(self):
        self.require_api_version('1.25')
        service = self.create_service('db', cpu_percent=12)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.CpuPercent') == 12

    @v2_2_only()
    def test_create_container_with_cpus(self):
        self.require_api_version('1.25')
        service = self.create_service('db', cpus=1)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.NanoCpus') == 1000000000

    def test_create_container_with_shm_size(self):
        self.require_api_version('1.22')
        service = self.create_service('db', shm_size=67108864)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.ShmSize') == 67108864

    def test_create_container_with_init_bool(self):
        self.require_api_version('1.25')
        service = self.create_service('db', init=True)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.Init') is True

    @pytest.mark.xfail(True, reason='Option has been removed in Engine 17.06.0')
    def test_create_container_with_init_path(self):
        self.require_api_version('1.25')
        docker_init_path = find_executable('docker-init')
        service = self.create_service('db', init=docker_init_path)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.InitPath') == docker_init_path

    @pytest.mark.xfail(True, reason='Some kernels/configs do not support pids_limit')
    def test_create_container_with_pids_limit(self):
        self.require_api_version('1.23')
        service = self.create_service('db', pids_limit=10)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.PidsLimit') == 10

    def test_create_container_with_extra_hosts_list(self):
        extra_hosts = ['somehost:162.242.195.82', 'otherhost:50.31.209.229']
        service = self.create_service('db', extra_hosts=extra_hosts)
        container = service.create_container()
        service.start_container(container)
        assert set(container.get('HostConfig.ExtraHosts')) == set(extra_hosts)

    def test_create_container_with_extra_hosts_dicts(self):
        extra_hosts = {'somehost': '162.242.195.82', 'otherhost': '50.31.209.229'}
        extra_hosts_list = ['somehost:162.242.195.82', 'otherhost:50.31.209.229']
        service = self.create_service('db', extra_hosts=extra_hosts)
        container = service.create_container()
        service.start_container(container)
        assert set(container.get('HostConfig.ExtraHosts')) == set(extra_hosts_list)

    def test_create_container_with_cpu_set(self):
        service = self.create_service('db', cpuset='0')
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.CpusetCpus') == '0'

    def test_create_container_with_read_only_root_fs(self):
        read_only = True
        service = self.create_service('db', read_only=read_only)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.ReadonlyRootfs') == read_only

    def test_create_container_with_blkio_config(self):
        blkio_config = {
            'weight': 300,
            'weight_device': [{'path': '/dev/sda', 'weight': 200}],
            'device_read_bps': [{'path': '/dev/sda', 'rate': 1024 * 1024 * 100}],
            'device_read_iops': [{'path': '/dev/sda', 'rate': 1000}],
            'device_write_bps': [{'path': '/dev/sda', 'rate': 1024 * 1024}],
            'device_write_iops': [{'path': '/dev/sda', 'rate': 800}]
        }
        service = self.create_service('web', blkio_config=blkio_config)
        container = service.create_container()
        assert container.get('HostConfig.BlkioWeight') == 300
        assert container.get('HostConfig.BlkioWeightDevice') == [{
            'Path': '/dev/sda', 'Weight': 200
        }]
        assert container.get('HostConfig.BlkioDeviceReadBps') == [{
            'Path': '/dev/sda', 'Rate': 1024 * 1024 * 100
        }]
        assert container.get('HostConfig.BlkioDeviceWriteBps') == [{
            'Path': '/dev/sda', 'Rate': 1024 * 1024
        }]
        assert container.get('HostConfig.BlkioDeviceReadIOps') == [{
            'Path': '/dev/sda', 'Rate': 1000
        }]
        assert container.get('HostConfig.BlkioDeviceWriteIOps') == [{
            'Path': '/dev/sda', 'Rate': 800
        }]

    def test_create_container_with_security_opt(self):
        security_opt = [SecurityOpt.parse('label:disable')]
        service = self.create_service('db', security_opt=security_opt)
        container = service.create_container()
        service.start_container(container)
        assert set(container.get('HostConfig.SecurityOpt')) == set([o.repr() for o in security_opt])

    @pytest.mark.xfail(True, reason='Not supported on most drivers')
    def test_create_container_with_storage_opt(self):
        storage_opt = {'size': '1G'}
        service = self.create_service('db', storage_opt=storage_opt)
        container = service.create_container()
        service.start_container(container)
        assert container.get('HostConfig.StorageOpt') == storage_opt

    def test_create_container_with_oom_kill_disable(self):
        self.require_api_version('1.20')
        service = self.create_service('db', oom_kill_disable=True)
        container = service.create_container()
        assert container.get('HostConfig.OomKillDisable') is True

    def test_create_container_with_mac_address(self):
        service = self.create_service('db', mac_address='02:42:ac:11:65:43')
        container = service.create_container()
        service.start_container(container)
        assert container.inspect()['Config']['MacAddress'] == '02:42:ac:11:65:43'

    def test_create_container_with_device_cgroup_rules(self):
        service = self.create_service('db', device_cgroup_rules=['c 7:128 rwm'])
        container = service.create_container()
        assert container.get('HostConfig.DeviceCgroupRules') == ['c 7:128 rwm']

    def test_create_container_with_specified_volume(self):
        host_path = '/tmp/host-path'
        container_path = '/container-path'

        service = self.create_service(
            'db',
            volumes=[VolumeSpec(host_path, container_path, 'rw')])
        container = service.create_container()
        service.start_container(container)
        assert container.get_mount(container_path)

        # Match the last component ("host-path"), because boot2docker symlinks /tmp
        actual_host_path = container.get_mount(container_path)['Source']

        assert path.basename(actual_host_path) == path.basename(host_path), (
            "Last component differs: %s, %s" % (actual_host_path, host_path)
        )

    @v2_3_only()
    def test_create_container_with_host_mount(self):
        host_path = '/tmp/host-path'
        container_path = '/container-path'

        create_custom_host_file(self.client, path.join(host_path, 'a.txt'), 'test')

        service = self.create_service(
            'db',
            volumes=[
                MountSpec(type='bind', source=host_path, target=container_path, read_only=True)
            ]
        )
        container = service.create_container()
        service.start_container(container)
        mount = container.get_mount(container_path)
        assert mount
        assert path.basename(mount['Source']) == path.basename(host_path)
        assert mount['RW'] is False

    @v2_3_only()
    def test_create_container_with_tmpfs_mount(self):
        container_path = '/container-tmpfs'
        service = self.create_service(
            'db',
            volumes=[MountSpec(type='tmpfs', target=container_path)]
        )
        container = service.create_container()
        service.start_container(container)
        mount = container.get_mount(container_path)
        assert mount
        assert mount['Type'] == 'tmpfs'

    @v2_3_only()
    def test_create_container_with_tmpfs_mount_tmpfs_size(self):
        container_path = '/container-tmpfs'
        service = self.create_service(
            'db',
            volumes=[MountSpec(type='tmpfs', target=container_path, tmpfs={'size': 5368709})]
        )
        container = service.create_container()
        service.start_container(container)
        mount = container.get_mount(container_path)
        assert mount
        print(container.dictionary)
        assert mount['Type'] == 'tmpfs'
        assert container.get('HostConfig.Mounts')[0]['TmpfsOptions'] == {
            'SizeBytes': 5368709
        }

    @v2_3_only()
    def test_create_container_with_volume_mount(self):
        container_path = '/container-volume'
        volume_name = 'composetest_abcde'
        self.client.create_volume(volume_name)
        service = self.create_service(
            'db',
            volumes=[MountSpec(type='volume', source=volume_name, target=container_path)]
        )
        container = service.create_container()
        service.start_container(container)
        mount = container.get_mount(container_path)
        assert mount
        assert mount['Name'] == volume_name

    @v3_only()
    def test_create_container_with_legacy_mount(self):
        # Ensure mounts are converted to volumes if API version < 1.30
        # Needed to support long syntax in the 3.2 format
        client = docker_client({}, version='1.25')
        container_path = '/container-volume'
        volume_name = 'composetest_abcde'
        self.client.create_volume(volume_name)
        service = Service('db', client=client, volumes=[
            MountSpec(type='volume', source=volume_name, target=container_path)
        ], image='busybox:latest', command=['top'], project='composetest')
        container = service.create_container()
        service.start_container(container)
        mount = container.get_mount(container_path)
        assert mount
        assert mount['Name'] == volume_name

    @v3_only()
    def test_create_container_with_legacy_tmpfs_mount(self):
        # Ensure tmpfs mounts are converted to tmpfs entries if API version < 1.30
        # Needed to support long syntax in the 3.2 format
        client = docker_client({}, version='1.25')
        container_path = '/container-tmpfs'
        service = Service('db', client=client, volumes=[
            MountSpec(type='tmpfs', target=container_path)
        ], image='busybox:latest', command=['top'], project='composetest')
        container = service.create_container()
        service.start_container(container)
        mount = container.get_mount(container_path)
        assert mount is None
        assert container_path in container.get('HostConfig.Tmpfs')

    def test_create_container_with_healthcheck_config(self):
        one_second = parse_nanoseconds_int('1s')
        healthcheck = {
            'test': ['true'],
            'interval': 2 * one_second,
            'timeout': 5 * one_second,
            'retries': 5,
            'start_period': 2 * one_second
        }
        service = self.create_service('db', healthcheck=healthcheck)
        container = service.create_container()
        remote_healthcheck = container.get('Config.Healthcheck')
        assert remote_healthcheck['Test'] == healthcheck['test']
        assert remote_healthcheck['Interval'] == healthcheck['interval']
        assert remote_healthcheck['Timeout'] == healthcheck['timeout']
        assert remote_healthcheck['Retries'] == healthcheck['retries']
        assert remote_healthcheck['StartPeriod'] == healthcheck['start_period']

    def test_recreate_preserves_volume_with_trailing_slash(self):
        """When the Compose file specifies a trailing slash in the container path, make
        sure we copy the volume over when recreating.
        """
        service = self.create_service('data', volumes=[VolumeSpec.parse('/data/')])
        old_container = create_and_start_container(service)
        volume_path = old_container.get_mount('/data')['Source']

        new_container = service.recreate_container(old_container)
        assert new_container.get_mount('/data')['Source'] == volume_path

    def test_recreate_volume_to_mount(self):
        # https://github.com/docker/compose/issues/6280
        service = Service(
            project='composetest',
            name='db',
            client=self.client,
            build={'context': 'tests/fixtures/dockerfile-with-volume'},
            volumes=[MountSpec.parse({
                'type': 'volume',
                'target': '/data',
            })]
        )
        old_container = create_and_start_container(service)
        new_container = service.recreate_container(old_container)
        assert new_container.get_mount('/data')['Source']

    def test_duplicate_volume_trailing_slash(self):
        """
        When an image specifies a volume, and the Compose file specifies a host path
        but adds a trailing slash, make sure that we don't create duplicate binds.
        """
        host_path = '/tmp/data'
        container_path = '/data'
        volumes = [VolumeSpec.parse('{}:{}/'.format(host_path, container_path))]

        tmp_container = self.client.create_container(
            'busybox', 'true',
            volumes={container_path: {}},
            labels={'com.docker.compose.test_image': 'true'},
            host_config={}
        )
        image = self.client.commit(tmp_container)['Id']

        service = self.create_service('db', image=image, volumes=volumes)
        old_container = create_and_start_container(service)

        assert old_container.get('Config.Volumes') == {container_path: {}}

        service = self.create_service('db', image=image, volumes=volumes)
        new_container = service.recreate_container(old_container)

        assert new_container.get('Config.Volumes') == {container_path: {}}

        assert service.containers(stopped=False) == [new_container]

    def test_create_container_with_volumes_from(self):
        volume_service = self.create_service('data')
        volume_container_1 = volume_service.create_container()
        volume_container_2 = Container.create(
            self.client,
            image='busybox:latest',
            command=["top"],
            labels={LABEL_PROJECT: 'composetest'},
            host_config={},
            environment=['affinity:container=={}'.format(volume_container_1.id)],
        )
        host_service = self.create_service(
            'host',
            volumes_from=[
                VolumeFromSpec(volume_service, 'rw', 'service'),
                VolumeFromSpec(volume_container_2, 'rw', 'container')
            ],
            environment=['affinity:container=={}'.format(volume_container_1.id)],
        )
        host_container = host_service.create_container()
        host_service.start_container(host_container)
        assert volume_container_1.id + ':rw' in host_container.get('HostConfig.VolumesFrom')
        assert volume_container_2.id + ':rw' in host_container.get('HostConfig.VolumesFrom')

    def test_execute_convergence_plan_recreate(self):
        service = self.create_service(
            'db',
            environment={'FOO': '1'},
            volumes=[VolumeSpec.parse('/etc')],
            entrypoint=['top'],
            command=['-d', '1']
        )
        old_container = service.create_container()
        assert old_container.get('Config.Entrypoint') == ['top']
        assert old_container.get('Config.Cmd') == ['-d', '1']
        assert 'FOO=1' in old_container.get('Config.Env')
        assert old_container.name.startswith('composetest_db_')
        service.start_container(old_container)
        old_container.inspect()  # reload volume data
        volume_path = old_container.get_mount('/etc')['Source']

        num_containers_before = len(self.client.containers(all=True))

        service.options['environment']['FOO'] = '2'
        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container]))

        assert new_container.get('Config.Entrypoint') == ['top']
        assert new_container.get('Config.Cmd') == ['-d', '1']
        assert 'FOO=2' in new_container.get('Config.Env')
        assert new_container.name.startswith('composetest_db_')
        assert new_container.get_mount('/etc')['Source'] == volume_path
        if not is_cluster(self.client):
            assert (
                'affinity:container==%s' % old_container.id in
                new_container.get('Config.Env')
            )
        else:
            # In Swarm, the env marker is consumed and the container should be deployed
            # on the same node.
            assert old_container.get('Node.Name') == new_container.get('Node.Name')

        assert len(self.client.containers(all=True)) == num_containers_before
        assert old_container.id != new_container.id
        with pytest.raises(APIError):
            self.client.inspect_container(old_container.id)

    def test_execute_convergence_plan_recreate_change_mount_target(self):
        service = self.create_service(
            'db',
            volumes=[MountSpec(target='/app1', type='volume')],
            entrypoint=['top'], command=['-d', '1']
        )
        old_container = create_and_start_container(service)
        assert (
            [mount['Destination'] for mount in old_container.get('Mounts')] ==
            ['/app1']
        )
        service.options['volumes'] = [MountSpec(target='/app2', type='volume')]

        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container])
        )

        assert (
            [mount['Destination'] for mount in new_container.get('Mounts')] ==
            ['/app2']
        )

    def test_execute_convergence_plan_recreate_twice(self):
        service = self.create_service(
            'db',
            volumes=[VolumeSpec.parse('/etc')],
            entrypoint=['top'],
            command=['-d', '1'])

        orig_container = service.create_container()
        service.start_container(orig_container)

        orig_container.inspect()  # reload volume data
        volume_path = orig_container.get_mount('/etc')['Source']

        # Do this twice to reproduce the bug
        for _ in range(2):
            new_container, = service.execute_convergence_plan(
                ConvergencePlan('recreate', [orig_container]))

            assert new_container.get_mount('/etc')['Source'] == volume_path
            if not is_cluster(self.client):
                assert ('affinity:container==%s' % orig_container.id in
                        new_container.get('Config.Env'))
            else:
                # In Swarm, the env marker is consumed and the container should be deployed
                # on the same node.
                assert orig_container.get('Node.Name') == new_container.get('Node.Name')

            orig_container = new_container

    @v2_3_only()
    def test_execute_convergence_plan_recreate_twice_with_mount(self):
        service = self.create_service(
            'db',
            volumes=[MountSpec(target='/etc', type='volume')],
            entrypoint=['top'],
            command=['-d', '1']
        )

        orig_container = service.create_container()
        service.start_container(orig_container)

        orig_container.inspect()  # reload volume data
        volume_path = orig_container.get_mount('/etc')['Source']

        # Do this twice to reproduce the bug
        for _ in range(2):
            new_container, = service.execute_convergence_plan(
                ConvergencePlan('recreate', [orig_container])
            )

            assert new_container.get_mount('/etc')['Source'] == volume_path
            if not is_cluster(self.client):
                assert ('affinity:container==%s' % orig_container.id in
                        new_container.get('Config.Env'))
            else:
                # In Swarm, the env marker is consumed and the container should be deployed
                # on the same node.
                assert orig_container.get('Node.Name') == new_container.get('Node.Name')

            orig_container = new_container

    def test_execute_convergence_plan_when_containers_are_stopped(self):
        service = self.create_service(
            'db',
            environment={'FOO': '1'},
            volumes=[VolumeSpec.parse('/var/db')],
            entrypoint=['top'],
            command=['-d', '1']
        )
        service.create_container()

        containers = service.containers(stopped=True)
        assert len(containers) == 1
        container, = containers
        assert not container.is_running

        service.execute_convergence_plan(ConvergencePlan('start', [container]))

        containers = service.containers()
        assert len(containers) == 1
        container.inspect()
        assert container == containers[0]
        assert container.is_running

    def test_execute_convergence_plan_with_image_declared_volume(self):
        service = Service(
            project='composetest',
            name='db',
            client=self.client,
            build={'context': 'tests/fixtures/dockerfile-with-volume'},
        )

        old_container = create_and_start_container(service)
        assert [mount['Destination'] for mount in old_container.get('Mounts')] == ['/data']
        volume_path = old_container.get_mount('/data')['Source']

        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container]))

        assert [mount['Destination'] for mount in new_container.get('Mounts')] == ['/data']
        assert new_container.get_mount('/data')['Source'] == volume_path

    def test_execute_convergence_plan_with_image_declared_volume_renew(self):
        service = Service(
            project='composetest',
            name='db',
            client=self.client,
            build={'context': 'tests/fixtures/dockerfile-with-volume'},
        )

        old_container = create_and_start_container(service)
        assert [mount['Destination'] for mount in old_container.get('Mounts')] == ['/data']
        volume_path = old_container.get_mount('/data')['Source']

        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container]), renew_anonymous_volumes=True
        )

        assert [mount['Destination'] for mount in new_container.get('Mounts')] == ['/data']
        assert new_container.get_mount('/data')['Source'] != volume_path

    def test_execute_convergence_plan_when_image_volume_masks_config(self):
        service = self.create_service(
            'db',
            build={'context': 'tests/fixtures/dockerfile-with-volume'},
        )

        old_container = create_and_start_container(service)
        assert [mount['Destination'] for mount in old_container.get('Mounts')] == ['/data']
        volume_path = old_container.get_mount('/data')['Source']

        service.options['volumes'] = [VolumeSpec.parse('/tmp:/data')]

        with mock.patch('compose.service.log') as mock_log:
            new_container, = service.execute_convergence_plan(
                ConvergencePlan('recreate', [old_container]))

        mock_log.warn.assert_called_once_with(mock.ANY)
        _, args, kwargs = mock_log.warn.mock_calls[0]
        assert "Service \"db\" is using volume \"/data\" from the previous container" in args[0]

        assert [mount['Destination'] for mount in new_container.get('Mounts')] == ['/data']
        assert new_container.get_mount('/data')['Source'] == volume_path

    def test_execute_convergence_plan_when_host_volume_is_removed(self):
        host_path = '/tmp/host-path'
        service = self.create_service(
            'db',
            build={'context': 'tests/fixtures/dockerfile-with-volume'},
            volumes=[VolumeSpec(host_path, '/data', 'rw')])

        old_container = create_and_start_container(service)
        assert (
            [mount['Destination'] for mount in old_container.get('Mounts')] ==
            ['/data']
        )
        service.options['volumes'] = []

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            new_container, = service.execute_convergence_plan(
                ConvergencePlan('recreate', [old_container]))

        assert not mock_log.warn.called
        assert (
            [mount['Destination'] for mount in new_container.get('Mounts')] ==
            ['/data']
        )
        assert new_container.get_mount('/data')['Source'] != host_path

    def test_execute_convergence_plan_anonymous_volume_renew(self):
        service = self.create_service(
            'db',
            image='busybox',
            volumes=[VolumeSpec(None, '/data', 'rw')])

        old_container = create_and_start_container(service)
        assert (
            [mount['Destination'] for mount in old_container.get('Mounts')] ==
            ['/data']
        )
        volume_path = old_container.get_mount('/data')['Source']

        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container]),
            renew_anonymous_volumes=True
        )

        assert (
            [mount['Destination'] for mount in new_container.get('Mounts')] ==
            ['/data']
        )
        assert new_container.get_mount('/data')['Source'] != volume_path

    def test_execute_convergence_plan_anonymous_volume_recreate_then_renew(self):
        service = self.create_service(
            'db',
            image='busybox',
            volumes=[VolumeSpec(None, '/data', 'rw')])

        old_container = create_and_start_container(service)
        assert (
            [mount['Destination'] for mount in old_container.get('Mounts')] ==
            ['/data']
        )
        volume_path = old_container.get_mount('/data')['Source']

        mid_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container]),
        )

        assert (
            [mount['Destination'] for mount in mid_container.get('Mounts')] ==
            ['/data']
        )
        assert mid_container.get_mount('/data')['Source'] == volume_path

        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [mid_container]),
            renew_anonymous_volumes=True
        )

        assert (
            [mount['Destination'] for mount in new_container.get('Mounts')] ==
            ['/data']
        )
        assert new_container.get_mount('/data')['Source'] != volume_path

    def test_execute_convergence_plan_without_start(self):
        service = self.create_service(
            'db',
            build={'context': 'tests/fixtures/dockerfile-with-volume'}
        )

        containers = service.execute_convergence_plan(ConvergencePlan('create', []), start=False)
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

        containers = service.execute_convergence_plan(
            ConvergencePlan('recreate', containers),
            start=False)
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

        service.execute_convergence_plan(ConvergencePlan('start', containers), start=False)
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

    def test_execute_convergence_plan_image_with_volume_is_removed(self):
        service = self.create_service(
            'db', build={'context': 'tests/fixtures/dockerfile-with-volume'}
        )

        old_container = create_and_start_container(service)
        assert (
            [mount['Destination'] for mount in old_container.get('Mounts')] ==
            ['/data']
        )
        volume_path = old_container.get_mount('/data')['Source']

        old_container.stop()
        self.client.remove_image(service.image(), force=True)

        service.ensure_image_exists()
        with pytest.raises(ImageNotFound):
            service.execute_convergence_plan(
                ConvergencePlan('recreate', [old_container])
            )
        old_container.inspect()  # retrieve new name from server

        new_container, = service.execute_convergence_plan(
            ConvergencePlan('recreate', [old_container]),
            reset_container_image=True
        )
        assert [mount['Destination'] for mount in new_container.get('Mounts')] == ['/data']
        assert new_container.get_mount('/data')['Source'] == volume_path

    def test_start_container_passes_through_options(self):
        db = self.create_service('db')
        create_and_start_container(db, environment={'FOO': 'BAR'})
        assert db.containers()[0].environment['FOO'] == 'BAR'

    def test_start_container_inherits_options_from_constructor(self):
        db = self.create_service('db', environment={'FOO': 'BAR'})
        create_and_start_container(db)
        assert db.containers()[0].environment['FOO'] == 'BAR'

    @no_cluster('No legacy links support in Swarm')
    def test_start_container_creates_links(self):
        db = self.create_service('db')
        web = self.create_service('web', links=[(db, None)])

        db1 = create_and_start_container(db)
        db2 = create_and_start_container(db)
        create_and_start_container(web)

        assert set(get_links(web.containers()[0])) == set([
            db1.name, db1.name_without_project,
            db2.name, db2.name_without_project,
            'db'
        ])

    @no_cluster('No legacy links support in Swarm')
    def test_start_container_creates_links_with_names(self):
        db = self.create_service('db')
        web = self.create_service('web', links=[(db, 'custom_link_name')])

        db1 = create_and_start_container(db)
        db2 = create_and_start_container(db)
        create_and_start_container(web)

        assert set(get_links(web.containers()[0])) == set([
            db1.name, db1.name_without_project,
            db2.name, db2.name_without_project,
            'custom_link_name'
        ])

    @no_cluster('No legacy links support in Swarm')
    def test_start_container_with_external_links(self):
        db = self.create_service('db')
        db_ctnrs = [create_and_start_container(db) for _ in range(3)]
        web = self.create_service(
            'web', external_links=[
                db_ctnrs[0].name,
                db_ctnrs[1].name,
                '{}:db_3'.format(db_ctnrs[2].name)
            ]
        )

        create_and_start_container(web)

        assert set(get_links(web.containers()[0])) == set([
            db_ctnrs[0].name,
            db_ctnrs[1].name,
            'db_3'
        ])

    @no_cluster('No legacy links support in Swarm')
    def test_start_normal_container_does_not_create_links_to_its_own_service(self):
        db = self.create_service('db')

        create_and_start_container(db)
        create_and_start_container(db)

        c = create_and_start_container(db)
        assert set(get_links(c)) == set([])

    @no_cluster('No legacy links support in Swarm')
    def test_start_one_off_container_creates_links_to_its_own_service(self):
        db = self.create_service('db')

        db1 = create_and_start_container(db)
        db2 = create_and_start_container(db)

        c = create_and_start_container(db, one_off=OneOffFilter.only)

        assert set(get_links(c)) == set([
            db1.name, db1.name_without_project,
            db2.name, db2.name_without_project,
            'db'
        ])

    def test_start_container_builds_images(self):
        service = Service(
            name='test',
            client=self.client,
            build={'context': 'tests/fixtures/simple-dockerfile'},
            project='composetest',
        )
        container = create_and_start_container(service)
        container.wait()
        assert b'success' in container.logs()
        assert len(self.client.images(name='composetest_test')) >= 1

    def test_start_container_uses_tagged_image_if_it_exists(self):
        self.check_build('tests/fixtures/simple-dockerfile', tag='composetest_test')
        service = Service(
            name='test',
            client=self.client,
            build={'context': 'this/does/not/exist/and/will/throw/error'},
            project='composetest',
        )
        container = create_and_start_container(service)
        container.wait()
        assert b'success' in container.logs()

    def test_start_container_creates_ports(self):
        service = self.create_service('web', ports=[8000])
        container = create_and_start_container(service).inspect()
        assert list(container['NetworkSettings']['Ports'].keys()) == ['8000/tcp']
        assert container['NetworkSettings']['Ports']['8000/tcp'][0]['HostPort'] != '8000'

    def test_build(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write("FROM busybox\n")

        service = self.create_service('web', build={'context': base_dir})
        service.build()
        self.addCleanup(self.client.remove_image, service.image_name)

        assert self.client.inspect_image('composetest_web')

    def test_build_non_ascii_filename(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write("FROM busybox\n")

        with open(os.path.join(base_dir.encode('utf8'), b'foo\xE2bar'), 'w') as f:
            f.write("hello world\n")

        service = self.create_service('web', build={'context': text_type(base_dir)})
        service.build()
        self.addCleanup(self.client.remove_image, service.image_name)
        assert self.client.inspect_image('composetest_web')

    def test_build_with_image_name(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write("FROM busybox\n")

        image_name = 'examples/composetest:latest'
        self.addCleanup(self.client.remove_image, image_name)
        self.create_service('web', build={'context': base_dir}, image=image_name).build()
        assert self.client.inspect_image(image_name)

    def test_build_with_git_url(self):
        build_url = "https://github.com/dnephin/docker-build-from-url.git"
        service = self.create_service('buildwithurl', build={'context': build_url})
        self.addCleanup(self.client.remove_image, service.image_name)
        service.build()
        assert service.image()

    def test_build_with_build_args(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write("FROM busybox\n")
            f.write("ARG build_version\n")
            f.write("RUN echo ${build_version}\n")

        service = self.create_service('buildwithargs',
                                      build={'context': text_type(base_dir),
                                             'args': {"build_version": "1"}})
        service.build()
        self.addCleanup(self.client.remove_image, service.image_name)
        assert service.image()
        assert "build_version=1" in service.image()['ContainerConfig']['Cmd']

    def test_build_with_build_args_override(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write("FROM busybox\n")
            f.write("ARG build_version\n")
            f.write("RUN echo ${build_version}\n")

        service = self.create_service('buildwithargs',
                                      build={'context': text_type(base_dir),
                                             'args': {"build_version": "1"}})
        service.build(build_args_override={'build_version': '2'})
        self.addCleanup(self.client.remove_image, service.image_name)

        assert service.image()
        assert "build_version=2" in service.image()['ContainerConfig']['Cmd']

    def test_build_with_build_labels(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('FROM busybox\n')

        service = self.create_service('buildlabels', build={
            'context': text_type(base_dir),
            'labels': {'com.docker.compose.test': 'true'}
        })
        service.build()
        self.addCleanup(self.client.remove_image, service.image_name)

        assert service.image()
        assert service.image()['Config']['Labels']['com.docker.compose.test'] == 'true'

    @no_cluster('Container networks not on Swarm')
    def test_build_with_network(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)
        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('FROM busybox\n')
            f.write('RUN ping -c1 google.local\n')

        net_container = self.client.create_container(
            'busybox', 'top', host_config=self.client.create_host_config(
                extra_hosts={'google.local': '127.0.0.1'}
            ), name='composetest_build_network'
        )

        self.addCleanup(self.client.remove_container, net_container, force=True)
        self.client.start(net_container)

        service = self.create_service('buildwithnet', build={
            'context': text_type(base_dir),
            'network': 'container:{}'.format(net_container['Id'])
        })

        service.build()
        self.addCleanup(self.client.remove_image, service.image_name)

        assert service.image()

    @v2_3_only()
    @no_cluster('Not supported on UCP 2.2.0-beta1')  # FIXME: remove once support is added
    def test_build_with_target(self):
        self.require_api_version('1.30')
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('FROM busybox as one\n')
            f.write('LABEL com.docker.compose.test=true\n')
            f.write('LABEL com.docker.compose.test.target=one\n')
            f.write('FROM busybox as two\n')
            f.write('LABEL com.docker.compose.test.target=two\n')

        service = self.create_service('buildtarget', build={
            'context': text_type(base_dir),
            'target': 'one'
        })

        service.build()
        assert service.image()
        assert service.image()['Config']['Labels']['com.docker.compose.test.target'] == 'one'

    @v2_3_only()
    def test_build_with_extra_hosts(self):
        self.require_api_version('1.27')
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('\n'.join([
                'FROM busybox',
                'RUN ping -c1 foobar',
                'RUN ping -c1 baz',
            ]))

        service = self.create_service('build_extra_hosts', build={
            'context': text_type(base_dir),
            'extra_hosts': {
                'foobar': '127.0.0.1',
                'baz': '127.0.0.1'
            }
        })
        service.build()
        assert service.image()

    def test_build_with_gzip(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)
        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('\n'.join([
                'FROM busybox',
                'COPY . /src',
                'RUN cat /src/hello.txt'
            ]))
        with open(os.path.join(base_dir, 'hello.txt'), 'w') as f:
            f.write('hello world\n')

        service = self.create_service('build_gzip', build={
            'context': text_type(base_dir),
        })
        service.build(gzip=True)
        assert service.image()

    @v2_1_only()
    def test_build_with_isolation(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)
        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('FROM busybox\n')

        service = self.create_service('build_isolation', build={
            'context': text_type(base_dir),
            'isolation': 'default',
        })
        service.build()
        assert service.image()

    def test_build_with_illegal_leading_chars(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)
        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write('FROM busybox\nRUN echo "Embodiment of Scarlet Devil"\n')
        service = Service(
            'build_leading_slug', client=self.client,
            project='___-composetest', build={
                'context': text_type(base_dir)
            }
        )
        assert service.image_name == 'composetest_build_leading_slug'
        service.build()
        assert service.image()

    def test_start_container_stays_unprivileged(self):
        service = self.create_service('web')
        container = create_and_start_container(service).inspect()
        assert container['HostConfig']['Privileged'] is False

    def test_start_container_becomes_privileged(self):
        service = self.create_service('web', privileged=True)
        container = create_and_start_container(service).inspect()
        assert container['HostConfig']['Privileged'] is True

    def test_expose_does_not_publish_ports(self):
        service = self.create_service('web', expose=["8000"])
        container = create_and_start_container(service).inspect()
        assert container['NetworkSettings']['Ports'] == {'8000/tcp': None}

    def test_start_container_creates_port_with_explicit_protocol(self):
        service = self.create_service('web', ports=['8000/udp'])
        container = create_and_start_container(service).inspect()
        assert list(container['NetworkSettings']['Ports'].keys()) == ['8000/udp']

    def test_start_container_creates_fixed_external_ports(self):
        service = self.create_service('web', ports=['8000:8000'])
        container = create_and_start_container(service).inspect()
        assert '8000/tcp' in container['NetworkSettings']['Ports']
        assert container['NetworkSettings']['Ports']['8000/tcp'][0]['HostPort'] == '8000'

    def test_start_container_creates_fixed_external_ports_when_it_is_different_to_internal_port(self):
        service = self.create_service('web', ports=['8001:8000'])
        container = create_and_start_container(service).inspect()
        assert '8000/tcp' in container['NetworkSettings']['Ports']
        assert container['NetworkSettings']['Ports']['8000/tcp'][0]['HostPort'] == '8001'

    def test_port_with_explicit_interface(self):
        service = self.create_service('web', ports=[
            '127.0.0.1:8001:8000',
            '0.0.0.0:9001:9000/udp',
        ])
        container = create_and_start_container(service).inspect()
        assert container['NetworkSettings']['Ports']['8000/tcp'] == [{
            'HostIp': '127.0.0.1',
            'HostPort': '8001',
        }]
        assert container['NetworkSettings']['Ports']['9000/udp'][0]['HostPort'] == '9001'
        if not is_cluster(self.client):
            assert container['NetworkSettings']['Ports']['9000/udp'][0]['HostIp'] == '0.0.0.0'
        # self.assertEqual(container['NetworkSettings']['Ports'], {
        #     '8000/tcp': [
        #         {
        #             'HostIp': '127.0.0.1',
        #             'HostPort': '8001',
        #         },
        #     ],
        #     '9000/udp': [
        #         {
        #             'HostIp': '0.0.0.0',
        #             'HostPort': '9001',
        #         },
        #     ],
        # })

    def test_create_with_image_id(self):
        # Get image id for the current busybox:latest
        pull_busybox(self.client)
        image_id = self.client.inspect_image('busybox:latest')['Id'][:12]
        service = self.create_service('foo', image=image_id)
        service.create_container()

    def test_scale(self):
        service = self.create_service('web')
        service.scale(1)
        assert len(service.containers()) == 1

        # Ensure containers don't have stdout or stdin connected
        container = service.containers()[0]
        config = container.inspect()['Config']
        assert not config['AttachStderr']
        assert not config['AttachStdout']
        assert not config['AttachStdin']

        service.scale(3)
        assert len(service.containers()) == 3
        service.scale(1)
        assert len(service.containers()) == 1
        service.scale(0)
        assert len(service.containers()) == 0

    @pytest.mark.skipif(
        SWARM_SKIP_CONTAINERS_ALL,
        reason='Swarm /containers/json bug'
    )
    def test_scale_with_stopped_containers(self):
        """
        Given there are some stopped containers and scale is called with a
        desired number that is the same as the number of stopped containers,
        test that those containers are restarted and not removed/recreated.
        """
        service = self.create_service('web')
        valid_numbers = [service._next_container_number(), service._next_container_number()]
        service.create_container(number=valid_numbers[0])
        service.create_container(number=valid_numbers[1])

        ParallelStreamWriter.instance = None
        with mock.patch('sys.stderr', new_callable=StringIO) as mock_stderr:
            service.scale(2)
        for container in service.containers():
            assert container.is_running
            assert container.number in valid_numbers

        captured_output = mock_stderr.getvalue()
        assert 'Creating' not in captured_output
        assert 'Starting' in captured_output

    def test_scale_with_stopped_containers_and_needing_creation(self):
        """
        Given there are some stopped containers and scale is called with a
        desired number that is greater than the number of stopped containers,
        test that those containers are restarted and required number are created.
        """
        service = self.create_service('web')
        next_number = service._next_container_number()
        service.create_container(number=next_number, quiet=True)

        for container in service.containers():
            assert not container.is_running

        ParallelStreamWriter.instance = None
        with mock.patch('sys.stderr', new_callable=StringIO) as mock_stderr:
            service.scale(2)

        assert len(service.containers()) == 2
        for container in service.containers():
            assert container.is_running

        captured_output = mock_stderr.getvalue()
        assert 'Creating' in captured_output
        assert 'Starting' in captured_output

    def test_scale_with_api_error(self):
        """Test that when scaling if the API returns an error, that error is handled
        and the remaining threads continue.
        """
        service = self.create_service('web')
        next_number = service._next_container_number()
        service.create_container(number=next_number, quiet=True)

        with mock.patch(
            'compose.container.Container.create',
            side_effect=APIError(
                message="testing",
                response={},
                explanation="Boom")):
            with mock.patch('sys.stderr', new_callable=StringIO) as mock_stderr:
                with pytest.raises(OperationFailedError):
                    service.scale(3)

        assert len(service.containers()) == 1
        assert service.containers()[0].is_running
        assert "ERROR: for composetest_web_" in mock_stderr.getvalue()
        assert "Cannot create container for service web: Boom" in mock_stderr.getvalue()

    def test_scale_with_unexpected_exception(self):
        """Test that when scaling if the API returns an error, that is not of type
        APIError, that error is re-raised.
        """
        service = self.create_service('web')
        next_number = service._next_container_number()
        service.create_container(number=next_number, quiet=True)

        with mock.patch(
            'compose.container.Container.create',
            side_effect=ValueError("BOOM")
        ):
            with pytest.raises(ValueError):
                service.scale(3)

        assert len(service.containers()) == 1
        assert service.containers()[0].is_running

    @mock.patch('compose.service.log')
    def test_scale_with_desired_number_already_achieved(self, mock_log):
        """
        Test that calling scale with a desired number that is equal to the
        number of containers already running results in no change.
        """
        service = self.create_service('web')
        next_number = service._next_container_number()
        container = service.create_container(number=next_number, quiet=True)
        container.start()

        container.inspect()
        assert container.is_running
        assert len(service.containers()) == 1

        service.scale(1)
        assert len(service.containers()) == 1
        container.inspect()
        assert container.is_running

        captured_output = mock_log.info.call_args[0]
        assert 'Desired container number already achieved' in captured_output

    @mock.patch('compose.service.log')
    def test_scale_with_custom_container_name_outputs_warning(self, mock_log):
        """Test that calling scale on a service that has a custom container name
        results in warning output.
        """
        service = self.create_service('app', container_name='custom-container')
        assert service.custom_container_name == 'custom-container'

        with pytest.raises(OperationFailedError):
            service.scale(3)

        captured_output = mock_log.warn.call_args[0][0]

        assert len(service.containers()) == 1
        assert "Remove the custom name to scale the service." in captured_output

    def test_scale_sets_ports(self):
        service = self.create_service('web', ports=['8000'])
        service.scale(2)
        containers = service.containers()
        assert len(containers) == 2
        for container in containers:
            assert list(container.get('HostConfig.PortBindings')) == ['8000/tcp']

    def test_scale_with_immediate_exit(self):
        service = self.create_service('web', image='busybox', command='true')
        service.scale(2)
        assert len(service.containers(stopped=True)) == 2

    def test_network_mode_none(self):
        service = self.create_service('web', network_mode=NetworkMode('none'))
        container = create_and_start_container(service)
        assert container.get('HostConfig.NetworkMode') == 'none'

    def test_network_mode_bridged(self):
        service = self.create_service('web', network_mode=NetworkMode('bridge'))
        container = create_and_start_container(service)
        assert container.get('HostConfig.NetworkMode') == 'bridge'

    def test_network_mode_host(self):
        service = self.create_service('web', network_mode=NetworkMode('host'))
        container = create_and_start_container(service)
        assert container.get('HostConfig.NetworkMode') == 'host'

    def test_pid_mode_none_defined(self):
        service = self.create_service('web', pid_mode=None)
        container = create_and_start_container(service)
        assert container.get('HostConfig.PidMode') == ''

    def test_pid_mode_host(self):
        service = self.create_service('web', pid_mode=PidMode('host'))
        container = create_and_start_container(service)
        assert container.get('HostConfig.PidMode') == 'host'

    @v2_1_only()
    def test_userns_mode_none_defined(self):
        service = self.create_service('web', userns_mode=None)
        container = create_and_start_container(service)
        assert container.get('HostConfig.UsernsMode') == ''

    @v2_1_only()
    def test_userns_mode_host(self):
        service = self.create_service('web', userns_mode='host')
        container = create_and_start_container(service)
        assert container.get('HostConfig.UsernsMode') == 'host'

    def test_dns_no_value(self):
        service = self.create_service('web')
        container = create_and_start_container(service)
        assert container.get('HostConfig.Dns') is None

    def test_dns_list(self):
        service = self.create_service('web', dns=['8.8.8.8', '9.9.9.9'])
        container = create_and_start_container(service)
        assert container.get('HostConfig.Dns') == ['8.8.8.8', '9.9.9.9']

    def test_mem_swappiness(self):
        service = self.create_service('web', mem_swappiness=11)
        container = create_and_start_container(service)
        assert container.get('HostConfig.MemorySwappiness') == 11

    def test_mem_reservation(self):
        service = self.create_service('web', mem_reservation='20m')
        container = create_and_start_container(service)
        assert container.get('HostConfig.MemoryReservation') == 20 * 1024 * 1024

    def test_restart_always_value(self):
        service = self.create_service('web', restart={'Name': 'always'})
        container = create_and_start_container(service)
        assert container.get('HostConfig.RestartPolicy.Name') == 'always'

    def test_oom_score_adj_value(self):
        service = self.create_service('web', oom_score_adj=500)
        container = create_and_start_container(service)
        assert container.get('HostConfig.OomScoreAdj') == 500

    def test_group_add_value(self):
        service = self.create_service('web', group_add=["root", "1"])
        container = create_and_start_container(service)

        host_container_groupadd = container.get('HostConfig.GroupAdd')
        assert "root" in host_container_groupadd
        assert "1" in host_container_groupadd

    def test_dns_opt_value(self):
        service = self.create_service('web', dns_opt=["use-vc", "no-tld-query"])
        container = create_and_start_container(service)

        dns_opt = container.get('HostConfig.DnsOptions')
        assert 'use-vc' in dns_opt
        assert 'no-tld-query' in dns_opt

    def test_restart_on_failure_value(self):
        service = self.create_service('web', restart={
            'Name': 'on-failure',
            'MaximumRetryCount': 5
        })
        container = create_and_start_container(service)
        assert container.get('HostConfig.RestartPolicy.Name') == 'on-failure'
        assert container.get('HostConfig.RestartPolicy.MaximumRetryCount') == 5

    def test_cap_add_list(self):
        service = self.create_service('web', cap_add=['SYS_ADMIN', 'NET_ADMIN'])
        container = create_and_start_container(service)
        assert container.get('HostConfig.CapAdd') == ['SYS_ADMIN', 'NET_ADMIN']

    def test_cap_drop_list(self):
        service = self.create_service('web', cap_drop=['SYS_ADMIN', 'NET_ADMIN'])
        container = create_and_start_container(service)
        assert container.get('HostConfig.CapDrop') == ['SYS_ADMIN', 'NET_ADMIN']

    def test_dns_search(self):
        service = self.create_service('web', dns_search=['dc1.example.com', 'dc2.example.com'])
        container = create_and_start_container(service)
        assert container.get('HostConfig.DnsSearch') == ['dc1.example.com', 'dc2.example.com']

    @v2_only()
    def test_tmpfs(self):
        service = self.create_service('web', tmpfs=['/run'])
        container = create_and_start_container(service)
        assert container.get('HostConfig.Tmpfs') == {'/run': ''}

    def test_working_dir_param(self):
        service = self.create_service('container', working_dir='/working/dir/sample')
        container = service.create_container()
        assert container.get('Config.WorkingDir') == '/working/dir/sample'

    def test_split_env(self):
        service = self.create_service(
            'web',
            environment=['NORMAL=F1', 'CONTAINS_EQUALS=F=2', 'TRAILING_EQUALS='])
        env = create_and_start_container(service).environment
        for k, v in {'NORMAL': 'F1', 'CONTAINS_EQUALS': 'F=2', 'TRAILING_EQUALS': ''}.items():
            assert env[k] == v

    def test_env_from_file_combined_with_env(self):
        service = self.create_service(
            'web',
            environment=['ONE=1', 'TWO=2', 'THREE=3'],
            env_file=['tests/fixtures/env/one.env', 'tests/fixtures/env/two.env'])
        env = create_and_start_container(service).environment
        for k, v in {
            'ONE': '1',
            'TWO': '2',
            'THREE': '3',
            'FOO': 'baz',
            'DOO': 'dah'
        }.items():
            assert env[k] == v

    @v3_only()
    def test_build_with_cachefrom(self):
        base_dir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, base_dir)

        with open(os.path.join(base_dir, 'Dockerfile'), 'w') as f:
            f.write("FROM busybox\n")

        service = self.create_service('cache_from',
                                      build={'context': base_dir,
                                             'cache_from': ['build1']})
        service.build()
        self.addCleanup(self.client.remove_image, service.image_name)

        assert service.image()

    @mock.patch.dict(os.environ)
    def test_resolve_env(self):
        os.environ['FILE_DEF'] = 'E1'
        os.environ['FILE_DEF_EMPTY'] = 'E2'
        os.environ['ENV_DEF'] = 'E3'
        service = self.create_service(
            'web',
            environment={
                'FILE_DEF': 'F1',
                'FILE_DEF_EMPTY': '',
                'ENV_DEF': None,
                'NO_DEF': None
            }
        )
        env = create_and_start_container(service).environment
        for k, v in {
            'FILE_DEF': 'F1',
            'FILE_DEF_EMPTY': '',
            'ENV_DEF': 'E3',
            'NO_DEF': None
        }.items():
            assert env[k] == v

    def test_with_high_enough_api_version_we_get_default_network_mode(self):
        # TODO: remove this test once minimum docker version is 1.8.x
        with mock.patch.object(self.client, '_version', '1.20'):
            service = self.create_service('web')
            service_config = service._get_container_host_config({})
            assert service_config['NetworkMode'] == 'default'

    def test_labels(self):
        labels_dict = {
            'com.example.description': "Accounting webapp",
            'com.example.department': "Finance",
            'com.example.label-with-empty-value': "",
        }

        compose_labels = {
            LABEL_ONE_OFF: 'False',
            LABEL_PROJECT: 'composetest',
            LABEL_SERVICE: 'web',
            LABEL_VERSION: __version__,
            LABEL_CONTAINER_NUMBER: '1'
        }
        expected = dict(labels_dict, **compose_labels)

        service = self.create_service('web', labels=labels_dict)
        ctnr = create_and_start_container(service)
        labels = ctnr.labels.items()
        for pair in expected.items():
            assert pair in labels
        assert ctnr.labels[LABEL_SLUG] == ctnr.full_slug

    def test_empty_labels(self):
        labels_dict = {'foo': '', 'bar': ''}
        service = self.create_service('web', labels=labels_dict)
        labels = create_and_start_container(service).labels.items()
        for name in labels_dict:
            assert (name, '') in labels

    def test_stop_signal(self):
        stop_signal = 'SIGINT'
        service = self.create_service('web', stop_signal=stop_signal)
        container = create_and_start_container(service)
        assert container.stop_signal == stop_signal

    def test_custom_container_name(self):
        service = self.create_service('web', container_name='my-web-container')
        assert service.custom_container_name == 'my-web-container'

        container = create_and_start_container(service)
        assert container.name == 'my-web-container'

        one_off_container = service.create_container(one_off=True)
        assert one_off_container.name != 'my-web-container'

    @pytest.mark.skipif(True, reason="Broken on 1.11.0 - 17.03.0")
    def test_log_drive_invalid(self):
        service = self.create_service('web', logging={'driver': 'xxx'})
        expected_error_msg = "logger: no log driver named 'xxx' is registered"

        with pytest.raises(APIError) as excinfo:
            create_and_start_container(service)
        assert re.search(expected_error_msg, excinfo.value)

    def test_log_drive_empty_default_jsonfile(self):
        service = self.create_service('web')
        log_config = create_and_start_container(service).log_config

        assert 'json-file' == log_config['Type']
        assert not log_config['Config']

    def test_log_drive_none(self):
        service = self.create_service('web', logging={'driver': 'none'})
        log_config = create_and_start_container(service).log_config

        assert 'none' == log_config['Type']
        assert not log_config['Config']

    def test_devices(self):
        service = self.create_service('web', devices=["/dev/random:/dev/mapped-random"])
        device_config = create_and_start_container(service).get('HostConfig.Devices')

        device_dict = {
            'PathOnHost': '/dev/random',
            'CgroupPermissions': 'rwm',
            'PathInContainer': '/dev/mapped-random'
        }

        assert 1 == len(device_config)
        assert device_dict == device_config[0]

    def test_duplicate_containers(self):
        service = self.create_service('web')

        options = service._get_container_create_options({}, service._next_container_number())
        original = Container.create(service.client, **options)

        assert set(service.containers(stopped=True)) == set([original])
        assert set(service.duplicate_containers()) == set()

        options['name'] = 'temporary_container_name'
        duplicate = Container.create(service.client, **options)

        assert set(service.containers(stopped=True)) == set([original, duplicate])
        assert set(service.duplicate_containers()) == set([duplicate])


def converge(service, strategy=ConvergenceStrategy.changed):
    """Create a converge plan from a strategy and execute the plan."""
    plan = service.convergence_plan(strategy)
    return service.execute_convergence_plan(plan, timeout=1)


class ConfigHashTest(DockerClientTestCase):

    def test_no_config_hash_when_one_off(self):
        web = self.create_service('web')
        container = web.create_container(one_off=True)
        assert LABEL_CONFIG_HASH not in container.labels

    def test_no_config_hash_when_overriding_options(self):
        web = self.create_service('web')
        container = web.create_container(environment={'FOO': '1'})
        assert LABEL_CONFIG_HASH not in container.labels

    def test_config_hash_with_custom_labels(self):
        web = self.create_service('web', labels={'foo': '1'})
        container = converge(web)[0]
        assert LABEL_CONFIG_HASH in container.labels
        assert 'foo' in container.labels

    def test_config_hash_sticks_around(self):
        web = self.create_service('web', command=["top"])
        container = converge(web)[0]
        assert LABEL_CONFIG_HASH in container.labels

        web = self.create_service('web', command=["top", "-d", "1"])
        container = converge(web)[0]
        assert LABEL_CONFIG_HASH in container.labels
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import six
from docker.errors import DockerException

from .testcases import DockerClientTestCase
from .testcases import no_cluster
from compose.const import LABEL_PROJECT
from compose.const import LABEL_VOLUME
from compose.volume import Volume


class VolumeTest(DockerClientTestCase):
    def setUp(self):
        self.tmp_volumes = []

    def tearDown(self):
        for volume in self.tmp_volumes:
            try:
                self.client.remove_volume(volume.full_name)
            except DockerException:
                pass
        del self.tmp_volumes
        super(VolumeTest, self).tearDown()

    def create_volume(self, name, driver=None, opts=None, external=None, custom_name=False):
        if external:
            custom_name = True
            if isinstance(external, six.text_type):
                name = external

        vol = Volume(
            self.client, 'composetest', name, driver=driver, driver_opts=opts,
            external=bool(external), custom_name=custom_name
        )
        self.tmp_volumes.append(vol)
        return vol

    def test_create_volume(self):
        vol = self.create_volume('volume01')
        vol.create()
        info = self.get_volume_data(vol.full_name)
        assert info['Name'].split('/')[-1] == vol.full_name

    def test_create_volume_custom_name(self):
        vol = self.create_volume('volume01', custom_name=True)
        assert vol.name == vol.full_name
        vol.create()
        info = self.get_volume_data(vol.full_name)
        assert info['Name'].split('/')[-1] == vol.name

    def test_recreate_existing_volume(self):
        vol = self.create_volume('volume01')

        vol.create()
        info = self.get_volume_data(vol.full_name)
        assert info['Name'].split('/')[-1] == vol.full_name

        vol.create()
        info = self.get_volume_data(vol.full_name)
        assert info['Name'].split('/')[-1] == vol.full_name

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_inspect_volume(self):
        vol = self.create_volume('volume01')
        vol.create()
        info = vol.inspect()
        assert info['Name'] == vol.full_name

    @no_cluster('remove volume by name defect on Swarm Classic')
    def test_remove_volume(self):
        vol = Volume(self.client, 'composetest', 'volume01')
        vol.create()
        vol.remove()
        volumes = self.client.volumes()['Volumes']
        assert len([v for v in volumes if v['Name'] == vol.full_name]) == 0

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_external_volume(self):
        vol = self.create_volume('composetest_volume_ext', external=True)
        assert vol.external is True
        assert vol.full_name == vol.name
        vol.create()
        info = vol.inspect()
        assert info['Name'] == vol.name

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_external_aliased_volume(self):
        alias_name = 'composetest_alias01'
        vol = self.create_volume('volume01', external=alias_name)
        assert vol.external is True
        assert vol.full_name == alias_name
        vol.create()
        info = vol.inspect()
        assert info['Name'] == alias_name

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_exists(self):
        vol = self.create_volume('volume01')
        assert vol.exists() is False
        vol.create()
        assert vol.exists() is True

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_exists_external(self):
        vol = self.create_volume('volume01', external=True)
        assert vol.exists() is False
        vol.create()
        assert vol.exists() is True

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_exists_external_aliased(self):
        vol = self.create_volume('volume01', external='composetest_alias01')
        assert vol.exists() is False
        vol.create()
        assert vol.exists() is True

    @no_cluster('inspect volume by name defect on Swarm Classic')
    def test_volume_default_labels(self):
        vol = self.create_volume('volume01')
        vol.create()
        vol_data = vol.inspect()
        labels = vol_data['Labels']
        assert labels[LABEL_VOLUME] == vol.name
        assert labels[LABEL_PROJECT] == vol.project
<EOF>
<BOF>
# encoding: utf-8
from __future__ import absolute_import
from __future__ import unicode_literals

from compose import utils


class TestJsonSplitter(object):

    def test_json_splitter_no_object(self):
        data = '{"foo": "bar'
        assert utils.json_splitter(data) is None

    def test_json_splitter_with_object(self):
        data = '{"foo": "bar"}\n  \n{"next": "obj"}'
        assert utils.json_splitter(data) == ({'foo': 'bar'}, '{"next": "obj"}')

    def test_json_splitter_leading_whitespace(self):
        data = '\n   \r{"foo": "bar"}\n\n   {"next": "obj"}'
        assert utils.json_splitter(data) == ({'foo': 'bar'}, '{"next": "obj"}')


class TestStreamAsText(object):

    def test_stream_with_non_utf_unicode_character(self):
        stream = [b'\xed\xf3\xf3']
        output, = utils.stream_as_text(stream)
        assert output == ''

    def test_stream_with_utf_character(self):
        stream = [''.encode('utf-8')]
        output, = utils.stream_as_text(stream)
        assert output == ''


class TestJsonStream(object):

    def test_with_falsy_entries(self):
        stream = [
            '{"one": "two"}\n{}\n',
            "[1, 2, 3]\n[]\n",
        ]
        output = list(utils.json_stream(stream))
        assert output == [
            {'one': 'two'},
            {},
            [1, 2, 3],
            [],
        ]

    def test_with_leading_whitespace(self):
        stream = [
            '\n  \r\n  {"one": "two"}{"x": 1}',
            '  {"three": "four"}\t\t{"x": 2}'
        ]
        output = list(utils.json_stream(stream))
        assert output == [
            {'one': 'two'},
            {'x': 1},
            {'three': 'four'},
            {'x': 2}
        ]


class TestParseBytes(object):
    def test_parse_bytes(self):
        assert utils.parse_bytes('123kb') == 123 * 1024
        assert utils.parse_bytes(123) == 123
        assert utils.parse_bytes('foobar') is None
        assert utils.parse_bytes('123') == 123


class TestMoreItertools(object):
    def test_unique_everseen(self):
        unique = utils.unique_everseen
        assert list(unique([2, 1, 2, 1])) == [2, 1]
        assert list(unique([2, 1, 2, 1], hash)) == [2, 1]
        assert list(unique([2, 1, 2, 1], lambda x: 'key_%s' % x)) == [2, 1]
<EOF>
<BOF>
# encoding: utf-8
from __future__ import absolute_import
from __future__ import unicode_literals

import datetime

import docker
import pytest
from docker.errors import NotFound

from .. import mock
from .. import unittest
from compose.config.config import Config
from compose.config.types import VolumeFromSpec
from compose.const import COMPOSEFILE_V1 as V1
from compose.const import COMPOSEFILE_V2_0 as V2_0
from compose.const import COMPOSEFILE_V2_4 as V2_4
from compose.const import LABEL_SERVICE
from compose.container import Container
from compose.errors import OperationFailedError
from compose.project import NoSuchService
from compose.project import Project
from compose.project import ProjectError
from compose.service import ImageType
from compose.service import Service


class ProjectTest(unittest.TestCase):
    def setUp(self):
        self.mock_client = mock.create_autospec(docker.APIClient)
        self.mock_client._general_configs = {}
        self.mock_client.api_version = docker.constants.DEFAULT_DOCKER_API_VERSION

    def test_from_config_v1(self):
        config = Config(
            version=V1,
            services=[
                {
                    'name': 'web',
                    'image': 'busybox:latest',
                },
                {
                    'name': 'db',
                    'image': 'busybox:latest',
                },
            ],
            networks=None,
            volumes=None,
            secrets=None,
            configs=None,
        )
        project = Project.from_config(
            name='composetest',
            config_data=config,
            client=None,
        )
        assert len(project.services) == 2
        assert project.get_service('web').name == 'web'
        assert project.get_service('web').options['image'] == 'busybox:latest'
        assert project.get_service('db').name == 'db'
        assert project.get_service('db').options['image'] == 'busybox:latest'
        assert not project.networks.use_networking

    @mock.patch('compose.network.Network.true_name', lambda n: n.full_name)
    def test_from_config_v2(self):
        config = Config(
            version=V2_0,
            services=[
                {
                    'name': 'web',
                    'image': 'busybox:latest',
                },
                {
                    'name': 'db',
                    'image': 'busybox:latest',
                },
            ],
            networks=None,
            volumes=None,
            secrets=None,
            configs=None,
        )
        project = Project.from_config('composetest', config, None)
        assert len(project.services) == 2
        assert project.networks.use_networking

    def test_get_service(self):
        web = Service(
            project='composetest',
            name='web',
            client=None,
            image="busybox:latest",
        )
        project = Project('test', [web], None)
        assert project.get_service('web') == web

    def test_get_services_returns_all_services_without_args(self):
        web = Service(
            project='composetest',
            name='web',
            image='foo',
        )
        console = Service(
            project='composetest',
            name='console',
            image='foo',
        )
        project = Project('test', [web, console], None)
        assert project.get_services() == [web, console]

    def test_get_services_returns_listed_services_with_args(self):
        web = Service(
            project='composetest',
            name='web',
            image='foo',
        )
        console = Service(
            project='composetest',
            name='console',
            image='foo',
        )
        project = Project('test', [web, console], None)
        assert project.get_services(['console']) == [console]

    def test_get_services_with_include_links(self):
        db = Service(
            project='composetest',
            name='db',
            image='foo',
        )
        web = Service(
            project='composetest',
            name='web',
            image='foo',
            links=[(db, 'database')]
        )
        cache = Service(
            project='composetest',
            name='cache',
            image='foo'
        )
        console = Service(
            project='composetest',
            name='console',
            image='foo',
            links=[(web, 'web')]
        )
        project = Project('test', [web, db, cache, console], None)
        assert project.get_services(['console'], include_deps=True) == [db, web, console]

    def test_get_services_removes_duplicates_following_links(self):
        db = Service(
            project='composetest',
            name='db',
            image='foo',
        )
        web = Service(
            project='composetest',
            name='web',
            image='foo',
            links=[(db, 'database')]
        )
        project = Project('test', [web, db], None)
        assert project.get_services(['web', 'db'], include_deps=True) == [db, web]

    def test_use_volumes_from_container(self):
        container_id = 'aabbccddee'
        container_dict = dict(Name='aaa', Id=container_id)
        self.mock_client.inspect_container.return_value = container_dict
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[{
                    'name': 'test',
                    'image': 'busybox:latest',
                    'volumes_from': [VolumeFromSpec('aaa', 'rw', 'container')]
                }],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )
        assert project.get_service('test')._get_volumes_from() == [container_id + ":rw"]

    def test_use_volumes_from_service_no_container(self):
        container_name = 'test_vol_1'
        self.mock_client.containers.return_value = [
            {
                "Name": container_name,
                "Names": [container_name],
                "Id": container_name,
                "Image": 'busybox:latest'
            }
        ]
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[
                    {
                        'name': 'vol',
                        'image': 'busybox:latest'
                    },
                    {
                        'name': 'test',
                        'image': 'busybox:latest',
                        'volumes_from': [VolumeFromSpec('vol', 'rw', 'service')]
                    }
                ],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )
        assert project.get_service('test')._get_volumes_from() == [container_name + ":rw"]

    @mock.patch('compose.network.Network.true_name', lambda n: n.full_name)
    def test_use_volumes_from_service_container(self):
        container_ids = ['aabbccddee', '12345']

        project = Project.from_config(
            name='test',
            client=None,
            config_data=Config(
                version=V2_0,
                services=[
                    {
                        'name': 'vol',
                        'image': 'busybox:latest'
                    },
                    {
                        'name': 'test',
                        'image': 'busybox:latest',
                        'volumes_from': [VolumeFromSpec('vol', 'rw', 'service')]
                    }
                ],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )
        with mock.patch.object(Service, 'containers') as mock_return:
            mock_return.return_value = [
                mock.Mock(id=container_id, spec=Container)
                for container_id in container_ids]
            assert (
                project.get_service('test')._get_volumes_from() ==
                [container_ids[0] + ':rw']
            )

    def test_events(self):
        services = [Service(name='web'), Service(name='db')]
        project = Project('test', services, self.mock_client)
        self.mock_client.events.return_value = iter([
            {
                'status': 'create',
                'from': 'example/image',
                'id': 'abcde',
                'time': 1420092061,
                'timeNano': 14200920610000002000,
            },
            {
                'status': 'attach',
                'from': 'example/image',
                'id': 'abcde',
                'time': 1420092061,
                'timeNano': 14200920610000003000,
            },
            {
                'status': 'create',
                'from': 'example/other',
                'id': 'bdbdbd',
                'time': 1420092061,
                'timeNano': 14200920610000005000,
            },
            {
                'status': 'create',
                'from': 'example/db',
                'id': 'ababa',
                'time': 1420092061,
                'timeNano': 14200920610000004000,
            },
            {
                'status': 'destroy',
                'from': 'example/db',
                'id': 'eeeee',
                'time': 1420092061,
                'timeNano': 14200920610000004000,
            },
        ])

        def dt_with_microseconds(dt, us):
            return datetime.datetime.fromtimestamp(dt).replace(microsecond=us)

        def get_container(cid):
            if cid == 'eeeee':
                raise NotFound(None, None, "oops")
            if cid == 'abcde':
                name = 'web'
                labels = {LABEL_SERVICE: name}
            elif cid == 'ababa':
                name = 'db'
                labels = {LABEL_SERVICE: name}
            else:
                labels = {}
                name = ''
            return {
                'Id': cid,
                'Config': {'Labels': labels},
                'Name': '/project_%s_1' % name,
            }

        self.mock_client.inspect_container.side_effect = get_container

        events = project.events()

        events_list = list(events)
        # Assert the return value is a generator
        assert not list(events)
        assert events_list == [
            {
                'type': 'container',
                'service': 'web',
                'action': 'create',
                'id': 'abcde',
                'attributes': {
                    'name': 'project_web_1',
                    'image': 'example/image',
                },
                'time': dt_with_microseconds(1420092061, 2),
                'container': Container(None, {'Id': 'abcde'}),
            },
            {
                'type': 'container',
                'service': 'web',
                'action': 'attach',
                'id': 'abcde',
                'attributes': {
                    'name': 'project_web_1',
                    'image': 'example/image',
                },
                'time': dt_with_microseconds(1420092061, 3),
                'container': Container(None, {'Id': 'abcde'}),
            },
            {
                'type': 'container',
                'service': 'db',
                'action': 'create',
                'id': 'ababa',
                'attributes': {
                    'name': 'project_db_1',
                    'image': 'example/db',
                },
                'time': dt_with_microseconds(1420092061, 4),
                'container': Container(None, {'Id': 'ababa'}),
            },
        ]

    def test_net_unset(self):
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V1,
                services=[
                    {
                        'name': 'test',
                        'image': 'busybox:latest',
                    }
                ],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )
        service = project.get_service('test')
        assert service.network_mode.id is None
        assert 'NetworkMode' not in service._get_container_host_config({})

    def test_use_net_from_container(self):
        container_id = 'aabbccddee'
        container_dict = dict(Name='aaa', Id=container_id)
        self.mock_client.inspect_container.return_value = container_dict
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[
                    {
                        'name': 'test',
                        'image': 'busybox:latest',
                        'network_mode': 'container:aaa'
                    },
                ],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )
        service = project.get_service('test')
        assert service.network_mode.mode == 'container:' + container_id

    def test_use_net_from_service(self):
        container_name = 'test_aaa_1'
        self.mock_client.containers.return_value = [
            {
                "Name": container_name,
                "Names": [container_name],
                "Id": container_name,
                "Image": 'busybox:latest'
            }
        ]
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[
                    {
                        'name': 'aaa',
                        'image': 'busybox:latest'
                    },
                    {
                        'name': 'test',
                        'image': 'busybox:latest',
                        'network_mode': 'service:aaa'
                    },
                ],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )

        service = project.get_service('test')
        assert service.network_mode.mode == 'container:' + container_name

    def test_uses_default_network_true(self):
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[
                    {
                        'name': 'foo',
                        'image': 'busybox:latest'
                    },
                ],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )

        assert 'default' in project.networks.networks

    def test_uses_default_network_false(self):
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[
                    {
                        'name': 'foo',
                        'image': 'busybox:latest',
                        'networks': {'custom': None}
                    },
                ],
                networks={'custom': {}},
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )

        assert 'default' not in project.networks.networks

    def test_container_without_name(self):
        self.mock_client.containers.return_value = [
            {'Image': 'busybox:latest', 'Id': '1', 'Name': '1'},
            {'Image': 'busybox:latest', 'Id': '2', 'Name': None},
            {'Image': 'busybox:latest', 'Id': '3'},
        ]
        self.mock_client.inspect_container.return_value = {
            'Id': '1',
            'Config': {
                'Labels': {
                    LABEL_SERVICE: 'web',
                },
            },
        }
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[{
                    'name': 'web',
                    'image': 'busybox:latest',
                }],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )
        assert [c.id for c in project.containers()] == ['1']

    def test_down_with_no_resources(self):
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[{
                    'name': 'web',
                    'image': 'busybox:latest',
                }],
                networks={'default': {}},
                volumes={'data': {}},
                secrets=None,
                configs=None,
            ),
        )
        self.mock_client.remove_network.side_effect = NotFound(None, None, 'oops')
        self.mock_client.remove_volume.side_effect = NotFound(None, None, 'oops')

        project.down(ImageType.all, True)
        self.mock_client.remove_image.assert_called_once_with("busybox:latest")

    def test_no_warning_on_stop(self):
        self.mock_client.info.return_value = {'Swarm': {'LocalNodeState': 'active'}}
        project = Project('composetest', [], self.mock_client)

        with mock.patch('compose.project.log') as fake_log:
            project.stop()
            assert fake_log.warn.call_count == 0

    def test_no_warning_in_normal_mode(self):
        self.mock_client.info.return_value = {'Swarm': {'LocalNodeState': 'inactive'}}
        project = Project('composetest', [], self.mock_client)

        with mock.patch('compose.project.log') as fake_log:
            project.up()
            assert fake_log.warn.call_count == 0

    def test_no_warning_with_no_swarm_info(self):
        self.mock_client.info.return_value = {}
        project = Project('composetest', [], self.mock_client)

        with mock.patch('compose.project.log') as fake_log:
            project.up()
            assert fake_log.warn.call_count == 0

    def test_no_such_service_unicode(self):
        assert NoSuchService(''.encode('utf-8')).msg == 'No such service: '
        assert NoSuchService('').msg == 'No such service: '

    def test_project_platform_value(self):
        service_config = {
            'name': 'web',
            'image': 'busybox:latest',
        }
        config_data = Config(
            version=V2_4, services=[service_config], networks={}, volumes={}, secrets=None, configs=None
        )

        project = Project.from_config(name='test', client=self.mock_client, config_data=config_data)
        assert project.get_service('web').platform is None

        project = Project.from_config(
            name='test', client=self.mock_client, config_data=config_data, default_platform='windows'
        )
        assert project.get_service('web').platform == 'windows'

        service_config['platform'] = 'linux/s390x'
        project = Project.from_config(name='test', client=self.mock_client, config_data=config_data)
        assert project.get_service('web').platform == 'linux/s390x'

        project = Project.from_config(
            name='test', client=self.mock_client, config_data=config_data, default_platform='windows'
        )
        assert project.get_service('web').platform == 'linux/s390x'

    @mock.patch('compose.parallel.ParallelStreamWriter._write_noansi')
    def test_error_parallel_pull(self, mock_write):
        project = Project.from_config(
            name='test',
            client=self.mock_client,
            config_data=Config(
                version=V2_0,
                services=[{
                    'name': 'web',
                    'image': 'busybox:latest',
                }],
                networks=None,
                volumes=None,
                secrets=None,
                configs=None,
            ),
        )

        self.mock_client.pull.side_effect = OperationFailedError('pull error')
        with pytest.raises(ProjectError):
            project.pull(parallel_pull=True)

        self.mock_client.pull.side_effect = OperationFailedError(b'pull error')
        with pytest.raises(ProjectError):
            project.pull(parallel_pull=True)
<EOF>
<BOF>
# encoding: utf-8
from __future__ import absolute_import
from __future__ import unicode_literals

import os
import shutil
import tempfile
from io import StringIO

import docker
import py
import pytest
from docker.constants import DEFAULT_DOCKER_API_VERSION

from .. import mock
from .. import unittest
from ..helpers import build_config
from compose.cli.command import get_project
from compose.cli.command import get_project_name
from compose.cli.docopt_command import NoSuchCommand
from compose.cli.errors import UserError
from compose.cli.main import TopLevelCommand
from compose.const import IS_WINDOWS_PLATFORM
from compose.project import Project


class CLITestCase(unittest.TestCase):

    def test_default_project_name(self):
        test_dir = py._path.local.LocalPath('tests/fixtures/simple-composefile')
        with test_dir.as_cwd():
            project_name = get_project_name('.')
            assert 'simple-composefile' == project_name

    def test_project_name_with_explicit_base_dir(self):
        base_dir = 'tests/fixtures/simple-composefile'
        project_name = get_project_name(base_dir)
        assert 'simple-composefile' == project_name

    def test_project_name_with_explicit_uppercase_base_dir(self):
        base_dir = 'tests/fixtures/UpperCaseDir'
        project_name = get_project_name(base_dir)
        assert 'uppercasedir' == project_name

    def test_project_name_with_explicit_project_name(self):
        name = 'explicit-project-name'
        project_name = get_project_name(None, project_name=name)
        assert 'explicit-project-name' == project_name

    @mock.patch.dict(os.environ)
    def test_project_name_from_environment_new_var(self):
        name = 'namefromenv'
        os.environ['COMPOSE_PROJECT_NAME'] = name
        project_name = get_project_name(None)
        assert project_name == name

    def test_project_name_with_empty_environment_var(self):
        base_dir = 'tests/fixtures/simple-composefile'
        with mock.patch.dict(os.environ):
            os.environ['COMPOSE_PROJECT_NAME'] = ''
            project_name = get_project_name(base_dir)
        assert 'simple-composefile' == project_name

    @mock.patch.dict(os.environ)
    def test_project_name_with_environment_file(self):
        base_dir = tempfile.mkdtemp()
        try:
            name = 'namefromenvfile'
            with open(os.path.join(base_dir, '.env'), 'w') as f:
                f.write('COMPOSE_PROJECT_NAME={}'.format(name))
            project_name = get_project_name(base_dir)
            assert project_name == name

            # Environment has priority over .env file
            os.environ['COMPOSE_PROJECT_NAME'] = 'namefromenv'
            assert get_project_name(base_dir) == os.environ['COMPOSE_PROJECT_NAME']
        finally:
            shutil.rmtree(base_dir)

    def test_get_project(self):
        base_dir = 'tests/fixtures/longer-filename-composefile'
        project = get_project(base_dir)
        assert project.name == 'longer-filename-composefile'
        assert project.client
        assert project.services

    def test_command_help(self):
        with mock.patch('sys.stdout', new=StringIO()) as fake_stdout:
            TopLevelCommand.help({'COMMAND': 'up'})

        assert "Usage: up" in fake_stdout.getvalue()

    def test_command_help_nonexistent(self):
        with pytest.raises(NoSuchCommand):
            TopLevelCommand.help({'COMMAND': 'nonexistent'})

    @pytest.mark.xfail(IS_WINDOWS_PLATFORM, reason="requires dockerpty")
    @mock.patch('compose.cli.main.RunOperation', autospec=True)
    @mock.patch('compose.cli.main.PseudoTerminal', autospec=True)
    @mock.patch.dict(os.environ)
    def test_run_interactive_passes_logs_false(self, mock_pseudo_terminal, mock_run_operation):
        os.environ['COMPOSE_INTERACTIVE_NO_CLI'] = 'true'
        mock_client = mock.create_autospec(docker.APIClient)
        mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        mock_client._general_configs = {}
        project = Project.from_config(
            name='composetest',
            client=mock_client,
            config_data=build_config({
                'service': {'image': 'busybox'}
            }),
        )
        command = TopLevelCommand(project)

        with pytest.raises(SystemExit):
            command.run({
                'SERVICE': 'service',
                'COMMAND': None,
                '-e': [],
                '--label': [],
                '--user': None,
                '--no-deps': None,
                '--detach': False,
                '-T': None,
                '--entrypoint': None,
                '--service-ports': None,
                '--use-aliases': None,
                '--publish': [],
                '--volume': [],
                '--rm': None,
                '--name': None,
                '--workdir': None,
            })

        _, _, call_kwargs = mock_run_operation.mock_calls[0]
        assert call_kwargs['logs'] is False

    def test_run_service_with_restart_always(self):
        mock_client = mock.create_autospec(docker.APIClient)
        mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        mock_client._general_configs = {}

        project = Project.from_config(
            name='composetest',
            client=mock_client,
            config_data=build_config({
                'service': {
                    'image': 'busybox',
                    'restart': 'always',
                }
            }),
        )

        command = TopLevelCommand(project)
        command.run({
            'SERVICE': 'service',
            'COMMAND': None,
            '-e': [],
            '--label': [],
            '--user': None,
            '--no-deps': None,
            '--detach': True,
            '-T': None,
            '--entrypoint': None,
            '--service-ports': None,
            '--use-aliases': None,
            '--publish': [],
            '--volume': [],
            '--rm': None,
            '--name': None,
            '--workdir': None,
        })

        # NOTE: The "run" command is supposed to be a one-off tool; therefore restart policy "no"
        #       (the default) is enforced despite explicit wish for "always" in the project
        #       configuration file
        assert not mock_client.create_host_config.call_args[1].get('restart_policy')

        command = TopLevelCommand(project)
        command.run({
            'SERVICE': 'service',
            'COMMAND': None,
            '-e': [],
            '--label': [],
            '--user': None,
            '--no-deps': None,
            '--detach': True,
            '-T': None,
            '--entrypoint': None,
            '--service-ports': None,
            '--use-aliases': None,
            '--publish': [],
            '--volume': [],
            '--rm': True,
            '--name': None,
            '--workdir': None,
        })

        assert not mock_client.create_host_config.call_args[1].get('restart_policy')

    def test_command_manual_and_service_ports_together(self):
        project = Project.from_config(
            name='composetest',
            client=None,
            config_data=build_config({
                'service': {'image': 'busybox'},
            }),
        )
        command = TopLevelCommand(project)

        with pytest.raises(UserError):
            command.run({
                'SERVICE': 'service',
                'COMMAND': None,
                '-e': [],
                '--label': [],
                '--user': None,
                '--no-deps': None,
                '--detach': True,
                '-T': None,
                '--entrypoint': None,
                '--service-ports': True,
                '--use-aliases': None,
                '--publish': ['80:80'],
                '--rm': None,
                '--name': None,
            })
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import docker

from .. import mock
from .. import unittest
from compose.const import LABEL_SLUG
from compose.container import Container
from compose.container import get_container_name


class ContainerTest(unittest.TestCase):

    def setUp(self):
        self.container_id = "abcabcabcbabc12345"
        self.container_dict = {
            "Id": self.container_id,
            "Image": "busybox:latest",
            "Command": "top",
            "Created": 1387384730,
            "Status": "Up 8 seconds",
            "Ports": None,
            "SizeRw": 0,
            "SizeRootFs": 0,
            "Names": ["/composetest_db_1", "/composetest_web_1/db"],
            "NetworkSettings": {
                "Ports": {},
            },
            "Config": {
                "Labels": {
                    "com.docker.compose.project": "composetest",
                    "com.docker.compose.service": "web",
                    "com.docker.compose.container-number": "7",
                    "com.docker.compose.slug": "092cd63296fdc446ad432d3905dd1fcbe12a2ba6b52"
                },
            }
        }

    def test_from_ps(self):
        container = Container.from_ps(None,
                                      self.container_dict,
                                      has_been_inspected=True)
        assert container.dictionary == {
            "Id": self.container_id,
            "Image": "busybox:latest",
            "Name": "/composetest_db_1",
        }

    def test_from_ps_prefixed(self):
        self.container_dict['Names'] = [
            '/swarm-host-1' + n for n in self.container_dict['Names']
        ]

        container = Container.from_ps(
            None,
            self.container_dict,
            has_been_inspected=True)
        assert container.dictionary == {
            "Id": self.container_id,
            "Image": "busybox:latest",
            "Name": "/composetest_db_1",
        }

    def test_environment(self):
        container = Container(None, {
            'Id': 'abc',
            'Config': {
                'Env': [
                    'FOO=BAR',
                    'BAZ=DOGE',
                ]
            }
        }, has_been_inspected=True)
        assert container.environment == {
            'FOO': 'BAR',
            'BAZ': 'DOGE',
        }

    def test_number(self):
        container = Container(None, self.container_dict, has_been_inspected=True)
        assert container.number == 7

    def test_name(self):
        container = Container.from_ps(None,
                                      self.container_dict,
                                      has_been_inspected=True)
        assert container.name == "composetest_db_1"

    def test_name_without_project(self):
        self.container_dict['Name'] = "/composetest_web_7_092cd63296fd"
        container = Container(None, self.container_dict, has_been_inspected=True)
        assert container.name_without_project == "web_7_092cd63296fd"

    def test_name_without_project_custom_container_name(self):
        self.container_dict['Name'] = "/custom_name_of_container"
        container = Container(None, self.container_dict, has_been_inspected=True)
        assert container.name_without_project == "custom_name_of_container"

    def test_name_without_project_noslug(self):
        self.container_dict['Name'] = "/composetest_web_7"
        del self.container_dict['Config']['Labels'][LABEL_SLUG]
        container = Container(None, self.container_dict, has_been_inspected=True)
        assert container.name_without_project == 'web_7'

    def test_inspect_if_not_inspected(self):
        mock_client = mock.create_autospec(docker.APIClient)
        container = Container(mock_client, dict(Id="the_id"))

        container.inspect_if_not_inspected()
        mock_client.inspect_container.assert_called_once_with("the_id")
        assert container.dictionary == mock_client.inspect_container.return_value
        assert container.has_been_inspected

        container.inspect_if_not_inspected()
        assert mock_client.inspect_container.call_count == 1

    def test_human_readable_ports_none(self):
        container = Container(None, self.container_dict, has_been_inspected=True)
        assert container.human_readable_ports == ''

    def test_human_readable_ports_public_and_private(self):
        self.container_dict['NetworkSettings']['Ports'].update({
            "45454/tcp": [{"HostIp": "0.0.0.0", "HostPort": "49197"}],
            "45453/tcp": [],
        })
        container = Container(None, self.container_dict, has_been_inspected=True)

        expected = "45453/tcp, 0.0.0.0:49197->45454/tcp"
        assert container.human_readable_ports == expected

    def test_get_local_port(self):
        self.container_dict['NetworkSettings']['Ports'].update({
            "45454/tcp": [{"HostIp": "0.0.0.0", "HostPort": "49197"}],
        })
        container = Container(None, self.container_dict, has_been_inspected=True)

        assert container.get_local_port(45454, protocol='tcp') == '0.0.0.0:49197'

    def test_human_readable_states_no_health(self):
        container = Container(None, {
            "State": {
                "Status": "running",
                "Running": True,
                "Paused": False,
                "Restarting": False,
                "OOMKilled": False,
                "Dead": False,
                "Pid": 7623,
                "ExitCode": 0,
                "Error": "",
                "StartedAt": "2018-01-29T00:34:25.2052414Z",
                "FinishedAt": "0001-01-01T00:00:00Z"
            },
        }, has_been_inspected=True)
        expected = "Up"
        assert container.human_readable_state == expected

    def test_human_readable_states_starting(self):
        container = Container(None, {
            "State": {
                "Status": "running",
                "Running": True,
                "Paused": False,
                "Restarting": False,
                "OOMKilled": False,
                "Dead": False,
                "Pid": 11744,
                "ExitCode": 0,
                "Error": "",
                "StartedAt": "2018-02-03T07:56:20.3591233Z",
                "FinishedAt": "2018-01-31T08:56:11.0505228Z",
                "Health": {
                    "Status": "starting",
                    "FailingStreak": 0,
                    "Log": []
                }
            }
        }, has_been_inspected=True)
        expected = "Up (health: starting)"
        assert container.human_readable_state == expected

    def test_human_readable_states_healthy(self):
        container = Container(None, {
            "State": {
                "Status": "running",
                "Running": True,
                "Paused": False,
                "Restarting": False,
                "OOMKilled": False,
                "Dead": False,
                "Pid": 5674,
                "ExitCode": 0,
                "Error": "",
                "StartedAt": "2018-02-03T08:32:05.3281831Z",
                "FinishedAt": "2018-02-03T08:11:35.7872706Z",
                "Health": {
                    "Status": "healthy",
                    "FailingStreak": 0,
                    "Log": []
                }
            }
        }, has_been_inspected=True)
        expected = "Up (healthy)"
        assert container.human_readable_state == expected

    def test_get(self):
        container = Container(None, {
            "Status": "Up 8 seconds",
            "HostConfig": {
                "VolumesFrom": ["volume_id"]
            },
        }, has_been_inspected=True)

        assert container.get('Status') == "Up 8 seconds"
        assert container.get('HostConfig.VolumesFrom') == ["volume_id"]
        assert container.get('Foo.Bar.DoesNotExist') is None

    def test_short_id(self):
        container = Container(None, self.container_dict, has_been_inspected=True)
        assert container.short_id == self.container_id[:12]

    def test_has_api_logs(self):
        container_dict = {
            'HostConfig': {
                'LogConfig': {
                    'Type': 'json-file'
                }
            }
        }

        container = Container(None, container_dict, has_been_inspected=True)
        assert container.has_api_logs is True

        container_dict['HostConfig']['LogConfig']['Type'] = 'none'
        container = Container(None, container_dict, has_been_inspected=True)
        assert container.has_api_logs is False

        container_dict['HostConfig']['LogConfig']['Type'] = 'syslog'
        container = Container(None, container_dict, has_been_inspected=True)
        assert container.has_api_logs is False

        container_dict['HostConfig']['LogConfig']['Type'] = 'journald'
        container = Container(None, container_dict, has_been_inspected=True)
        assert container.has_api_logs is True

        container_dict['HostConfig']['LogConfig']['Type'] = 'foobar'
        container = Container(None, container_dict, has_been_inspected=True)
        assert container.has_api_logs is False


class GetContainerNameTestCase(unittest.TestCase):

    def test_get_container_name(self):
        assert get_container_name({}) is None
        assert get_container_name({'Name': 'myproject_db_1'}) == 'myproject_db_1'
        assert get_container_name(
            {'Names': ['/myproject_db_1', '/myproject_web_1/db']}
        ) == 'myproject_db_1'
        assert get_container_name({
            'Names': [
                '/swarm-host-1/myproject_db_1',
                '/swarm-host-1/myproject_web_1/db'
            ]
        }) == 'myproject_db_1'
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest

from .. import mock
from .. import unittest
from compose.network import check_remote_network_config
from compose.network import Network
from compose.network import NetworkConfigChangedError


class NetworkTest(unittest.TestCase):
    def test_check_remote_network_config_success(self):
        options = {'com.docker.network.driver.foo': 'bar'}
        ipam_config = {
            'driver': 'default',
            'config': [
                {'subnet': '172.0.0.1/16', },
                {
                    'subnet': '156.0.0.1/25',
                    'gateway': '156.0.0.1',
                    'aux_addresses': ['11.0.0.1', '24.25.26.27'],
                    'ip_range': '156.0.0.1-254'
                }
            ],
            'options': {
                'iface': 'eth0',
            }
        }
        labels = {
            'com.project.tests.istest': 'true',
            'com.project.sound.track': 'way out of here',
        }
        remote_labels = labels.copy()
        remote_labels.update({
            'com.docker.compose.project': 'compose_test',
            'com.docker.compose.network': 'net1',
        })
        net = Network(
            None, 'compose_test', 'net1', 'bridge',
            options, enable_ipv6=True, ipam=ipam_config,
            labels=labels
        )
        check_remote_network_config(
            {
                'Driver': 'bridge',
                'Options': options,
                'EnableIPv6': True,
                'Internal': False,
                'Attachable': True,
                'IPAM': {
                    'Driver': 'default',
                    'Config': [{
                        'Subnet': '156.0.0.1/25',
                        'Gateway': '156.0.0.1',
                        'AuxiliaryAddresses': ['24.25.26.27', '11.0.0.1'],
                        'IPRange': '156.0.0.1-254'
                    }, {
                        'Subnet': '172.0.0.1/16',
                        'Gateway': '172.0.0.1'
                    }],
                    'Options': {
                        'iface': 'eth0',
                    },
                },
                'Labels': remote_labels
            },
            net
        )

    def test_check_remote_network_config_whitelist(self):
        options = {'com.docker.network.driver.foo': 'bar'}
        remote_options = {
            'com.docker.network.driver.overlay.vxlanid_list': '257',
            'com.docker.network.driver.foo': 'bar',
            'com.docker.network.windowsshim.hnsid': 'aac3fd4887daaec1e3b',
        }
        net = Network(
            None, 'compose_test', 'net1', 'overlay',
            options
        )
        check_remote_network_config(
            {'Driver': 'overlay', 'Options': remote_options}, net
        )

    @mock.patch('compose.network.Network.true_name', lambda n: n.full_name)
    def test_check_remote_network_config_driver_mismatch(self):
        net = Network(None, 'compose_test', 'net1', 'overlay')
        with pytest.raises(NetworkConfigChangedError) as e:
            check_remote_network_config(
                {'Driver': 'bridge', 'Options': {}}, net
            )

        assert 'driver has changed' in str(e.value)

    @mock.patch('compose.network.Network.true_name', lambda n: n.full_name)
    def test_check_remote_network_config_options_mismatch(self):
        net = Network(None, 'compose_test', 'net1', 'overlay')
        with pytest.raises(NetworkConfigChangedError) as e:
            check_remote_network_config({'Driver': 'overlay', 'Options': {
                'com.docker.network.driver.foo': 'baz'
            }}, net)

        assert 'option "com.docker.network.driver.foo" has changed' in str(e.value)

    def test_check_remote_network_config_null_remote(self):
        net = Network(None, 'compose_test', 'net1', 'overlay')
        check_remote_network_config(
            {'Driver': 'overlay', 'Options': None}, net
        )

    def test_check_remote_network_config_null_remote_ipam_options(self):
        ipam_config = {
            'driver': 'default',
            'config': [
                {'subnet': '172.0.0.1/16', },
                {
                    'subnet': '156.0.0.1/25',
                    'gateway': '156.0.0.1',
                    'aux_addresses': ['11.0.0.1', '24.25.26.27'],
                    'ip_range': '156.0.0.1-254'
                }
            ]
        }
        net = Network(
            None, 'compose_test', 'net1', 'bridge', ipam=ipam_config,
        )

        check_remote_network_config(
            {
                'Driver': 'bridge',
                'Attachable': True,
                'IPAM': {
                    'Driver': 'default',
                    'Config': [{
                        'Subnet': '156.0.0.1/25',
                        'Gateway': '156.0.0.1',
                        'AuxiliaryAddresses': ['24.25.26.27', '11.0.0.1'],
                        'IPRange': '156.0.0.1-254'
                    }, {
                        'Subnet': '172.0.0.1/16',
                        'Gateway': '172.0.0.1'
                    }],
                    'Options': None
                },
            },
            net
        )

    @mock.patch('compose.network.Network.true_name', lambda n: n.full_name)
    def test_check_remote_network_labels_mismatch(self):
        net = Network(None, 'compose_test', 'net1', 'overlay', labels={
            'com.project.touhou.character': 'sakuya.izayoi'
        })
        remote = {
            'Driver': 'overlay',
            'Options': None,
            'Labels': {
                'com.docker.compose.network': 'net1',
                'com.docker.compose.project': 'compose_test',
                'com.project.touhou.character': 'marisa.kirisame',
            }
        }
        with mock.patch('compose.network.log') as mock_log:
            check_remote_network_config(remote, net)

        mock_log.warn.assert_called_once_with(mock.ANY)
        _, args, kwargs = mock_log.warn.mock_calls[0]
        assert 'label "com.project.touhou.character" has changed' in args[0]
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from .. import unittest
from compose.utils import split_buffer


class SplitBufferTest(unittest.TestCase):
    def test_single_line_chunks(self):
        def reader():
            yield b'abc\n'
            yield b'def\n'
            yield b'ghi\n'

        self.assert_produces(reader, ['abc\n', 'def\n', 'ghi\n'])

    def test_no_end_separator(self):
        def reader():
            yield b'abc\n'
            yield b'def\n'
            yield b'ghi'

        self.assert_produces(reader, ['abc\n', 'def\n', 'ghi'])

    def test_multiple_line_chunk(self):
        def reader():
            yield b'abc\ndef\nghi'

        self.assert_produces(reader, ['abc\n', 'def\n', 'ghi'])

    def test_chunked_line(self):
        def reader():
            yield b'a'
            yield b'b'
            yield b'c'
            yield b'\n'
            yield b'd'

        self.assert_produces(reader, ['abc\n', 'd'])

    def test_preserves_unicode_sequences_within_lines(self):
        string = u"a\u2022c\n"

        def reader():
            yield string.encode('utf-8')

        self.assert_produces(reader, [string])

    def assert_produces(self, reader, expectations):
        split = split_buffer(reader())

        for (actual, expected) in zip(split, expectations):
            assert type(actual) == type(expected)
            assert actual == expected
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from compose import timeparse


def test_milli():
    assert timeparse.timeparse('5ms') == 0.005


def test_milli_float():
    assert timeparse.timeparse('50.5ms') == 0.0505


def test_second_milli():
    assert timeparse.timeparse('200s5ms') == 200.005


def test_second_milli_micro():
    assert timeparse.timeparse('200s5ms10us') == 200.00501


def test_second():
    assert timeparse.timeparse('200s') == 200


def test_second_as_float():
    assert timeparse.timeparse('20.5s') == 20.5


def test_minute():
    assert timeparse.timeparse('32m') == 1920


def test_hour_minute():
    assert timeparse.timeparse('2h32m') == 9120


def test_minute_as_float():
    assert timeparse.timeparse('1.5m') == 90


def test_hour_minute_second():
    assert timeparse.timeparse('5h34m56s') == 20096


def test_invalid_with_space():
    assert timeparse.timeparse('5h 34m 56s') is None


def test_invalid_with_comma():
    assert timeparse.timeparse('5h,34m,56s') is None


def test_invalid_with_empty_string():
    assert timeparse.timeparse('') is None
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import docker
import pytest

from .. import mock
from compose import bundle
from compose import service
from compose.cli.errors import UserError
from compose.config.config import Config
from compose.const import COMPOSEFILE_V2_0 as V2_0


@pytest.fixture
def mock_service():
    return mock.create_autospec(
        service.Service,
        client=mock.create_autospec(docker.APIClient),
        options={})


def test_get_image_digest_exists(mock_service):
    mock_service.options['image'] = 'abcd'
    mock_service.image.return_value = {'RepoDigests': ['digest1']}
    digest = bundle.get_image_digest(mock_service)
    assert digest == 'digest1'


def test_get_image_digest_image_uses_digest(mock_service):
    mock_service.options['image'] = image_id = 'redis@sha256:digest'

    digest = bundle.get_image_digest(mock_service)
    assert digest == image_id
    assert not mock_service.image.called


def test_get_image_digest_no_image(mock_service):
    with pytest.raises(UserError) as exc:
        bundle.get_image_digest(service.Service(name='theservice'))

    assert "doesn't define an image tag" in exc.exconly()


def test_push_image_with_saved_digest(mock_service):
    mock_service.options['build'] = '.'
    mock_service.options['image'] = image_id = 'abcd'
    mock_service.push.return_value = expected = 'sha256:thedigest'
    mock_service.image.return_value = {'RepoDigests': ['digest1']}

    digest = bundle.push_image(mock_service)
    assert digest == image_id + '@' + expected

    mock_service.push.assert_called_once_with()
    assert not mock_service.client.push.called


def test_push_image(mock_service):
    mock_service.options['build'] = '.'
    mock_service.options['image'] = image_id = 'abcd'
    mock_service.push.return_value = expected = 'sha256:thedigest'
    mock_service.image.return_value = {'RepoDigests': []}

    digest = bundle.push_image(mock_service)
    assert digest == image_id + '@' + expected

    mock_service.push.assert_called_once_with()
    mock_service.client.pull.assert_called_once_with(digest)


def test_to_bundle():
    image_digests = {'a': 'aaaa', 'b': 'bbbb'}
    services = [
        {'name': 'a', 'build': '.', },
        {'name': 'b', 'build': './b'},
    ]
    config = Config(
        version=V2_0,
        services=services,
        volumes={'special': {}},
        networks={'extra': {}},
        secrets={},
        configs={}
    )

    with mock.patch('compose.bundle.log.warn', autospec=True) as mock_log:
        output = bundle.to_bundle(config, image_digests)

    assert mock_log.mock_calls == [
        mock.call("Unsupported top level key 'networks' - ignoring"),
        mock.call("Unsupported top level key 'volumes' - ignoring"),
    ]

    assert output == {
        'Version': '0.1',
        'Services': {
            'a': {'Image': 'aaaa', 'Networks': ['default']},
            'b': {'Image': 'bbbb', 'Networks': ['default']},
        }
    }


def test_convert_service_to_bundle():
    name = 'theservice'
    image_digest = 'thedigest'
    service_dict = {
        'ports': ['80'],
        'expose': ['1234'],
        'networks': {'extra': {}},
        'command': 'foo',
        'entrypoint': 'entry',
        'environment': {'BAZ': 'ENV'},
        'build': '.',
        'working_dir': '/tmp',
        'user': 'root',
        'labels': {'FOO': 'LABEL'},
        'privileged': True,
    }

    with mock.patch('compose.bundle.log.warn', autospec=True) as mock_log:
        config = bundle.convert_service_to_bundle(name, service_dict, image_digest)

    mock_log.assert_called_once_with(
        "Unsupported key 'privileged' in services.theservice - ignoring")

    assert config == {
        'Image': image_digest,
        'Ports': [
            {'Protocol': 'tcp', 'Port': 80},
            {'Protocol': 'tcp', 'Port': 1234},
        ],
        'Networks': ['extra'],
        'Command': ['entry', 'foo'],
        'Env': ['BAZ=ENV'],
        'WorkingDir': '/tmp',
        'User': 'root',
        'Labels': {'FOO': 'LABEL'},
    }


def test_set_command_and_args_none():
    config = {}
    bundle.set_command_and_args(config, [], [])
    assert config == {}


def test_set_command_and_args_from_command():
    config = {}
    bundle.set_command_and_args(config, [], "echo ok")
    assert config == {'Args': ['echo', 'ok']}


def test_set_command_and_args_from_entrypoint():
    config = {}
    bundle.set_command_and_args(config, "echo entry", [])
    assert config == {'Command': ['echo', 'entry']}


def test_set_command_and_args_from_both():
    config = {}
    bundle.set_command_and_args(config, "echo entry", ["extra", "arg"])
    assert config == {'Command': ['echo', 'entry', "extra", "arg"]}


def test_make_service_networks_default():
    name = 'theservice'
    service_dict = {}

    with mock.patch('compose.bundle.log.warn', autospec=True) as mock_log:
        networks = bundle.make_service_networks(name, service_dict)

    assert not mock_log.called
    assert networks == ['default']


def test_make_service_networks():
    name = 'theservice'
    service_dict = {
        'networks': {
            'foo': {
                'aliases': ['one', 'two'],
            },
            'bar': {}
        },
    }

    with mock.patch('compose.bundle.log.warn', autospec=True) as mock_log:
        networks = bundle.make_service_networks(name, service_dict)

    mock_log.assert_called_once_with(
        "Unsupported key 'aliases' in services.theservice.networks.foo - ignoring")
    assert sorted(networks) == sorted(service_dict['networks'])


def test_make_port_specs():
    service_dict = {
        'expose': ['80', '500/udp'],
        'ports': [
            '400:80',
            '222',
            '127.0.0.1:8001:8001',
            '127.0.0.1:5000-5001:3000-3001'],
    }
    port_specs = bundle.make_port_specs(service_dict)
    assert port_specs == [
        {'Protocol': 'tcp', 'Port': 80},
        {'Protocol': 'tcp', 'Port': 222},
        {'Protocol': 'tcp', 'Port': 8001},
        {'Protocol': 'tcp', 'Port': 3000},
        {'Protocol': 'tcp', 'Port': 3001},
        {'Protocol': 'udp', 'Port': 500},
    ]


def test_make_port_spec_with_protocol():
    port_spec = bundle.make_port_spec("5000/udp")
    assert port_spec == {'Protocol': 'udp', 'Port': 5000}


def test_make_port_spec_default_protocol():
    port_spec = bundle.make_port_spec("50000")
    assert port_spec == {'Protocol': 'tcp', 'Port': 50000}
<EOF>
<BOF>
# ~*~ encoding: utf-8 ~*~
from __future__ import absolute_import
from __future__ import unicode_literals

import io
import os
import random
import shutil
import tempfile

from six import StringIO

from compose import progress_stream
from tests import unittest


class ProgressStreamTestCase(unittest.TestCase):
    def test_stream_output(self):
        output = [
            b'{"status": "Downloading", "progressDetail": {"current": '
            b'31019763, "start": 1413653874, "total": 62763875}, '
            b'"progress": "..."}',
        ]
        events = list(progress_stream.stream_output(output, StringIO()))
        assert len(events) == 1

    def test_stream_output_div_zero(self):
        output = [
            b'{"status": "Downloading", "progressDetail": {"current": '
            b'0, "start": 1413653874, "total": 0}, '
            b'"progress": "..."}',
        ]
        events = list(progress_stream.stream_output(output, StringIO()))
        assert len(events) == 1

    def test_stream_output_null_total(self):
        output = [
            b'{"status": "Downloading", "progressDetail": {"current": '
            b'0, "start": 1413653874, "total": null}, '
            b'"progress": "..."}',
        ]
        events = list(progress_stream.stream_output(output, StringIO()))
        assert len(events) == 1

    def test_stream_output_progress_event_tty(self):
        events = [
            b'{"status": "Already exists", "progressDetail": {}, "id": "8d05e3af52b0"}'
        ]

        class TTYStringIO(StringIO):
            def isatty(self):
                return True

        output = TTYStringIO()
        events = list(progress_stream.stream_output(events, output))
        assert len(output.getvalue()) > 0

    def test_stream_output_progress_event_no_tty(self):
        events = [
            b'{"status": "Already exists", "progressDetail": {}, "id": "8d05e3af52b0"}'
        ]
        output = StringIO()

        events = list(progress_stream.stream_output(events, output))
        assert len(output.getvalue()) == 0

    def test_stream_output_no_progress_event_no_tty(self):
        events = [
            b'{"status": "Pulling from library/xy", "id": "latest"}'
        ]
        output = StringIO()

        events = list(progress_stream.stream_output(events, output))
        assert len(output.getvalue()) > 0

    def test_mismatched_encoding_stream_write(self):
        tmpdir = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, tmpdir, True)

        def mktempfile(encoding):
            fname = os.path.join(tmpdir, hex(random.getrandbits(128))[2:-1])
            return io.open(fname, mode='w+', encoding=encoding)

        text = ''
        with mktempfile(encoding='utf-8') as tf:
            progress_stream.write_to_stream(text, tf)
            tf.seek(0)
            assert tf.read() == text

        with mktempfile(encoding='utf-32') as tf:
            progress_stream.write_to_stream(text, tf)
            tf.seek(0)
            assert tf.read() == text

        with mktempfile(encoding='ascii') as tf:
            progress_stream.write_to_stream(text, tf)
            tf.seek(0)
            assert tf.read() == '???'


def test_get_digest_from_push():
    digest = "sha256:abcd"
    events = [
        {"status": "..."},
        {"status": "..."},
        {"progressDetail": {}, "aux": {"Digest": digest}},
    ]
    assert progress_stream.get_digest_from_push(events) == digest


def test_get_digest_from_pull():
    digest = "sha256:abcd"
    events = [
        {"status": "..."},
        {"status": "..."},
        {"status": "Digest: %s" % digest},
    ]
    assert progress_stream.get_digest_from_pull(events) == digest
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import docker
import pytest
from docker.constants import DEFAULT_DOCKER_API_VERSION
from docker.errors import APIError
from docker.errors import NotFound

from .. import mock
from .. import unittest
from compose.config.errors import DependencyError
from compose.config.types import MountSpec
from compose.config.types import ServicePort
from compose.config.types import ServiceSecret
from compose.config.types import VolumeFromSpec
from compose.config.types import VolumeSpec
from compose.const import API_VERSIONS
from compose.const import LABEL_CONFIG_HASH
from compose.const import LABEL_ONE_OFF
from compose.const import LABEL_PROJECT
from compose.const import LABEL_SERVICE
from compose.const import SECRETS_PATH
from compose.const import WINDOWS_LONGPATH_PREFIX
from compose.container import Container
from compose.errors import OperationFailedError
from compose.parallel import ParallelStreamWriter
from compose.project import OneOffFilter
from compose.service import build_ulimits
from compose.service import build_volume_binding
from compose.service import BuildAction
from compose.service import ContainerNetworkMode
from compose.service import format_environment
from compose.service import formatted_ports
from compose.service import get_container_data_volumes
from compose.service import ImageType
from compose.service import merge_volume_bindings
from compose.service import NeedsBuildError
from compose.service import NetworkMode
from compose.service import NoSuchImageError
from compose.service import parse_repository_tag
from compose.service import rewrite_build_path
from compose.service import Service
from compose.service import ServiceNetworkMode
from compose.service import warn_on_masked_volume


class ServiceTest(unittest.TestCase):

    def setUp(self):
        self.mock_client = mock.create_autospec(docker.APIClient)
        self.mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        self.mock_client._general_configs = {}

    def test_containers(self):
        service = Service('db', self.mock_client, 'myproject', image='foo')
        self.mock_client.containers.return_value = []
        assert list(service.containers()) == []

    def test_containers_with_containers(self):
        self.mock_client.containers.return_value = [
            dict(Name=str(i), Image='foo', Id=i) for i in range(3)
        ]
        service = Service('db', self.mock_client, 'myproject', image='foo')
        assert [c.id for c in service.containers()] == list(range(3))

        expected_labels = [
            '{0}=myproject'.format(LABEL_PROJECT),
            '{0}=db'.format(LABEL_SERVICE),
            '{0}=False'.format(LABEL_ONE_OFF),
        ]

        self.mock_client.containers.assert_called_once_with(
            all=False,
            filters={'label': expected_labels})

    def test_container_without_name(self):
        self.mock_client.containers.return_value = [
            {'Image': 'foo', 'Id': '1', 'Name': '1'},
            {'Image': 'foo', 'Id': '2', 'Name': None},
            {'Image': 'foo', 'Id': '3'},
        ]
        service = Service('db', self.mock_client, 'myproject', image='foo')

        assert [c.id for c in service.containers()] == ['1']
        assert service._next_container_number() == 2
        assert service.get_container(1).id == '1'

    def test_get_volumes_from_container(self):
        container_id = 'aabbccddee'
        service = Service(
            'test',
            image='foo',
            volumes_from=[
                VolumeFromSpec(
                    mock.Mock(id=container_id, spec=Container),
                    'rw',
                    'container')])

        assert service._get_volumes_from() == [container_id + ':rw']

    def test_get_volumes_from_container_read_only(self):
        container_id = 'aabbccddee'
        service = Service(
            'test',
            image='foo',
            volumes_from=[
                VolumeFromSpec(
                    mock.Mock(id=container_id, spec=Container),
                    'ro',
                    'container')])

        assert service._get_volumes_from() == [container_id + ':ro']

    def test_get_volumes_from_service_container_exists(self):
        container_ids = ['aabbccddee', '12345']
        from_service = mock.create_autospec(Service)
        from_service.containers.return_value = [
            mock.Mock(id=container_id, spec=Container)
            for container_id in container_ids
        ]
        service = Service(
            'test',
            volumes_from=[VolumeFromSpec(from_service, 'rw', 'service')],
            image='foo')

        assert service._get_volumes_from() == [container_ids[0] + ":rw"]

    def test_get_volumes_from_service_container_exists_with_flags(self):
        for mode in ['ro', 'rw', 'z', 'rw,z', 'z,rw']:
            container_ids = ['aabbccddee:' + mode, '12345:' + mode]
            from_service = mock.create_autospec(Service)
            from_service.containers.return_value = [
                mock.Mock(id=container_id.split(':')[0], spec=Container)
                for container_id in container_ids
            ]
            service = Service(
                'test',
                volumes_from=[VolumeFromSpec(from_service, mode, 'service')],
                image='foo')

            assert service._get_volumes_from() == [container_ids[0]]

    def test_get_volumes_from_service_no_container(self):
        container_id = 'abababab'
        from_service = mock.create_autospec(Service)
        from_service.containers.return_value = []
        from_service.create_container.return_value = mock.Mock(
            id=container_id,
            spec=Container)
        service = Service(
            'test',
            image='foo',
            volumes_from=[VolumeFromSpec(from_service, 'rw', 'service')])

        assert service._get_volumes_from() == [container_id + ':rw']
        from_service.create_container.assert_called_once_with()

    def test_memory_swap_limit(self):
        self.mock_client.create_host_config.return_value = {}

        service = Service(
            name='foo',
            image='foo',
            hostname='name',
            client=self.mock_client,
            mem_limit=1000000000,
            memswap_limit=2000000000)
        service._get_container_create_options({'some': 'overrides'}, 1)

        assert self.mock_client.create_host_config.called
        assert self.mock_client.create_host_config.call_args[1]['mem_limit'] == 1000000000
        assert self.mock_client.create_host_config.call_args[1]['memswap_limit'] == 2000000000

    def test_self_reference_external_link(self):
        service = Service(
            name='foo',
            external_links=['default_foo_1_bdfa3ed91e2c']
        )
        with pytest.raises(DependencyError):
            service.get_container_name('foo', 1, 'bdfa3ed91e2c')

    def test_mem_reservation(self):
        self.mock_client.create_host_config.return_value = {}

        service = Service(
            name='foo',
            image='foo',
            hostname='name',
            client=self.mock_client,
            mem_reservation='512m'
        )
        service._get_container_create_options({'some': 'overrides'}, 1)
        assert self.mock_client.create_host_config.called is True
        assert self.mock_client.create_host_config.call_args[1]['mem_reservation'] == '512m'

    def test_cgroup_parent(self):
        self.mock_client.create_host_config.return_value = {}

        service = Service(
            name='foo',
            image='foo',
            hostname='name',
            client=self.mock_client,
            cgroup_parent='test')
        service._get_container_create_options({'some': 'overrides'}, 1)

        assert self.mock_client.create_host_config.called
        assert self.mock_client.create_host_config.call_args[1]['cgroup_parent'] == 'test'

    def test_log_opt(self):
        self.mock_client.create_host_config.return_value = {}

        log_opt = {'syslog-address': 'tcp://192.168.0.42:123'}
        logging = {'driver': 'syslog', 'options': log_opt}
        service = Service(
            name='foo',
            image='foo',
            hostname='name',
            client=self.mock_client,
            log_driver='syslog',
            logging=logging)
        service._get_container_create_options({'some': 'overrides'}, 1)

        assert self.mock_client.create_host_config.called
        assert self.mock_client.create_host_config.call_args[1]['log_config'] == {
            'Type': 'syslog', 'Config': {'syslog-address': 'tcp://192.168.0.42:123'}
        }

    def test_stop_grace_period(self):
        self.mock_client.api_version = '1.25'
        self.mock_client.create_host_config.return_value = {}
        service = Service(
            'foo',
            image='foo',
            client=self.mock_client,
            stop_grace_period="1m35s")
        opts = service._get_container_create_options({'image': 'foo'}, 1)
        assert opts['stop_timeout'] == 95

    def test_split_domainname_none(self):
        service = Service(
            'foo',
            image='foo',
            hostname='name.domain.tld',
            client=self.mock_client)
        opts = service._get_container_create_options({'image': 'foo'}, 1)
        assert opts['hostname'] == 'name.domain.tld', 'hostname'
        assert not ('domainname' in opts), 'domainname'

    def test_split_domainname_fqdn(self):
        self.mock_client.api_version = '1.22'
        service = Service(
            'foo',
            hostname='name.domain.tld',
            image='foo',
            client=self.mock_client)
        opts = service._get_container_create_options({'image': 'foo'}, 1)
        assert opts['hostname'] == 'name', 'hostname'
        assert opts['domainname'] == 'domain.tld', 'domainname'

    def test_split_domainname_both(self):
        self.mock_client.api_version = '1.22'
        service = Service(
            'foo',
            hostname='name',
            image='foo',
            domainname='domain.tld',
            client=self.mock_client)
        opts = service._get_container_create_options({'image': 'foo'}, 1)
        assert opts['hostname'] == 'name', 'hostname'
        assert opts['domainname'] == 'domain.tld', 'domainname'

    def test_split_domainname_weird(self):
        self.mock_client.api_version = '1.22'
        service = Service(
            'foo',
            hostname='name.sub',
            domainname='domain.tld',
            image='foo',
            client=self.mock_client)
        opts = service._get_container_create_options({'image': 'foo'}, 1)
        assert opts['hostname'] == 'name.sub', 'hostname'
        assert opts['domainname'] == 'domain.tld', 'domainname'

    def test_no_default_hostname_when_not_using_networking(self):
        service = Service(
            'foo',
            image='foo',
            use_networking=False,
            client=self.mock_client,
        )
        opts = service._get_container_create_options({'image': 'foo'}, 1)
        assert opts.get('hostname') is None

    def test_get_container_create_options_with_name_option(self):
        service = Service(
            'foo',
            image='foo',
            client=self.mock_client,
            container_name='foo1')
        name = 'the_new_name'
        opts = service._get_container_create_options(
            {'name': name},
            1,
            one_off=OneOffFilter.only)
        assert opts['name'] == name

    def test_get_container_create_options_does_not_mutate_options(self):
        labels = {'thing': 'real'}
        environment = {'also': 'real'}
        service = Service(
            'foo',
            image='foo',
            labels=dict(labels),
            client=self.mock_client,
            environment=dict(environment),
        )
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        prev_container = mock.Mock(
            id='ababab',
            image_config={'ContainerConfig': {}}
        )
        prev_container.full_slug = 'abcdefff1234'
        prev_container.get.return_value = None

        opts = service._get_container_create_options(
            {}, 1, previous_container=prev_container
        )

        assert service.options['labels'] == labels
        assert service.options['environment'] == environment

        assert opts['labels'][LABEL_CONFIG_HASH] == \
            '2524a06fcb3d781aa2c981fc40bcfa08013bb318e4273bfa388df22023e6f2aa'
        assert opts['environment'] == ['also=real']

    def test_get_container_create_options_sets_affinity_with_binds(self):
        service = Service(
            'foo',
            image='foo',
            client=self.mock_client,
        )
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        prev_container = mock.Mock(
            id='ababab',
            image_config={'ContainerConfig': {'Volumes': ['/data']}})

        def container_get(key):
            return {
                'Mounts': [
                    {
                        'Destination': '/data',
                        'Source': '/some/path',
                        'Name': 'abab1234',
                    },
                ]
            }.get(key, None)

        prev_container.get.side_effect = container_get
        prev_container.full_slug = 'abcdefff1234'

        opts = service._get_container_create_options(
            {},
            1,
            previous_container=prev_container
        )

        assert opts['environment'] == ['affinity:container==ababab']

    def test_get_container_create_options_no_affinity_without_binds(self):
        service = Service('foo', image='foo', client=self.mock_client)
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        prev_container = mock.Mock(
            id='ababab',
            image_config={'ContainerConfig': {}})
        prev_container.get.return_value = None
        prev_container.full_slug = 'abcdefff1234'

        opts = service._get_container_create_options(
            {},
            1,
            previous_container=prev_container)
        assert opts['environment'] == []

    def test_get_container_not_found(self):
        self.mock_client.containers.return_value = []
        service = Service('foo', client=self.mock_client, image='foo')

        with pytest.raises(ValueError):
            service.get_container()

    @mock.patch('compose.service.Container', autospec=True)
    def test_get_container(self, mock_container_class):
        container_dict = dict(Name='default_foo_2_bdfa3ed91e2c')
        self.mock_client.containers.return_value = [container_dict]
        service = Service('foo', image='foo', client=self.mock_client)

        container = service.get_container(number=2)
        assert container == mock_container_class.from_ps.return_value
        mock_container_class.from_ps.assert_called_once_with(
            self.mock_client, container_dict)

    @mock.patch('compose.service.log', autospec=True)
    def test_pull_image(self, mock_log):
        service = Service('foo', client=self.mock_client, image='someimage:sometag')
        service.pull()
        self.mock_client.pull.assert_called_once_with(
            'someimage',
            tag='sometag',
            stream=True,
            platform=None)
        mock_log.info.assert_called_once_with('Pulling foo (someimage:sometag)...')

    def test_pull_image_no_tag(self):
        service = Service('foo', client=self.mock_client, image='ababab')
        service.pull()
        self.mock_client.pull.assert_called_once_with(
            'ababab',
            tag='latest',
            stream=True,
            platform=None)

    @mock.patch('compose.service.log', autospec=True)
    def test_pull_image_digest(self, mock_log):
        service = Service('foo', client=self.mock_client, image='someimage@sha256:1234')
        service.pull()
        self.mock_client.pull.assert_called_once_with(
            'someimage',
            tag='sha256:1234',
            stream=True,
            platform=None)
        mock_log.info.assert_called_once_with('Pulling foo (someimage@sha256:1234)...')

    @mock.patch('compose.service.log', autospec=True)
    def test_pull_image_with_platform(self, mock_log):
        self.mock_client.api_version = '1.35'
        service = Service(
            'foo', client=self.mock_client, image='someimage:sometag', platform='windows/x86_64'
        )
        service.pull()
        assert self.mock_client.pull.call_count == 1
        call_args = self.mock_client.pull.call_args
        assert call_args[1]['platform'] == 'windows/x86_64'

    @mock.patch('compose.service.log', autospec=True)
    def test_pull_image_with_platform_unsupported_api(self, mock_log):
        self.mock_client.api_version = '1.33'
        service = Service(
            'foo', client=self.mock_client, image='someimage:sometag', platform='linux/arm'
        )
        with pytest.raises(OperationFailedError):
            service.pull()

    def test_pull_image_with_default_platform(self):
        self.mock_client.api_version = '1.35'

        service = Service(
            'foo', client=self.mock_client, image='someimage:sometag',
            default_platform='linux'
        )
        assert service.platform == 'linux'
        service.pull()

        assert self.mock_client.pull.call_count == 1
        call_args = self.mock_client.pull.call_args
        assert call_args[1]['platform'] == 'linux'

    @mock.patch('compose.service.Container', autospec=True)
    def test_recreate_container(self, _):
        mock_container = mock.create_autospec(Container)
        mock_container.full_slug = 'abcdefff1234'
        service = Service('foo', client=self.mock_client, image='someimage')
        service.image = lambda: {'Id': 'abc123'}
        new_container = service.recreate_container(mock_container)

        mock_container.stop.assert_called_once_with(timeout=10)
        mock_container.rename_to_tmp_name.assert_called_once_with()

        new_container.start.assert_called_once_with()
        mock_container.remove.assert_called_once_with()

    @mock.patch('compose.service.Container', autospec=True)
    def test_recreate_container_with_timeout(self, _):
        mock_container = mock.create_autospec(Container)
        mock_container.full_slug = 'abcdefff1234'
        self.mock_client.inspect_image.return_value = {'Id': 'abc123'}
        service = Service('foo', client=self.mock_client, image='someimage')
        service.recreate_container(mock_container, timeout=1)

        mock_container.stop.assert_called_once_with(timeout=1)

    def test_parse_repository_tag(self):
        assert parse_repository_tag("root") == ("root", "", ":")
        assert parse_repository_tag("root:tag") == ("root", "tag", ":")
        assert parse_repository_tag("user/repo") == ("user/repo", "", ":")
        assert parse_repository_tag("user/repo:tag") == ("user/repo", "tag", ":")
        assert parse_repository_tag("url:5000/repo") == ("url:5000/repo", "", ":")
        assert parse_repository_tag("url:5000/repo:tag") == ("url:5000/repo", "tag", ":")
        assert parse_repository_tag("root@sha256:digest") == ("root", "sha256:digest", "@")
        assert parse_repository_tag("user/repo@sha256:digest") == ("user/repo", "sha256:digest", "@")
        assert parse_repository_tag("url:5000/repo@sha256:digest") == (
            "url:5000/repo", "sha256:digest", "@"
        )

    def test_create_container(self):
        service = Service('foo', client=self.mock_client, build={'context': '.'})
        self.mock_client.inspect_image.side_effect = [
            NoSuchImageError,
            {'Id': 'abc123'},
        ]
        self.mock_client.build.return_value = [
            '{"stream": "Successfully built abcd"}',
        ]

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            service.create_container()
            assert mock_log.warn.called
            _, args, _ = mock_log.warn.mock_calls[0]
            assert 'was built because it did not already exist' in args[0]

        assert self.mock_client.build.call_count == 1
        self.mock_client.build.call_args[1]['tag'] == 'default_foo'

    def test_ensure_image_exists_no_build(self):
        service = Service('foo', client=self.mock_client, build={'context': '.'})
        self.mock_client.inspect_image.return_value = {'Id': 'abc123'}

        service.ensure_image_exists(do_build=BuildAction.skip)
        assert not self.mock_client.build.called

    def test_ensure_image_exists_no_build_but_needs_build(self):
        service = Service('foo', client=self.mock_client, build={'context': '.'})
        self.mock_client.inspect_image.side_effect = NoSuchImageError
        with pytest.raises(NeedsBuildError):
            service.ensure_image_exists(do_build=BuildAction.skip)

    def test_ensure_image_exists_force_build(self):
        service = Service('foo', client=self.mock_client, build={'context': '.'})
        self.mock_client.inspect_image.return_value = {'Id': 'abc123'}
        self.mock_client.build.return_value = [
            '{"stream": "Successfully built abcd"}',
        ]

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            service.ensure_image_exists(do_build=BuildAction.force)

        assert not mock_log.warn.called
        assert self.mock_client.build.call_count == 1
        self.mock_client.build.call_args[1]['tag'] == 'default_foo'

    def test_build_does_not_pull(self):
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service('foo', client=self.mock_client, build={'context': '.'})
        service.build()

        assert self.mock_client.build.call_count == 1
        assert not self.mock_client.build.call_args[1]['pull']

    def test_build_with_platform(self):
        self.mock_client.api_version = '1.35'
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service('foo', client=self.mock_client, build={'context': '.'}, platform='linux')
        service.build()

        assert self.mock_client.build.call_count == 1
        call_args = self.mock_client.build.call_args
        assert call_args[1]['platform'] == 'linux'

    def test_build_with_default_platform(self):
        self.mock_client.api_version = '1.35'
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service(
            'foo', client=self.mock_client, build={'context': '.'},
            default_platform='linux'
        )
        assert service.platform == 'linux'
        service.build()

        assert self.mock_client.build.call_count == 1
        call_args = self.mock_client.build.call_args
        assert call_args[1]['platform'] == 'linux'

    def test_service_platform_precedence(self):
        self.mock_client.api_version = '1.35'

        service = Service(
            'foo', client=self.mock_client, platform='linux/arm',
            default_platform='osx'
        )
        assert service.platform == 'linux/arm'

    def test_service_ignore_default_platform_with_unsupported_api(self):
        self.mock_client.api_version = '1.32'
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service(
            'foo', client=self.mock_client, default_platform='windows', build={'context': '.'}
        )
        assert service.platform is None
        service.build()
        assert self.mock_client.build.call_count == 1
        call_args = self.mock_client.build.call_args
        assert call_args[1]['platform'] is None

    def test_build_with_override_build_args(self):
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        build_args = {
            'arg1': 'arg1_new_value',
        }
        service = Service('foo', client=self.mock_client,
                          build={'context': '.', 'args': {'arg1': 'arg1', 'arg2': 'arg2'}})
        service.build(build_args_override=build_args)

        called_build_args = self.mock_client.build.call_args[1]['buildargs']

        assert called_build_args['arg1'] == build_args['arg1']
        assert called_build_args['arg2'] == 'arg2'

    def test_build_with_isolation_from_service_config(self):
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service('foo', client=self.mock_client, build={'context': '.'}, isolation='hyperv')
        service.build()

        assert self.mock_client.build.call_count == 1
        called_build_args = self.mock_client.build.call_args[1]
        assert called_build_args['isolation'] == 'hyperv'

    def test_build_isolation_from_build_override_service_config(self):
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service(
            'foo', client=self.mock_client, build={'context': '.', 'isolation': 'default'},
            isolation='hyperv'
        )
        service.build()

        assert self.mock_client.build.call_count == 1
        called_build_args = self.mock_client.build.call_args[1]
        assert called_build_args['isolation'] == 'default'

    def test_config_dict(self):
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        service = Service(
            'foo',
            image='example.com/foo',
            client=self.mock_client,
            network_mode=ServiceNetworkMode(Service('other')),
            networks={'default': None},
            links=[(Service('one'), 'one')],
            volumes_from=[VolumeFromSpec(Service('two'), 'rw', 'service')])

        config_dict = service.config_dict()
        expected = {
            'image_id': 'abcd',
            'options': {'image': 'example.com/foo'},
            'links': [('one', 'one')],
            'net': 'other',
            'networks': {'default': None},
            'volumes_from': [('two', 'rw')],
        }
        assert config_dict == expected

    def test_config_dict_with_network_mode_from_container(self):
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        container = Container(
            self.mock_client,
            {'Id': 'aaabbb', 'Name': '/foo_1'})
        service = Service(
            'foo',
            image='example.com/foo',
            client=self.mock_client,
            network_mode=ContainerNetworkMode(container))

        config_dict = service.config_dict()
        expected = {
            'image_id': 'abcd',
            'options': {'image': 'example.com/foo'},
            'links': [],
            'networks': {},
            'net': 'aaabbb',
            'volumes_from': [],
        }
        assert config_dict == expected

    def test_config_hash_matches_label(self):
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        service = Service(
            'foo',
            image='example.com/foo',
            client=self.mock_client,
            network_mode=NetworkMode('bridge'),
            networks={'bridge': {}, 'net2': {}},
            links=[(Service('one', client=self.mock_client), 'one')],
            volumes_from=[VolumeFromSpec(Service('two', client=self.mock_client), 'rw', 'service')],
            volumes=[VolumeSpec('/ext', '/int', 'ro')],
            build={'context': 'some/random/path'},
        )
        config_hash = service.config_hash

        for api_version in set(API_VERSIONS.values()):
            self.mock_client.api_version = api_version
            assert service._get_container_create_options(
                {}, 1
            )['labels'][LABEL_CONFIG_HASH] == config_hash

    def test_remove_image_none(self):
        web = Service('web', image='example', client=self.mock_client)
        assert not web.remove_image(ImageType.none)
        assert not self.mock_client.remove_image.called

    def test_remove_image_local_with_image_name_doesnt_remove(self):
        web = Service('web', image='example', client=self.mock_client)
        assert not web.remove_image(ImageType.local)
        assert not self.mock_client.remove_image.called

    def test_remove_image_local_without_image_name_does_remove(self):
        web = Service('web', build='.', client=self.mock_client)
        assert web.remove_image(ImageType.local)
        self.mock_client.remove_image.assert_called_once_with(web.image_name)

    def test_remove_image_all_does_remove(self):
        web = Service('web', image='example', client=self.mock_client)
        assert web.remove_image(ImageType.all)
        self.mock_client.remove_image.assert_called_once_with(web.image_name)

    def test_remove_image_with_error(self):
        self.mock_client.remove_image.side_effect = error = APIError(
            message="testing",
            response={},
            explanation="Boom")

        web = Service('web', image='example', client=self.mock_client)
        with mock.patch('compose.service.log', autospec=True) as mock_log:
            assert not web.remove_image(ImageType.all)
        mock_log.error.assert_called_once_with(
            "Failed to remove image for service %s: %s", web.name, error)

    def test_specifies_host_port_with_no_ports(self):
        service = Service(
            'foo',
            image='foo')
        assert not service.specifies_host_port()

    def test_specifies_host_port_with_container_port(self):
        service = Service(
            'foo',
            image='foo',
            ports=["2000"])
        assert not service.specifies_host_port()

    def test_specifies_host_port_with_host_port(self):
        service = Service(
            'foo',
            image='foo',
            ports=["1000:2000"])
        assert service.specifies_host_port()

    def test_specifies_host_port_with_host_ip_no_port(self):
        service = Service(
            'foo',
            image='foo',
            ports=["127.0.0.1::2000"])
        assert not service.specifies_host_port()

    def test_specifies_host_port_with_host_ip_and_port(self):
        service = Service(
            'foo',
            image='foo',
            ports=["127.0.0.1:1000:2000"])
        assert service.specifies_host_port()

    def test_specifies_host_port_with_container_port_range(self):
        service = Service(
            'foo',
            image='foo',
            ports=["2000-3000"])
        assert not service.specifies_host_port()

    def test_specifies_host_port_with_host_port_range(self):
        service = Service(
            'foo',
            image='foo',
            ports=["1000-2000:2000-3000"])
        assert service.specifies_host_port()

    def test_specifies_host_port_with_host_ip_no_port_range(self):
        service = Service(
            'foo',
            image='foo',
            ports=["127.0.0.1::2000-3000"])
        assert not service.specifies_host_port()

    def test_specifies_host_port_with_host_ip_and_port_range(self):
        service = Service(
            'foo',
            image='foo',
            ports=["127.0.0.1:1000-2000:2000-3000"])
        assert service.specifies_host_port()

    def test_image_name_from_config(self):
        image_name = 'example/web:latest'
        service = Service('foo', image=image_name)
        assert service.image_name == image_name

    def test_image_name_default(self):
        service = Service('foo', project='testing')
        assert service.image_name == 'testing_foo'

    @mock.patch('compose.service.log', autospec=True)
    def test_only_log_warning_when_host_ports_clash(self, mock_log):
        self.mock_client.inspect_image.return_value = {'Id': 'abcd'}
        ParallelStreamWriter.instance = None
        name = 'foo'
        service = Service(
            name,
            client=self.mock_client,
            ports=["8080:80"])

        service.scale(0)
        assert not mock_log.warn.called

        service.scale(1)
        assert not mock_log.warn.called

        service.scale(2)
        mock_log.warn.assert_called_once_with(
            'The "{}" service specifies a port on the host. If multiple containers '
            'for this service are created on a single host, the port will clash.'.format(name))

    def test_parse_proxy_config(self):
        default_proxy_config = {
            'httpProxy': 'http://proxy.mycorp.com:3128',
            'httpsProxy': 'https://user:password@proxy.mycorp.com:3129',
            'ftpProxy': 'http://ftpproxy.mycorp.com:21',
            'noProxy': '*.intra.mycorp.com',
        }

        self.mock_client.base_url = 'http+docker://localunixsocket'
        self.mock_client._general_configs = {
            'proxies': {
                'default': default_proxy_config,
            }
        }

        service = Service('foo', client=self.mock_client)

        assert service._parse_proxy_config() == {
            'HTTP_PROXY': default_proxy_config['httpProxy'],
            'http_proxy': default_proxy_config['httpProxy'],
            'HTTPS_PROXY': default_proxy_config['httpsProxy'],
            'https_proxy': default_proxy_config['httpsProxy'],
            'FTP_PROXY': default_proxy_config['ftpProxy'],
            'ftp_proxy': default_proxy_config['ftpProxy'],
            'NO_PROXY': default_proxy_config['noProxy'],
            'no_proxy': default_proxy_config['noProxy'],
        }

    def test_parse_proxy_config_per_host(self):
        default_proxy_config = {
            'httpProxy': 'http://proxy.mycorp.com:3128',
            'httpsProxy': 'https://user:password@proxy.mycorp.com:3129',
            'ftpProxy': 'http://ftpproxy.mycorp.com:21',
            'noProxy': '*.intra.mycorp.com',
        }
        host_specific_proxy_config = {
            'httpProxy': 'http://proxy.example.com:3128',
            'httpsProxy': 'https://user:password@proxy.example.com:3129',
            'ftpProxy': 'http://ftpproxy.example.com:21',
            'noProxy': '*.intra.example.com'
        }

        self.mock_client.base_url = 'http+docker://localunixsocket'
        self.mock_client._general_configs = {
            'proxies': {
                'default': default_proxy_config,
                'tcp://example.docker.com:2376': host_specific_proxy_config,
            }
        }

        service = Service('foo', client=self.mock_client)

        assert service._parse_proxy_config() == {
            'HTTP_PROXY': default_proxy_config['httpProxy'],
            'http_proxy': default_proxy_config['httpProxy'],
            'HTTPS_PROXY': default_proxy_config['httpsProxy'],
            'https_proxy': default_proxy_config['httpsProxy'],
            'FTP_PROXY': default_proxy_config['ftpProxy'],
            'ftp_proxy': default_proxy_config['ftpProxy'],
            'NO_PROXY': default_proxy_config['noProxy'],
            'no_proxy': default_proxy_config['noProxy'],
        }

        self.mock_client._original_base_url = 'tcp://example.docker.com:2376'

        assert service._parse_proxy_config() == {
            'HTTP_PROXY': host_specific_proxy_config['httpProxy'],
            'http_proxy': host_specific_proxy_config['httpProxy'],
            'HTTPS_PROXY': host_specific_proxy_config['httpsProxy'],
            'https_proxy': host_specific_proxy_config['httpsProxy'],
            'FTP_PROXY': host_specific_proxy_config['ftpProxy'],
            'ftp_proxy': host_specific_proxy_config['ftpProxy'],
            'NO_PROXY': host_specific_proxy_config['noProxy'],
            'no_proxy': host_specific_proxy_config['noProxy'],
        }

    def test_build_service_with_proxy_config(self):
        default_proxy_config = {
            'httpProxy': 'http://proxy.mycorp.com:3128',
            'httpsProxy': 'https://user:password@proxy.example.com:3129',
        }
        buildargs = {
            'HTTPS_PROXY': 'https://rdcf.th08.jp:8911',
            'https_proxy': 'https://rdcf.th08.jp:8911',
        }
        self.mock_client._general_configs = {
            'proxies': {
                'default': default_proxy_config,
            }
        }
        self.mock_client.base_url = 'http+docker://localunixsocket'
        self.mock_client.build.return_value = [
            b'{"stream": "Successfully built 12345"}',
        ]

        service = Service('foo', client=self.mock_client, build={'context': '.', 'args': buildargs})
        service.build()

        assert self.mock_client.build.call_count == 1
        assert self.mock_client.build.call_args[1]['buildargs'] == {
            'HTTP_PROXY': default_proxy_config['httpProxy'],
            'http_proxy': default_proxy_config['httpProxy'],
            'HTTPS_PROXY': buildargs['HTTPS_PROXY'],
            'https_proxy': buildargs['HTTPS_PROXY'],
        }

    def test_get_create_options_with_proxy_config(self):
        default_proxy_config = {
            'httpProxy': 'http://proxy.mycorp.com:3128',
            'httpsProxy': 'https://user:password@proxy.mycorp.com:3129',
            'ftpProxy': 'http://ftpproxy.mycorp.com:21',
        }
        self.mock_client._general_configs = {
            'proxies': {
                'default': default_proxy_config,
            }
        }
        self.mock_client.base_url = 'http+docker://localunixsocket'

        override_options = {
            'environment': {
                'FTP_PROXY': 'ftp://xdge.exo.au:21',
                'ftp_proxy': 'ftp://xdge.exo.au:21',
            }
        }
        environment = {
            'HTTPS_PROXY': 'https://rdcf.th08.jp:8911',
            'https_proxy': 'https://rdcf.th08.jp:8911',
        }

        service = Service('foo', client=self.mock_client, environment=environment)

        create_opts = service._get_container_create_options(override_options, 1)
        assert set(create_opts['environment']) == set(format_environment({
            'HTTP_PROXY': default_proxy_config['httpProxy'],
            'http_proxy': default_proxy_config['httpProxy'],
            'HTTPS_PROXY': environment['HTTPS_PROXY'],
            'https_proxy': environment['HTTPS_PROXY'],
            'FTP_PROXY': override_options['environment']['FTP_PROXY'],
            'ftp_proxy': override_options['environment']['FTP_PROXY'],
        }))

    def test_create_when_removed_containers_are_listed(self):
        # This is aimed at simulating a race between the API call to list the
        # containers, and the ones to inspect each of the listed containers.
        # It can happen that a container has been removed after we listed it.

        # containers() returns a container that is about to be removed
        self.mock_client.containers.return_value = [
            {'Id': 'rm_cont_id', 'Name': 'rm_cont', 'Image': 'img_id'},
        ]

        # inspect_container() will raise a NotFound when trying to inspect
        # rm_cont_id, which at this point has been removed
        def inspect(name):
            if name == 'rm_cont_id':
                raise NotFound(message='Not Found')

            if name == 'new_cont_id':
                return {'Id': 'new_cont_id'}

            raise NotImplementedError("incomplete mock")

        self.mock_client.inspect_container.side_effect = inspect

        self.mock_client.inspect_image.return_value = {'Id': 'imageid'}

        self.mock_client.create_container.return_value = {'Id': 'new_cont_id'}

        # We should nonetheless be able to create a new container
        service = Service('foo', client=self.mock_client)

        assert service.create_container().id == 'new_cont_id'

    def test_build_volume_options_duplicate_binds(self):
        self.mock_client.api_version = '1.29'  # Trigger 3.2 format workaround
        service = Service('foo', client=self.mock_client)
        ctnr_opts, override_opts = service._build_container_volume_options(
            previous_container=None,
            container_options={
                'volumes': [
                    MountSpec.parse({'source': 'vol', 'target': '/data', 'type': 'volume'}),
                    VolumeSpec.parse('vol:/data:rw'),
                ],
                'environment': {},
            },
            override_options={},
        )
        assert 'binds' in override_opts
        assert len(override_opts['binds']) == 1
        assert override_opts['binds'][0] == 'vol:/data:rw'

    def test_volumes_order_is_preserved(self):
        service = Service('foo', client=self.mock_client)
        volumes = [
            VolumeSpec.parse(cfg) for cfg in [
                '/v{0}:/v{0}:rw'.format(i) for i in range(6)
            ]
        ]
        ctnr_opts, override_opts = service._build_container_volume_options(
            previous_container=None,
            container_options={
                'volumes': volumes,
                'environment': {},
            },
            override_options={},
        )
        assert override_opts['binds'] == [vol.repr() for vol in volumes]


class TestServiceNetwork(unittest.TestCase):
    def setUp(self):
        self.mock_client = mock.create_autospec(docker.APIClient)
        self.mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        self.mock_client._general_configs = {}

    def test_connect_container_to_networks_short_aliase_exists(self):
        service = Service(
            'db',
            self.mock_client,
            'myproject',
            image='foo',
            networks={'project_default': {}})
        container = Container(
            None,
            {
                'Id': 'abcdef',
                'NetworkSettings': {
                    'Networks': {
                        'project_default': {
                            'Aliases': ['analias', 'abcdef'],
                        },
                    },
                },
            },
            True)
        service.connect_container_to_networks(container)

        assert not self.mock_client.disconnect_container_from_network.call_count
        assert not self.mock_client.connect_container_to_network.call_count


def sort_by_name(dictionary_list):
    return sorted(dictionary_list, key=lambda k: k['name'])


class BuildUlimitsTestCase(unittest.TestCase):

    def test_build_ulimits_with_dict(self):
        ulimits = build_ulimits(
            {
                'nofile': {'soft': 10000, 'hard': 20000},
                'nproc': {'soft': 65535, 'hard': 65535}
            }
        )
        expected = [
            {'name': 'nofile', 'soft': 10000, 'hard': 20000},
            {'name': 'nproc', 'soft': 65535, 'hard': 65535}
        ]
        assert sort_by_name(ulimits) == sort_by_name(expected)

    def test_build_ulimits_with_ints(self):
        ulimits = build_ulimits({'nofile': 20000, 'nproc': 65535})
        expected = [
            {'name': 'nofile', 'soft': 20000, 'hard': 20000},
            {'name': 'nproc', 'soft': 65535, 'hard': 65535}
        ]
        assert sort_by_name(ulimits) == sort_by_name(expected)

    def test_build_ulimits_with_integers_and_dicts(self):
        ulimits = build_ulimits(
            {
                'nproc': 65535,
                'nofile': {'soft': 10000, 'hard': 20000}
            }
        )
        expected = [
            {'name': 'nofile', 'soft': 10000, 'hard': 20000},
            {'name': 'nproc', 'soft': 65535, 'hard': 65535}
        ]
        assert sort_by_name(ulimits) == sort_by_name(expected)


class NetTestCase(unittest.TestCase):
    def setUp(self):
        self.mock_client = mock.create_autospec(docker.APIClient)
        self.mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        self.mock_client._general_configs = {}

    def test_network_mode(self):
        network_mode = NetworkMode('host')
        assert network_mode.id == 'host'
        assert network_mode.mode == 'host'
        assert network_mode.service_name is None

    def test_network_mode_container(self):
        container_id = 'abcd'
        network_mode = ContainerNetworkMode(Container(None, {'Id': container_id}))
        assert network_mode.id == container_id
        assert network_mode.mode == 'container:' + container_id
        assert network_mode.service_name is None

    def test_network_mode_service(self):
        container_id = 'bbbb'
        service_name = 'web'
        self.mock_client.containers.return_value = [
            {'Id': container_id, 'Name': container_id, 'Image': 'abcd'},
        ]

        service = Service(name=service_name, client=self.mock_client)
        network_mode = ServiceNetworkMode(service)

        assert network_mode.id == service_name
        assert network_mode.mode == 'container:' + container_id
        assert network_mode.service_name == service_name

    def test_network_mode_service_no_containers(self):
        service_name = 'web'
        self.mock_client.containers.return_value = []

        service = Service(name=service_name, client=self.mock_client)
        network_mode = ServiceNetworkMode(service)

        assert network_mode.id == service_name
        assert network_mode.mode is None
        assert network_mode.service_name == service_name


class ServicePortsTest(unittest.TestCase):
    def test_formatted_ports(self):
        ports = [
            '3000',
            '0.0.0.0:4025-4030:23000-23005',
            ServicePort(6000, None, None, None, None),
            ServicePort(8080, 8080, None, None, None),
            ServicePort('20000', '20000', 'udp', 'ingress', None),
            ServicePort(30000, '30000', 'tcp', None, '127.0.0.1'),
        ]
        formatted = formatted_ports(ports)
        assert ports[0] in formatted
        assert ports[1] in formatted
        assert '6000/tcp' in formatted
        assert '8080:8080/tcp' in formatted
        assert '20000:20000/udp' in formatted
        assert '127.0.0.1:30000:30000/tcp' in formatted


def build_mount(destination, source, mode='rw'):
    return {'Source': source, 'Destination': destination, 'Mode': mode}


class ServiceVolumesTest(unittest.TestCase):

    def setUp(self):
        self.mock_client = mock.create_autospec(docker.APIClient)
        self.mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        self.mock_client._general_configs = {}

    def test_build_volume_binding(self):
        binding = build_volume_binding(VolumeSpec.parse('/outside:/inside', True))
        assert binding == ('/inside', '/outside:/inside:rw')

    def test_get_container_data_volumes(self):
        options = [VolumeSpec.parse(v) for v in [
            '/host/volume:/host/volume:ro',
            '/new/volume',
            '/existing/volume',
            'named:/named/vol',
            '/dev/tmpfs'
        ]]

        self.mock_client.inspect_image.return_value = {
            'ContainerConfig': {
                'Volumes': {
                    '/mnt/image/data': {},
                }
            }
        }
        container = Container(self.mock_client, {
            'Image': 'ababab',
            'Mounts': [
                {
                    'Source': '/host/volume',
                    'Destination': '/host/volume',
                    'Mode': '',
                    'RW': True,
                    'Name': 'hostvolume',
                }, {
                    'Source': '/var/lib/docker/aaaaaaaa',
                    'Destination': '/existing/volume',
                    'Mode': '',
                    'RW': True,
                    'Name': 'existingvolume',
                }, {
                    'Source': '/var/lib/docker/bbbbbbbb',
                    'Destination': '/removed/volume',
                    'Mode': '',
                    'RW': True,
                    'Name': 'removedvolume',
                }, {
                    'Source': '/var/lib/docker/cccccccc',
                    'Destination': '/mnt/image/data',
                    'Mode': '',
                    'RW': True,
                    'Name': 'imagedata',
                },
            ]
        }, has_been_inspected=True)

        expected = [
            VolumeSpec.parse('existingvolume:/existing/volume:rw'),
            VolumeSpec.parse('imagedata:/mnt/image/data:rw'),
        ]

        volumes, _ = get_container_data_volumes(container, options, ['/dev/tmpfs'], [])
        assert sorted(volumes) == sorted(expected)

    def test_merge_volume_bindings(self):
        options = [
            VolumeSpec.parse(v, True) for v in [
                '/host/volume:/host/volume:ro',
                '/host/rw/volume:/host/rw/volume',
                '/new/volume',
                '/existing/volume',
                '/dev/tmpfs'
            ]
        ]

        self.mock_client.inspect_image.return_value = {
            'ContainerConfig': {'Volumes': {}}
        }

        previous_container = Container(self.mock_client, {
            'Id': 'cdefab',
            'Image': 'ababab',
            'Mounts': [{
                'Source': '/var/lib/docker/aaaaaaaa',
                'Destination': '/existing/volume',
                'Mode': '',
                'RW': True,
                'Name': 'existingvolume',
            }],
        }, has_been_inspected=True)

        expected = [
            '/host/volume:/host/volume:ro',
            '/host/rw/volume:/host/rw/volume:rw',
            'existingvolume:/existing/volume:rw',
        ]

        binds, affinity = merge_volume_bindings(options, ['/dev/tmpfs'], previous_container, [])
        assert sorted(binds) == sorted(expected)
        assert affinity == {'affinity:container': '=cdefab'}

    def test_mount_same_host_path_to_two_volumes(self):
        service = Service(
            'web',
            image='busybox',
            volumes=[
                VolumeSpec.parse('/host/path:/data1', True),
                VolumeSpec.parse('/host/path:/data2', True),
            ],
            client=self.mock_client,
        )

        self.mock_client.inspect_image.return_value = {
            'Id': 'ababab',
            'ContainerConfig': {
                'Volumes': {}
            }
        }

        service._get_container_create_options(
            override_options={},
            number=1,
        )

        assert set(self.mock_client.create_host_config.call_args[1]['binds']) == set([
            '/host/path:/data1:rw',
            '/host/path:/data2:rw',
        ])

    def test_get_container_create_options_with_different_host_path_in_container_json(self):
        service = Service(
            'web',
            image='busybox',
            volumes=[VolumeSpec.parse('/host/path:/data')],
            client=self.mock_client,
        )
        volume_name = 'abcdefff1234'

        self.mock_client.inspect_image.return_value = {
            'Id': 'ababab',
            'ContainerConfig': {
                'Volumes': {
                    '/data': {},
                }
            }
        }

        self.mock_client.inspect_container.return_value = {
            'Id': '123123123',
            'Image': 'ababab',
            'Mounts': [
                {
                    'Destination': '/data',
                    'Source': '/mnt/sda1/host/path',
                    'Mode': '',
                    'RW': True,
                    'Driver': 'local',
                    'Name': volume_name,
                },
            ]
        }

        service._get_container_create_options(
            override_options={},
            number=1,
            previous_container=Container(self.mock_client, {'Id': '123123123'}),
        )

        assert (
            self.mock_client.create_host_config.call_args[1]['binds'] ==
            ['{}:/data:rw'.format(volume_name)]
        )

    def test_warn_on_masked_volume_no_warning_when_no_container_volumes(self):
        volumes_option = [VolumeSpec('/home/user', '/path', 'rw')]
        container_volumes = []
        service = 'service_name'

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            warn_on_masked_volume(volumes_option, container_volumes, service)

        assert not mock_log.warn.called

    def test_warn_on_masked_volume_when_masked(self):
        volumes_option = [VolumeSpec('/home/user', '/path', 'rw')]
        container_volumes = [
            VolumeSpec('/var/lib/docker/path', '/path', 'rw'),
            VolumeSpec('/var/lib/docker/path', '/other', 'rw'),
        ]
        service = 'service_name'

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            warn_on_masked_volume(volumes_option, container_volumes, service)

        mock_log.warn.assert_called_once_with(mock.ANY)

    def test_warn_on_masked_no_warning_with_same_path(self):
        volumes_option = [VolumeSpec('/home/user', '/path', 'rw')]
        container_volumes = [VolumeSpec('/home/user', '/path', 'rw')]
        service = 'service_name'

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            warn_on_masked_volume(volumes_option, container_volumes, service)

        assert not mock_log.warn.called

    def test_warn_on_masked_no_warning_with_container_only_option(self):
        volumes_option = [VolumeSpec(None, '/path', 'rw')]
        container_volumes = [
            VolumeSpec('/var/lib/docker/volume/path', '/path', 'rw')
        ]
        service = 'service_name'

        with mock.patch('compose.service.log', autospec=True) as mock_log:
            warn_on_masked_volume(volumes_option, container_volumes, service)

        assert not mock_log.warn.called

    def test_create_with_special_volume_mode(self):
        self.mock_client.inspect_image.return_value = {'Id': 'imageid'}

        self.mock_client.create_container.return_value = {'Id': 'containerid'}

        volume = '/tmp:/foo:z'
        Service(
            'web',
            client=self.mock_client,
            image='busybox',
            volumes=[VolumeSpec.parse(volume, True)],
        ).create_container()

        assert self.mock_client.create_container.call_count == 1
        assert self.mock_client.create_host_config.call_args[1]['binds'] == [volume]


class ServiceSecretTest(unittest.TestCase):
    def setUp(self):
        self.mock_client = mock.create_autospec(docker.APIClient)
        self.mock_client.api_version = DEFAULT_DOCKER_API_VERSION
        self.mock_client._general_configs = {}

    def test_get_secret_volumes(self):
        secret1 = {
            'secret': ServiceSecret.parse({'source': 'secret1', 'target': 'b.txt'}),
            'file': 'a.txt'
        }
        service = Service(
            'web',
            client=self.mock_client,
            image='busybox',
            secrets=[secret1]
        )
        volumes = service.get_secret_volumes()

        assert volumes[0].source == secret1['file']
        assert volumes[0].target == '{}/{}'.format(SECRETS_PATH, secret1['secret'].target)

    def test_get_secret_volumes_abspath(self):
        secret1 = {
            'secret': ServiceSecret.parse({'source': 'secret1', 'target': '/d.txt'}),
            'file': 'c.txt'
        }
        service = Service(
            'web',
            client=self.mock_client,
            image='busybox',
            secrets=[secret1]
        )
        volumes = service.get_secret_volumes()

        assert volumes[0].source == secret1['file']
        assert volumes[0].target == secret1['secret'].target

    def test_get_secret_volumes_no_target(self):
        secret1 = {
            'secret': ServiceSecret.parse({'source': 'secret1'}),
            'file': 'c.txt'
        }
        service = Service(
            'web',
            client=self.mock_client,
            image='busybox',
            secrets=[secret1]
        )
        volumes = service.get_secret_volumes()

        assert volumes[0].source == secret1['file']
        assert volumes[0].target == '{}/{}'.format(SECRETS_PATH, secret1['secret'].source)


class RewriteBuildPathTest(unittest.TestCase):
    @mock.patch('compose.service.IS_WINDOWS_PLATFORM', True)
    def test_rewrite_url_no_prefix(self):
        urls = [
            'http://test.com',
            'https://test.com',
            'git://test.com',
            'github.com/test/test',
            'git@test.com',
        ]
        for u in urls:
            assert rewrite_build_path(u) == u

    @mock.patch('compose.service.IS_WINDOWS_PLATFORM', True)
    def test_rewrite_windows_path(self):
        assert rewrite_build_path('C:\\context') == WINDOWS_LONGPATH_PREFIX + 'C:\\context'
        assert rewrite_build_path(
            rewrite_build_path('C:\\context')
        ) == rewrite_build_path('C:\\context')

    @mock.patch('compose.service.IS_WINDOWS_PLATFORM', False)
    def test_rewrite_unix_path(self):
        assert rewrite_build_path('/context') == '/context'
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import unittest
from threading import Lock

import six
from docker.errors import APIError

from compose.parallel import GlobalLimit
from compose.parallel import parallel_execute
from compose.parallel import parallel_execute_iter
from compose.parallel import ParallelStreamWriter
from compose.parallel import UpstreamError


web = 'web'
db = 'db'
data_volume = 'data_volume'
cache = 'cache'

objects = [web, db, data_volume, cache]

deps = {
    web: [db, cache],
    db: [data_volume],
    data_volume: [],
    cache: [],
}


def get_deps(obj):
    return [(dep, None) for dep in deps[obj]]


class ParallelTest(unittest.TestCase):

    def test_parallel_execute(self):
        results, errors = parallel_execute(
            objects=[1, 2, 3, 4, 5],
            func=lambda x: x * 2,
            get_name=six.text_type,
            msg="Doubling",
        )

        assert sorted(results) == [2, 4, 6, 8, 10]
        assert errors == {}

    def test_parallel_execute_with_limit(self):
        limit = 1
        tasks = 20
        lock = Lock()

        def f(obj):
            locked = lock.acquire(False)
            # we should always get the lock because we're the only thread running
            assert locked
            lock.release()
            return None

        results, errors = parallel_execute(
            objects=list(range(tasks)),
            func=f,
            get_name=six.text_type,
            msg="Testing",
            limit=limit,
        )

        assert results == tasks * [None]
        assert errors == {}

    def test_parallel_execute_with_global_limit(self):
        GlobalLimit.set_global_limit(1)
        self.addCleanup(GlobalLimit.set_global_limit, None)
        tasks = 20
        lock = Lock()

        def f(obj):
            locked = lock.acquire(False)
            # we should always get the lock because we're the only thread running
            assert locked
            lock.release()
            return None

        results, errors = parallel_execute(
            objects=list(range(tasks)),
            func=f,
            get_name=six.text_type,
            msg="Testing",
        )

        assert results == tasks * [None]
        assert errors == {}

    def test_parallel_execute_with_deps(self):
        log = []

        def process(x):
            log.append(x)

        parallel_execute(
            objects=objects,
            func=process,
            get_name=lambda obj: obj,
            msg="Processing",
            get_deps=get_deps,
        )

        assert sorted(log) == sorted(objects)

        assert log.index(data_volume) < log.index(db)
        assert log.index(db) < log.index(web)
        assert log.index(cache) < log.index(web)

    def test_parallel_execute_with_upstream_errors(self):
        log = []

        def process(x):
            if x is data_volume:
                raise APIError(None, None, "Something went wrong")
            log.append(x)

        parallel_execute(
            objects=objects,
            func=process,
            get_name=lambda obj: obj,
            msg="Processing",
            get_deps=get_deps,
        )

        assert log == [cache]

        events = [
            (obj, result, type(exception))
            for obj, result, exception
            in parallel_execute_iter(objects, process, get_deps, None)
        ]

        assert (cache, None, type(None)) in events
        assert (data_volume, None, APIError) in events
        assert (db, None, UpstreamError) in events
        assert (web, None, UpstreamError) in events


def test_parallel_execute_alignment(capsys):
    ParallelStreamWriter.instance = None
    results, errors = parallel_execute(
        objects=["short", "a very long name"],
        func=lambda x: x,
        get_name=six.text_type,
        msg="Aligning",
    )

    assert errors == {}

    _, err = capsys.readouterr()
    a, b = err.split('\n')[:2]
    assert a.index('...') == b.index('...')


def test_parallel_execute_ansi(capsys):
    ParallelStreamWriter.instance = None
    ParallelStreamWriter.set_noansi(value=False)
    results, errors = parallel_execute(
        objects=["something", "something more"],
        func=lambda x: x,
        get_name=six.text_type,
        msg="Control characters",
    )

    assert errors == {}

    _, err = capsys.readouterr()
    assert "\x1b" in err


def test_parallel_execute_noansi(capsys):
    ParallelStreamWriter.instance = None
    ParallelStreamWriter.set_noansi()
    results, errors = parallel_execute(
        objects=["something", "something more"],
        func=lambda x: x,
        get_name=six.text_type,
        msg="Control characters",
    )

    assert errors == {}

    _, err = capsys.readouterr()
    assert "\x1b" not in err
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import docker
import pytest

from compose import volume
from tests import mock


@pytest.fixture
def mock_client():
    return mock.create_autospec(docker.APIClient)


class TestVolume(object):

    def test_remove_local_volume(self, mock_client):
        vol = volume.Volume(mock_client, 'foo', 'project')
        vol.remove()
        mock_client.remove_volume.assert_called_once_with('foo_project')

    def test_remove_external_volume(self, mock_client):
        vol = volume.Volume(mock_client, 'foo', 'project', external=True)
        vol.remove()
        assert not mock_client.remove_volume.called
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import os
import platform
import ssl

import docker
import pytest

import compose
from compose.cli import errors
from compose.cli.docker_client import docker_client
from compose.cli.docker_client import get_tls_version
from compose.cli.docker_client import tls_config_from_options
from compose.config.environment import Environment
from tests import mock
from tests import unittest


class DockerClientTestCase(unittest.TestCase):

    def test_docker_client_no_home(self):
        with mock.patch.dict(os.environ):
            try:
                del os.environ['HOME']
            except KeyError:
                pass
            docker_client(os.environ)

    @mock.patch.dict(os.environ)
    def test_docker_client_with_custom_timeout(self):
        os.environ['COMPOSE_HTTP_TIMEOUT'] = '123'
        client = docker_client(os.environ)
        assert client.timeout == 123

    @mock.patch.dict(os.environ)
    def test_custom_timeout_error(self):
        os.environ['COMPOSE_HTTP_TIMEOUT'] = '123'
        client = docker_client(os.environ)

        with mock.patch('compose.cli.errors.log') as fake_log:
            with pytest.raises(errors.ConnectionError):
                with errors.handle_connection_errors(client):
                    raise errors.RequestsConnectionError(
                        errors.ReadTimeoutError(None, None, None))

        assert fake_log.error.call_count == 1
        assert '123' in fake_log.error.call_args[0][0]

        with mock.patch('compose.cli.errors.log') as fake_log:
            with pytest.raises(errors.ConnectionError):
                with errors.handle_connection_errors(client):
                    raise errors.ReadTimeout()

        assert fake_log.error.call_count == 1
        assert '123' in fake_log.error.call_args[0][0]

    def test_user_agent(self):
        client = docker_client(os.environ)
        expected = "docker-compose/{0} docker-py/{1} {2}/{3}".format(
            compose.__version__,
            docker.__version__,
            platform.system(),
            platform.release()
        )
        assert client.headers['User-Agent'] == expected


class TLSConfigTestCase(unittest.TestCase):
    cert_path = 'tests/fixtures/tls/'
    ca_cert = os.path.join(cert_path, 'ca.pem')
    client_cert = os.path.join(cert_path, 'cert.pem')
    key = os.path.join(cert_path, 'key.pem')

    def test_simple_tls(self):
        options = {'--tls': True}
        result = tls_config_from_options(options)
        assert result is True

    def test_tls_ca_cert(self):
        options = {
            '--tlscacert': self.ca_cert, '--tlsverify': True
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.ca_cert == options['--tlscacert']
        assert result.verify is True

    def test_tls_ca_cert_explicit(self):
        options = {
            '--tlscacert': self.ca_cert, '--tls': True,
            '--tlsverify': True
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.ca_cert == options['--tlscacert']
        assert result.verify is True

    def test_tls_client_cert(self):
        options = {
            '--tlscert': self.client_cert, '--tlskey': self.key
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (options['--tlscert'], options['--tlskey'])

    def test_tls_client_cert_explicit(self):
        options = {
            '--tlscert': self.client_cert, '--tlskey': self.key,
            '--tls': True
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (options['--tlscert'], options['--tlskey'])

    def test_tls_client_and_ca(self):
        options = {
            '--tlscert': self.client_cert, '--tlskey': self.key,
            '--tlsverify': True, '--tlscacert': self.ca_cert
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (options['--tlscert'], options['--tlskey'])
        assert result.ca_cert == options['--tlscacert']
        assert result.verify is True

    def test_tls_client_and_ca_explicit(self):
        options = {
            '--tlscert': self.client_cert, '--tlskey': self.key,
            '--tlsverify': True, '--tlscacert': self.ca_cert,
            '--tls': True
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (options['--tlscert'], options['--tlskey'])
        assert result.ca_cert == options['--tlscacert']
        assert result.verify is True

    def test_tls_client_missing_key(self):
        options = {'--tlscert': self.client_cert}
        with pytest.raises(docker.errors.TLSParameterError):
            tls_config_from_options(options)

        options = {'--tlskey': self.key}
        with pytest.raises(docker.errors.TLSParameterError):
            tls_config_from_options(options)

    def test_assert_hostname_explicit_skip(self):
        options = {'--tlscacert': self.ca_cert, '--skip-hostname-check': True}
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.assert_hostname is False

    def test_tls_client_and_ca_quoted_paths(self):
        options = {
            '--tlscacert': '"{0}"'.format(self.ca_cert),
            '--tlscert': '"{0}"'.format(self.client_cert),
            '--tlskey': '"{0}"'.format(self.key),
            '--tlsverify': True
        }
        result = tls_config_from_options(options)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (self.client_cert, self.key)
        assert result.ca_cert == self.ca_cert
        assert result.verify is True

    def test_tls_simple_with_tls_version(self):
        tls_version = 'TLSv1'
        options = {'--tls': True}
        environment = Environment({'COMPOSE_TLS_VERSION': tls_version})
        result = tls_config_from_options(options, environment)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.ssl_version == ssl.PROTOCOL_TLSv1

    def test_tls_mixed_environment_and_flags(self):
        options = {'--tls': True, '--tlsverify': False}
        environment = Environment({'DOCKER_CERT_PATH': 'tests/fixtures/tls/'})
        result = tls_config_from_options(options, environment)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (self.client_cert, self.key)
        assert result.ca_cert == self.ca_cert
        assert result.verify is False

    def test_tls_flags_override_environment(self):
        environment = Environment({
            'DOCKER_CERT_PATH': '/completely/wrong/path',
            'DOCKER_TLS_VERIFY': 'false'
        })
        options = {
            '--tlscacert': '"{0}"'.format(self.ca_cert),
            '--tlscert': '"{0}"'.format(self.client_cert),
            '--tlskey': '"{0}"'.format(self.key),
            '--tlsverify': True
        }

        result = tls_config_from_options(options, environment)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.cert == (self.client_cert, self.key)
        assert result.ca_cert == self.ca_cert
        assert result.verify is True

    def test_tls_verify_flag_no_override(self):
        environment = Environment({
            'DOCKER_TLS_VERIFY': 'true',
            'COMPOSE_TLS_VERSION': 'TLSv1',
            'DOCKER_CERT_PATH': self.cert_path
        })
        options = {'--tls': True, '--tlsverify': False}

        result = tls_config_from_options(options, environment)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.ssl_version == ssl.PROTOCOL_TLSv1
        # verify is a special case - since `--tlsverify` = False means it
        # wasn't used, we set it if either the environment or the flag is True
        # see https://github.com/docker/compose/issues/5632
        assert result.verify is True

    def test_tls_verify_env_falsy_value(self):
        environment = Environment({'DOCKER_TLS_VERIFY': '0'})
        options = {'--tls': True}
        assert tls_config_from_options(options, environment) is True

    def test_tls_verify_default_cert_path(self):
        environment = Environment({'DOCKER_TLS_VERIFY': '1'})
        options = {'--tls': True}
        with mock.patch('compose.cli.docker_client.default_cert_path') as dcp:
            dcp.return_value = 'tests/fixtures/tls/'
            result = tls_config_from_options(options, environment)
        assert isinstance(result, docker.tls.TLSConfig)
        assert result.verify is True
        assert result.ca_cert == self.ca_cert
        assert result.cert == (self.client_cert, self.key)


class TestGetTlsVersion(object):
    def test_get_tls_version_default(self):
        environment = {}
        assert get_tls_version(environment) is None

    @pytest.mark.skipif(not hasattr(ssl, 'PROTOCOL_TLSv1_2'), reason='TLS v1.2 unsupported')
    def test_get_tls_version_upgrade(self):
        environment = {'COMPOSE_TLS_VERSION': 'TLSv1_2'}
        assert get_tls_version(environment) == ssl.PROTOCOL_TLSv1_2

    def test_get_tls_version_unavailable(self):
        environment = {'COMPOSE_TLS_VERSION': 'TLSv5_5'}
        with mock.patch('compose.cli.docker_client.log') as mock_log:
            tls_version = get_tls_version(environment)
        mock_log.warn.assert_called_once_with(mock.ANY)
        assert tls_version is None
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import unittest

from compose.utils import unquote_path


class UnquotePathTest(unittest.TestCase):
    def test_no_quotes(self):
        assert unquote_path('hello') == 'hello'

    def test_simple_quotes(self):
        assert unquote_path('"hello"') == 'hello'

    def test_uneven_quotes(self):
        assert unquote_path('"hello') == '"hello'
        assert unquote_path('hello"') == 'hello"'

    def test_nested_quotes(self):
        assert unquote_path('""hello""') == '"hello"'
        assert unquote_path('"hel"lo"') == 'hel"lo'
        assert unquote_path('"hello""') == 'hello"'
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging

import docker
import pytest

from compose import container
from compose.cli.errors import UserError
from compose.cli.formatter import ConsoleWarningFormatter
from compose.cli.main import call_docker
from compose.cli.main import convergence_strategy_from_opts
from compose.cli.main import filter_containers_to_service_names
from compose.cli.main import setup_console_handler
from compose.cli.main import warn_for_swarm_mode
from compose.service import ConvergenceStrategy
from tests import mock


def mock_container(service, number):
    return mock.create_autospec(
        container.Container,
        service=service,
        number=number,
        name_without_project='{0}_{1}'.format(service, number))


@pytest.fixture
def logging_handler():
    stream = mock.Mock()
    stream.isatty.return_value = True
    return logging.StreamHandler(stream=stream)


class TestCLIMainTestCase(object):

    def test_filter_containers_to_service_names(self):
        containers = [
            mock_container('web', 1),
            mock_container('web', 2),
            mock_container('db', 1),
            mock_container('other', 1),
            mock_container('another', 1),
        ]
        service_names = ['web', 'db']
        actual = filter_containers_to_service_names(containers, service_names)
        assert actual == containers[:3]

    def test_filter_containers_to_service_names_all(self):
        containers = [
            mock_container('web', 1),
            mock_container('db', 1),
            mock_container('other', 1),
        ]
        service_names = []
        actual = filter_containers_to_service_names(containers, service_names)
        assert actual == containers

    def test_warning_in_swarm_mode(self):
        mock_client = mock.create_autospec(docker.APIClient)
        mock_client.info.return_value = {'Swarm': {'LocalNodeState': 'active'}}

        with mock.patch('compose.cli.main.log') as fake_log:
            warn_for_swarm_mode(mock_client)
            assert fake_log.warn.call_count == 1


class TestSetupConsoleHandlerTestCase(object):

    def test_with_tty_verbose(self, logging_handler):
        setup_console_handler(logging_handler, True)
        assert type(logging_handler.formatter) == ConsoleWarningFormatter
        assert '%(name)s' in logging_handler.formatter._fmt
        assert '%(funcName)s' in logging_handler.formatter._fmt

    def test_with_tty_not_verbose(self, logging_handler):
        setup_console_handler(logging_handler, False)
        assert type(logging_handler.formatter) == ConsoleWarningFormatter
        assert '%(name)s' not in logging_handler.formatter._fmt
        assert '%(funcName)s' not in logging_handler.formatter._fmt

    def test_with_not_a_tty(self, logging_handler):
        logging_handler.stream.isatty.return_value = False
        setup_console_handler(logging_handler, False)
        assert type(logging_handler.formatter) == logging.Formatter


class TestConvergeStrategyFromOptsTestCase(object):

    def test_invalid_opts(self):
        options = {'--force-recreate': True, '--no-recreate': True}
        with pytest.raises(UserError):
            convergence_strategy_from_opts(options)

    def test_always(self):
        options = {'--force-recreate': True, '--no-recreate': False}
        assert (
            convergence_strategy_from_opts(options) ==
            ConvergenceStrategy.always
        )

    def test_never(self):
        options = {'--force-recreate': False, '--no-recreate': True}
        assert (
            convergence_strategy_from_opts(options) ==
            ConvergenceStrategy.never
        )

    def test_changed(self):
        options = {'--force-recreate': False, '--no-recreate': False}
        assert (
            convergence_strategy_from_opts(options) ==
            ConvergenceStrategy.changed
        )


def mock_find_executable(exe):
    return exe


@mock.patch('compose.cli.main.find_executable', mock_find_executable)
class TestCallDocker(object):
    def test_simple_no_options(self):
        with mock.patch('subprocess.call') as fake_call:
            call_docker(['ps'], {})

        assert fake_call.call_args[0][0] == ['docker', 'ps']

    def test_simple_tls_option(self):
        with mock.patch('subprocess.call') as fake_call:
            call_docker(['ps'], {'--tls': True})

        assert fake_call.call_args[0][0] == ['docker', '--tls', 'ps']

    def test_advanced_tls_options(self):
        with mock.patch('subprocess.call') as fake_call:
            call_docker(['ps'], {
                '--tls': True,
                '--tlscacert': './ca.pem',
                '--tlscert': './cert.pem',
                '--tlskey': './key.pem',
            })

        assert fake_call.call_args[0][0] == [
            'docker', '--tls', '--tlscacert', './ca.pem', '--tlscert',
            './cert.pem', '--tlskey', './key.pem', 'ps'
        ]

    def test_with_host_option(self):
        with mock.patch('subprocess.call') as fake_call:
            call_docker(['ps'], {'--host': 'tcp://mydocker.net:2333'})

        assert fake_call.call_args[0][0] == [
            'docker', '--host', 'tcp://mydocker.net:2333', 'ps'
        ]

    def test_with_http_host(self):
        with mock.patch('subprocess.call') as fake_call:
            call_docker(['ps'], {'--host': 'http://mydocker.net:2333'})

        assert fake_call.call_args[0][0] == [
            'docker', '--host', 'tcp://mydocker.net:2333', 'ps',
        ]

    def test_with_host_option_shorthand_equal(self):
        with mock.patch('subprocess.call') as fake_call:
            call_docker(['ps'], {'--host': '=tcp://mydocker.net:2333'})

        assert fake_call.call_args[0][0] == [
            'docker', '--host', 'tcp://mydocker.net:2333', 'ps'
        ]
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest
from docker.errors import APIError
from requests.exceptions import ConnectionError

from compose.cli import errors
from compose.cli.errors import handle_connection_errors
from compose.const import IS_WINDOWS_PLATFORM
from tests import mock


@pytest.yield_fixture
def mock_logging():
    with mock.patch('compose.cli.errors.log', autospec=True) as mock_log:
        yield mock_log


def patch_find_executable(side_effect):
    return mock.patch(
        'compose.cli.errors.find_executable',
        autospec=True,
        side_effect=side_effect)


class TestHandleConnectionErrors(object):

    def test_generic_connection_error(self, mock_logging):
        with pytest.raises(errors.ConnectionError):
            with patch_find_executable(['/bin/docker', None]):
                with handle_connection_errors(mock.Mock()):
                    raise ConnectionError()

        _, args, _ = mock_logging.error.mock_calls[0]
        assert "Couldn't connect to Docker daemon" in args[0]

    def test_api_error_version_mismatch(self, mock_logging):
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise APIError(None, None, b"client is newer than server")

        _, args, _ = mock_logging.error.mock_calls[0]
        assert "Docker Engine of version 1.10.0 or greater" in args[0]

    def test_api_error_version_mismatch_unicode_explanation(self, mock_logging):
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise APIError(None, None, u"client is newer than server")

        _, args, _ = mock_logging.error.mock_calls[0]
        assert "Docker Engine of version 1.10.0 or greater" in args[0]

    def test_api_error_version_other(self, mock_logging):
        msg = b"Something broke!"
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise APIError(None, None, msg)

        mock_logging.error.assert_called_once_with(msg.decode('utf-8'))

    def test_api_error_version_other_unicode_explanation(self, mock_logging):
        msg = u"Something broke!"
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise APIError(None, None, msg)

        mock_logging.error.assert_called_once_with(msg)

    @pytest.mark.skipif(not IS_WINDOWS_PLATFORM, reason='Needs pywin32')
    def test_windows_pipe_error_no_data(self, mock_logging):
        import pywintypes
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise pywintypes.error(232, 'WriteFile', 'The pipe is being closed.')

        _, args, _ = mock_logging.error.mock_calls[0]
        assert "The current Compose file version is not compatible with your engine version." in args[0]

    @pytest.mark.skipif(not IS_WINDOWS_PLATFORM, reason='Needs pywin32')
    def test_windows_pipe_error_misc(self, mock_logging):
        import pywintypes
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise pywintypes.error(231, 'WriteFile', 'The pipe is busy.')

        _, args, _ = mock_logging.error.mock_calls[0]
        assert "Windows named pipe error: The pipe is busy. (code: 231)" == args[0]

    @pytest.mark.skipif(not IS_WINDOWS_PLATFORM, reason='Needs pywin32')
    def test_windows_pipe_error_encoding_issue(self, mock_logging):
        import pywintypes
        with pytest.raises(errors.ConnectionError):
            with handle_connection_errors(mock.Mock(api_version='1.22')):
                raise pywintypes.error(9999, 'WriteFile', 'I use weird characters \xe9')

        _, args, _ = mock_logging.error.mock_calls[0]
        assert 'Windows named pipe error: I use weird characters \xe9 (code: 9999)' == args[0]
<EOF>
<BOF>
# ~*~ encoding: utf-8 ~*~
from __future__ import absolute_import
from __future__ import unicode_literals

import os

import pytest
import six

from compose.cli.command import get_config_path_from_options
from compose.config.environment import Environment
from compose.const import IS_WINDOWS_PLATFORM
from tests import mock


class TestGetConfigPathFromOptions(object):

    def test_path_from_options(self):
        paths = ['one.yml', 'two.yml']
        opts = {'--file': paths}
        environment = Environment.from_env_file('.')
        assert get_config_path_from_options('.', opts, environment) == paths

    def test_single_path_from_env(self):
        with mock.patch.dict(os.environ):
            os.environ['COMPOSE_FILE'] = 'one.yml'
            environment = Environment.from_env_file('.')
            assert get_config_path_from_options('.', {}, environment) == ['one.yml']

    @pytest.mark.skipif(IS_WINDOWS_PLATFORM, reason='posix separator')
    def test_multiple_path_from_env(self):
        with mock.patch.dict(os.environ):
            os.environ['COMPOSE_FILE'] = 'one.yml:two.yml'
            environment = Environment.from_env_file('.')
            assert get_config_path_from_options(
                '.', {}, environment
            ) == ['one.yml', 'two.yml']

    @pytest.mark.skipif(not IS_WINDOWS_PLATFORM, reason='windows separator')
    def test_multiple_path_from_env_windows(self):
        with mock.patch.dict(os.environ):
            os.environ['COMPOSE_FILE'] = 'one.yml;two.yml'
            environment = Environment.from_env_file('.')
            assert get_config_path_from_options(
                '.', {}, environment
            ) == ['one.yml', 'two.yml']

    def test_multiple_path_from_env_custom_separator(self):
        with mock.patch.dict(os.environ):
            os.environ['COMPOSE_PATH_SEPARATOR'] = '^'
            os.environ['COMPOSE_FILE'] = 'c:\\one.yml^.\\semi;colon.yml'
            environment = Environment.from_env_file('.')
            assert get_config_path_from_options(
                '.', {}, environment
            ) == ['c:\\one.yml', '.\\semi;colon.yml']

    def test_no_path(self):
        environment = Environment.from_env_file('.')
        assert not get_config_path_from_options('.', {}, environment)

    def test_unicode_path_from_options(self):
        paths = [b'\xe5\xb0\xb1\xe5\x90\x83\xe9\xa5\xad/docker-compose.yml']
        opts = {'--file': paths}
        environment = Environment.from_env_file('.')
        assert get_config_path_from_options(
            '.', opts, environment
        ) == ['/docker-compose.yml']

    @pytest.mark.skipif(six.PY3, reason='Env values in Python 3 are already Unicode')
    def test_unicode_path_from_env(self):
        with mock.patch.dict(os.environ):
            os.environ['COMPOSE_FILE'] = b'\xe5\xb0\xb1\xe5\x90\x83\xe9\xa5\xad/docker-compose.yml'
            environment = Environment.from_env_file('.')
            assert get_config_path_from_options(
                '.', {}, environment
            ) == ['/docker-compose.yml']
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import itertools

import pytest
import requests
import six
from docker.errors import APIError
from six.moves.queue import Queue

from compose.cli.log_printer import build_log_generator
from compose.cli.log_printer import build_log_presenters
from compose.cli.log_printer import build_no_log_generator
from compose.cli.log_printer import consume_queue
from compose.cli.log_printer import QueueItem
from compose.cli.log_printer import wait_on_exit
from compose.cli.log_printer import watch_events
from compose.container import Container
from tests import mock


@pytest.fixture
def output_stream():
    output = six.StringIO()
    output.flush = mock.Mock()
    return output


@pytest.fixture
def mock_container():
    return mock.Mock(spec=Container, name_without_project='web_1')


class TestLogPresenter(object):

    def test_monochrome(self, mock_container):
        presenters = build_log_presenters(['foo', 'bar'], True)
        presenter = next(presenters)
        actual = presenter.present(mock_container, "this line")
        assert actual == "web_1  | this line"

    def test_polychrome(self, mock_container):
        presenters = build_log_presenters(['foo', 'bar'], False)
        presenter = next(presenters)
        actual = presenter.present(mock_container, "this line")
        assert '\033[' in actual


def test_wait_on_exit():
    exit_status = 3
    mock_container = mock.Mock(
        spec=Container,
        name='cname',
        wait=mock.Mock(return_value=exit_status))

    expected = '{} exited with code {}\n'.format(mock_container.name, exit_status)
    assert expected == wait_on_exit(mock_container)


def test_wait_on_exit_raises():
    status_code = 500

    def mock_wait():
        resp = requests.Response()
        resp.status_code = status_code
        raise APIError('Bad server', resp)

    mock_container = mock.Mock(
        spec=Container,
        name='cname',
        wait=mock_wait
    )

    expected = 'Unexpected API error for {} (HTTP code {})\n'.format(
        mock_container.name, status_code,
    )
    assert expected in wait_on_exit(mock_container)


def test_build_no_log_generator(mock_container):
    mock_container.has_api_logs = False
    mock_container.log_driver = 'none'
    output, = build_no_log_generator(mock_container, None)
    assert "WARNING: no logs are available with the 'none' log driver\n" in output
    assert "exited with code" not in output


class TestBuildLogGenerator(object):

    def test_no_log_stream(self, mock_container):
        mock_container.log_stream = None
        mock_container.logs.return_value = iter([b"hello\nworld"])
        log_args = {'follow': True}

        generator = build_log_generator(mock_container, log_args)
        assert next(generator) == "hello\n"
        assert next(generator) == "world"
        mock_container.logs.assert_called_once_with(
            stdout=True,
            stderr=True,
            stream=True,
            **log_args)

    def test_with_log_stream(self, mock_container):
        mock_container.log_stream = iter([b"hello\nworld"])
        log_args = {'follow': True}

        generator = build_log_generator(mock_container, log_args)
        assert next(generator) == "hello\n"
        assert next(generator) == "world"

    def test_unicode(self, output_stream):
        glyph = u'\u2022\n'
        mock_container.log_stream = iter([glyph.encode('utf-8')])

        generator = build_log_generator(mock_container, {})
        assert next(generator) == glyph


@pytest.fixture
def thread_map():
    return {'cid': mock.Mock()}


@pytest.fixture
def mock_presenters():
    return itertools.cycle([mock.Mock()])


class TestWatchEvents(object):

    def test_stop_event(self, thread_map, mock_presenters):
        event_stream = [{'action': 'stop', 'id': 'cid'}]
        watch_events(thread_map, event_stream, mock_presenters, ())
        assert not thread_map

    def test_start_event(self, thread_map, mock_presenters):
        container_id = 'abcd'
        event = {'action': 'start', 'id': container_id, 'container': mock.Mock()}
        event_stream = [event]
        thread_args = 'foo', 'bar'

        with mock.patch(
            'compose.cli.log_printer.build_thread',
            autospec=True
        ) as mock_build_thread:
            watch_events(thread_map, event_stream, mock_presenters, thread_args)
            mock_build_thread.assert_called_once_with(
                event['container'],
                next(mock_presenters),
                *thread_args)
        assert container_id in thread_map

    def test_other_event(self, thread_map, mock_presenters):
        container_id = 'abcd'
        event_stream = [{'action': 'create', 'id': container_id}]
        watch_events(thread_map, event_stream, mock_presenters, ())
        assert container_id not in thread_map


class TestConsumeQueue(object):

    def test_item_is_an_exception(self):

        class Problem(Exception):
            pass

        queue = Queue()
        error = Problem('oops')
        for item in QueueItem.new('a'), QueueItem.new('b'), QueueItem.exception(error):
            queue.put(item)

        generator = consume_queue(queue, False)
        assert next(generator) == 'a'
        assert next(generator) == 'b'
        with pytest.raises(Problem):
            next(generator)

    def test_item_is_stop_without_cascade_stop(self):
        queue = Queue()
        for item in QueueItem.stop(), QueueItem.new('a'), QueueItem.new('b'):
            queue.put(item)

        generator = consume_queue(queue, False)
        assert next(generator) == 'a'
        assert next(generator) == 'b'

    def test_item_is_stop_with_cascade_stop(self):
        """Return the name of the container that caused the cascade_stop"""
        queue = Queue()
        for item in QueueItem.stop('foobar-1'), QueueItem.new('a'), QueueItem.new('b'):
            queue.put(item)

        generator = consume_queue(queue, True)
        assert next(generator) is 'foobar-1'

    def test_item_is_none_when_timeout_is_hit(self):
        queue = Queue()
        generator = consume_queue(queue, False)
        assert next(generator) is None
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import logging

from compose.cli import colors
from compose.cli.formatter import ConsoleWarningFormatter
from tests import unittest


MESSAGE = 'this is the message'


def make_log_record(level, message=None):
    return logging.LogRecord('name', level, 'pathame', 0, message or MESSAGE, (), None)


class ConsoleWarningFormatterTestCase(unittest.TestCase):

    def setUp(self):
        self.formatter = ConsoleWarningFormatter()

    def test_format_warn(self):
        output = self.formatter.format(make_log_record(logging.WARN))
        expected = colors.yellow('WARNING') + ': '
        assert output == expected + MESSAGE

    def test_format_error(self):
        output = self.formatter.format(make_log_record(logging.ERROR))
        expected = colors.red('ERROR') + ': '
        assert output == expected + MESSAGE

    def test_format_info(self):
        output = self.formatter.format(make_log_record(logging.INFO))
        assert output == MESSAGE

    def test_format_unicode_info(self):
        message = b'\xec\xa0\x95\xec\x88\x98\xec\xa0\x95'
        output = self.formatter.format(make_log_record(logging.INFO, message))
        assert output == message.decode('utf-8')

    def test_format_unicode_warn(self):
        message = b'\xec\xa0\x95\xec\x88\x98\xec\xa0\x95'
        output = self.formatter.format(make_log_record(logging.WARN, message))
        expected = colors.yellow('WARNING') + ': '
        assert output == '{0}{1}'.format(expected, message.decode('utf-8'))

    def test_format_unicode_error(self):
        message = b'\xec\xa0\x95\xec\x88\x98\xec\xa0\x95'
        output = self.formatter.format(make_log_record(logging.ERROR, message))
        expected = colors.red('ERROR') + ': '
        assert output == '{0}{1}'.format(expected, message.decode('utf-8'))
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import six

from compose.cli import verbose_proxy
from tests import unittest


class VerboseProxyTestCase(unittest.TestCase):

    def test_format_call(self):
        prefix = '' if six.PY3 else 'u'
        expected = "(%(p)s'arg1', True, key=%(p)s'value')" % dict(p=prefix)
        actual = verbose_proxy.format_call(
            ("arg1", True),
            {'key': 'value'})

        assert expected == actual

    def test_format_return_sequence(self):
        expected = "(list with 10 items)"
        actual = verbose_proxy.format_return(list(range(10)), 2)
        assert expected == actual

    def test_format_return(self):
        expected = repr({'Id': 'ok'})
        actual = verbose_proxy.format_return({'Id': 'ok'}, 2)
        assert expected == actual

    def test_format_return_no_result(self):
        actual = verbose_proxy.format_return(None, 2)
        assert actual is None
<EOF>
<BOF>
# encoding: utf-8
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import codecs
import os
import shutil
import tempfile
from operator import itemgetter
from random import shuffle

import py
import pytest
import yaml

from ...helpers import build_config_details
from compose.config import config
from compose.config import types
from compose.config.config import resolve_build_args
from compose.config.config import resolve_environment
from compose.config.environment import Environment
from compose.config.errors import ConfigurationError
from compose.config.errors import VERSION_EXPLANATION
from compose.config.serialize import denormalize_service_dict
from compose.config.serialize import serialize_config
from compose.config.serialize import serialize_ns_time_value
from compose.config.types import VolumeSpec
from compose.const import COMPOSEFILE_V1 as V1
from compose.const import COMPOSEFILE_V2_0 as V2_0
from compose.const import COMPOSEFILE_V2_1 as V2_1
from compose.const import COMPOSEFILE_V2_2 as V2_2
from compose.const import COMPOSEFILE_V2_3 as V2_3
from compose.const import COMPOSEFILE_V3_0 as V3_0
from compose.const import COMPOSEFILE_V3_1 as V3_1
from compose.const import COMPOSEFILE_V3_2 as V3_2
from compose.const import COMPOSEFILE_V3_3 as V3_3
from compose.const import COMPOSEFILE_V3_5 as V3_5
from compose.const import IS_WINDOWS_PLATFORM
from tests import mock
from tests import unittest

DEFAULT_VERSION = V2_0


def make_service_dict(name, service_dict, working_dir='.', filename=None):
    """Test helper function to construct a ServiceExtendsResolver
    """
    resolver = config.ServiceExtendsResolver(
        config.ServiceConfig(
            working_dir=working_dir,
            filename=filename,
            name=name,
            config=service_dict),
        config.ConfigFile(filename=filename, config={}),
        environment=Environment.from_env_file(working_dir)
    )
    return config.process_service(resolver.run())


def service_sort(services):
    return sorted(services, key=itemgetter('name'))


def secret_sort(secrets):
    return sorted(secrets, key=itemgetter('source'))


class ConfigTest(unittest.TestCase):

    def test_load(self):
        service_dicts = config.load(
            build_config_details(
                {
                    'foo': {'image': 'busybox'},
                    'bar': {'image': 'busybox', 'environment': ['FOO=1']},
                },
                'tests/fixtures/extends',
                'common.yml'
            )
        ).services

        assert service_sort(service_dicts) == service_sort([
            {
                'name': 'bar',
                'image': 'busybox',
                'environment': {'FOO': '1'},
            },
            {
                'name': 'foo',
                'image': 'busybox',
            }
        ])

    def test_load_v2(self):
        config_data = config.load(
            build_config_details({
                'version': '2',
                'services': {
                    'foo': {'image': 'busybox'},
                    'bar': {'image': 'busybox', 'environment': ['FOO=1']},
                },
                'volumes': {
                    'hello': {
                        'driver': 'default',
                        'driver_opts': {'beep': 'boop'}
                    }
                },
                'networks': {
                    'default': {
                        'driver': 'bridge',
                        'driver_opts': {'beep': 'boop'}
                    },
                    'with_ipam': {
                        'ipam': {
                            'driver': 'default',
                            'config': [
                                {'subnet': '172.28.0.0/16'}
                            ]
                        }
                    },
                    'internal': {
                        'driver': 'bridge',
                        'internal': True
                    }
                }
            }, 'working_dir', 'filename.yml')
        )
        service_dicts = config_data.services
        volume_dict = config_data.volumes
        networks_dict = config_data.networks
        assert service_sort(service_dicts) == service_sort([
            {
                'name': 'bar',
                'image': 'busybox',
                'environment': {'FOO': '1'},
            },
            {
                'name': 'foo',
                'image': 'busybox',
            }
        ])
        assert volume_dict == {
            'hello': {
                'driver': 'default',
                'driver_opts': {'beep': 'boop'}
            }
        }
        assert networks_dict == {
            'default': {
                'driver': 'bridge',
                'driver_opts': {'beep': 'boop'}
            },
            'with_ipam': {
                'ipam': {
                    'driver': 'default',
                    'config': [
                        {'subnet': '172.28.0.0/16'}
                    ]
                }
            },
            'internal': {
                'driver': 'bridge',
                'internal': True
            }
        }

    def test_valid_versions(self):
        for version in ['2', '2.0']:
            cfg = config.load(build_config_details({'version': version}))
            assert cfg.version == V2_0

        cfg = config.load(build_config_details({'version': '2.1'}))
        assert cfg.version == V2_1

        cfg = config.load(build_config_details({'version': '2.2'}))
        assert cfg.version == V2_2

        cfg = config.load(build_config_details({'version': '2.3'}))
        assert cfg.version == V2_3

        for version in ['3', '3.0']:
            cfg = config.load(build_config_details({'version': version}))
            assert cfg.version == V3_0

        cfg = config.load(build_config_details({'version': '3.1'}))
        assert cfg.version == V3_1

    def test_v1_file_version(self):
        cfg = config.load(build_config_details({'web': {'image': 'busybox'}}))
        assert cfg.version == V1
        assert list(s['name'] for s in cfg.services) == ['web']

        cfg = config.load(build_config_details({'version': {'image': 'busybox'}}))
        assert cfg.version == V1
        assert list(s['name'] for s in cfg.services) == ['version']

    def test_wrong_version_type(self):
        for version in [None, 1, 2, 2.0]:
            with pytest.raises(ConfigurationError) as excinfo:
                config.load(
                    build_config_details(
                        {'version': version},
                        filename='filename.yml',
                    )
                )

            assert 'Version in "filename.yml" is invalid - it should be a string.' \
                in excinfo.exconly()

    def test_unsupported_version(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {'version': '2.18'},
                    filename='filename.yml',
                )
            )

        assert 'Version in "filename.yml" is unsupported' in excinfo.exconly()
        assert VERSION_EXPLANATION in excinfo.exconly()

    def test_version_1_is_invalid(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'version': '1',
                        'web': {'image': 'busybox'},
                    },
                    filename='filename.yml',
                )
            )

        assert 'Version in "filename.yml" is invalid' in excinfo.exconly()
        assert VERSION_EXPLANATION in excinfo.exconly()

    def test_v1_file_with_version_is_invalid(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'version': '2',
                        'web': {'image': 'busybox'},
                    },
                    filename='filename.yml',
                )
            )

        assert 'Invalid top-level property "web"' in excinfo.exconly()
        assert VERSION_EXPLANATION in excinfo.exconly()

    def test_named_volume_config_empty(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'simple': {'image': 'busybox'}
            },
            'volumes': {
                'simple': None,
                'other': {},
            }
        })
        config_result = config.load(config_details)
        volumes = config_result.volumes
        assert 'simple' in volumes
        assert volumes['simple'] == {}
        assert volumes['other'] == {}

    def test_named_volume_numeric_driver_opt(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'simple': {'image': 'busybox'}
            },
            'volumes': {
                'simple': {'driver_opts': {'size': 42}},
            }
        })
        cfg = config.load(config_details)
        assert cfg.volumes['simple']['driver_opts']['size'] == '42'

    def test_volume_invalid_driver_opt(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'simple': {'image': 'busybox'}
            },
            'volumes': {
                'simple': {'driver_opts': {'size': True}},
            }
        })
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert 'driver_opts.size contains an invalid type' in exc.exconly()

    def test_named_volume_invalid_type_list(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'simple': {'image': 'busybox'}
            },
            'volumes': []
        })
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert "volume must be a mapping, not an array" in exc.exconly()

    def test_networks_invalid_type_list(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'simple': {'image': 'busybox'}
            },
            'networks': []
        })
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert "network must be a mapping, not an array" in exc.exconly()

    def test_load_service_with_name_version(self):
        with mock.patch('compose.config.config.log') as mock_logging:
            config_data = config.load(
                build_config_details({
                    'version': {
                        'image': 'busybox'
                    }
                }, 'working_dir', 'filename.yml')
            )

        assert 'Unexpected type for "version" key in "filename.yml"' \
            in mock_logging.warn.call_args[0][0]

        service_dicts = config_data.services
        assert service_sort(service_dicts) == service_sort([
            {
                'name': 'version',
                'image': 'busybox',
            }
        ])

    def test_load_throws_error_when_not_dict(self):
        with pytest.raises(ConfigurationError):
            config.load(
                build_config_details(
                    {'web': 'busybox:latest'},
                    'working_dir',
                    'filename.yml'
                )
            )

    def test_load_throws_error_when_not_dict_v2(self):
        with pytest.raises(ConfigurationError):
            config.load(
                build_config_details(
                    {'version': '2', 'services': {'web': 'busybox:latest'}},
                    'working_dir',
                    'filename.yml'
                )
            )

    def test_load_throws_error_with_invalid_network_fields(self):
        with pytest.raises(ConfigurationError):
            config.load(
                build_config_details({
                    'version': '2',
                    'services': {'web': 'busybox:latest'},
                    'networks': {
                        'invalid': {'foo', 'bar'}
                    }
                }, 'working_dir', 'filename.yml')
            )

    def test_load_config_link_local_ips_network(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': str(V2_1),
                'services': {
                    'web': {
                        'image': 'example/web',
                        'networks': {
                            'foobar': {
                                'aliases': ['foo', 'bar'],
                                'link_local_ips': ['169.254.8.8']
                            }
                        }
                    }
                },
                'networks': {'foobar': {}}
            }
        )

        details = config.ConfigDetails('.', [base_file])
        web_service = config.load(details).services[0]
        assert web_service['networks'] == {
            'foobar': {
                'aliases': ['foo', 'bar'],
                'link_local_ips': ['169.254.8.8']
            }
        }

    def test_load_config_service_labels(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2.1',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'labels': ['label_key=label_val']
                    },
                    'db': {
                        'image': 'example/db',
                        'labels': {
                            'label_key': 'label_val'
                        }
                    }
                },
            }
        )
        details = config.ConfigDetails('.', [base_file])
        service_dicts = config.load(details).services
        for service in service_dicts:
            assert service['labels'] == {
                'label_key': 'label_val'
            }

    def test_load_config_custom_resource_names(self):
        base_file = config.ConfigFile(
            'base.yaml', {
                'version': '3.5',
                'volumes': {
                    'abc': {
                        'name': 'xyz'
                    }
                },
                'networks': {
                    'abc': {
                        'name': 'xyz'
                    }
                },
                'secrets': {
                    'abc': {
                        'name': 'xyz'
                    }
                },
                'configs': {
                    'abc': {
                        'name': 'xyz'
                    }
                }
            }
        )
        details = config.ConfigDetails('.', [base_file])
        loaded_config = config.load(details)

        assert loaded_config.networks['abc'] == {'name': 'xyz'}
        assert loaded_config.volumes['abc'] == {'name': 'xyz'}
        assert loaded_config.secrets['abc']['name'] == 'xyz'
        assert loaded_config.configs['abc']['name'] == 'xyz'

    def test_load_config_volume_and_network_labels(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2.1',
                'services': {
                    'web': {
                        'image': 'example/web',
                    },
                },
                'networks': {
                    'with_label': {
                        'labels': {
                            'label_key': 'label_val'
                        }
                    }
                },
                'volumes': {
                    'with_label': {
                        'labels': {
                            'label_key': 'label_val'
                        }
                    }
                }
            }
        )

        details = config.ConfigDetails('.', [base_file])
        loaded_config = config.load(details)

        assert loaded_config.networks == {
            'with_label': {
                'labels': {
                    'label_key': 'label_val'
                }
            }
        }

        assert loaded_config.volumes == {
            'with_label': {
                'labels': {
                    'label_key': 'label_val'
                }
            }
        }

    def test_load_config_invalid_service_names(self):
        for invalid_name in ['?not?allowed', ' ', '', '!', '/', '\xe2']:
            with pytest.raises(ConfigurationError) as exc:
                config.load(build_config_details(
                    {invalid_name: {'image': 'busybox'}}))
            assert 'Invalid service name \'%s\'' % invalid_name in exc.exconly()

    def test_load_config_invalid_service_names_v2(self):
        for invalid_name in ['?not?allowed', ' ', '', '!', '/', '\xe2']:
            with pytest.raises(ConfigurationError) as exc:
                config.load(build_config_details(
                    {
                        'version': '2',
                        'services': {invalid_name: {'image': 'busybox'}},
                    }))
            assert 'Invalid service name \'%s\'' % invalid_name in exc.exconly()

    def test_load_with_invalid_field_name(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details(
                {
                    'version': '2',
                    'services': {
                        'web': {'image': 'busybox', 'name': 'bogus'},
                    }
                },
                'working_dir',
                'filename.yml',
            ))

        assert "Unsupported config option for services.web: 'name'" in exc.exconly()

    def test_load_with_invalid_field_name_v1(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details(
                {
                    'web': {'image': 'busybox', 'name': 'bogus'},
                },
                'working_dir',
                'filename.yml',
            ))

        assert "Unsupported config option for web: 'name'" in exc.exconly()

    def test_load_invalid_service_definition(self):
        config_details = build_config_details(
            {'web': 'wrong'},
            'working_dir',
            'filename.yml')
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert "service 'web' must be a mapping not a string." in exc.exconly()

    def test_load_with_empty_build_args(self):
        config_details = build_config_details(
            {
                'version': '2',
                'services': {
                    'web': {
                        'build': {
                            'context': os.getcwd(),
                            'args': None,
                        },
                    },
                },
            }
        )
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert (
            "services.web.build.args contains an invalid type, it should be an "
            "object, or an array" in exc.exconly()
        )

    def test_config_integer_service_name_raise_validation_error(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {1: {'image': 'busybox'}},
                    'working_dir',
                    'filename.yml'
                )
            )

        assert (
            "In file 'filename.yml', the service name 1 must be a quoted string, i.e. '1'" in
            excinfo.exconly()
        )

    def test_config_integer_service_name_raise_validation_error_v2(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'version': '2',
                        'services': {1: {'image': 'busybox'}}
                    },
                    'working_dir',
                    'filename.yml'
                )
            )

        assert (
            "In file 'filename.yml', the service name 1 must be a quoted string, i.e. '1'." in
            excinfo.exconly()
        )

    def test_config_integer_service_property_raise_validation_error(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details({
                    'version': '2.1',
                    'services': {'foobar': {'image': 'busybox', 1234: 'hah'}}
                }, 'working_dir', 'filename.yml')
            )

        assert (
            "Unsupported config option for services.foobar: '1234'" in excinfo.exconly()
        )

    def test_config_invalid_service_name_raise_validation_error(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details({
                    'version': '2',
                    'services': {
                        'test_app': {'build': '.'},
                        'mong\\o': {'image': 'mongo'},
                    }
                })
            )

            assert 'Invalid service name \'mong\\o\'' in excinfo.exconly()

    def test_config_duplicate_cache_from_values_validation_error(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(
                build_config_details({
                    'version': '2.3',
                    'services': {
                        'test': {'build': {'context': '.', 'cache_from': ['a', 'b', 'a']}}
                    }

                })
            )

        assert 'build.cache_from contains non-unique items' in exc.exconly()

    def test_load_with_multiple_files_v1(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'web': {
                    'image': 'example/web',
                    'links': ['db'],
                },
                'db': {
                    'image': 'example/db',
                },
            })
        override_file = config.ConfigFile(
            'override.yaml',
            {
                'web': {
                    'build': '/',
                    'volumes': ['/home/user/project:/code'],
                },
            })
        details = config.ConfigDetails('.', [base_file, override_file])

        service_dicts = config.load(details).services
        expected = [
            {
                'name': 'web',
                'build': {'context': os.path.abspath('/')},
                'volumes': [VolumeSpec.parse('/home/user/project:/code')],
                'links': ['db'],
            },
            {
                'name': 'db',
                'image': 'example/db',
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    def test_load_with_multiple_files_and_empty_override(self):
        base_file = config.ConfigFile(
            'base.yml',
            {'web': {'image': 'example/web'}})
        override_file = config.ConfigFile('override.yml', None)
        details = config.ConfigDetails('.', [base_file, override_file])

        with pytest.raises(ConfigurationError) as exc:
            config.load(details)
        error_msg = "Top level object in 'override.yml' needs to be an object"
        assert error_msg in exc.exconly()

    def test_load_with_multiple_files_and_empty_override_v2(self):
        base_file = config.ConfigFile(
            'base.yml',
            {'version': '2', 'services': {'web': {'image': 'example/web'}}})
        override_file = config.ConfigFile('override.yml', None)
        details = config.ConfigDetails('.', [base_file, override_file])

        with pytest.raises(ConfigurationError) as exc:
            config.load(details)
        error_msg = "Top level object in 'override.yml' needs to be an object"
        assert error_msg in exc.exconly()

    def test_load_with_multiple_files_and_empty_base(self):
        base_file = config.ConfigFile('base.yml', None)
        override_file = config.ConfigFile(
            'override.yml',
            {'web': {'image': 'example/web'}})
        details = config.ConfigDetails('.', [base_file, override_file])

        with pytest.raises(ConfigurationError) as exc:
            config.load(details)
        assert "Top level object in 'base.yml' needs to be an object" in exc.exconly()

    def test_load_with_multiple_files_and_empty_base_v2(self):
        base_file = config.ConfigFile('base.yml', None)
        override_file = config.ConfigFile(
            'override.tml',
            {'version': '2', 'services': {'web': {'image': 'example/web'}}}
        )
        details = config.ConfigDetails('.', [base_file, override_file])
        with pytest.raises(ConfigurationError) as exc:
            config.load(details)
        assert "Top level object in 'base.yml' needs to be an object" in exc.exconly()

    def test_load_with_multiple_files_and_extends_in_override_file(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'web': {'image': 'example/web'},
            })
        override_file = config.ConfigFile(
            'override.yaml',
            {
                'web': {
                    'extends': {
                        'file': 'common.yml',
                        'service': 'base',
                    },
                    'volumes': ['/home/user/project:/code'],
                },
            })
        details = config.ConfigDetails('.', [base_file, override_file])

        tmpdir = py.test.ensuretemp('config_test')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('common.yml').write("""
            base:
              labels: ['label=one']
        """)
        with tmpdir.as_cwd():
            service_dicts = config.load(details).services

        expected = [
            {
                'name': 'web',
                'image': 'example/web',
                'volumes': [VolumeSpec.parse('/home/user/project:/code')],
                'labels': {'label': 'one'},
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    def test_load_mixed_extends_resolution(self):
        main_file = config.ConfigFile(
            'main.yml', {
                'version': '2.2',
                'services': {
                    'prodweb': {
                        'extends': {
                            'service': 'web',
                            'file': 'base.yml'
                        },
                        'environment': {'PROD': 'true'},
                    },
                },
            }
        )

        tmpdir = pytest.ensuretemp('config_test')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('base.yml').write("""
            version: '2.2'
            services:
              base:
                image: base
              web:
                extends: base
        """)

        details = config.ConfigDetails('.', [main_file])
        with tmpdir.as_cwd():
            service_dicts = config.load(details).services
            assert service_dicts[0] == {
                'name': 'prodweb',
                'image': 'base',
                'environment': {'PROD': 'true'},
            }

    def test_load_with_multiple_files_and_invalid_override(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {'web': {'image': 'example/web'}})
        override_file = config.ConfigFile(
            'override.yaml',
            {'bogus': 'thing'})
        details = config.ConfigDetails('.', [base_file, override_file])

        with pytest.raises(ConfigurationError) as exc:
            config.load(details)
        assert "service 'bogus' must be a mapping not a string." in exc.exconly()
        assert "In file 'override.yaml'" in exc.exconly()

    def test_load_sorts_in_dependency_order(self):
        config_details = build_config_details({
            'web': {
                'image': 'busybox:latest',
                'links': ['db'],
            },
            'db': {
                'image': 'busybox:latest',
                'volumes_from': ['volume:ro']
            },
            'volume': {
                'image': 'busybox:latest',
                'volumes': ['/tmp'],
            }
        })
        services = config.load(config_details).services

        assert services[0]['name'] == 'volume'
        assert services[1]['name'] == 'db'
        assert services[2]['name'] == 'web'

    def test_load_with_extensions(self):
        config_details = build_config_details({
            'version': '2.3',
            'x-data': {
                'lambda': 3,
                'excess': [True, {}]
            }
        })

        config_data = config.load(config_details)
        assert config_data.services == []

    def test_config_build_configuration(self):
        service = config.load(
            build_config_details(
                {'web': {
                    'build': '.',
                    'dockerfile': 'Dockerfile-alt'
                }},
                'tests/fixtures/extends',
                'filename.yml'
            )
        ).services
        assert 'context' in service[0]['build']
        assert service[0]['build']['dockerfile'] == 'Dockerfile-alt'

    def test_config_build_configuration_v2(self):
        # service.dockerfile is invalid in v2
        with pytest.raises(ConfigurationError):
            config.load(
                build_config_details(
                    {
                        'version': '2',
                        'services': {
                            'web': {
                                'build': '.',
                                'dockerfile': 'Dockerfile-alt'
                            }
                        }
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        service = config.load(
            build_config_details({
                'version': '2',
                'services': {
                    'web': {
                        'build': '.'
                    }
                }
            }, 'tests/fixtures/extends', 'filename.yml')
        ).services[0]
        assert 'context' in service['build']

        service = config.load(
            build_config_details(
                {
                    'version': '2',
                    'services': {
                        'web': {
                            'build': {
                                'context': '.',
                                'dockerfile': 'Dockerfile-alt'
                            }
                        }
                    }
                },
                'tests/fixtures/extends',
                'filename.yml'
            )
        ).services
        assert 'context' in service[0]['build']
        assert service[0]['build']['dockerfile'] == 'Dockerfile-alt'

    def test_load_with_buildargs(self):
        service = config.load(
            build_config_details(
                {
                    'version': '2',
                    'services': {
                        'web': {
                            'build': {
                                'context': '.',
                                'dockerfile': 'Dockerfile-alt',
                                'args': {
                                    'opt1': 42,
                                    'opt2': 'foobar'
                                }
                            }
                        }
                    }
                },
                'tests/fixtures/extends',
                'filename.yml'
            )
        ).services[0]
        assert 'args' in service['build']
        assert 'opt1' in service['build']['args']
        assert isinstance(service['build']['args']['opt1'], str)
        assert service['build']['args']['opt1'] == '42'
        assert service['build']['args']['opt2'] == 'foobar'

    def test_load_build_labels_dict(self):
        service = config.load(
            build_config_details(
                {
                    'version': str(V3_3),
                    'services': {
                        'web': {
                            'build': {
                                'context': '.',
                                'dockerfile': 'Dockerfile-alt',
                                'labels': {
                                    'label1': 42,
                                    'label2': 'foobar'
                                }
                            }
                        }
                    }
                },
                'tests/fixtures/extends',
                'filename.yml'
            )
        ).services[0]
        assert 'labels' in service['build']
        assert 'label1' in service['build']['labels']
        assert service['build']['labels']['label1'] == '42'
        assert service['build']['labels']['label2'] == 'foobar'

    def test_load_build_labels_list(self):
        base_file = config.ConfigFile(
            'base.yml',
            {
                'version': '2.3',
                'services': {
                    'web': {
                        'build': {
                            'context': '.',
                            'labels': ['foo=bar', 'baz=true', 'foobar=1']
                        },
                    },
                },
            }
        )

        details = config.ConfigDetails('.', [base_file])
        service = config.load(details).services[0]
        assert service['build']['labels'] == {
            'foo': 'bar', 'baz': 'true', 'foobar': '1'
        }

    def test_build_args_allow_empty_properties(self):
        service = config.load(
            build_config_details(
                {
                    'version': '2',
                    'services': {
                        'web': {
                            'build': {
                                'context': '.',
                                'dockerfile': 'Dockerfile-alt',
                                'args': {
                                    'foo': None
                                }
                            }
                        }
                    }
                },
                'tests/fixtures/extends',
                'filename.yml'
            )
        ).services[0]
        assert 'args' in service['build']
        assert 'foo' in service['build']['args']
        assert service['build']['args']['foo'] == ''

    # If build argument is None then it will be converted to the empty
    # string. Make sure that int zero kept as it is, i.e. not converted to
    # the empty string
    def test_build_args_check_zero_preserved(self):
        service = config.load(
            build_config_details(
                {
                    'version': '2',
                    'services': {
                        'web': {
                            'build': {
                                'context': '.',
                                'dockerfile': 'Dockerfile-alt',
                                'args': {
                                    'foo': 0
                                }
                            }
                        }
                    }
                },
                'tests/fixtures/extends',
                'filename.yml'
            )
        ).services[0]
        assert 'args' in service['build']
        assert 'foo' in service['build']['args']
        assert service['build']['args']['foo'] == '0'

    def test_load_with_multiple_files_mismatched_networks_format(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'networks': {
                            'foobar': {'aliases': ['foo', 'bar']}
                        }
                    }
                },
                'networks': {'foobar': {}, 'baz': {}}
            }
        )

        override_file = config.ConfigFile(
            'override.yaml',
            {
                'version': '2',
                'services': {
                    'web': {
                        'networks': ['baz']
                    }
                }
            }
        )

        details = config.ConfigDetails('.', [base_file, override_file])
        web_service = config.load(details).services[0]
        assert web_service['networks'] == {
            'foobar': {'aliases': ['foo', 'bar']},
            'baz': None
        }

    def test_load_with_multiple_files_v2(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'depends_on': ['db'],
                    },
                    'db': {
                        'image': 'example/db',
                    }
                },
            })
        override_file = config.ConfigFile(
            'override.yaml',
            {
                'version': '2',
                'services': {
                    'web': {
                        'build': '/',
                        'volumes': ['/home/user/project:/code'],
                        'depends_on': ['other'],
                    },
                    'other': {
                        'image': 'example/other',
                    }
                }
            })
        details = config.ConfigDetails('.', [base_file, override_file])

        service_dicts = config.load(details).services
        expected = [
            {
                'name': 'web',
                'build': {'context': os.path.abspath('/')},
                'image': 'example/web',
                'volumes': [VolumeSpec.parse('/home/user/project:/code')],
                'depends_on': {
                    'db': {'condition': 'service_started'},
                    'other': {'condition': 'service_started'},
                },
            },
            {
                'name': 'db',
                'image': 'example/db',
            },
            {
                'name': 'other',
                'image': 'example/other',
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    @mock.patch.dict(os.environ)
    def test_load_with_multiple_files_v3_2(self):
        os.environ['COMPOSE_CONVERT_WINDOWS_PATHS'] = 'true'
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.2',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'volumes': [
                            {'source': '/a', 'target': '/b', 'type': 'bind'},
                            {'source': 'vol', 'target': '/x', 'type': 'volume', 'read_only': True}
                        ],
                        'stop_grace_period': '30s',
                    }
                },
                'volumes': {'vol': {}}
            }
        )

        override_file = config.ConfigFile(
            'override.yaml',
            {
                'version': '3.2',
                'services': {
                    'web': {
                        'volumes': ['/c:/b', '/anonymous']
                    }
                }
            }
        )
        details = config.ConfigDetails('.', [base_file, override_file])
        service_dicts = config.load(details).services
        svc_volumes = map(lambda v: v.repr(), service_dicts[0]['volumes'])
        for vol in svc_volumes:
            assert vol in [
                '/anonymous',
                '/c:/b:rw',
                {'source': 'vol', 'target': '/x', 'type': 'volume', 'read_only': True}
            ]
        assert service_dicts[0]['stop_grace_period'] == '30s'

    @mock.patch.dict(os.environ)
    def test_volume_mode_override(self):
        os.environ['COMPOSE_CONVERT_WINDOWS_PATHS'] = 'true'
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2.3',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'volumes': ['/c:/b:rw']
                    }
                },
            }
        )

        override_file = config.ConfigFile(
            'override.yaml',
            {
                'version': '2.3',
                'services': {
                    'web': {
                        'volumes': ['/c:/b:ro']
                    }
                }
            }
        )
        details = config.ConfigDetails('.', [base_file, override_file])
        service_dicts = config.load(details).services
        svc_volumes = list(map(lambda v: v.repr(), service_dicts[0]['volumes']))
        assert svc_volumes == ['/c:/b:ro']

    def test_undeclared_volume_v2(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2',
                'services': {
                    'web': {
                        'image': 'busybox:latest',
                        'volumes': ['data0028:/data:ro'],
                    },
                },
            }
        )
        details = config.ConfigDetails('.', [base_file])
        with pytest.raises(ConfigurationError):
            config.load(details)

        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '2',
                'services': {
                    'web': {
                        'image': 'busybox:latest',
                        'volumes': ['./data0028:/data:ro'],
                    },
                },
            }
        )
        details = config.ConfigDetails('.', [base_file])
        config_data = config.load(details)
        volume = config_data.services[0].get('volumes')[0]
        assert not volume.is_named_volume

    def test_undeclared_volume_v1(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'web': {
                    'image': 'busybox:latest',
                    'volumes': ['data0028:/data:ro'],
                },
            }
        )
        details = config.ConfigDetails('.', [base_file])
        config_data = config.load(details)
        volume = config_data.services[0].get('volumes')[0]
        assert volume.external == 'data0028'
        assert volume.is_named_volume

    def test_volumes_long_syntax(self):
        base_file = config.ConfigFile(
            'base.yaml', {
                'version': '2.3',
                'services': {
                    'web': {
                        'image': 'busybox:latest',
                        'volumes': [
                            {
                                'target': '/anonymous', 'type': 'volume'
                            }, {
                                'source': '/abc', 'target': '/xyz', 'type': 'bind'
                            }, {
                                'source': '\\\\.\\pipe\\abcd', 'target': '/named_pipe', 'type': 'npipe'
                            }, {
                                'type': 'tmpfs', 'target': '/tmpfs'
                            }
                        ]
                    },
                },
            },
        )
        details = config.ConfigDetails('.', [base_file])
        config_data = config.load(details)
        volumes = config_data.services[0].get('volumes')
        anon_volume = [v for v in volumes if v.target == '/anonymous'][0]
        tmpfs_mount = [v for v in volumes if v.type == 'tmpfs'][0]
        host_mount = [v for v in volumes if v.type == 'bind'][0]
        npipe_mount = [v for v in volumes if v.type == 'npipe'][0]

        assert anon_volume.type == 'volume'
        assert not anon_volume.is_named_volume

        assert tmpfs_mount.target == '/tmpfs'
        assert not tmpfs_mount.is_named_volume

        assert host_mount.source == '/abc'
        assert host_mount.target == '/xyz'
        assert not host_mount.is_named_volume

        assert npipe_mount.source == '\\\\.\\pipe\\abcd'
        assert npipe_mount.target == '/named_pipe'
        assert not npipe_mount.is_named_volume

    def test_load_bind_mount_relative_path(self):
        expected_source = 'C:\\tmp\\web' if IS_WINDOWS_PLATFORM else '/tmp/web'
        base_file = config.ConfigFile(
            'base.yaml', {
                'version': '3.4',
                'services': {
                    'web': {
                        'image': 'busybox:latest',
                        'volumes': [
                            {'type': 'bind', 'source': './web', 'target': '/web'},
                        ],
                    },
                },
            },
        )

        details = config.ConfigDetails('/tmp', [base_file])
        config_data = config.load(details)
        mount = config_data.services[0].get('volumes')[0]
        assert mount.target == '/web'
        assert mount.type == 'bind'
        assert mount.source == expected_source

    def test_load_bind_mount_relative_path_with_tilde(self):
        base_file = config.ConfigFile(
            'base.yaml', {
                'version': '3.4',
                'services': {
                    'web': {
                        'image': 'busybox:latest',
                        'volumes': [
                            {'type': 'bind', 'source': '~/web', 'target': '/web'},
                        ],
                    },
                },
            },
        )

        details = config.ConfigDetails('.', [base_file])
        config_data = config.load(details)
        mount = config_data.services[0].get('volumes')[0]
        assert mount.target == '/web'
        assert mount.type == 'bind'
        assert (
            not mount.source.startswith('~') and mount.source.endswith(
                '{}web'.format(os.path.sep)
            )
        )

    def test_config_invalid_ipam_config(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'version': str(V2_1),
                        'networks': {
                            'foo': {
                                'driver': 'default',
                                'ipam': {
                                    'driver': 'default',
                                    'config': ['172.18.0.0/16'],
                                }
                            }
                        }
                    },
                    filename='filename.yml',
                )
            )
        assert ('networks.foo.ipam.config contains an invalid type,'
                ' it should be an object') in excinfo.exconly()

    def test_config_valid_ipam_config(self):
        ipam_config = {
            'subnet': '172.28.0.0/16',
            'ip_range': '172.28.5.0/24',
            'gateway': '172.28.5.254',
            'aux_addresses': {
                'host1': '172.28.1.5',
                'host2': '172.28.1.6',
                'host3': '172.28.1.7',
            },
        }
        networks = config.load(
            build_config_details(
                {
                    'version': str(V2_1),
                    'networks': {
                        'foo': {
                            'driver': 'default',
                            'ipam': {
                                'driver': 'default',
                                'config': [ipam_config],
                            }
                        }
                    }
                },
                filename='filename.yml',
            )
        ).networks

        assert 'foo' in networks
        assert networks['foo']['ipam']['config'] == [ipam_config]

    def test_config_valid_service_names(self):
        for valid_name in ['_', '-', '.__.', '_what-up.', 'what_.up----', 'whatup']:
            services = config.load(
                build_config_details(
                    {valid_name: {'image': 'busybox'}},
                    'tests/fixtures/extends',
                    'common.yml')).services
            assert services[0]['name'] == valid_name

    def test_config_hint(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'foo': {'image': 'busybox', 'privilige': 'something'},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "(did you mean 'privileged'?)" in excinfo.exconly()

    def test_load_errors_on_uppercase_with_no_image(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details({
                'Foo': {'build': '.'},
            }, 'tests/fixtures/build-ctx'))
            assert "Service 'Foo' contains uppercase characters" in exc.exconly()

    def test_invalid_config_v1(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'foo': {'image': 1},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "foo.image contains an invalid type, it should be a string" \
            in excinfo.exconly()

    def test_invalid_config_v2(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'version': '2',
                        'services': {
                            'foo': {'image': 1},
                        },
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "services.foo.image contains an invalid type, it should be a string" \
            in excinfo.exconly()

    def test_invalid_config_build_and_image_specified_v1(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'foo': {'image': 'busybox', 'build': '.'},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "foo has both an image and build path specified." in excinfo.exconly()

    def test_invalid_config_type_should_be_an_array(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'foo': {'image': 'busybox', 'links': 'an_link'},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "foo.links contains an invalid type, it should be an array" \
            in excinfo.exconly()

    def test_invalid_config_not_a_dictionary(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    ['foo', 'lol'],
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "Top level object in 'filename.yml' needs to be an object" \
            in excinfo.exconly()

    def test_invalid_config_not_unique_items(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'web': {'build': '.', 'devices': ['/dev/foo:/dev/foo', '/dev/foo:/dev/foo']}
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "has non-unique elements" in excinfo.exconly()

    def test_invalid_list_of_strings_format(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'web': {'build': '.', 'command': [1]}
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "web.command contains 1, which is an invalid type, it should be a string" \
            in excinfo.exconly()

    def test_load_config_dockerfile_without_build_raises_error_v1(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details({
                'web': {
                    'image': 'busybox',
                    'dockerfile': 'Dockerfile.alt'
                }
            }))

        assert "web has both an image and alternate Dockerfile." in exc.exconly()

    def test_config_extra_hosts_string_raises_validation_error(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {'web': {
                        'image': 'busybox',
                        'extra_hosts': 'somehost:162.242.195.82'
                    }},
                    'working_dir',
                    'filename.yml'
                )
            )

        assert "web.extra_hosts contains an invalid type" \
            in excinfo.exconly()

    def test_config_extra_hosts_list_of_dicts_validation_error(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {'web': {
                        'image': 'busybox',
                        'extra_hosts': [
                            {'somehost': '162.242.195.82'},
                            {'otherhost': '50.31.209.229'}
                        ]
                    }},
                    'working_dir',
                    'filename.yml'
                )
            )

        assert "web.extra_hosts contains {\"somehost\": \"162.242.195.82\"}, " \
               "which is an invalid type, it should be a string" \
            in excinfo.exconly()

    def test_config_ulimits_invalid_keys_validation_error(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details(
                {
                    'web': {
                        'image': 'busybox',
                        'ulimits': {
                            'nofile': {
                                "not_soft_or_hard": 100,
                                "soft": 10000,
                                "hard": 20000,
                            }
                        }
                    }
                },
                'working_dir',
                'filename.yml'))

        assert "web.ulimits.nofile contains unsupported option: 'not_soft_or_hard'" \
            in exc.exconly()

    def test_config_ulimits_required_keys_validation_error(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details(
                {
                    'web': {
                        'image': 'busybox',
                        'ulimits': {'nofile': {"soft": 10000}}
                    }
                },
                'working_dir',
                'filename.yml'))
        assert "web.ulimits.nofile" in exc.exconly()
        assert "'hard' is a required property" in exc.exconly()

    def test_config_ulimits_soft_greater_than_hard_error(self):
        expected = "'soft' value can not be greater than 'hard' value"

        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details(
                {
                    'web': {
                        'image': 'busybox',
                        'ulimits': {
                            'nofile': {"soft": 10000, "hard": 1000}
                        }
                    }
                },
                'working_dir',
                'filename.yml'))
        assert expected in exc.exconly()

    def test_valid_config_which_allows_two_type_definitions(self):
        expose_values = [["8000"], [8000]]
        for expose in expose_values:
            service = config.load(
                build_config_details(
                    {'web': {
                        'image': 'busybox',
                        'expose': expose
                    }},
                    'working_dir',
                    'filename.yml'
                )
            ).services
            assert service[0]['expose'] == expose

    def test_valid_config_oneof_string_or_list(self):
        entrypoint_values = [["sh"], "sh"]
        for entrypoint in entrypoint_values:
            service = config.load(
                build_config_details(
                    {'web': {
                        'image': 'busybox',
                        'entrypoint': entrypoint
                    }},
                    'working_dir',
                    'filename.yml'
                )
            ).services
            assert service[0]['entrypoint'] == entrypoint

    def test_logs_warning_for_boolean_in_environment(self):
        config_details = build_config_details({
            'web': {
                'image': 'busybox',
                'environment': {'SHOW_STUFF': True}
            }
        })

        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)

        assert "contains true, which is an invalid type" in exc.exconly()

    def test_config_valid_environment_dict_key_contains_dashes(self):
        services = config.load(
            build_config_details(
                {'web': {
                    'image': 'busybox',
                    'environment': {'SPRING_JPA_HIBERNATE_DDL-AUTO': 'none'}
                }},
                'working_dir',
                'filename.yml'
            )
        ).services
        assert services[0]['environment']['SPRING_JPA_HIBERNATE_DDL-AUTO'] == 'none'

    def test_load_yaml_with_yaml_error(self):
        tmpdir = py.test.ensuretemp('invalid_yaml_test')
        self.addCleanup(tmpdir.remove)
        invalid_yaml_file = tmpdir.join('docker-compose.yml')
        invalid_yaml_file.write("""
            web:
              this is bogus: ok: what
        """)
        with pytest.raises(ConfigurationError) as exc:
            config.load_yaml(str(invalid_yaml_file))

        assert 'line 3, column 32' in exc.exconly()

    def test_load_yaml_with_bom(self):
        tmpdir = py.test.ensuretemp('bom_yaml')
        self.addCleanup(tmpdir.remove)
        bom_yaml = tmpdir.join('docker-compose.yml')
        with codecs.open(str(bom_yaml), 'w', encoding='utf-8') as f:
            f.write('''\ufeff
                version: '2.3'
                volumes:
                    park_bom:
            ''')
        assert config.load_yaml(str(bom_yaml)) == {
            'version': '2.3',
            'volumes': {'park_bom': None}
        }

    def test_validate_extra_hosts_invalid(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details({
                'web': {
                    'image': 'alpine',
                    'extra_hosts': "www.example.com: 192.168.0.17",
                }
            }))
        assert "web.extra_hosts contains an invalid type" in exc.exconly()

    def test_validate_extra_hosts_invalid_list(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details({
                'web': {
                    'image': 'alpine',
                    'extra_hosts': [
                        {'www.example.com': '192.168.0.17'},
                        {'api.example.com': '192.168.0.18'}
                    ],
                }
            }))
        assert "which is an invalid type" in exc.exconly()

    def test_normalize_dns_options(self):
        actual = config.load(build_config_details({
            'web': {
                'image': 'alpine',
                'dns': '8.8.8.8',
                'dns_search': 'domain.local',
            }
        }))
        assert actual.services == [
            {
                'name': 'web',
                'image': 'alpine',
                'dns': ['8.8.8.8'],
                'dns_search': ['domain.local'],
            }
        ]

    def test_tmpfs_option(self):
        actual = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'alpine',
                    'tmpfs': '/run',
                }
            }
        }))
        assert actual.services == [
            {
                'name': 'web',
                'image': 'alpine',
                'tmpfs': ['/run'],
            }
        ]

    def test_oom_score_adj_option(self):

        actual = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'alpine',
                    'oom_score_adj': 500
                }
            }
        }))

        assert actual.services == [
            {
                'name': 'web',
                'image': 'alpine',
                'oom_score_adj': 500
            }
        ]

    def test_swappiness_option(self):
        actual = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'alpine',
                    'mem_swappiness': 10,
                }
            }
        }))
        assert actual.services == [
            {
                'name': 'web',
                'image': 'alpine',
                'mem_swappiness': 10,
            }
        ]

    def test_group_add_option(self):
        actual = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'alpine',
                    'group_add': ["docker", 777]
                }
            }
        }))

        assert actual.services == [
            {
                'name': 'web',
                'image': 'alpine',
                'group_add': ["docker", 777]
            }
        ]

    def test_dns_opt_option(self):
        actual = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'alpine',
                    'dns_opt': ["use-vc", "no-tld-query"]
                }
            }
        }))

        assert actual.services == [
            {
                'name': 'web',
                'image': 'alpine',
                'dns_opt': ["use-vc", "no-tld-query"]
            }
        ]

    def test_isolation_option(self):
        actual = config.load(build_config_details({
            'version': str(V2_1),
            'services': {
                'web': {
                    'image': 'win10',
                    'isolation': 'hyperv'
                }
            }
        }))

        assert actual.services == [
            {
                'name': 'web',
                'image': 'win10',
                'isolation': 'hyperv',
            }
        ]

    def test_runtime_option(self):
        actual = config.load(build_config_details({
            'version': str(V2_3),
            'services': {
                'web': {
                    'image': 'nvidia/cuda',
                    'runtime': 'nvidia'
                }
            }
        }))

        assert actual.services == [
            {
                'name': 'web',
                'image': 'nvidia/cuda',
                'runtime': 'nvidia',
            }
        ]

    def test_merge_service_dicts_from_files_with_extends_in_base(self):
        base = {
            'volumes': ['.:/app'],
            'extends': {'service': 'app'}
        }
        override = {
            'image': 'alpine:edge',
        }
        actual = config.merge_service_dicts_from_files(
            base,
            override,
            DEFAULT_VERSION)
        assert actual == {
            'image': 'alpine:edge',
            'volumes': ['.:/app'],
            'extends': {'service': 'app'}
        }

    def test_merge_service_dicts_from_files_with_extends_in_override(self):
        base = {
            'volumes': ['.:/app'],
            'extends': {'service': 'app'}
        }
        override = {
            'image': 'alpine:edge',
            'extends': {'service': 'foo'}
        }
        actual = config.merge_service_dicts_from_files(
            base,
            override,
            DEFAULT_VERSION)
        assert actual == {
            'image': 'alpine:edge',
            'volumes': ['.:/app'],
            'extends': {'service': 'foo'}
        }

    def test_merge_service_dicts_heterogeneous(self):
        base = {
            'volumes': ['.:/app'],
            'ports': ['5432']
        }
        override = {
            'image': 'alpine:edge',
            'ports': [5432]
        }
        actual = config.merge_service_dicts_from_files(
            base,
            override,
            DEFAULT_VERSION)
        assert actual == {
            'image': 'alpine:edge',
            'volumes': ['.:/app'],
            'ports': types.ServicePort.parse('5432')
        }

    def test_merge_service_dicts_heterogeneous_2(self):
        base = {
            'volumes': ['.:/app'],
            'ports': [5432]
        }
        override = {
            'image': 'alpine:edge',
            'ports': ['5432']
        }
        actual = config.merge_service_dicts_from_files(
            base,
            override,
            DEFAULT_VERSION)
        assert actual == {
            'image': 'alpine:edge',
            'volumes': ['.:/app'],
            'ports': types.ServicePort.parse('5432')
        }

    def test_merge_service_dicts_ports_sorting(self):
        base = {
            'ports': [5432]
        }
        override = {
            'image': 'alpine:edge',
            'ports': ['5432/udp']
        }
        actual = config.merge_service_dicts_from_files(
            base,
            override,
            DEFAULT_VERSION)
        assert len(actual['ports']) == 2
        assert types.ServicePort.parse('5432')[0] in actual['ports']
        assert types.ServicePort.parse('5432/udp')[0] in actual['ports']

    def test_merge_service_dicts_heterogeneous_volumes(self):
        base = {
            'volumes': ['/a:/b', '/x:/z'],
        }

        override = {
            'image': 'alpine:edge',
            'volumes': [
                {'source': '/e', 'target': '/b', 'type': 'bind'},
                {'source': '/c', 'target': '/d', 'type': 'bind'}
            ]
        }

        actual = config.merge_service_dicts_from_files(
            base, override, V3_2
        )

        assert actual['volumes'] == [
            {'source': '/e', 'target': '/b', 'type': 'bind'},
            {'source': '/c', 'target': '/d', 'type': 'bind'},
            '/x:/z'
        ]

    def test_merge_logging_v1(self):
        base = {
            'image': 'alpine:edge',
            'log_driver': 'something',
            'log_opt': {'foo': 'three'},
        }
        override = {
            'image': 'alpine:edge',
            'command': 'true',
        }
        actual = config.merge_service_dicts(base, override, V1)
        assert actual == {
            'image': 'alpine:edge',
            'log_driver': 'something',
            'log_opt': {'foo': 'three'},
            'command': 'true',
        }

    def test_merge_logging_v2(self):
        base = {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000',
                    'timeout': '23'
                }
            }
        }
        override = {
            'logging': {
                'options': {
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000',
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

    def test_merge_logging_v2_override_driver(self):
        base = {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000',
                    'timeout': '23'
                }
            }
        }
        override = {
            'logging': {
                'driver': 'syslog',
                'options': {
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'syslog',
                'options': {
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

    def test_merge_logging_v2_no_base_driver(self):
        base = {
            'image': 'alpine:edge',
            'logging': {
                'options': {
                    'frequency': '2000',
                    'timeout': '23'
                }
            }
        }
        override = {
            'logging': {
                'driver': 'json-file',
                'options': {
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000',
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

    def test_merge_logging_v2_no_drivers(self):
        base = {
            'image': 'alpine:edge',
            'logging': {
                'options': {
                    'frequency': '2000',
                    'timeout': '23'
                }
            }
        }
        override = {
            'logging': {
                'options': {
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'options': {
                    'frequency': '2000',
                    'timeout': '360',
                    'pretty-print': 'on'
                }
            }
        }

    def test_merge_logging_v2_no_override_options(self):
        base = {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000',
                    'timeout': '23'
                }
            }
        }
        override = {
            'logging': {
                'driver': 'syslog'
            }
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'syslog',
            }
        }

    def test_merge_logging_v2_no_base(self):
        base = {
            'image': 'alpine:edge'
        }
        override = {
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000'
                }
            }
        }
        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'json-file',
                'options': {
                    'frequency': '2000'
                }
            }
        }

    def test_merge_logging_v2_no_override(self):
        base = {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'syslog',
                'options': {
                    'frequency': '2000'
                }
            }
        }
        override = {}
        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'alpine:edge',
            'logging': {
                'driver': 'syslog',
                'options': {
                    'frequency': '2000'
                }
            }
        }

    def test_merge_mixed_ports(self):
        base = {
            'image': 'busybox:latest',
            'command': 'top',
            'ports': [
                {
                    'target': '1245',
                    'published': '1245',
                    'protocol': 'udp',
                }
            ]
        }

        override = {
            'ports': ['1245:1245/udp']
        }

        actual = config.merge_service_dicts(base, override, V3_1)
        assert actual == {
            'image': 'busybox:latest',
            'command': 'top',
            'ports': [types.ServicePort('1245', '1245', 'udp', None, None)]
        }

    def test_merge_depends_on_no_override(self):
        base = {
            'image': 'busybox',
            'depends_on': {
                'app1': {'condition': 'service_started'},
                'app2': {'condition': 'service_healthy'}
            }
        }
        override = {}
        actual = config.merge_service_dicts(base, override, V2_1)
        assert actual == base

    def test_merge_depends_on_mixed_syntax(self):
        base = {
            'image': 'busybox',
            'depends_on': {
                'app1': {'condition': 'service_started'},
                'app2': {'condition': 'service_healthy'}
            }
        }
        override = {
            'depends_on': ['app3']
        }

        actual = config.merge_service_dicts(base, override, V2_1)
        assert actual == {
            'image': 'busybox',
            'depends_on': {
                'app1': {'condition': 'service_started'},
                'app2': {'condition': 'service_healthy'},
                'app3': {'condition': 'service_started'}
            }
        }

    def test_empty_environment_key_allowed(self):
        service_dict = config.load(
            build_config_details(
                {
                    'web': {
                        'build': '.',
                        'environment': {
                            'POSTGRES_PASSWORD': ''
                        },
                    },
                },
                '.',
                None,
            )
        ).services[0]
        assert service_dict['environment']['POSTGRES_PASSWORD'] == ''

    def test_merge_pid(self):
        # Regression: https://github.com/docker/compose/issues/4184
        base = {
            'image': 'busybox',
            'pid': 'host'
        }

        override = {
            'labels': {'com.docker.compose.test': 'yes'}
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual == {
            'image': 'busybox',
            'pid': 'host',
            'labels': {'com.docker.compose.test': 'yes'}
        }

    def test_merge_different_secrets(self):
        base = {
            'image': 'busybox',
            'secrets': [
                {'source': 'src.txt'}
            ]
        }
        override = {'secrets': ['other-src.txt']}

        actual = config.merge_service_dicts(base, override, V3_1)
        assert secret_sort(actual['secrets']) == secret_sort([
            {'source': 'src.txt'},
            {'source': 'other-src.txt'}
        ])

    def test_merge_secrets_override(self):
        base = {
            'image': 'busybox',
            'secrets': ['src.txt'],
        }
        override = {
            'secrets': [
                {
                    'source': 'src.txt',
                    'target': 'data.txt',
                    'mode': 0o400
                }
            ]
        }
        actual = config.merge_service_dicts(base, override, V3_1)
        assert actual['secrets'] == override['secrets']

    def test_merge_different_configs(self):
        base = {
            'image': 'busybox',
            'configs': [
                {'source': 'src.txt'}
            ]
        }
        override = {'configs': ['other-src.txt']}

        actual = config.merge_service_dicts(base, override, V3_3)
        assert secret_sort(actual['configs']) == secret_sort([
            {'source': 'src.txt'},
            {'source': 'other-src.txt'}
        ])

    def test_merge_configs_override(self):
        base = {
            'image': 'busybox',
            'configs': ['src.txt'],
        }
        override = {
            'configs': [
                {
                    'source': 'src.txt',
                    'target': 'data.txt',
                    'mode': 0o400
                }
            ]
        }
        actual = config.merge_service_dicts(base, override, V3_3)
        assert actual['configs'] == override['configs']

    def test_merge_deploy(self):
        base = {
            'image': 'busybox',
        }
        override = {
            'deploy': {
                'mode': 'global',
                'restart_policy': {
                    'condition': 'on-failure'
                }
            }
        }
        actual = config.merge_service_dicts(base, override, V3_0)
        assert actual['deploy'] == override['deploy']

    def test_merge_deploy_override(self):
        base = {
            'deploy': {
                'endpoint_mode': 'vip',
                'labels': ['com.docker.compose.a=1', 'com.docker.compose.b=2'],
                'mode': 'replicated',
                'placement': {
                    'constraints': [
                        'node.role == manager', 'engine.labels.aws == true'
                    ],
                    'preferences': [
                        {'spread': 'node.labels.zone'}, {'spread': 'x.d.z'}
                    ]
                },
                'replicas': 3,
                'resources': {
                    'limits': {'cpus': '0.50', 'memory': '50m'},
                    'reservations': {
                        'cpus': '0.1',
                        'generic_resources': [
                            {'discrete_resource_spec': {'kind': 'abc', 'value': 123}}
                        ],
                        'memory': '15m'
                    }
                },
                'restart_policy': {'condition': 'any', 'delay': '10s'},
                'update_config': {'delay': '10s', 'max_failure_ratio': 0.3}
            },
            'image': 'hello-world'
        }
        override = {
            'deploy': {
                'labels': {
                    'com.docker.compose.b': '21', 'com.docker.compose.c': '3'
                },
                'placement': {
                    'constraints': ['node.role == worker', 'engine.labels.dev == true'],
                    'preferences': [{'spread': 'node.labels.zone'}, {'spread': 'x.d.s'}]
                },
                'resources': {
                    'limits': {'memory': '200m'},
                    'reservations': {
                        'cpus': '0.78',
                        'generic_resources': [
                            {'discrete_resource_spec': {'kind': 'abc', 'value': 134}},
                            {'discrete_resource_spec': {'kind': 'xyz', 'value': 0.1}}
                        ]
                    }
                },
                'restart_policy': {'condition': 'on-failure', 'max_attempts': 42},
                'update_config': {'max_failure_ratio': 0.712, 'parallelism': 4}
            }
        }
        actual = config.merge_service_dicts(base, override, V3_5)
        assert actual['deploy'] == {
            'mode': 'replicated',
            'endpoint_mode': 'vip',
            'labels': {
                'com.docker.compose.a': '1',
                'com.docker.compose.b': '21',
                'com.docker.compose.c': '3'
            },
            'placement': {
                'constraints': [
                    'engine.labels.aws == true', 'engine.labels.dev == true',
                    'node.role == manager', 'node.role == worker'
                ],
                'preferences': [
                    {'spread': 'node.labels.zone'}, {'spread': 'x.d.s'}, {'spread': 'x.d.z'}
                ]
            },
            'replicas': 3,
            'resources': {
                'limits': {'cpus': '0.50', 'memory': '200m'},
                'reservations': {
                    'cpus': '0.78',
                    'memory': '15m',
                    'generic_resources': [
                        {'discrete_resource_spec': {'kind': 'abc', 'value': 134}},
                        {'discrete_resource_spec': {'kind': 'xyz', 'value': 0.1}},
                    ]
                }
            },
            'restart_policy': {
                'condition': 'on-failure',
                'delay': '10s',
                'max_attempts': 42,
            },
            'update_config': {
                'max_failure_ratio': 0.712,
                'delay': '10s',
                'parallelism': 4
            }
        }

    def test_merge_credential_spec(self):
        base = {
            'image': 'bb',
            'credential_spec': {
                'file': '/hello-world',
            }
        }

        override = {
            'credential_spec': {
                'registry': 'revolution.com',
            }
        }

        actual = config.merge_service_dicts(base, override, V3_3)
        assert actual['credential_spec'] == override['credential_spec']

    def test_merge_scale(self):
        base = {
            'image': 'bar',
            'scale': 2,
        }

        override = {
            'scale': 4,
        }

        actual = config.merge_service_dicts(base, override, V2_2)
        assert actual == {'image': 'bar', 'scale': 4}

    def test_merge_blkio_config(self):
        base = {
            'image': 'bar',
            'blkio_config': {
                'weight': 300,
                'weight_device': [
                    {'path': '/dev/sda1', 'weight': 200}
                ],
                'device_read_iops': [
                    {'path': '/dev/sda1', 'rate': 300}
                ],
                'device_write_iops': [
                    {'path': '/dev/sda1', 'rate': 1000}
                ]
            }
        }

        override = {
            'blkio_config': {
                'weight': 450,
                'weight_device': [
                    {'path': '/dev/sda2', 'weight': 400}
                ],
                'device_read_iops': [
                    {'path': '/dev/sda1', 'rate': 2000}
                ],
                'device_read_bps': [
                    {'path': '/dev/sda1', 'rate': 1024}
                ]
            }
        }

        actual = config.merge_service_dicts(base, override, V2_2)
        assert actual == {
            'image': 'bar',
            'blkio_config': {
                'weight': override['blkio_config']['weight'],
                'weight_device': (
                    base['blkio_config']['weight_device'] +
                    override['blkio_config']['weight_device']
                ),
                'device_read_iops': override['blkio_config']['device_read_iops'],
                'device_read_bps': override['blkio_config']['device_read_bps'],
                'device_write_iops': base['blkio_config']['device_write_iops']
            }
        }

    def test_merge_extra_hosts(self):
        base = {
            'image': 'bar',
            'extra_hosts': {
                'foo': '1.2.3.4',
            }
        }

        override = {
            'extra_hosts': ['bar:5.6.7.8', 'foo:127.0.0.1']
        }

        actual = config.merge_service_dicts(base, override, V2_0)
        assert actual['extra_hosts'] == {
            'foo': '127.0.0.1',
            'bar': '5.6.7.8',
        }

    def test_merge_healthcheck_config(self):
        base = {
            'image': 'bar',
            'healthcheck': {
                'start_period': 1000,
                'interval': 3000,
                'test': ['true']
            }
        }

        override = {
            'healthcheck': {
                'interval': 5000,
                'timeout': 10000,
                'test': ['echo', 'OK'],
            }
        }

        actual = config.merge_service_dicts(base, override, V2_3)
        assert actual['healthcheck'] == {
            'start_period': base['healthcheck']['start_period'],
            'test': override['healthcheck']['test'],
            'interval': override['healthcheck']['interval'],
            'timeout': override['healthcheck']['timeout'],
        }

    def test_merge_healthcheck_override_disables(self):
        base = {
            'image': 'bar',
            'healthcheck': {
                'start_period': 1000,
                'interval': 3000,
                'timeout': 2000,
                'retries': 3,
                'test': ['true']
            }
        }

        override = {
            'healthcheck': {
                'disabled': True
            }
        }

        actual = config.merge_service_dicts(base, override, V2_3)
        assert actual['healthcheck'] == {'disabled': True}

    def test_merge_healthcheck_override_enables(self):
        base = {
            'image': 'bar',
            'healthcheck': {
                'disabled': True
            }
        }

        override = {
            'healthcheck': {
                'disabled': False,
                'start_period': 1000,
                'interval': 3000,
                'timeout': 2000,
                'retries': 3,
                'test': ['true']
            }
        }

        actual = config.merge_service_dicts(base, override, V2_3)
        assert actual['healthcheck'] == override['healthcheck']

    def test_merge_device_cgroup_rules(self):
        base = {
            'image': 'bar',
            'device_cgroup_rules': ['c 7:128 rwm', 'x 3:244 rw']
        }

        override = {
            'device_cgroup_rules': ['c 7:128 rwm', 'f 0:128 n']
        }

        actual = config.merge_service_dicts(base, override, V2_3)
        assert sorted(actual['device_cgroup_rules']) == sorted(
            ['c 7:128 rwm', 'x 3:244 rw', 'f 0:128 n']
        )

    def test_merge_isolation(self):
        base = {
            'image': 'bar',
            'isolation': 'default',
        }

        override = {
            'isolation': 'hyperv',
        }

        actual = config.merge_service_dicts(base, override, V2_3)
        assert actual == {
            'image': 'bar',
            'isolation': 'hyperv',
        }

    def test_merge_storage_opt(self):
        base = {
            'image': 'bar',
            'storage_opt': {
                'size': '1G',
                'readonly': 'false',
            }
        }

        override = {
            'storage_opt': {
                'size': '2G',
                'encryption': 'aes',
            }
        }

        actual = config.merge_service_dicts(base, override, V2_3)
        assert actual['storage_opt'] == {
            'size': '2G',
            'readonly': 'false',
            'encryption': 'aes',
        }

    def test_external_volume_config(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'bogus': {'image': 'busybox'}
            },
            'volumes': {
                'ext': {'external': True},
                'ext2': {'external': {'name': 'aliased'}}
            }
        })
        config_result = config.load(config_details)
        volumes = config_result.volumes
        assert 'ext' in volumes
        assert volumes['ext']['external'] is True
        assert 'ext2' in volumes
        assert volumes['ext2']['external']['name'] == 'aliased'

    def test_external_volume_invalid_config(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'bogus': {'image': 'busybox'}
            },
            'volumes': {
                'ext': {'external': True, 'driver': 'foo'}
            }
        })
        with pytest.raises(ConfigurationError):
            config.load(config_details)

    def test_depends_on_orders_services(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'one': {'image': 'busybox', 'depends_on': ['three', 'two']},
                'two': {'image': 'busybox', 'depends_on': ['three']},
                'three': {'image': 'busybox'},
            },
        })
        actual = config.load(config_details)
        assert (
            [service['name'] for service in actual.services] ==
            ['three', 'two', 'one']
        )

    def test_depends_on_unknown_service_errors(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'one': {'image': 'busybox', 'depends_on': ['three']},
            },
        })
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert "Service 'one' depends on service 'three'" in exc.exconly()

    def test_linked_service_is_undefined(self):
        with pytest.raises(ConfigurationError):
            config.load(
                build_config_details({
                    'version': '2',
                    'services': {
                        'web': {'image': 'busybox', 'links': ['db:db']},
                    },
                })
            )

    def test_load_dockerfile_without_context(self):
        config_details = build_config_details({
            'version': '2',
            'services': {
                'one': {'build': {'dockerfile': 'Dockerfile.foo'}},
            },
        })
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)
        assert 'has neither an image nor a build context' in exc.exconly()

    def test_load_secrets(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.1',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'secrets': [
                            'one',
                            {
                                'source': 'source',
                                'target': 'target',
                                'uid': '100',
                                'gid': '200',
                                'mode': 0o777,
                            },
                        ],
                    },
                },
                'secrets': {
                    'one': {'file': 'secret.txt'},
                },
            })
        details = config.ConfigDetails('.', [base_file])
        service_dicts = config.load(details).services
        expected = [
            {
                'name': 'web',
                'image': 'example/web',
                'secrets': [
                    types.ServiceSecret('one', None, None, None, None, None),
                    types.ServiceSecret('source', 'target', '100', '200', 0o777, None),
                ],
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    def test_load_secrets_multi_file(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.1',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'secrets': ['one'],
                    },
                },
                'secrets': {
                    'one': {'file': 'secret.txt'},
                },
            })
        override_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.1',
                'services': {
                    'web': {
                        'secrets': [
                            {
                                'source': 'source',
                                'target': 'target',
                                'uid': '100',
                                'gid': '200',
                                'mode': 0o777,
                            },
                        ],
                    },
                },
            })
        details = config.ConfigDetails('.', [base_file, override_file])
        service_dicts = config.load(details).services
        expected = [
            {
                'name': 'web',
                'image': 'example/web',
                'secrets': [
                    types.ServiceSecret('one', None, None, None, None, None),
                    types.ServiceSecret('source', 'target', '100', '200', 0o777, None),
                ],
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    def test_load_configs(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.3',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'configs': [
                            'one',
                            {
                                'source': 'source',
                                'target': 'target',
                                'uid': '100',
                                'gid': '200',
                                'mode': 0o777,
                            },
                        ],
                    },
                },
                'configs': {
                    'one': {'file': 'secret.txt'},
                },
            })
        details = config.ConfigDetails('.', [base_file])
        service_dicts = config.load(details).services
        expected = [
            {
                'name': 'web',
                'image': 'example/web',
                'configs': [
                    types.ServiceConfig('one', None, None, None, None, None),
                    types.ServiceConfig('source', 'target', '100', '200', 0o777, None),
                ],
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    def test_load_configs_multi_file(self):
        base_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.3',
                'services': {
                    'web': {
                        'image': 'example/web',
                        'configs': ['one'],
                    },
                },
                'configs': {
                    'one': {'file': 'secret.txt'},
                },
            })
        override_file = config.ConfigFile(
            'base.yaml',
            {
                'version': '3.3',
                'services': {
                    'web': {
                        'configs': [
                            {
                                'source': 'source',
                                'target': 'target',
                                'uid': '100',
                                'gid': '200',
                                'mode': 0o777,
                            },
                        ],
                    },
                },
            })
        details = config.ConfigDetails('.', [base_file, override_file])
        service_dicts = config.load(details).services
        expected = [
            {
                'name': 'web',
                'image': 'example/web',
                'configs': [
                    types.ServiceConfig('one', None, None, None, None, None),
                    types.ServiceConfig('source', 'target', '100', '200', 0o777, None),
                ],
            },
        ]
        assert service_sort(service_dicts) == service_sort(expected)

    def test_config_convertible_label_types(self):
        config_details = build_config_details(
            {
                'version': '3.5',
                'services': {
                    'web': {
                        'build': {
                            'labels': {'testbuild': True},
                            'context': os.getcwd()
                        },
                        'labels': {
                            "key": 12345
                        }
                    },
                },
                'networks': {
                    'foo': {
                        'labels': {'network.ips.max': 1023}
                    }
                },
                'volumes': {
                    'foo': {
                        'labels': {'volume.is_readonly': False}
                    }
                },
                'secrets': {
                    'foo': {
                        'labels': {'secret.data.expires': 1546282120}
                    }
                },
                'configs': {
                    'foo': {
                        'labels': {'config.data.correction.value': -0.1412}
                    }
                }
            }
        )
        loaded_config = config.load(config_details)

        assert loaded_config.services[0]['build']['labels'] == {'testbuild': 'True'}
        assert loaded_config.services[0]['labels'] == {'key': '12345'}
        assert loaded_config.networks['foo']['labels']['network.ips.max'] == '1023'
        assert loaded_config.volumes['foo']['labels']['volume.is_readonly'] == 'False'
        assert loaded_config.secrets['foo']['labels']['secret.data.expires'] == '1546282120'
        assert loaded_config.configs['foo']['labels']['config.data.correction.value'] == '-0.1412'

    def test_config_invalid_label_types(self):
        config_details = build_config_details({
            'version': '2.3',
            'volumes': {
                'foo': {'labels': [1, 2, 3]}
            }
        })
        with pytest.raises(ConfigurationError):
            config.load(config_details)

    def test_service_volume_invalid_config(self):
        config_details = build_config_details(
            {
                'version': '3.2',
                'services': {
                    'web': {
                        'build': {
                            'context': '.',
                            'args': None,
                        },
                        'volumes': [
                            {
                                "type": "volume",
                                "source": "/data",
                                "garbage": {
                                    "and": "error"
                                }
                            }
                        ]
                    }
                }
            }
        )
        with pytest.raises(ConfigurationError) as exc:
            config.load(config_details)

        assert "services.web.volumes contains unsupported option: 'garbage'" in exc.exconly()

    def test_config_valid_service_label_validation(self):
        config_details = build_config_details(
            {
                'version': '3.5',
                'services': {
                    'web': {
                        'image': 'busybox',
                        'labels': {
                            "key": "string"
                        }
                    },
                },
            }
        )
        config.load(config_details)


class NetworkModeTest(unittest.TestCase):

    def test_network_mode_standard(self):
        config_data = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'busybox',
                    'command': "top",
                    'network_mode': 'bridge',
                },
            },
        }))

        assert config_data.services[0]['network_mode'] == 'bridge'

    def test_network_mode_standard_v1(self):
        config_data = config.load(build_config_details({
            'web': {
                'image': 'busybox',
                'command': "top",
                'net': 'bridge',
            },
        }))

        assert config_data.services[0]['network_mode'] == 'bridge'
        assert 'net' not in config_data.services[0]

    def test_network_mode_container(self):
        config_data = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'busybox',
                    'command': "top",
                    'network_mode': 'container:foo',
                },
            },
        }))

        assert config_data.services[0]['network_mode'] == 'container:foo'

    def test_network_mode_container_v1(self):
        config_data = config.load(build_config_details({
            'web': {
                'image': 'busybox',
                'command': "top",
                'net': 'container:foo',
            },
        }))

        assert config_data.services[0]['network_mode'] == 'container:foo'

    def test_network_mode_service(self):
        config_data = config.load(build_config_details({
            'version': '2',
            'services': {
                'web': {
                    'image': 'busybox',
                    'command': "top",
                    'network_mode': 'service:foo',
                },
                'foo': {
                    'image': 'busybox',
                    'command': "top",
                },
            },
        }))

        assert config_data.services[1]['network_mode'] == 'service:foo'

    def test_network_mode_service_v1(self):
        config_data = config.load(build_config_details({
            'web': {
                'image': 'busybox',
                'command': "top",
                'net': 'container:foo',
            },
            'foo': {
                'image': 'busybox',
                'command': "top",
            },
        }))

        assert config_data.services[1]['network_mode'] == 'service:foo'

    def test_network_mode_service_nonexistent(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(build_config_details({
                'version': '2',
                'services': {
                    'web': {
                        'image': 'busybox',
                        'command': "top",
                        'network_mode': 'service:foo',
                    },
                },
            }))

        assert "service 'foo' which is undefined" in excinfo.exconly()

    def test_network_mode_plus_networks_is_invalid(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(build_config_details({
                'version': '2',
                'services': {
                    'web': {
                        'image': 'busybox',
                        'command': "top",
                        'network_mode': 'bridge',
                        'networks': ['front'],
                    },
                },
                'networks': {
                    'front': None,
                }
            }))

        assert "'network_mode' and 'networks' cannot be combined" in excinfo.exconly()


class PortsTest(unittest.TestCase):
    INVALID_PORTS_TYPES = [
        {"1": "8000"},
        False,
        "8000",
        8000,
    ]

    NON_UNIQUE_SINGLE_PORTS = [
        ["8000", "8000"],
    ]

    INVALID_PORT_MAPPINGS = [
        ["8000-8004:8000-8002"],
        ["4242:4242-4244"],
    ]

    VALID_SINGLE_PORTS = [
        ["8000"],
        ["8000/tcp"],
        ["8000", "9000"],
        [8000],
        [8000, 9000],
    ]

    VALID_PORT_MAPPINGS = [
        ["8000:8050"],
        ["49153-49154:3002-3003"],
    ]

    def test_config_invalid_ports_type_validation(self):
        for invalid_ports in self.INVALID_PORTS_TYPES:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config({'ports': invalid_ports})

            assert "contains an invalid type" in exc.value.msg

    def test_config_non_unique_ports_validation(self):
        for invalid_ports in self.NON_UNIQUE_SINGLE_PORTS:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config({'ports': invalid_ports})

            assert "non-unique" in exc.value.msg

    def test_config_invalid_ports_format_validation(self):
        for invalid_ports in self.INVALID_PORT_MAPPINGS:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config({'ports': invalid_ports})

            assert "Port ranges don't match in length" in exc.value.msg

    def test_config_valid_ports_format_validation(self):
        for valid_ports in self.VALID_SINGLE_PORTS + self.VALID_PORT_MAPPINGS:
            self.check_config({'ports': valid_ports})

    def test_config_invalid_expose_type_validation(self):
        for invalid_expose in self.INVALID_PORTS_TYPES:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config({'expose': invalid_expose})

            assert "contains an invalid type" in exc.value.msg

    def test_config_non_unique_expose_validation(self):
        for invalid_expose in self.NON_UNIQUE_SINGLE_PORTS:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config({'expose': invalid_expose})

            assert "non-unique" in exc.value.msg

    def test_config_invalid_expose_format_validation(self):
        # Valid port mappings ARE NOT valid 'expose' entries
        for invalid_expose in self.INVALID_PORT_MAPPINGS + self.VALID_PORT_MAPPINGS:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config({'expose': invalid_expose})

            assert "should be of the format" in exc.value.msg

    def test_config_valid_expose_format_validation(self):
        # Valid single ports ARE valid 'expose' entries
        for valid_expose in self.VALID_SINGLE_PORTS:
            self.check_config({'expose': valid_expose})

    def check_config(self, cfg):
        config.load(
            build_config_details({
                'version': '2.3',
                'services': {
                    'web': dict(image='busybox', **cfg)
                },
            }, 'working_dir', 'filename.yml')
        )


class SubnetTest(unittest.TestCase):
    INVALID_SUBNET_TYPES = [
        None,
        False,
        10,
    ]

    INVALID_SUBNET_MAPPINGS = [
        "",
        "192.168.0.1/sdfsdfs",
        "192.168.0.1/",
        "192.168.0.1/33",
        "192.168.0.1/01",
        "192.168.0.1",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156/sdfsdfs",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156/",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156/129",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156/01",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156",
        "ge80:0000:0000:0000:0204:61ff:fe9d:f156/128",
        "192.168.0.1/31/31",
    ]

    VALID_SUBNET_MAPPINGS = [
        "192.168.0.1/0",
        "192.168.0.1/32",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156/0",
        "fe80:0000:0000:0000:0204:61ff:fe9d:f156/128",
        "1:2:3:4:5:6:7:8/0",
        "1::/0",
        "1:2:3:4:5:6:7::/0",
        "1::8/0",
        "1:2:3:4:5:6::8/0",
        "::/0",
        "::8/0",
        "::2:3:4:5:6:7:8/0",
        "fe80::7:8%eth0/0",
        "fe80::7:8%1/0",
        "::255.255.255.255/0",
        "::ffff:255.255.255.255/0",
        "::ffff:0:255.255.255.255/0",
        "2001:db8:3:4::192.0.2.33/0",
        "64:ff9b::192.0.2.33/0",
    ]

    def test_config_invalid_subnet_type_validation(self):
        for invalid_subnet in self.INVALID_SUBNET_TYPES:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config(invalid_subnet)

            assert "contains an invalid type" in exc.value.msg

    def test_config_invalid_subnet_format_validation(self):
        for invalid_subnet in self.INVALID_SUBNET_MAPPINGS:
            with pytest.raises(ConfigurationError) as exc:
                self.check_config(invalid_subnet)

            assert "should use the CIDR format" in exc.value.msg

    def test_config_valid_subnet_format_validation(self):
        for valid_subnet in self.VALID_SUBNET_MAPPINGS:
            self.check_config(valid_subnet)

    def check_config(self, subnet):
        config.load(
            build_config_details({
                'version': '3.5',
                'services': {
                    'web': {
                        'image': 'busybox'
                    }
                },
                'networks': {
                    'default': {
                        'ipam': {
                            'config': [
                                {
                                    'subnet': subnet
                                }
                            ],
                            'driver': 'default'
                        }
                    }
                }
            })
        )


class InterpolationTest(unittest.TestCase):

    @mock.patch.dict(os.environ)
    def test_config_file_with_environment_file(self):
        project_dir = 'tests/fixtures/default-env-file'
        service_dicts = config.load(
            config.find(
                project_dir, None, Environment.from_env_file(project_dir)
            )
        ).services

        assert service_dicts[0] == {
            'name': 'web',
            'image': 'alpine:latest',
            'ports': [
                types.ServicePort.parse('5643')[0],
                types.ServicePort.parse('9999')[0]
            ],
            'command': 'true'
        }

    @mock.patch.dict(os.environ)
    def test_config_file_with_environment_variable(self):
        project_dir = 'tests/fixtures/environment-interpolation'
        os.environ.update(
            IMAGE="busybox",
            HOST_PORT="80",
            LABEL_VALUE="myvalue",
        )

        service_dicts = config.load(
            config.find(
                project_dir, None, Environment.from_env_file(project_dir)
            )
        ).services

        assert service_dicts == [
            {
                'name': 'web',
                'image': 'busybox',
                'ports': types.ServicePort.parse('80:8000'),
                'labels': {'mylabel': 'myvalue'},
                'hostname': 'host-',
                'command': '${ESCAPED}',
            }
        ]

    @mock.patch.dict(os.environ)
    def test_config_file_with_environment_variable_with_defaults(self):
        project_dir = 'tests/fixtures/environment-interpolation-with-defaults'
        os.environ.update(
            IMAGE="busybox",
        )

        service_dicts = config.load(
            config.find(
                project_dir, None, Environment.from_env_file(project_dir)
            )
        ).services

        assert service_dicts == [
            {
                'name': 'web',
                'image': 'busybox',
                'ports': types.ServicePort.parse('80:8000'),
                'hostname': 'host-',
            }
        ]

    @mock.patch.dict(os.environ)
    def test_unset_variable_produces_warning(self):
        os.environ.pop('FOO', None)
        os.environ.pop('BAR', None)
        config_details = build_config_details(
            {
                'web': {
                    'image': '${FOO}',
                    'command': '${BAR}',
                    'container_name': '${BAR}',
                },
            },
            '.',
            None,
        )

        with mock.patch('compose.config.environment.log') as log:
            config.load(config_details)

            assert 2 == log.warn.call_count
            warnings = sorted(args[0][0] for args in log.warn.call_args_list)
            assert 'BAR' in warnings[0]
            assert 'FOO' in warnings[1]

    def test_compatibility_mode_warnings(self):
        config_details = build_config_details({
            'version': '3.5',
            'services': {
                'web': {
                    'deploy': {
                        'labels': ['abc=def'],
                        'endpoint_mode': 'dnsrr',
                        'update_config': {'max_failure_ratio': 0.4},
                        'placement': {'constraints': ['node.id==deadbeef']},
                        'resources': {
                            'reservations': {'cpus': '0.2'}
                        },
                        'restart_policy': {
                            'delay': '2s',
                            'window': '12s'
                        }
                    },
                    'image': 'busybox'
                }
            }
        })

        with mock.patch('compose.config.config.log') as log:
            config.load(config_details, compatibility=True)

        assert log.warn.call_count == 1
        warn_message = log.warn.call_args[0][0]
        assert warn_message.startswith(
            'The following deploy sub-keys are not supported in compatibility mode'
        )
        assert 'labels' in warn_message
        assert 'endpoint_mode' in warn_message
        assert 'update_config' in warn_message
        assert 'placement' in warn_message
        assert 'resources.reservations.cpus' in warn_message
        assert 'restart_policy.delay' in warn_message
        assert 'restart_policy.window' in warn_message

    def test_compatibility_mode_load(self):
        config_details = build_config_details({
            'version': '3.5',
            'services': {
                'foo': {
                    'image': 'alpine:3.7',
                    'deploy': {
                        'replicas': 3,
                        'restart_policy': {
                            'condition': 'any',
                            'max_attempts': 7,
                        },
                        'resources': {
                            'limits': {'memory': '300M', 'cpus': '0.7'},
                            'reservations': {'memory': '100M'},
                        },
                    },
                },
            },
        })

        with mock.patch('compose.config.config.log') as log:
            cfg = config.load(config_details, compatibility=True)

        assert log.warn.call_count == 0

        service_dict = cfg.services[0]
        assert service_dict == {
            'image': 'alpine:3.7',
            'scale': 3,
            'restart': {'MaximumRetryCount': 7, 'Name': 'always'},
            'mem_limit': '300M',
            'mem_reservation': '100M',
            'cpus': 0.7,
            'name': 'foo'
        }

    @mock.patch.dict(os.environ)
    def test_invalid_interpolation(self):
        with pytest.raises(config.ConfigurationError) as cm:
            config.load(
                build_config_details(
                    {'web': {'image': '${'}},
                    'working_dir',
                    'filename.yml'
                )
            )

        assert 'Invalid' in cm.value.msg
        assert 'for "image" option' in cm.value.msg
        assert 'in service "web"' in cm.value.msg
        assert '"${"' in cm.value.msg

    @mock.patch.dict(os.environ)
    def test_interpolation_secrets_section(self):
        os.environ['FOO'] = 'baz.bar'
        config_dict = config.load(build_config_details({
            'version': '3.1',
            'secrets': {
                'secretdata': {
                    'external': {'name': '$FOO'}
                }
            }
        }))
        assert config_dict.secrets == {
            'secretdata': {
                'external': {'name': 'baz.bar'},
                'name': 'baz.bar'
            }
        }

    @mock.patch.dict(os.environ)
    def test_interpolation_configs_section(self):
        os.environ['FOO'] = 'baz.bar'
        config_dict = config.load(build_config_details({
            'version': '3.3',
            'configs': {
                'configdata': {
                    'external': {'name': '$FOO'}
                }
            }
        }))
        assert config_dict.configs == {
            'configdata': {
                'external': {'name': 'baz.bar'},
                'name': 'baz.bar'
            }
        }


class VolumeConfigTest(unittest.TestCase):

    def test_no_binding(self):
        d = make_service_dict('foo', {'build': '.', 'volumes': ['/data']}, working_dir='.')
        assert d['volumes'] == ['/data']

    @mock.patch.dict(os.environ)
    def test_volume_binding_with_environment_variable(self):
        os.environ['VOLUME_PATH'] = '/host/path'

        d = config.load(
            build_config_details(
                {'foo': {'build': '.', 'volumes': ['${VOLUME_PATH}:/container/path']}},
                '.',
                None,
            )
        ).services[0]
        assert d['volumes'] == [VolumeSpec.parse('/host/path:/container/path')]

    @pytest.mark.skipif(IS_WINDOWS_PLATFORM, reason='posix paths')
    def test_volumes_order_is_preserved(self):
        volumes = ['/{0}:/{0}'.format(i) for i in range(0, 6)]
        shuffle(volumes)
        cfg = make_service_dict('foo', {'build': '.', 'volumes': volumes})
        assert cfg['volumes'] == volumes

    @pytest.mark.skipif(IS_WINDOWS_PLATFORM, reason='posix paths')
    @mock.patch.dict(os.environ)
    def test_volume_binding_with_home(self):
        os.environ['HOME'] = '/home/user'
        d = make_service_dict('foo', {'build': '.', 'volumes': ['~:/container/path']}, working_dir='.')
        assert d['volumes'] == ['/home/user:/container/path']

    def test_name_does_not_expand(self):
        d = make_service_dict('foo', {'build': '.', 'volumes': ['mydatavolume:/data']}, working_dir='.')
        assert d['volumes'] == ['mydatavolume:/data']

    def test_absolute_posix_path_does_not_expand(self):
        d = make_service_dict('foo', {'build': '.', 'volumes': ['/var/lib/data:/data']}, working_dir='.')
        assert d['volumes'] == ['/var/lib/data:/data']

    def test_absolute_windows_path_does_not_expand(self):
        d = make_service_dict('foo', {'build': '.', 'volumes': ['c:\\data:/data']}, working_dir='.')
        assert d['volumes'] == ['c:\\data:/data']

    @pytest.mark.skipif(IS_WINDOWS_PLATFORM, reason='posix paths')
    def test_relative_path_does_expand_posix(self):
        d = make_service_dict(
            'foo',
            {'build': '.', 'volumes': ['./data:/data']},
            working_dir='/home/me/myproject')
        assert d['volumes'] == ['/home/me/myproject/data:/data']

        d = make_service_dict(
            'foo',
            {'build': '.', 'volumes': ['.:/data']},
            working_dir='/home/me/myproject')
        assert d['volumes'] == ['/home/me/myproject:/data']

        d = make_service_dict(
            'foo',
            {'build': '.', 'volumes': ['../otherproject:/data']},
            working_dir='/home/me/myproject')
        assert d['volumes'] == ['/home/me/otherproject:/data']

    @pytest.mark.skipif(not IS_WINDOWS_PLATFORM, reason='windows paths')
    def test_relative_path_does_expand_windows(self):
        d = make_service_dict(
            'foo',
            {'build': '.', 'volumes': ['./data:/data']},
            working_dir='c:\\Users\\me\\myproject')
        assert d['volumes'] == ['c:\\Users\\me\\myproject\\data:/data']

        d = make_service_dict(
            'foo',
            {'build': '.', 'volumes': ['.:/data']},
            working_dir='c:\\Users\\me\\myproject')
        assert d['volumes'] == ['c:\\Users\\me\\myproject:/data']

        d = make_service_dict(
            'foo',
            {'build': '.', 'volumes': ['../otherproject:/data']},
            working_dir='c:\\Users\\me\\myproject')
        assert d['volumes'] == ['c:\\Users\\me\\otherproject:/data']

    @mock.patch.dict(os.environ)
    def test_home_directory_with_driver_does_not_expand(self):
        os.environ['NAME'] = 'surprise!'
        d = make_service_dict('foo', {
            'build': '.',
            'volumes': ['~:/data'],
            'volume_driver': 'foodriver',
        }, working_dir='.')
        assert d['volumes'] == ['~:/data']

    def test_volume_path_with_non_ascii_directory(self):
        volume = u'/F/data:/data'
        container_path = config.resolve_volume_path(".", volume)
        assert container_path == volume


class MergePathMappingTest(object):
    config_name = ""

    def test_empty(self):
        service_dict = config.merge_service_dicts({}, {}, DEFAULT_VERSION)
        assert self.config_name not in service_dict

    def test_no_override(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: ['/foo:/code', '/data']},
            {},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(['/foo:/code', '/data'])

    def test_no_base(self):
        service_dict = config.merge_service_dicts(
            {},
            {self.config_name: ['/bar:/code']},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(['/bar:/code'])

    def test_override_explicit_path(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: ['/foo:/code', '/data']},
            {self.config_name: ['/bar:/code']},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(['/bar:/code', '/data'])

    def test_add_explicit_path(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: ['/foo:/code', '/data']},
            {self.config_name: ['/bar:/code', '/quux:/data']},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(['/bar:/code', '/quux:/data'])

    def test_remove_explicit_path(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: ['/foo:/code', '/quux:/data']},
            {self.config_name: ['/bar:/code', '/data']},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(['/bar:/code', '/data'])


class MergeVolumesTest(unittest.TestCase, MergePathMappingTest):
    config_name = 'volumes'


class MergeDevicesTest(unittest.TestCase, MergePathMappingTest):
    config_name = 'devices'


class BuildOrImageMergeTest(unittest.TestCase):

    def test_merge_build_or_image_no_override(self):
        assert config.merge_service_dicts({'build': '.'}, {}, V1) == {'build': '.'}

        assert config.merge_service_dicts({'image': 'redis'}, {}, V1) == {'image': 'redis'}

    def test_merge_build_or_image_override_with_same(self):
        assert config.merge_service_dicts({'build': '.'}, {'build': './web'}, V1) == {'build': './web'}

        assert config.merge_service_dicts({'image': 'redis'}, {'image': 'postgres'}, V1) == {
            'image': 'postgres'
        }

    def test_merge_build_or_image_override_with_other(self):
        assert config.merge_service_dicts({'build': '.'}, {'image': 'redis'}, V1) == {
            'image': 'redis'
        }

        assert config.merge_service_dicts({'image': 'redis'}, {'build': '.'}, V1) == {'build': '.'}


class MergeListsTest(object):
    config_name = ""
    base_config = []
    override_config = []

    def merged_config(self):
        return set(self.base_config) | set(self.override_config)

    def test_empty(self):
        assert self.config_name not in config.merge_service_dicts({}, {}, DEFAULT_VERSION)

    def test_no_override(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: self.base_config},
            {},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(self.base_config)

    def test_no_base(self):
        service_dict = config.merge_service_dicts(
            {},
            {self.config_name: self.base_config},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(self.base_config)

    def test_add_item(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: self.base_config},
            {self.config_name: self.override_config},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == set(self.merged_config())


class MergePortsTest(unittest.TestCase, MergeListsTest):
    config_name = 'ports'
    base_config = ['10:8000', '9000']
    override_config = ['20:8000']

    def merged_config(self):
        return self.convert(self.base_config) | self.convert(self.override_config)

    def convert(self, port_config):
        return set(config.merge_service_dicts(
            {self.config_name: port_config},
            {self.config_name: []},
            DEFAULT_VERSION
        )[self.config_name])

    def test_duplicate_port_mappings(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: self.base_config},
            {self.config_name: self.base_config},
            DEFAULT_VERSION
        )
        assert set(service_dict[self.config_name]) == self.convert(self.base_config)

    def test_no_override(self):
        service_dict = config.merge_service_dicts(
            {self.config_name: self.base_config},
            {},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == self.convert(self.base_config)

    def test_no_base(self):
        service_dict = config.merge_service_dicts(
            {},
            {self.config_name: self.base_config},
            DEFAULT_VERSION)
        assert set(service_dict[self.config_name]) == self.convert(self.base_config)


class MergeNetworksTest(unittest.TestCase, MergeListsTest):
    config_name = 'networks'
    base_config = ['frontend', 'backend']
    override_config = ['monitoring']


class MergeStringsOrListsTest(unittest.TestCase):

    def test_no_override(self):
        service_dict = config.merge_service_dicts(
            {'dns': '8.8.8.8'},
            {},
            DEFAULT_VERSION)
        assert set(service_dict['dns']) == set(['8.8.8.8'])

    def test_no_base(self):
        service_dict = config.merge_service_dicts(
            {},
            {'dns': '8.8.8.8'},
            DEFAULT_VERSION)
        assert set(service_dict['dns']) == set(['8.8.8.8'])

    def test_add_string(self):
        service_dict = config.merge_service_dicts(
            {'dns': ['8.8.8.8']},
            {'dns': '9.9.9.9'},
            DEFAULT_VERSION)
        assert set(service_dict['dns']) == set(['8.8.8.8', '9.9.9.9'])

    def test_add_list(self):
        service_dict = config.merge_service_dicts(
            {'dns': '8.8.8.8'},
            {'dns': ['9.9.9.9']},
            DEFAULT_VERSION)
        assert set(service_dict['dns']) == set(['8.8.8.8', '9.9.9.9'])


class MergeLabelsTest(unittest.TestCase):

    def test_empty(self):
        assert 'labels' not in config.merge_service_dicts({}, {}, DEFAULT_VERSION)

    def test_no_override(self):
        service_dict = config.merge_service_dicts(
            make_service_dict('foo', {'build': '.', 'labels': ['foo=1', 'bar']}, 'tests/'),
            make_service_dict('foo', {'build': '.'}, 'tests/'),
            DEFAULT_VERSION)
        assert service_dict['labels'] == {'foo': '1', 'bar': ''}

    def test_no_base(self):
        service_dict = config.merge_service_dicts(
            make_service_dict('foo', {'build': '.'}, 'tests/'),
            make_service_dict('foo', {'build': '.', 'labels': ['foo=2']}, 'tests/'),
            DEFAULT_VERSION)
        assert service_dict['labels'] == {'foo': '2'}

    def test_override_explicit_value(self):
        service_dict = config.merge_service_dicts(
            make_service_dict('foo', {'build': '.', 'labels': ['foo=1', 'bar']}, 'tests/'),
            make_service_dict('foo', {'build': '.', 'labels': ['foo=2']}, 'tests/'),
            DEFAULT_VERSION)
        assert service_dict['labels'] == {'foo': '2', 'bar': ''}

    def test_add_explicit_value(self):
        service_dict = config.merge_service_dicts(
            make_service_dict('foo', {'build': '.', 'labels': ['foo=1', 'bar']}, 'tests/'),
            make_service_dict('foo', {'build': '.', 'labels': ['bar=2']}, 'tests/'),
            DEFAULT_VERSION)
        assert service_dict['labels'] == {'foo': '1', 'bar': '2'}

    def test_remove_explicit_value(self):
        service_dict = config.merge_service_dicts(
            make_service_dict('foo', {'build': '.', 'labels': ['foo=1', 'bar=2']}, 'tests/'),
            make_service_dict('foo', {'build': '.', 'labels': ['bar']}, 'tests/'),
            DEFAULT_VERSION)
        assert service_dict['labels'] == {'foo': '1', 'bar': ''}


class MergeBuildTest(unittest.TestCase):
    def test_full(self):
        base = {
            'context': '.',
            'dockerfile': 'Dockerfile',
            'args': {
                'x': '1',
                'y': '2',
            },
            'cache_from': ['ubuntu'],
            'labels': ['com.docker.compose.test=true']
        }

        override = {
            'context': './prod',
            'dockerfile': 'Dockerfile.prod',
            'args': ['x=12'],
            'cache_from': ['debian'],
            'labels': {
                'com.docker.compose.test': 'false',
                'com.docker.compose.prod': 'true',
            }
        }

        result = config.merge_build(None, {'build': base}, {'build': override})
        assert result['context'] == override['context']
        assert result['dockerfile'] == override['dockerfile']
        assert result['args'] == {'x': '12', 'y': '2'}
        assert set(result['cache_from']) == set(['ubuntu', 'debian'])
        assert result['labels'] == override['labels']

    def test_empty_override(self):
        base = {
            'context': '.',
            'dockerfile': 'Dockerfile',
            'args': {
                'x': '1',
                'y': '2',
            },
            'cache_from': ['ubuntu'],
            'labels': {
                'com.docker.compose.test': 'true'
            }
        }

        override = {}

        result = config.merge_build(None, {'build': base}, {'build': override})
        assert result == base

    def test_empty_base(self):
        base = {}

        override = {
            'context': './prod',
            'dockerfile': 'Dockerfile.prod',
            'args': {'x': '12'},
            'cache_from': ['debian'],
            'labels': {
                'com.docker.compose.test': 'false',
                'com.docker.compose.prod': 'true',
            }
        }

        result = config.merge_build(None, {'build': base}, {'build': override})
        assert result == override


class MemoryOptionsTest(unittest.TestCase):

    def test_validation_fails_with_just_memswap_limit(self):
        """
        When you set a 'memswap_limit' it is invalid config unless you also set
        a mem_limit
        """
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'foo': {'image': 'busybox', 'memswap_limit': 2000000},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "foo.memswap_limit is invalid: when defining " \
               "'memswap_limit' you must set 'mem_limit' as well" \
            in excinfo.exconly()

    def test_validation_with_correct_memswap_values(self):
        service_dict = config.load(
            build_config_details(
                {'foo': {'image': 'busybox', 'mem_limit': 1000000, 'memswap_limit': 2000000}},
                'tests/fixtures/extends',
                'common.yml'
            )
        ).services
        assert service_dict[0]['memswap_limit'] == 2000000

    def test_memswap_can_be_a_string(self):
        service_dict = config.load(
            build_config_details(
                {'foo': {'image': 'busybox', 'mem_limit': "1G", 'memswap_limit': "512M"}},
                'tests/fixtures/extends',
                'common.yml'
            )
        ).services
        assert service_dict[0]['memswap_limit'] == "512M"


class EnvTest(unittest.TestCase):

    def test_parse_environment_as_list(self):
        environment = [
            'NORMAL=F1',
            'CONTAINS_EQUALS=F=2',
            'TRAILING_EQUALS=',
        ]
        assert config.parse_environment(environment) == {
            'NORMAL': 'F1', 'CONTAINS_EQUALS': 'F=2', 'TRAILING_EQUALS': ''
        }

    def test_parse_environment_as_dict(self):
        environment = {
            'NORMAL': 'F1',
            'CONTAINS_EQUALS': 'F=2',
            'TRAILING_EQUALS': None,
        }
        assert config.parse_environment(environment) == environment

    def test_parse_environment_invalid(self):
        with pytest.raises(ConfigurationError):
            config.parse_environment('a=b')

    def test_parse_environment_empty(self):
        assert config.parse_environment(None) == {}

    @mock.patch.dict(os.environ)
    def test_resolve_environment(self):
        os.environ['FILE_DEF'] = 'E1'
        os.environ['FILE_DEF_EMPTY'] = 'E2'
        os.environ['ENV_DEF'] = 'E3'

        service_dict = {
            'build': '.',
            'environment': {
                'FILE_DEF': 'F1',
                'FILE_DEF_EMPTY': '',
                'ENV_DEF': None,
                'NO_DEF': None
            },
        }
        assert resolve_environment(
            service_dict, Environment.from_env_file(None)
        ) == {'FILE_DEF': 'F1', 'FILE_DEF_EMPTY': '', 'ENV_DEF': 'E3', 'NO_DEF': None}

    def test_resolve_environment_from_env_file(self):
        assert resolve_environment({'env_file': ['tests/fixtures/env/one.env']}) == {
            'ONE': '2', 'TWO': '1', 'THREE': '3', 'FOO': 'bar'
        }

    def test_environment_overrides_env_file(self):
        assert resolve_environment({
            'environment': {'FOO': 'baz'},
            'env_file': ['tests/fixtures/env/one.env'],
        }) == {'ONE': '2', 'TWO': '1', 'THREE': '3', 'FOO': 'baz'}

    def test_resolve_environment_with_multiple_env_files(self):
        service_dict = {
            'env_file': [
                'tests/fixtures/env/one.env',
                'tests/fixtures/env/two.env'
            ]
        }
        assert resolve_environment(service_dict) == {
            'ONE': '2', 'TWO': '1', 'THREE': '3', 'FOO': 'baz', 'DOO': 'dah'
        }

    def test_resolve_environment_nonexistent_file(self):
        with pytest.raises(ConfigurationError) as exc:
            config.load(build_config_details(
                {'foo': {'image': 'example', 'env_file': 'nonexistent.env'}},
                working_dir='tests/fixtures/env'))

        assert 'Couldn\'t find env file' in exc.exconly()
        assert 'nonexistent.env' in exc.exconly()

    @mock.patch.dict(os.environ)
    def test_resolve_environment_from_env_file_with_empty_values(self):
        os.environ['FILE_DEF'] = 'E1'
        os.environ['FILE_DEF_EMPTY'] = 'E2'
        os.environ['ENV_DEF'] = 'E3'
        assert resolve_environment(
            {'env_file': ['tests/fixtures/env/resolve.env']},
            Environment.from_env_file(None)
        ) == {
            'FILE_DEF': u'br',
            'FILE_DEF_EMPTY': '',
            'ENV_DEF': 'E3',
            'NO_DEF': None
        }

    @mock.patch.dict(os.environ)
    def test_resolve_build_args(self):
        os.environ['env_arg'] = 'value2'

        build = {
            'context': '.',
            'args': {
                'arg1': 'value1',
                'empty_arg': '',
                'env_arg': None,
                'no_env': None
            }
        }
        assert resolve_build_args(build['args'], Environment.from_env_file(build['context'])) == {
            'arg1': 'value1', 'empty_arg': '', 'env_arg': 'value2', 'no_env': None
        }

    @pytest.mark.xfail(IS_WINDOWS_PLATFORM, reason='paths use slash')
    @mock.patch.dict(os.environ)
    def test_resolve_path(self):
        os.environ['HOSTENV'] = '/tmp'
        os.environ['CONTAINERENV'] = '/host/tmp'

        service_dict = config.load(
            build_config_details(
                {'foo': {'build': '.', 'volumes': ['$HOSTENV:$CONTAINERENV']}},
                "tests/fixtures/env",
            )
        ).services[0]
        assert set(service_dict['volumes']) == set([VolumeSpec.parse('/tmp:/host/tmp')])

        service_dict = config.load(
            build_config_details(
                {'foo': {'build': '.', 'volumes': ['/opt${HOSTENV}:/opt${CONTAINERENV}']}},
                "tests/fixtures/env",
            )
        ).services[0]
        assert set(service_dict['volumes']) == set([VolumeSpec.parse('/opt/tmp:/opt/host/tmp')])


def load_from_filename(filename, override_dir=None):
    return config.load(
        config.find('.', [filename], Environment.from_env_file('.'), override_dir=override_dir)
    ).services


class ExtendsTest(unittest.TestCase):

    def test_extends(self):
        service_dicts = load_from_filename('tests/fixtures/extends/docker-compose.yml')

        assert service_sort(service_dicts) == service_sort([
            {
                'name': 'mydb',
                'image': 'busybox',
                'command': 'top',
            },
            {
                'name': 'myweb',
                'image': 'busybox',
                'command': 'top',
                'network_mode': 'bridge',
                'links': ['mydb:db'],
                'environment': {
                    "FOO": "1",
                    "BAR": "2",
                    "BAZ": "2",
                },
            }
        ])

    def test_merging_env_labels_ulimits(self):
        service_dicts = load_from_filename('tests/fixtures/extends/common-env-labels-ulimits.yml')

        assert service_sort(service_dicts) == service_sort([
            {
                'name': 'web',
                'image': 'busybox',
                'command': '/bin/true',
                'network_mode': 'host',
                'environment': {
                    "FOO": "2",
                    "BAR": "1",
                    "BAZ": "3",
                },
                'labels': {'label': 'one'},
                'ulimits': {'nproc': 65535, 'memlock': {'soft': 1024, 'hard': 2048}}
            }
        ])

    def test_nested(self):
        service_dicts = load_from_filename('tests/fixtures/extends/nested.yml')

        assert service_dicts == [
            {
                'name': 'myweb',
                'image': 'busybox',
                'command': '/bin/true',
                'network_mode': 'host',
                'environment': {
                    "FOO": "2",
                    "BAR": "2",
                },
            },
        ]

    def test_self_referencing_file(self):
        """
        We specify a 'file' key that is the filename we're already in.
        """
        service_dicts = load_from_filename('tests/fixtures/extends/specify-file-as-self.yml')
        assert service_sort(service_dicts) == service_sort([
            {
                'environment':
                {
                    'YEP': '1', 'BAR': '1', 'BAZ': '3'
                },
                'image': 'busybox',
                'name': 'myweb'
            },
            {
                'environment':
                {'YEP': '1'},
                'image': 'busybox',
                'name': 'otherweb'
            },
            {
                'environment':
                {'YEP': '1', 'BAZ': '3'},
                'image': 'busybox',
                'name': 'web'
            }
        ])

    def test_circular(self):
        with pytest.raises(config.CircularReference) as exc:
            load_from_filename('tests/fixtures/extends/circle-1.yml')

        path = [
            (os.path.basename(filename), service_name)
            for (filename, service_name) in exc.value.trail
        ]
        expected = [
            ('circle-1.yml', 'web'),
            ('circle-2.yml', 'other'),
            ('circle-1.yml', 'web'),
        ]
        assert path == expected

    def test_extends_validation_empty_dictionary(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'web': {'image': 'busybox', 'extends': {}},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert 'service' in excinfo.exconly()

    def test_extends_validation_missing_service_key(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'web': {'image': 'busybox', 'extends': {'file': 'common.yml'}},
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "'service' is a required property" in excinfo.exconly()

    def test_extends_validation_invalid_key(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'web': {
                            'image': 'busybox',
                            'extends': {
                                'file': 'common.yml',
                                'service': 'web',
                                'rogue_key': 'is not allowed'
                            }
                        },
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "web.extends contains unsupported option: 'rogue_key'" \
            in excinfo.exconly()

    def test_extends_validation_sub_property_key(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details(
                    {
                        'web': {
                            'image': 'busybox',
                            'extends': {
                                'file': 1,
                                'service': 'web',
                            }
                        },
                    },
                    'tests/fixtures/extends',
                    'filename.yml'
                )
            )

        assert "web.extends.file contains 1, which is an invalid type, it should be a string" \
            in excinfo.exconly()

    def test_extends_validation_no_file_key_no_filename_set(self):
        dictionary = {'extends': {'service': 'web'}}

        with pytest.raises(ConfigurationError) as excinfo:
            make_service_dict('myweb', dictionary, working_dir='tests/fixtures/extends')

        assert 'file' in excinfo.exconly()

    def test_extends_validation_valid_config(self):
        service = config.load(
            build_config_details(
                {
                    'web': {'image': 'busybox', 'extends': {'service': 'web', 'file': 'common.yml'}},
                },
                'tests/fixtures/extends',
                'common.yml'
            )
        ).services

        assert len(service) == 1
        assert isinstance(service[0], dict)
        assert service[0]['command'] == "/bin/true"

    def test_extended_service_with_invalid_config(self):
        with pytest.raises(ConfigurationError) as exc:
            load_from_filename('tests/fixtures/extends/service-with-invalid-schema.yml')
        assert (
            "myweb has neither an image nor a build context specified" in
            exc.exconly()
        )

    def test_extended_service_with_valid_config(self):
        service = load_from_filename('tests/fixtures/extends/service-with-valid-composite-extends.yml')
        assert service[0]['command'] == "top"

    def test_extends_file_defaults_to_self(self):
        """
        Test not specifying a file in our extends options that the
        config is valid and correctly extends from itself.
        """
        service_dicts = load_from_filename('tests/fixtures/extends/no-file-specified.yml')
        assert service_sort(service_dicts) == service_sort([
            {
                'name': 'myweb',
                'image': 'busybox',
                'environment': {
                    "BAR": "1",
                    "BAZ": "3",
                }
            },
            {
                'name': 'web',
                'image': 'busybox',
                'environment': {
                    "BAZ": "3",
                }
            }
        ])

    def test_invalid_links_in_extended_service(self):
        with pytest.raises(ConfigurationError) as excinfo:
            load_from_filename('tests/fixtures/extends/invalid-links.yml')

        assert "services with 'links' cannot be extended" in excinfo.exconly()

    def test_invalid_volumes_from_in_extended_service(self):
        with pytest.raises(ConfigurationError) as excinfo:
            load_from_filename('tests/fixtures/extends/invalid-volumes.yml')

        assert "services with 'volumes_from' cannot be extended" in excinfo.exconly()

    def test_invalid_net_in_extended_service(self):
        with pytest.raises(ConfigurationError) as excinfo:
            load_from_filename('tests/fixtures/extends/invalid-net-v2.yml')

        assert 'network_mode: service' in excinfo.exconly()
        assert 'cannot be extended' in excinfo.exconly()

        with pytest.raises(ConfigurationError) as excinfo:
            load_from_filename('tests/fixtures/extends/invalid-net.yml')

        assert 'net: container' in excinfo.exconly()
        assert 'cannot be extended' in excinfo.exconly()

    @mock.patch.dict(os.environ)
    def test_load_config_runs_interpolation_in_extended_service(self):
        os.environ.update(HOSTNAME_VALUE="penguin")
        expected_interpolated_value = "host-penguin"
        service_dicts = load_from_filename(
            'tests/fixtures/extends/valid-interpolation.yml')
        for service in service_dicts:
            assert service['hostname'] == expected_interpolated_value

    @pytest.mark.xfail(IS_WINDOWS_PLATFORM, reason='paths use slash')
    def test_volume_path(self):
        dicts = load_from_filename('tests/fixtures/volume-path/docker-compose.yml')

        paths = [
            VolumeSpec(
                os.path.abspath('tests/fixtures/volume-path/common/foo'),
                '/foo',
                'rw'),
            VolumeSpec(
                os.path.abspath('tests/fixtures/volume-path/bar'),
                '/bar',
                'rw')
        ]

        assert set(dicts[0]['volumes']) == set(paths)

    def test_parent_build_path_dne(self):
        child = load_from_filename('tests/fixtures/extends/nonexistent-path-child.yml')

        assert child == [
            {
                'name': 'dnechild',
                'image': 'busybox',
                'command': '/bin/true',
                'environment': {
                    "FOO": "1",
                    "BAR": "2",
                },
            },
        ]

    def test_load_throws_error_when_base_service_does_not_exist(self):
        with pytest.raises(ConfigurationError) as excinfo:
            load_from_filename('tests/fixtures/extends/nonexistent-service.yml')

        assert "Cannot extend service 'foo'" in excinfo.exconly()
        assert "Service not found" in excinfo.exconly()

    def test_partial_service_config_in_extends_is_still_valid(self):
        dicts = load_from_filename('tests/fixtures/extends/valid-common-config.yml')
        assert dicts[0]['environment'] == {'FOO': '1'}

    def test_extended_service_with_verbose_and_shorthand_way(self):
        services = load_from_filename('tests/fixtures/extends/verbose-and-shorthand.yml')
        assert service_sort(services) == service_sort([
            {
                'name': 'base',
                'image': 'busybox',
                'environment': {'BAR': '1'},
            },
            {
                'name': 'verbose',
                'image': 'busybox',
                'environment': {'BAR': '1', 'FOO': '1'},
            },
            {
                'name': 'shorthand',
                'image': 'busybox',
                'environment': {'BAR': '1', 'FOO': '2'},
            },
        ])

    @mock.patch.dict(os.environ)
    def test_extends_with_environment_and_env_files(self):
        tmpdir = py.test.ensuretemp('test_extends_with_environment')
        self.addCleanup(tmpdir.remove)
        commondir = tmpdir.mkdir('common')
        commondir.join('base.yml').write("""
            app:
                image: 'example/app'
                env_file:
                    - 'envs'
                environment:
                    - SECRET
                    - TEST_ONE=common
                    - TEST_TWO=common
        """)
        tmpdir.join('docker-compose.yml').write("""
            ext:
                extends:
                    file: common/base.yml
                    service: app
                env_file:
                    - 'envs'
                environment:
                    - THING
                    - TEST_ONE=top
        """)
        commondir.join('envs').write("""
            COMMON_ENV_FILE
            TEST_ONE=common-env-file
            TEST_TWO=common-env-file
            TEST_THREE=common-env-file
            TEST_FOUR=common-env-file
        """)
        tmpdir.join('envs').write("""
            TOP_ENV_FILE
            TEST_ONE=top-env-file
            TEST_TWO=top-env-file
            TEST_THREE=top-env-file
        """)

        expected = [
            {
                'name': 'ext',
                'image': 'example/app',
                'environment': {
                    'SECRET': 'secret',
                    'TOP_ENV_FILE': 'secret',
                    'COMMON_ENV_FILE': 'secret',
                    'THING': 'thing',
                    'TEST_ONE': 'top',
                    'TEST_TWO': 'common',
                    'TEST_THREE': 'top-env-file',
                    'TEST_FOUR': 'common-env-file',
                },
            },
        ]

        os.environ['SECRET'] = 'secret'
        os.environ['THING'] = 'thing'
        os.environ['COMMON_ENV_FILE'] = 'secret'
        os.environ['TOP_ENV_FILE'] = 'secret'
        config = load_from_filename(str(tmpdir.join('docker-compose.yml')))

        assert config == expected

    def test_extends_with_mixed_versions_is_error(self):
        tmpdir = py.test.ensuretemp('test_extends_with_mixed_version')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('docker-compose.yml').write("""
            version: "2"
            services:
              web:
                extends:
                  file: base.yml
                  service: base
                image: busybox
        """)
        tmpdir.join('base.yml').write("""
            base:
              volumes: ['/foo']
              ports: ['3000:3000']
        """)

        with pytest.raises(ConfigurationError) as exc:
            load_from_filename(str(tmpdir.join('docker-compose.yml')))
        assert 'Version mismatch' in exc.exconly()

    def test_extends_with_defined_version_passes(self):
        tmpdir = py.test.ensuretemp('test_extends_with_defined_version')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('docker-compose.yml').write("""
            version: "2"
            services:
              web:
                extends:
                  file: base.yml
                  service: base
                image: busybox
        """)
        tmpdir.join('base.yml').write("""
            version: "2"
            services:
                base:
                  volumes: ['/foo']
                  ports: ['3000:3000']
                  command: top
        """)

        service = load_from_filename(str(tmpdir.join('docker-compose.yml')))
        assert service[0]['command'] == "top"

    def test_extends_with_depends_on(self):
        tmpdir = py.test.ensuretemp('test_extends_with_depends_on')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('docker-compose.yml').write("""
            version: "2"
            services:
              base:
                image: example
              web:
                extends: base
                image: busybox
                depends_on: ['other']
              other:
                image: example
        """)
        services = load_from_filename(str(tmpdir.join('docker-compose.yml')))
        assert service_sort(services)[2]['depends_on'] == {
            'other': {'condition': 'service_started'}
        }

    def test_extends_with_healthcheck(self):
        service_dicts = load_from_filename('tests/fixtures/extends/healthcheck-2.yml')
        assert service_sort(service_dicts) == [{
            'name': 'demo',
            'image': 'foobar:latest',
            'healthcheck': {
                'test': ['CMD', '/health.sh'],
                'interval': 10000000000,
                'timeout': 5000000000,
                'retries': 36,
            }
        }]

    def test_extends_with_ports(self):
        tmpdir = py.test.ensuretemp('test_extends_with_ports')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('docker-compose.yml').write("""
            version: '2'

            services:
              a:
                image: nginx
                ports:
                  - 80

              b:
                extends:
                  service: a
        """)
        services = load_from_filename(str(tmpdir.join('docker-compose.yml')))

        assert len(services) == 2
        for svc in services:
            assert svc['ports'] == [types.ServicePort('80', None, None, None, None)]

    def test_extends_with_security_opt(self):
        tmpdir = py.test.ensuretemp('test_extends_with_ports')
        self.addCleanup(tmpdir.remove)
        tmpdir.join('docker-compose.yml').write("""
            version: '2'

            services:
              a:
                image: nginx
                security_opt:
                  - apparmor:unconfined
                  - seccomp:unconfined

              b:
                extends:
                  service: a
        """)
        services = load_from_filename(str(tmpdir.join('docker-compose.yml')))
        assert len(services) == 2
        for svc in services:
            assert types.SecurityOpt.parse('apparmor:unconfined') in svc['security_opt']
            assert types.SecurityOpt.parse('seccomp:unconfined') in svc['security_opt']


@pytest.mark.xfail(IS_WINDOWS_PLATFORM, reason='paths use slash')
class ExpandPathTest(unittest.TestCase):
    working_dir = '/home/user/somedir'

    def test_expand_path_normal(self):
        result = config.expand_path(self.working_dir, 'myfile')
        assert result == self.working_dir + '/' + 'myfile'

    def test_expand_path_absolute(self):
        abs_path = '/home/user/otherdir/somefile'
        result = config.expand_path(self.working_dir, abs_path)
        assert result == abs_path

    def test_expand_path_with_tilde(self):
        test_path = '~/otherdir/somefile'
        with mock.patch.dict(os.environ):
            os.environ['HOME'] = user_path = '/home/user/'
            result = config.expand_path(self.working_dir, test_path)

        assert result == user_path + 'otherdir/somefile'


class VolumePathTest(unittest.TestCase):

    def test_split_path_mapping_with_windows_path(self):
        host_path = "c:\\Users\\msamblanet\\Documents\\anvil\\connect\\config"
        windows_volume_path = host_path + ":/opt/connect/config:ro"
        expected_mapping = ("/opt/connect/config", (host_path, 'ro'))

        mapping = config.split_path_mapping(windows_volume_path)
        assert mapping == expected_mapping

    def test_split_path_mapping_with_windows_path_in_container(self):
        host_path = 'c:\\Users\\remilia\\data'
        container_path = 'c:\\scarletdevil\\data'
        expected_mapping = (container_path, (host_path, None))

        mapping = config.split_path_mapping('{0}:{1}'.format(host_path, container_path))
        assert mapping == expected_mapping

    def test_split_path_mapping_with_root_mount(self):
        host_path = '/'
        container_path = '/var/hostroot'
        expected_mapping = (container_path, (host_path, None))
        mapping = config.split_path_mapping('{0}:{1}'.format(host_path, container_path))
        assert mapping == expected_mapping


@pytest.mark.xfail(IS_WINDOWS_PLATFORM, reason='paths use slash')
class BuildPathTest(unittest.TestCase):

    def setUp(self):
        self.abs_context_path = os.path.join(os.getcwd(), 'tests/fixtures/build-ctx')

    def test_nonexistent_path(self):
        with pytest.raises(ConfigurationError):
            config.load(
                build_config_details(
                    {
                        'foo': {'build': 'nonexistent.path'},
                    },
                    'working_dir',
                    'filename.yml'
                )
            )

    def test_relative_path(self):
        relative_build_path = '../build-ctx/'
        service_dict = make_service_dict(
            'relpath',
            {'build': relative_build_path},
            working_dir='tests/fixtures/build-path'
        )
        assert service_dict['build'] == self.abs_context_path

    def test_absolute_path(self):
        service_dict = make_service_dict(
            'abspath',
            {'build': self.abs_context_path},
            working_dir='tests/fixtures/build-path'
        )
        assert service_dict['build'] == self.abs_context_path

    def test_from_file(self):
        service_dict = load_from_filename('tests/fixtures/build-path/docker-compose.yml')
        assert service_dict == [{'name': 'foo', 'build': {'context': self.abs_context_path}}]

    def test_from_file_override_dir(self):
        override_dir = os.path.join(os.getcwd(), 'tests/fixtures/')
        service_dict = load_from_filename(
            'tests/fixtures/build-path-override-dir/docker-compose.yml', override_dir=override_dir)
        assert service_dict == [{'name': 'foo', 'build': {'context': self.abs_context_path}}]

    def test_valid_url_in_build_path(self):
        valid_urls = [
            'git://github.com/docker/docker',
            'git@github.com:docker/docker.git',
            'git@bitbucket.org:atlassianlabs/atlassian-docker.git',
            'https://github.com/docker/docker.git',
            'http://github.com/docker/docker.git',
            'github.com/docker/docker.git',
        ]
        for valid_url in valid_urls:
            service_dict = config.load(build_config_details({
                'validurl': {'build': valid_url},
            }, '.', None)).services
            assert service_dict[0]['build'] == {'context': valid_url}

    def test_invalid_url_in_build_path(self):
        invalid_urls = [
            'example.com/bogus',
            'ftp://example.com/',
            '/path/does/not/exist',
        ]
        for invalid_url in invalid_urls:
            with pytest.raises(ConfigurationError) as exc:
                config.load(build_config_details({
                    'invalidurl': {'build': invalid_url},
                }, '.', None))
            assert 'build path' in exc.exconly()


class HealthcheckTest(unittest.TestCase):
    def test_healthcheck(self):
        config_dict = config.load(
            build_config_details({
                'version': '2.3',
                'services': {
                    'test': {
                        'image': 'busybox',
                        'healthcheck': {
                            'test': ['CMD', 'true'],
                            'interval': '1s',
                            'timeout': '1m',
                            'retries': 3,
                            'start_period': '10s',
                        }
                    }
                }

            })
        )

        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_service = serialized_config['services']['test']

        assert serialized_service['healthcheck'] == {
            'test': ['CMD', 'true'],
            'interval': '1s',
            'timeout': '1m',
            'retries': 3,
            'start_period': '10s'
        }

    def test_disable(self):
        config_dict = config.load(
            build_config_details({
                'version': '2.3',
                'services': {
                    'test': {
                        'image': 'busybox',
                        'healthcheck': {
                            'disable': True,
                        }
                    }
                }

            })
        )

        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_service = serialized_config['services']['test']

        assert serialized_service['healthcheck'] == {
            'test': ['NONE'],
        }

    def test_disable_with_other_config_is_invalid(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details({
                    'version': '2.3',
                    'services': {
                        'invalid-healthcheck': {
                            'image': 'busybox',
                            'healthcheck': {
                                'disable': True,
                                'interval': '1s',
                            }
                        }
                    }

                })
            )

        assert 'invalid-healthcheck' in excinfo.exconly()
        assert '"disable: true" cannot be combined with other options' in excinfo.exconly()

    def test_healthcheck_with_invalid_test(self):
        with pytest.raises(ConfigurationError) as excinfo:
            config.load(
                build_config_details({
                    'version': '2.3',
                    'services': {
                        'invalid-healthcheck': {
                            'image': 'busybox',
                            'healthcheck': {
                                'test': ['true'],
                                'interval': '1s',
                                'timeout': '1m',
                                'retries': 3,
                                'start_period': '10s',
                            }
                        }
                    }

                })
            )

        assert 'invalid-healthcheck' in excinfo.exconly()
        assert 'the first item must be either NONE, CMD or CMD-SHELL' in excinfo.exconly()


class GetDefaultConfigFilesTestCase(unittest.TestCase):

    files = [
        'docker-compose.yml',
        'docker-compose.yaml',
    ]

    def test_get_config_path_default_file_in_basedir(self):
        for index, filename in enumerate(self.files):
            assert filename == get_config_filename_for_files(self.files[index:])
        with pytest.raises(config.ComposeFileNotFound):
            get_config_filename_for_files([])

    def test_get_config_path_default_file_in_parent_dir(self):
        """Test with files placed in the subdir"""

        def get_config_in_subdir(files):
            return get_config_filename_for_files(files, subdir=True)

        for index, filename in enumerate(self.files):
            assert filename == get_config_in_subdir(self.files[index:])
        with pytest.raises(config.ComposeFileNotFound):
            get_config_in_subdir([])


def get_config_filename_for_files(filenames, subdir=None):
    def make_files(dirname, filenames):
        for fname in filenames:
            with open(os.path.join(dirname, fname), 'w') as f:
                f.write('')

    project_dir = tempfile.mkdtemp()
    try:
        make_files(project_dir, filenames)
        if subdir:
            base_dir = tempfile.mkdtemp(dir=project_dir)
        else:
            base_dir = project_dir
        filename, = config.get_default_config_files(base_dir)
        return os.path.basename(filename)
    finally:
        shutil.rmtree(project_dir)


class SerializeTest(unittest.TestCase):
    def test_denormalize_depends_on_v3(self):
        service_dict = {
            'image': 'busybox',
            'command': 'true',
            'depends_on': {
                'service2': {'condition': 'service_started'},
                'service3': {'condition': 'service_started'},
            }
        }

        assert denormalize_service_dict(service_dict, V3_0) == {
            'image': 'busybox',
            'command': 'true',
            'depends_on': ['service2', 'service3']
        }

    def test_denormalize_depends_on_v2_1(self):
        service_dict = {
            'image': 'busybox',
            'command': 'true',
            'depends_on': {
                'service2': {'condition': 'service_started'},
                'service3': {'condition': 'service_started'},
            }
        }

        assert denormalize_service_dict(service_dict, V2_1) == service_dict

    def test_serialize_time(self):
        data = {
            9: '9ns',
            9000: '9us',
            9000000: '9ms',
            90000000: '90ms',
            900000000: '900ms',
            999999999: '999999999ns',
            1000000000: '1s',
            60000000000: '1m',
            60000000001: '60000000001ns',
            9000000000000: '150m',
            90000000000000: '25h',
        }

        for k, v in data.items():
            assert serialize_ns_time_value(k) == v

    def test_denormalize_healthcheck(self):
        service_dict = {
            'image': 'test',
            'healthcheck': {
                'test': 'exit 1',
                'interval': '1m40s',
                'timeout': '30s',
                'retries': 5,
                'start_period': '2s90ms'
            }
        }
        processed_service = config.process_service(config.ServiceConfig(
            '.', 'test', 'test', service_dict
        ))
        denormalized_service = denormalize_service_dict(processed_service, V2_3)
        assert denormalized_service['healthcheck']['interval'] == '100s'
        assert denormalized_service['healthcheck']['timeout'] == '30s'
        assert denormalized_service['healthcheck']['start_period'] == '2090ms'

    def test_denormalize_image_has_digest(self):
        service_dict = {
            'image': 'busybox'
        }
        image_digest = 'busybox@sha256:abcde'

        assert denormalize_service_dict(service_dict, V3_0, image_digest) == {
            'image': 'busybox@sha256:abcde'
        }

    def test_denormalize_image_no_digest(self):
        service_dict = {
            'image': 'busybox'
        }

        assert denormalize_service_dict(service_dict, V3_0) == {
            'image': 'busybox'
        }

    def test_serialize_secrets(self):
        service_dict = {
            'image': 'example/web',
            'secrets': [
                {'source': 'one'},
                {
                    'source': 'source',
                    'target': 'target',
                    'uid': '100',
                    'gid': '200',
                    'mode': 0o777,
                }
            ]
        }
        secrets_dict = {
            'one': {'file': '/one.txt'},
            'source': {'file': '/source.pem'},
            'two': {'external': True},
        }
        config_dict = config.load(build_config_details({
            'version': '3.1',
            'services': {'web': service_dict},
            'secrets': secrets_dict
        }))

        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_service = serialized_config['services']['web']
        assert secret_sort(serialized_service['secrets']) == secret_sort(service_dict['secrets'])
        assert 'secrets' in serialized_config
        assert serialized_config['secrets']['two'] == secrets_dict['two']

    def test_serialize_ports(self):
        config_dict = config.Config(version=V2_0, services=[
            {
                'ports': [types.ServicePort('80', '8080', None, None, None)],
                'image': 'alpine',
                'name': 'web'
            }
        ], volumes={}, networks={}, secrets={}, configs={})

        serialized_config = yaml.load(serialize_config(config_dict))
        assert '8080:80/tcp' in serialized_config['services']['web']['ports']

    def test_serialize_ports_with_ext_ip(self):
        config_dict = config.Config(version=V3_5, services=[
            {
                'ports': [types.ServicePort('80', '8080', None, None, '127.0.0.1')],
                'image': 'alpine',
                'name': 'web'
            }
        ], volumes={}, networks={}, secrets={}, configs={})

        serialized_config = yaml.load(serialize_config(config_dict))
        assert '127.0.0.1:8080:80/tcp' in serialized_config['services']['web']['ports']

    def test_serialize_configs(self):
        service_dict = {
            'image': 'example/web',
            'configs': [
                {'source': 'one'},
                {
                    'source': 'source',
                    'target': 'target',
                    'uid': '100',
                    'gid': '200',
                    'mode': 0o777,
                }
            ]
        }
        configs_dict = {
            'one': {'file': '/one.txt'},
            'source': {'file': '/source.pem'},
            'two': {'external': True},
        }
        config_dict = config.load(build_config_details({
            'version': '3.3',
            'services': {'web': service_dict},
            'configs': configs_dict
        }))

        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_service = serialized_config['services']['web']
        assert secret_sort(serialized_service['configs']) == secret_sort(service_dict['configs'])
        assert 'configs' in serialized_config
        assert serialized_config['configs']['two'] == configs_dict['two']

    def test_serialize_bool_string(self):
        cfg = {
            'version': '2.2',
            'services': {
                'web': {
                    'image': 'example/web',
                    'command': 'true',
                    'environment': {'FOO': 'Y', 'BAR': 'on'}
                }
            }
        }
        config_dict = config.load(build_config_details(cfg))

        serialized_config = serialize_config(config_dict)
        assert 'command: "true"\n' in serialized_config
        assert 'FOO: "Y"\n' in serialized_config
        assert 'BAR: "on"\n' in serialized_config

    def test_serialize_escape_dollar_sign(self):
        cfg = {
            'version': '2.2',
            'services': {
                'web': {
                    'image': 'busybox',
                    'command': 'echo $$FOO',
                    'environment': {
                        'CURRENCY': '$$'
                    },
                    'entrypoint': ['$$SHELL', '-c'],
                }
            }
        }
        config_dict = config.load(build_config_details(cfg))

        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_service = serialized_config['services']['web']
        assert serialized_service['environment']['CURRENCY'] == '$$'
        assert serialized_service['command'] == 'echo $$FOO'
        assert serialized_service['entrypoint'][0] == '$$SHELL'

    def test_serialize_unicode_values(self):
        cfg = {
            'version': '2.3',
            'services': {
                'web': {
                    'image': 'busybox',
                    'command': 'echo '
                }
            }
        }

        config_dict = config.load(build_config_details(cfg))

        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_service = serialized_config['services']['web']
        assert serialized_service['command'] == 'echo '

    def test_serialize_external_false(self):
        cfg = {
            'version': '3.4',
            'volumes': {
                'test': {
                    'name': 'test-false',
                    'external': False
                }
            }
        }

        config_dict = config.load(build_config_details(cfg))
        serialized_config = yaml.load(serialize_config(config_dict))
        serialized_volume = serialized_config['volumes']['test']
        assert serialized_volume['external'] is False
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest

from compose.config.errors import ConfigurationError
from compose.config.types import parse_extra_hosts
from compose.config.types import ServicePort
from compose.config.types import VolumeFromSpec
from compose.config.types import VolumeSpec
from compose.const import COMPOSEFILE_V1 as V1
from compose.const import COMPOSEFILE_V2_0 as V2_0


def test_parse_extra_hosts_list():
    expected = {'www.example.com': '192.168.0.17'}
    assert parse_extra_hosts(["www.example.com:192.168.0.17"]) == expected

    expected = {'www.example.com': '192.168.0.17'}
    assert parse_extra_hosts(["www.example.com: 192.168.0.17"]) == expected

    assert parse_extra_hosts([
        "www.example.com: 192.168.0.17",
        "static.example.com:192.168.0.19",
        "api.example.com: 192.168.0.18",
        "v6.example.com: ::1"
    ]) == {
        'www.example.com': '192.168.0.17',
        'static.example.com': '192.168.0.19',
        'api.example.com': '192.168.0.18',
        'v6.example.com': '::1'
    }


def test_parse_extra_hosts_dict():
    assert parse_extra_hosts({
        'www.example.com': '192.168.0.17',
        'api.example.com': '192.168.0.18'
    }) == {
        'www.example.com': '192.168.0.17',
        'api.example.com': '192.168.0.18'
    }


class TestServicePort(object):
    def test_parse_dict(self):
        data = {
            'target': 8000,
            'published': 8000,
            'protocol': 'udp',
            'mode': 'global',
        }
        ports = ServicePort.parse(data)
        assert len(ports) == 1
        assert ports[0].repr() == data

    def test_parse_simple_target_port(self):
        ports = ServicePort.parse(8000)
        assert len(ports) == 1
        assert ports[0].target == 8000

    def test_parse_complete_port_definition(self):
        port_def = '1.1.1.1:3000:3000/udp'
        ports = ServicePort.parse(port_def)
        assert len(ports) == 1
        assert ports[0].repr() == {
            'target': 3000,
            'published': 3000,
            'external_ip': '1.1.1.1',
            'protocol': 'udp',
        }
        assert ports[0].legacy_repr() == port_def

    def test_parse_ext_ip_no_published_port(self):
        port_def = '1.1.1.1::3000'
        ports = ServicePort.parse(port_def)
        assert len(ports) == 1
        assert ports[0].legacy_repr() == port_def + '/tcp'
        assert ports[0].repr() == {
            'target': 3000,
            'external_ip': '1.1.1.1',
        }

    def test_repr_published_port_0(self):
        port_def = '0:4000'
        ports = ServicePort.parse(port_def)
        assert len(ports) == 1
        assert ports[0].legacy_repr() == port_def + '/tcp'

    def test_parse_port_range(self):
        ports = ServicePort.parse('25000-25001:4000-4001')
        assert len(ports) == 2
        reprs = [p.repr() for p in ports]
        assert {
            'target': 4000,
            'published': 25000
        } in reprs
        assert {
            'target': 4001,
            'published': 25001
        } in reprs

    def test_parse_port_publish_range(self):
        ports = ServicePort.parse('4440-4450:4000')
        assert len(ports) == 1
        reprs = [p.repr() for p in ports]
        assert {
            'target': 4000,
            'published': '4440-4450'
        } in reprs

    def test_parse_invalid_port(self):
        port_def = '4000p'
        with pytest.raises(ConfigurationError):
            ServicePort.parse(port_def)

    def test_parse_invalid_publish_range(self):
        port_def = '-4000:4000'
        with pytest.raises(ConfigurationError):
            ServicePort.parse(port_def)

        port_def = 'asdf:4000'
        with pytest.raises(ConfigurationError):
            ServicePort.parse(port_def)

        port_def = '1234-12f:4000'
        with pytest.raises(ConfigurationError):
            ServicePort.parse(port_def)

        port_def = '1234-1235-1239:4000'
        with pytest.raises(ConfigurationError):
            ServicePort.parse(port_def)


class TestVolumeSpec(object):

    def test_parse_volume_spec_only_one_path(self):
        spec = VolumeSpec.parse('/the/volume')
        assert spec == (None, '/the/volume', 'rw')

    def test_parse_volume_spec_internal_and_external(self):
        spec = VolumeSpec.parse('external:interval')
        assert spec == ('external', 'interval', 'rw')

    def test_parse_volume_spec_with_mode(self):
        spec = VolumeSpec.parse('external:interval:ro')
        assert spec == ('external', 'interval', 'ro')

        spec = VolumeSpec.parse('external:interval:z')
        assert spec == ('external', 'interval', 'z')

    def test_parse_volume_spec_too_many_parts(self):
        with pytest.raises(ConfigurationError) as exc:
            VolumeSpec.parse('one:two:three:four')
        assert 'has incorrect format' in exc.exconly()

    def test_parse_volume_windows_absolute_path_normalized(self):
        windows_path = "c:\\Users\\me\\Documents\\shiny\\config:/opt/shiny/config:ro"
        assert VolumeSpec._parse_win32(windows_path, True) == (
            "/c/Users/me/Documents/shiny/config",
            "/opt/shiny/config",
            "ro"
        )

    def test_parse_volume_windows_absolute_path_native(self):
        windows_path = "c:\\Users\\me\\Documents\\shiny\\config:/opt/shiny/config:ro"
        assert VolumeSpec._parse_win32(windows_path, False) == (
            "c:\\Users\\me\\Documents\\shiny\\config",
            "/opt/shiny/config",
            "ro"
        )

    def test_parse_volume_windows_internal_path_normalized(self):
        windows_path = 'C:\\Users\\reimu\\scarlet:C:\\scarlet\\app:ro'
        assert VolumeSpec._parse_win32(windows_path, True) == (
            '/c/Users/reimu/scarlet',
            'C:\\scarlet\\app',
            'ro'
        )

    def test_parse_volume_windows_internal_path_native(self):
        windows_path = 'C:\\Users\\reimu\\scarlet:C:\\scarlet\\app:ro'
        assert VolumeSpec._parse_win32(windows_path, False) == (
            'C:\\Users\\reimu\\scarlet',
            'C:\\scarlet\\app',
            'ro'
        )

    def test_parse_volume_windows_just_drives_normalized(self):
        windows_path = 'E:\\:C:\\:ro'
        assert VolumeSpec._parse_win32(windows_path, True) == (
            '/e/',
            'C:\\',
            'ro'
        )

    def test_parse_volume_windows_just_drives_native(self):
        windows_path = 'E:\\:C:\\:ro'
        assert VolumeSpec._parse_win32(windows_path, False) == (
            'E:\\',
            'C:\\',
            'ro'
        )

    def test_parse_volume_windows_mixed_notations_normalized(self):
        windows_path = 'C:\\Foo:/root/foo'
        assert VolumeSpec._parse_win32(windows_path, True) == (
            '/c/Foo',
            '/root/foo',
            'rw'
        )

    def test_parse_volume_windows_mixed_notations_native(self):
        windows_path = 'C:\\Foo:/root/foo'
        assert VolumeSpec._parse_win32(windows_path, False) == (
            'C:\\Foo',
            '/root/foo',
            'rw'
        )


class TestVolumesFromSpec(object):

    services = ['servicea', 'serviceb']

    def test_parse_v1_from_service(self):
        volume_from = VolumeFromSpec.parse('servicea', self.services, V1)
        assert volume_from == VolumeFromSpec('servicea', 'rw', 'service')

    def test_parse_v1_from_container(self):
        volume_from = VolumeFromSpec.parse('foo:ro', self.services, V1)
        assert volume_from == VolumeFromSpec('foo', 'ro', 'container')

    def test_parse_v1_invalid(self):
        with pytest.raises(ConfigurationError):
            VolumeFromSpec.parse('unknown:format:ro', self.services, V1)

    def test_parse_v2_from_service(self):
        volume_from = VolumeFromSpec.parse('servicea', self.services, V2_0)
        assert volume_from == VolumeFromSpec('servicea', 'rw', 'service')

    def test_parse_v2_from_service_with_mode(self):
        volume_from = VolumeFromSpec.parse('servicea:ro', self.services, V2_0)
        assert volume_from == VolumeFromSpec('servicea', 'ro', 'service')

    def test_parse_v2_from_container(self):
        volume_from = VolumeFromSpec.parse('container:foo', self.services, V2_0)
        assert volume_from == VolumeFromSpec('foo', 'rw', 'container')

    def test_parse_v2_from_container_with_mode(self):
        volume_from = VolumeFromSpec.parse('container:foo:ro', self.services, V2_0)
        assert volume_from == VolumeFromSpec('foo', 'ro', 'container')

    def test_parse_v2_invalid_type(self):
        with pytest.raises(ConfigurationError) as exc:
            VolumeFromSpec.parse('bogus:foo:ro', self.services, V2_0)
        assert "Unknown volumes_from type 'bogus'" in exc.exconly()

    def test_parse_v2_invalid(self):
        with pytest.raises(ConfigurationError):
            VolumeFromSpec.parse('unknown:format:ro', self.services, V2_0)
<EOF>
<BOF>
# encoding: utf-8
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest

from compose.config.environment import Environment
from compose.config.errors import ConfigurationError
from compose.config.interpolation import interpolate_environment_variables
from compose.config.interpolation import Interpolator
from compose.config.interpolation import InvalidInterpolation
from compose.config.interpolation import TemplateWithDefaults
from compose.config.interpolation import UnsetRequiredSubstitution
from compose.const import COMPOSEFILE_V2_0 as V2_0
from compose.const import COMPOSEFILE_V2_3 as V2_3
from compose.const import COMPOSEFILE_V3_4 as V3_4


@pytest.fixture
def mock_env():
    return Environment({
        'USER': 'jenny',
        'FOO': 'bar',
        'TRUE': 'True',
        'FALSE': 'OFF',
        'POSINT': '50',
        'NEGINT': '-200',
        'FLOAT': '0.145',
        'MODE': '0600',
        'BYTES': '512m',
    })


@pytest.fixture
def variable_mapping():
    return Environment({'FOO': 'first', 'BAR': ''})


@pytest.fixture
def defaults_interpolator(variable_mapping):
    return Interpolator(TemplateWithDefaults, variable_mapping).interpolate


def test_interpolate_environment_variables_in_services(mock_env):
    services = {
        'servicea': {
            'image': 'example:${USER}',
            'volumes': ['$FOO:/target'],
            'logging': {
                'driver': '${FOO}',
                'options': {
                    'user': '$USER',
                }
            }
        }
    }
    expected = {
        'servicea': {
            'image': 'example:jenny',
            'volumes': ['bar:/target'],
            'logging': {
                'driver': 'bar',
                'options': {
                    'user': 'jenny',
                }
            }
        }
    }
    value = interpolate_environment_variables(V2_0, services, 'service', mock_env)
    assert value == expected


def test_interpolate_environment_variables_in_volumes(mock_env):
    volumes = {
        'data': {
            'driver': '$FOO',
            'driver_opts': {
                'max': 2,
                'user': '${USER}'
            }
        },
        'other': None,
    }
    expected = {
        'data': {
            'driver': 'bar',
            'driver_opts': {
                'max': 2,
                'user': 'jenny'
            }
        },
        'other': {},
    }
    value = interpolate_environment_variables(V2_0, volumes, 'volume', mock_env)
    assert value == expected


def test_interpolate_environment_variables_in_secrets(mock_env):
    secrets = {
        'secretservice': {
            'file': '$FOO',
            'labels': {
                'max': 2,
                'user': '${USER}'
            }
        },
        'other': None,
    }
    expected = {
        'secretservice': {
            'file': 'bar',
            'labels': {
                'max': '2',
                'user': 'jenny'
            }
        },
        'other': {},
    }
    value = interpolate_environment_variables(V3_4, secrets, 'secret', mock_env)
    assert value == expected


def test_interpolate_environment_services_convert_types_v2(mock_env):
    entry = {
        'service1': {
            'blkio_config': {
                'weight': '${POSINT}',
                'weight_device': [{'file': '/dev/sda1', 'weight': '${POSINT}'}]
            },
            'cpus': '${FLOAT}',
            'cpu_count': '$POSINT',
            'healthcheck': {
                'retries': '${POSINT:-3}',
                'disable': '${FALSE}',
                'command': 'true'
            },
            'mem_swappiness': '${DEFAULT:-127}',
            'oom_score_adj': '${NEGINT}',
            'scale': '${POSINT}',
            'ulimits': {
                'nproc': '${POSINT}',
                'nofile': {
                    'soft': '${POSINT}',
                    'hard': '${DEFAULT:-40000}'
                },
            },
            'privileged': '${TRUE}',
            'read_only': '${DEFAULT:-no}',
            'tty': '${DEFAULT:-N}',
            'stdin_open': '${DEFAULT-on}',
            'volumes': [
                {'type': 'tmpfs', 'target': '/target', 'tmpfs': {'size': '$BYTES'}}
            ]
        }
    }

    expected = {
        'service1': {
            'blkio_config': {
                'weight': 50,
                'weight_device': [{'file': '/dev/sda1', 'weight': 50}]
            },
            'cpus': 0.145,
            'cpu_count': 50,
            'healthcheck': {
                'retries': 50,
                'disable': False,
                'command': 'true'
            },
            'mem_swappiness': 127,
            'oom_score_adj': -200,
            'scale': 50,
            'ulimits': {
                'nproc': 50,
                'nofile': {
                    'soft': 50,
                    'hard': 40000
                },
            },
            'privileged': True,
            'read_only': False,
            'tty': False,
            'stdin_open': True,
            'volumes': [
                {'type': 'tmpfs', 'target': '/target', 'tmpfs': {'size': 536870912}}
            ]
        }
    }

    value = interpolate_environment_variables(V2_3, entry, 'service', mock_env)
    assert value == expected


def test_interpolate_environment_services_convert_types_v3(mock_env):
    entry = {
        'service1': {
            'healthcheck': {
                'retries': '${POSINT:-3}',
                'disable': '${FALSE}',
                'command': 'true'
            },
            'ulimits': {
                'nproc': '${POSINT}',
                'nofile': {
                    'soft': '${POSINT}',
                    'hard': '${DEFAULT:-40000}'
                },
            },
            'privileged': '${TRUE}',
            'read_only': '${DEFAULT:-no}',
            'tty': '${DEFAULT:-N}',
            'stdin_open': '${DEFAULT-on}',
            'deploy': {
                'update_config': {
                    'parallelism': '${DEFAULT:-2}',
                    'max_failure_ratio': '${FLOAT}',
                },
                'restart_policy': {
                    'max_attempts': '$POSINT',
                },
                'replicas': '${DEFAULT-3}'
            },
            'ports': [{'target': '${POSINT}', 'published': '${DEFAULT:-5000}'}],
            'configs': [{'mode': '${MODE}', 'source': 'config1'}],
            'secrets': [{'mode': '${MODE}', 'source': 'secret1'}],
        }
    }

    expected = {
        'service1': {
            'healthcheck': {
                'retries': 50,
                'disable': False,
                'command': 'true'
            },
            'ulimits': {
                'nproc': 50,
                'nofile': {
                    'soft': 50,
                    'hard': 40000
                },
            },
            'privileged': True,
            'read_only': False,
            'tty': False,
            'stdin_open': True,
            'deploy': {
                'update_config': {
                    'parallelism': 2,
                    'max_failure_ratio': 0.145,
                },
                'restart_policy': {
                    'max_attempts': 50,
                },
                'replicas': 3
            },
            'ports': [{'target': 50, 'published': 5000}],
            'configs': [{'mode': 0o600, 'source': 'config1'}],
            'secrets': [{'mode': 0o600, 'source': 'secret1'}],
        }
    }

    value = interpolate_environment_variables(V3_4, entry, 'service', mock_env)
    assert value == expected


def test_interpolate_environment_services_convert_types_invalid(mock_env):
    entry = {'service1': {'privileged': '${POSINT}'}}

    with pytest.raises(ConfigurationError) as exc:
        interpolate_environment_variables(V2_3, entry, 'service', mock_env)

    assert 'Error while attempting to convert service.service1.privileged to '\
        'appropriate type: "50" is not a valid boolean value' in exc.exconly()

    entry = {'service1': {'cpus': '${TRUE}'}}
    with pytest.raises(ConfigurationError) as exc:
        interpolate_environment_variables(V2_3, entry, 'service', mock_env)

    assert 'Error while attempting to convert service.service1.cpus to '\
        'appropriate type: "True" is not a valid float' in exc.exconly()

    entry = {'service1': {'ulimits': {'nproc': '${FLOAT}'}}}
    with pytest.raises(ConfigurationError) as exc:
        interpolate_environment_variables(V2_3, entry, 'service', mock_env)

    assert 'Error while attempting to convert service.service1.ulimits.nproc to '\
        'appropriate type: "0.145" is not a valid integer' in exc.exconly()


def test_interpolate_environment_network_convert_types(mock_env):
    entry = {
        'network1': {
            'external': '${FALSE}',
            'attachable': '${TRUE}',
            'internal': '${DEFAULT:-false}'
        }
    }

    expected = {
        'network1': {
            'external': False,
            'attachable': True,
            'internal': False,
        }
    }

    value = interpolate_environment_variables(V3_4, entry, 'network', mock_env)
    assert value == expected


def test_interpolate_environment_external_resource_convert_types(mock_env):
    entry = {
        'resource1': {
            'external': '${TRUE}',
        }
    }

    expected = {
        'resource1': {
            'external': True,
        }
    }

    value = interpolate_environment_variables(V3_4, entry, 'network', mock_env)
    assert value == expected
    value = interpolate_environment_variables(V3_4, entry, 'volume', mock_env)
    assert value == expected
    value = interpolate_environment_variables(V3_4, entry, 'secret', mock_env)
    assert value == expected
    value = interpolate_environment_variables(V3_4, entry, 'config', mock_env)
    assert value == expected


def test_interpolate_service_name_uses_dot(mock_env):
    entry = {
        'service.1': {
            'image': 'busybox',
            'ulimits': {
                'nproc': '${POSINT}',
                'nofile': {
                    'soft': '${POSINT}',
                    'hard': '${DEFAULT:-40000}'
                },
            },
        }
    }

    expected = {
        'service.1': {
            'image': 'busybox',
            'ulimits': {
                'nproc': 50,
                'nofile': {
                    'soft': 50,
                    'hard': 40000
                },
            },
        }
    }

    value = interpolate_environment_variables(V3_4, entry, 'service', mock_env)
    assert value == expected


def test_escaped_interpolation(defaults_interpolator):
    assert defaults_interpolator('$${foo}') == '${foo}'


def test_invalid_interpolation(defaults_interpolator):
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('${')
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('$}')
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('${}')
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('${ }')
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('${ foo}')
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('${foo }')
    with pytest.raises(InvalidInterpolation):
        defaults_interpolator('${foo!}')


def test_interpolate_missing_no_default(defaults_interpolator):
    assert defaults_interpolator("This ${missing} var") == "This  var"
    assert defaults_interpolator("This ${BAR} var") == "This  var"


def test_interpolate_with_value(defaults_interpolator):
    assert defaults_interpolator("This $FOO var") == "This first var"
    assert defaults_interpolator("This ${FOO} var") == "This first var"


def test_interpolate_missing_with_default(defaults_interpolator):
    assert defaults_interpolator("ok ${missing:-def}") == "ok def"
    assert defaults_interpolator("ok ${missing-def}") == "ok def"


def test_interpolate_with_empty_and_default_value(defaults_interpolator):
    assert defaults_interpolator("ok ${BAR:-def}") == "ok def"
    assert defaults_interpolator("ok ${BAR-def}") == "ok "


def test_interpolate_mandatory_values(defaults_interpolator):
    assert defaults_interpolator("ok ${FOO:?bar}") == "ok first"
    assert defaults_interpolator("ok ${FOO?bar}") == "ok first"
    assert defaults_interpolator("ok ${BAR?bar}") == "ok "

    with pytest.raises(UnsetRequiredSubstitution) as e:
        defaults_interpolator("not ok ${BAR:?high bar}")
    assert e.value.err == 'high bar'

    with pytest.raises(UnsetRequiredSubstitution) as e:
        defaults_interpolator("not ok ${BAZ?dropped the bazz}")
    assert e.value.err == 'dropped the bazz'


def test_interpolate_mandatory_no_err_msg(defaults_interpolator):
    with pytest.raises(UnsetRequiredSubstitution) as e:
        defaults_interpolator("not ok ${BAZ?}")

    assert e.value.err == ''


def test_interpolate_mixed_separators(defaults_interpolator):
    assert defaults_interpolator("ok ${BAR:-/non:-alphanumeric}") == "ok /non:-alphanumeric"
    assert defaults_interpolator("ok ${BAR:-:?wwegegr??:?}") == "ok :?wwegegr??:?"
    assert defaults_interpolator("ok ${BAR-:-hello}") == 'ok '

    with pytest.raises(UnsetRequiredSubstitution) as e:
        defaults_interpolator("not ok ${BAR:?xazz:-redf}")
    assert e.value.err == 'xazz:-redf'

    assert defaults_interpolator("ok ${BAR?...:?bar}") == "ok "


def test_unbraced_separators(defaults_interpolator):
    assert defaults_interpolator("ok $FOO:-bar") == "ok first:-bar"
    assert defaults_interpolator("ok $BAZ?error") == "ok ?error"


def test_interpolate_unicode_values():
    variable_mapping = {
        'FOO': ''.encode('utf-8'),
        'BAR': ''
    }
    interpol = Interpolator(TemplateWithDefaults, variable_mapping).interpolate

    interpol("$FOO") == ''
    interpol("${BAR}") == ''


def test_interpolate_no_fallthrough():
    # Test regression on docker/compose#5829
    variable_mapping = {
        'TEST:-': 'hello',
        'TEST-': 'hello',
    }
    interpol = Interpolator(TemplateWithDefaults, variable_mapping).interpolate

    assert interpol('${TEST:-}') == ''
    assert interpol('${TEST-}') == ''
<EOF>
<BOF>
# encoding: utf-8
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import codecs

import pytest

from compose.config.environment import env_vars_from_file
from compose.config.environment import Environment
from tests import unittest


class EnvironmentTest(unittest.TestCase):
    def test_get_simple(self):
        env = Environment({
            'FOO': 'bar',
            'BAR': '1',
            'BAZ': ''
        })

        assert env.get('FOO') == 'bar'
        assert env.get('BAR') == '1'
        assert env.get('BAZ') == ''

    def test_get_undefined(self):
        env = Environment({
            'FOO': 'bar'
        })
        assert env.get('FOOBAR') is None

    def test_get_boolean(self):
        env = Environment({
            'FOO': '',
            'BAR': '0',
            'BAZ': 'FALSE',
            'FOOBAR': 'true',
        })

        assert env.get_boolean('FOO') is False
        assert env.get_boolean('BAR') is False
        assert env.get_boolean('BAZ') is False
        assert env.get_boolean('FOOBAR') is True
        assert env.get_boolean('UNDEFINED') is False

    def test_env_vars_from_file_bom(self):
        tmpdir = pytest.ensuretemp('env_file')
        self.addCleanup(tmpdir.remove)
        with codecs.open('{}/bom.env'.format(str(tmpdir)), 'w', encoding='utf-8') as f:
            f.write('\ufeffPARK_BOM=\n')
        assert env_vars_from_file(str(tmpdir.join('bom.env'))) == {
            'PARK_BOM': ''
        }
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import pytest

from compose.config.errors import DependencyError
from compose.config.sort_services import sort_service_dicts
from compose.config.types import VolumeFromSpec


class TestSortService(object):
    def test_sort_service_dicts_1(self):
        services = [
            {
                'links': ['redis'],
                'name': 'web'
            },
            {
                'name': 'grunt'
            },
            {
                'name': 'redis'
            }
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 3
        assert sorted_services[0]['name'] == 'grunt'
        assert sorted_services[1]['name'] == 'redis'
        assert sorted_services[2]['name'] == 'web'

    def test_sort_service_dicts_2(self):
        services = [
            {
                'links': ['redis', 'postgres'],
                'name': 'web'
            },
            {
                'name': 'postgres',
                'links': ['redis']
            },
            {
                'name': 'redis'
            }
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 3
        assert sorted_services[0]['name'] == 'redis'
        assert sorted_services[1]['name'] == 'postgres'
        assert sorted_services[2]['name'] == 'web'

    def test_sort_service_dicts_3(self):
        services = [
            {
                'name': 'child'
            },
            {
                'name': 'parent',
                'links': ['child']
            },
            {
                'links': ['parent'],
                'name': 'grandparent'
            },
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 3
        assert sorted_services[0]['name'] == 'child'
        assert sorted_services[1]['name'] == 'parent'
        assert sorted_services[2]['name'] == 'grandparent'

    def test_sort_service_dicts_4(self):
        services = [
            {
                'name': 'child'
            },
            {
                'name': 'parent',
                'volumes_from': [VolumeFromSpec('child', 'rw', 'service')]
            },
            {
                'links': ['parent'],
                'name': 'grandparent'
            },
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 3
        assert sorted_services[0]['name'] == 'child'
        assert sorted_services[1]['name'] == 'parent'
        assert sorted_services[2]['name'] == 'grandparent'

    def test_sort_service_dicts_5(self):
        services = [
            {
                'links': ['parent'],
                'name': 'grandparent'
            },
            {
                'name': 'parent',
                'network_mode': 'service:child'
            },
            {
                'name': 'child'
            }
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 3
        assert sorted_services[0]['name'] == 'child'
        assert sorted_services[1]['name'] == 'parent'
        assert sorted_services[2]['name'] == 'grandparent'

    def test_sort_service_dicts_6(self):
        services = [
            {
                'links': ['parent'],
                'name': 'grandparent'
            },
            {
                'name': 'parent',
                'volumes_from': [VolumeFromSpec('child', 'ro', 'service')]
            },
            {
                'name': 'child'
            }
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 3
        assert sorted_services[0]['name'] == 'child'
        assert sorted_services[1]['name'] == 'parent'
        assert sorted_services[2]['name'] == 'grandparent'

    def test_sort_service_dicts_7(self):
        services = [
            {
                'network_mode': 'service:three',
                'name': 'four'
            },
            {
                'links': ['two'],
                'name': 'three'
            },
            {
                'name': 'two',
                'volumes_from': [VolumeFromSpec('one', 'rw', 'service')]
            },
            {
                'name': 'one'
            }
        ]

        sorted_services = sort_service_dicts(services)
        assert len(sorted_services) == 4
        assert sorted_services[0]['name'] == 'one'
        assert sorted_services[1]['name'] == 'two'
        assert sorted_services[2]['name'] == 'three'
        assert sorted_services[3]['name'] == 'four'

    def test_sort_service_dicts_circular_imports(self):
        services = [
            {
                'links': ['redis'],
                'name': 'web'
            },
            {
                'name': 'redis',
                'links': ['web']
            },
        ]

        with pytest.raises(DependencyError) as exc:
            sort_service_dicts(services)
        assert 'redis' in exc.exconly()
        assert 'web' in exc.exconly()

    def test_sort_service_dicts_circular_imports_2(self):
        services = [
            {
                'links': ['postgres', 'redis'],
                'name': 'web'
            },
            {
                'name': 'redis',
                'links': ['web']
            },
            {
                'name': 'postgres'
            }
        ]

        with pytest.raises(DependencyError) as exc:
            sort_service_dicts(services)
        assert 'redis' in exc.exconly()
        assert 'web' in exc.exconly()

    def test_sort_service_dicts_circular_imports_3(self):
        services = [
            {
                'links': ['b'],
                'name': 'a'
            },
            {
                'name': 'b',
                'links': ['c']
            },
            {
                'name': 'c',
                'links': ['a']
            }
        ]

        with pytest.raises(DependencyError) as exc:
            sort_service_dicts(services)
        assert 'a' in exc.exconly()
        assert 'b' in exc.exconly()

    def test_sort_service_dicts_self_imports(self):
        services = [
            {
                'links': ['web'],
                'name': 'web'
            },
        ]

        with pytest.raises(DependencyError) as exc:
            sort_service_dicts(services)
        assert 'web' in exc.exconly()

    def test_sort_service_dicts_depends_on_self(self):
        services = [
            {
                'depends_on': ['web'],
                'name': 'web'
            },
        ]

        with pytest.raises(DependencyError) as exc:
            sort_service_dicts(services)
        assert 'A service can not depend on itself: web' in exc.exconly()
<EOF>
<BOF>
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import unicode_literals

import datetime
import json
import os
import os.path
import re
import signal
import subprocess
import time
from collections import Counter
from collections import namedtuple
from operator import attrgetter

import pytest
import six
import yaml
from docker import errors

from .. import mock
from ..helpers import create_host_file
from compose.cli.command import get_project
from compose.config.errors import DuplicateOverrideFileFound
from compose.container import Container
from compose.project import OneOffFilter
from compose.utils import nanoseconds_from_time_seconds
from tests.integration.testcases import DockerClientTestCase
from tests.integration.testcases import get_links
from tests.integration.testcases import is_cluster
from tests.integration.testcases import no_cluster
from tests.integration.testcases import pull_busybox
from tests.integration.testcases import SWARM_SKIP_RM_VOLUMES
from tests.integration.testcases import v2_1_only
from tests.integration.testcases import v2_2_only
from tests.integration.testcases import v2_only
from tests.integration.testcases import v3_only

ProcessResult = namedtuple('ProcessResult', 'stdout stderr')


BUILD_CACHE_TEXT = 'Using cache'
BUILD_PULL_TEXT = 'Status: Image is up to date for busybox:latest'


def start_process(base_dir, options):
    proc = subprocess.Popen(
        ['docker-compose'] + options,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        cwd=base_dir)
    print("Running process: %s" % proc.pid)
    return proc


def wait_on_process(proc, returncode=0):
    stdout, stderr = proc.communicate()
    if proc.returncode != returncode:
        print("Stderr: {}".format(stderr))
        print("Stdout: {}".format(stdout))
        assert proc.returncode == returncode
    return ProcessResult(stdout.decode('utf-8'), stderr.decode('utf-8'))


def wait_on_condition(condition, delay=0.1, timeout=40):
    start_time = time.time()
    while not condition():
        if time.time() - start_time > timeout:
            raise AssertionError("Timeout: %s" % condition)
        time.sleep(delay)


def kill_service(service):
    for container in service.containers():
        if container.is_running:
            container.kill()


class ContainerCountCondition(object):

    def __init__(self, project, expected):
        self.project = project
        self.expected = expected

    def __call__(self):
        return len([c for c in self.project.containers() if c.is_running]) == self.expected

    def __str__(self):
        return "waiting for counter count == %s" % self.expected


class ContainerStateCondition(object):

    def __init__(self, client, name, status):
        self.client = client
        self.name = name
        self.status = status

    def __call__(self):
        try:
            if self.name.endswith('*'):
                ctnrs = self.client.containers(all=True, filters={'name': self.name[:-1]})
                if len(ctnrs) > 0:
                    container = self.client.inspect_container(ctnrs[0]['Id'])
                else:
                    return False
            else:
                container = self.client.inspect_container(self.name)
            return container['State']['Status'] == self.status
        except errors.APIError:
            return False

    def __str__(self):
        return "waiting for container to be %s" % self.status


class CLITestCase(DockerClientTestCase):

    def setUp(self):
        super(CLITestCase, self).setUp()
        self.base_dir = 'tests/fixtures/simple-composefile'
        self.override_dir = None

    def tearDown(self):
        if self.base_dir:
            self.project.kill()
            self.project.down(None, True)

            for container in self.project.containers(stopped=True, one_off=OneOffFilter.only):
                container.remove(force=True)
            networks = self.client.networks()
            for n in networks:
                if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name)):
                    self.client.remove_network(n['Name'])
            volumes = self.client.volumes().get('Volumes') or []
            for v in volumes:
                if v['Name'].split('/')[-1].startswith('{}_'.format(self.project.name)):
                    self.client.remove_volume(v['Name'])
        if hasattr(self, '_project'):
            del self._project

        super(CLITestCase, self).tearDown()

    @property
    def project(self):
        # Hack: allow project to be overridden
        if not hasattr(self, '_project'):
            self._project = get_project(self.base_dir, override_dir=self.override_dir)
        return self._project

    def dispatch(self, options, project_options=None, returncode=0):
        project_options = project_options or []
        proc = start_process(self.base_dir, project_options + options)
        return wait_on_process(proc, returncode=returncode)

    def execute(self, container, cmd):
        # Remove once Hijack and CloseNotifier sign a peace treaty
        self.client.close()
        exc = self.client.exec_create(container.id, cmd)
        self.client.exec_start(exc)
        return self.client.exec_inspect(exc)['ExitCode']

    def lookup(self, container, hostname):
        return self.execute(container, ["nslookup", hostname]) == 0

    def test_help(self):
        self.base_dir = 'tests/fixtures/no-composefile'
        result = self.dispatch(['help', 'up'], returncode=0)
        assert 'Usage: up [options] [--scale SERVICE=NUM...] [SERVICE...]' in result.stdout
        # Prevent tearDown from trying to create a project
        self.base_dir = None

    def test_help_nonexistent(self):
        self.base_dir = 'tests/fixtures/no-composefile'
        result = self.dispatch(['help', 'foobar'], returncode=1)
        assert 'No such command' in result.stderr
        self.base_dir = None

    def test_shorthand_host_opt(self):
        self.dispatch(
            ['-H={0}'.format(os.environ.get('DOCKER_HOST', 'unix://')),
             'up', '-d'],
            returncode=0
        )

    def test_shorthand_host_opt_interactive(self):
        self.dispatch(
            ['-H={0}'.format(os.environ.get('DOCKER_HOST', 'unix://')),
             'run', 'another', 'ls'],
            returncode=0
        )

    def test_host_not_reachable(self):
        result = self.dispatch(['-H=tcp://doesnotexist:8000', 'ps'], returncode=1)
        assert "Couldn't connect to Docker daemon" in result.stderr

    def test_host_not_reachable_volumes_from_container(self):
        self.base_dir = 'tests/fixtures/volumes-from-container'

        container = self.client.create_container(
            'busybox', 'true', name='composetest_data_container',
            host_config={}
        )
        self.addCleanup(self.client.remove_container, container)

        result = self.dispatch(['-H=tcp://doesnotexist:8000', 'ps'], returncode=1)
        assert "Couldn't connect to Docker daemon" in result.stderr

    def test_config_list_services(self):
        self.base_dir = 'tests/fixtures/v2-full'
        result = self.dispatch(['config', '--services'])
        assert set(result.stdout.rstrip().split('\n')) == {'web', 'other'}

    def test_config_list_volumes(self):
        self.base_dir = 'tests/fixtures/v2-full'
        result = self.dispatch(['config', '--volumes'])
        assert set(result.stdout.rstrip().split('\n')) == {'data'}

    def test_config_quiet_with_error(self):
        self.base_dir = None
        result = self.dispatch([
            '-f', 'tests/fixtures/invalid-composefile/invalid.yml',
            'config', '--quiet'
        ], returncode=1)
        assert "'notaservice' must be a mapping" in result.stderr

    def test_config_quiet(self):
        self.base_dir = 'tests/fixtures/v2-full'
        assert self.dispatch(['config', '--quiet']).stdout == ''

    def test_config_with_hash_option(self):
        self.base_dir = 'tests/fixtures/v2-full'
        result = self.dispatch(['config', '--hash=*'])
        for service in self.project.get_services():
            assert '{} {}\n'.format(service.name, service.config_hash) in result.stdout

        svc = self.project.get_service('other')
        result = self.dispatch(['config', '--hash=other'])
        assert result.stdout == '{} {}\n'.format(svc.name, svc.config_hash)

    def test_config_default(self):
        self.base_dir = 'tests/fixtures/v2-full'
        result = self.dispatch(['config'])
        # assert there are no python objects encoded in the output
        assert '!!' not in result.stdout

        output = yaml.load(result.stdout)
        expected = {
            'version': '2.0',
            'volumes': {'data': {'driver': 'local'}},
            'networks': {'front': {}},
            'services': {
                'web': {
                    'build': {
                        'context': os.path.abspath(self.base_dir),
                    },
                    'networks': {'front': None, 'default': None},
                    'volumes_from': ['service:other:rw'],
                },
                'other': {
                    'image': 'busybox:latest',
                    'command': 'top',
                    'volumes': ['/data'],
                },
            },
        }
        assert output == expected

    def test_config_restart(self):
        self.base_dir = 'tests/fixtures/restart'
        result = self.dispatch(['config'])
        assert yaml.load(result.stdout) == {
            'version': '2.0',
            'services': {
                'never': {
                    'image': 'busybox',
                    'restart': 'no',
                },
                'always': {
                    'image': 'busybox',
                    'restart': 'always',
                },
                'on-failure': {
                    'image': 'busybox',
                    'restart': 'on-failure',
                },
                'on-failure-5': {
                    'image': 'busybox',
                    'restart': 'on-failure:5',
                },
                'restart-null': {
                    'image': 'busybox',
                    'restart': ''
                },
            },
        }

    def test_config_external_network(self):
        self.base_dir = 'tests/fixtures/networks'
        result = self.dispatch(['-f', 'external-networks.yml', 'config'])
        json_result = yaml.load(result.stdout)
        assert 'networks' in json_result
        assert json_result['networks'] == {
            'networks_foo': {
                'external': True  # {'name': 'networks_foo'}
            },
            'bar': {
                'external': {'name': 'networks_bar'}
            }
        }

    def test_config_with_dot_env(self):
        self.base_dir = 'tests/fixtures/default-env-file'
        result = self.dispatch(['config'])
        json_result = yaml.load(result.stdout)
        assert json_result == {
            'services': {
                'web': {
                    'command': 'true',
                    'image': 'alpine:latest',
                    'ports': ['5643/tcp', '9999/tcp']
                }
            },
            'version': '2.4'
        }

    def test_config_with_dot_env_and_override_dir(self):
        self.base_dir = 'tests/fixtures/default-env-file'
        result = self.dispatch(['--project-directory', 'alt/', 'config'])
        json_result = yaml.load(result.stdout)
        assert json_result == {
            'services': {
                'web': {
                    'command': 'echo uwu',
                    'image': 'alpine:3.4',
                    'ports': ['3341/tcp', '4449/tcp']
                }
            },
            'version': '2.4'
        }

    def test_config_external_volume_v2(self):
        self.base_dir = 'tests/fixtures/volumes'
        result = self.dispatch(['-f', 'external-volumes-v2.yml', 'config'])
        json_result = yaml.load(result.stdout)
        assert 'volumes' in json_result
        assert json_result['volumes'] == {
            'foo': {
                'external': True,
            },
            'bar': {
                'external': {
                    'name': 'some_bar',
                },
            }
        }

    def test_config_external_volume_v2_x(self):
        self.base_dir = 'tests/fixtures/volumes'
        result = self.dispatch(['-f', 'external-volumes-v2-x.yml', 'config'])
        json_result = yaml.load(result.stdout)
        assert 'volumes' in json_result
        assert json_result['volumes'] == {
            'foo': {
                'external': True,
                'name': 'some_foo',
            },
            'bar': {
                'external': True,
                'name': 'some_bar',
            }
        }

    def test_config_external_volume_v3_x(self):
        self.base_dir = 'tests/fixtures/volumes'
        result = self.dispatch(['-f', 'external-volumes-v3-x.yml', 'config'])
        json_result = yaml.load(result.stdout)
        assert 'volumes' in json_result
        assert json_result['volumes'] == {
            'foo': {
                'external': True,
            },
            'bar': {
                'external': {
                    'name': 'some_bar',
                },
            }
        }

    def test_config_external_volume_v3_4(self):
        self.base_dir = 'tests/fixtures/volumes'
        result = self.dispatch(['-f', 'external-volumes-v3-4.yml', 'config'])
        json_result = yaml.load(result.stdout)
        assert 'volumes' in json_result
        assert json_result['volumes'] == {
            'foo': {
                'external': True,
                'name': 'some_foo',
            },
            'bar': {
                'external': True,
                'name': 'some_bar',
            }
        }

    def test_config_external_network_v3_5(self):
        self.base_dir = 'tests/fixtures/networks'
        result = self.dispatch(['-f', 'external-networks-v3-5.yml', 'config'])
        json_result = yaml.load(result.stdout)
        assert 'networks' in json_result
        assert json_result['networks'] == {
            'foo': {
                'external': True,
                'name': 'some_foo',
            },
            'bar': {
                'external': True,
                'name': 'some_bar',
            },
        }

    def test_config_v1(self):
        self.base_dir = 'tests/fixtures/v1-config'
        result = self.dispatch(['config'])
        assert yaml.load(result.stdout) == {
            'version': '2.1',
            'services': {
                'net': {
                    'image': 'busybox',
                    'network_mode': 'bridge',
                },
                'volume': {
                    'image': 'busybox',
                    'volumes': ['/data'],
                    'network_mode': 'bridge',
                },
                'app': {
                    'image': 'busybox',
                    'volumes_from': ['service:volume:rw'],
                    'network_mode': 'service:net',
                },
            },
        }

    @v3_only()
    def test_config_v3(self):
        self.base_dir = 'tests/fixtures/v3-full'
        result = self.dispatch(['config'])

        assert yaml.load(result.stdout) == {
            'version': '3.5',
            'volumes': {
                'foobar': {
                    'labels': {
                        'com.docker.compose.test': 'true',
                    },
                },
            },
            'services': {
                'web': {
                    'image': 'busybox',
                    'deploy': {
                        'mode': 'replicated',
                        'replicas': 6,
                        'labels': ['FOO=BAR'],
                        'update_config': {
                            'parallelism': 3,
                            'delay': '10s',
                            'failure_action': 'continue',
                            'monitor': '60s',
                            'max_failure_ratio': 0.3,
                        },
                        'resources': {
                            'limits': {
                                'cpus': '0.05',
                                'memory': '50M',
                            },
                            'reservations': {
                                'cpus': '0.01',
                                'memory': '20M',
                            },
                        },
                        'restart_policy': {
                            'condition': 'on-failure',
                            'delay': '5s',
                            'max_attempts': 3,
                            'window': '120s',
                        },
                        'placement': {
                            'constraints': [
                                'node.hostname==foo', 'node.role != manager'
                            ],
                            'preferences': [{'spread': 'node.labels.datacenter'}]
                        },
                    },

                    'healthcheck': {
                        'test': 'cat /etc/passwd',
                        'interval': '10s',
                        'timeout': '1s',
                        'retries': 5,
                    },
                    'volumes': [{
                        'read_only': True,
                        'source': '/host/path',
                        'target': '/container/path',
                        'type': 'bind'
                    }, {
                        'source': 'foobar', 'target': '/container/volumepath', 'type': 'volume'
                    }, {
                        'target': '/anonymous', 'type': 'volume'
                    }, {
                        'source': 'foobar',
                        'target': '/container/volumepath2',
                        'type': 'volume',
                        'volume': {'nocopy': True}
                    }],
                    'stop_grace_period': '20s',
                },
            },
        }

    def test_config_compatibility_mode(self):
        self.base_dir = 'tests/fixtures/compatibility-mode'
        result = self.dispatch(['--compatibility', 'config'])

        assert yaml.load(result.stdout) == {
            'version': '2.3',
            'volumes': {'foo': {'driver': 'default'}},
            'networks': {'bar': {}},
            'services': {
                'foo': {
                    'command': '/bin/true',
                    'image': 'alpine:3.7',
                    'scale': 3,
                    'restart': 'always:7',
                    'mem_limit': '300M',
                    'mem_reservation': '100M',
                    'cpus': 0.7,
                    'volumes': ['foo:/bar:rw'],
                    'networks': {'bar': None},
                }
            },
        }

    def test_ps(self):
        self.project.get_service('simple').create_container()
        result = self.dispatch(['ps'])
        assert 'simple-composefile_simple_1' in result.stdout

    def test_ps_default_composefile(self):
        self.base_dir = 'tests/fixtures/multiple-composefiles'
        self.dispatch(['up', '-d'])
        result = self.dispatch(['ps'])

        assert 'multiple-composefiles_simple_1' in result.stdout
        assert 'multiple-composefiles_another_1' in result.stdout
        assert 'multiple-composefiles_yetanother_1' not in result.stdout

    def test_ps_alternate_composefile(self):
        config_path = os.path.abspath(
            'tests/fixtures/multiple-composefiles/compose2.yml')
        self._project = get_project(self.base_dir, [config_path])

        self.base_dir = 'tests/fixtures/multiple-composefiles'
        self.dispatch(['-f', 'compose2.yml', 'up', '-d'])
        result = self.dispatch(['-f', 'compose2.yml', 'ps'])

        assert 'multiple-composefiles_simple_1' not in result.stdout
        assert 'multiple-composefiles_another_1' not in result.stdout
        assert 'multiple-composefiles_yetanother_1' in result.stdout

    def test_ps_services_filter_option(self):
        self.base_dir = 'tests/fixtures/ps-services-filter'
        image = self.dispatch(['ps', '--services', '--filter', 'source=image'])
        build = self.dispatch(['ps', '--services', '--filter', 'source=build'])
        all_services = self.dispatch(['ps', '--services'])

        assert 'with_build' in all_services.stdout
        assert 'with_image' in all_services.stdout
        assert 'with_build' in build.stdout
        assert 'with_build' not in image.stdout
        assert 'with_image' in image.stdout
        assert 'with_image' not in build.stdout

    def test_ps_services_filter_status(self):
        self.base_dir = 'tests/fixtures/ps-services-filter'
        self.dispatch(['up', '-d'])
        self.dispatch(['pause', 'with_image'])
        paused = self.dispatch(['ps', '--services', '--filter', 'status=paused'])
        stopped = self.dispatch(['ps', '--services', '--filter', 'status=stopped'])
        running = self.dispatch(['ps', '--services', '--filter', 'status=running'])

        assert 'with_build' not in stopped.stdout
        assert 'with_image' not in stopped.stdout
        assert 'with_build' not in paused.stdout
        assert 'with_image' in paused.stdout
        assert 'with_build' in running.stdout
        assert 'with_image' in running.stdout

    def test_ps_all(self):
        self.project.get_service('simple').create_container(one_off='blahblah')
        result = self.dispatch(['ps'])
        assert 'simple-composefile_simple_run_1' not in result.stdout

        result2 = self.dispatch(['ps', '--all'])
        assert 'simple-composefile_simple_run_1' in result2.stdout

    def test_pull(self):
        result = self.dispatch(['pull'])
        assert 'Pulling simple' in result.stderr
        assert 'Pulling another' in result.stderr

    def test_pull_with_digest(self):
        result = self.dispatch(['-f', 'digest.yml', 'pull', '--no-parallel'])

        assert 'Pulling simple (busybox:latest)...' in result.stderr
        assert ('Pulling digest (busybox@'
                'sha256:38a203e1986cf79639cfb9b2e1d6e773de84002feea2d4eb006b520'
                '04ee8502d)...') in result.stderr

    def test_pull_with_ignore_pull_failures(self):
        result = self.dispatch([
            '-f', 'ignore-pull-failures.yml',
            'pull', '--ignore-pull-failures', '--no-parallel']
        )

        assert 'Pulling simple (busybox:latest)...' in result.stderr
        assert 'Pulling another (nonexisting-image:latest)...' in result.stderr
        assert ('repository nonexisting-image not found' in result.stderr or
                'image library/nonexisting-image:latest not found' in result.stderr or
                'pull access denied for nonexisting-image' in result.stderr)

    def test_pull_with_quiet(self):
        assert self.dispatch(['pull', '--quiet']).stderr == ''
        assert self.dispatch(['pull', '--quiet']).stdout == ''

    def test_pull_with_parallel_failure(self):
        result = self.dispatch([
            '-f', 'ignore-pull-failures.yml', 'pull'],
            returncode=1
        )

        assert re.search(re.compile('^Pulling simple', re.MULTILINE), result.stderr)
        assert re.search(re.compile('^Pulling another', re.MULTILINE), result.stderr)
        assert re.search(
            re.compile('^ERROR: for another .*does not exist.*', re.MULTILINE),
            result.stderr
        )
        assert re.search(
            re.compile('''^(ERROR: )?(b')?.* nonexisting-image''', re.MULTILINE),
            result.stderr
        )

    def test_pull_with_no_deps(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        result = self.dispatch(['pull', '--no-parallel', 'web'])
        assert sorted(result.stderr.split('\n'))[1:] == [
            'Pulling web (busybox:latest)...',
        ]

    def test_pull_with_include_deps(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        result = self.dispatch(['pull', '--no-parallel', '--include-deps', 'web'])
        assert sorted(result.stderr.split('\n'))[1:] == [
            'Pulling db (busybox:latest)...',
            'Pulling web (busybox:latest)...',
        ]

    def test_build_plain(self):
        self.base_dir = 'tests/fixtures/simple-dockerfile'
        self.dispatch(['build', 'simple'])

        result = self.dispatch(['build', 'simple'])
        assert BUILD_PULL_TEXT not in result.stdout

    def test_build_no_cache(self):
        self.base_dir = 'tests/fixtures/simple-dockerfile'
        self.dispatch(['build', 'simple'])

        result = self.dispatch(['build', '--no-cache', 'simple'])
        assert BUILD_CACHE_TEXT not in result.stdout
        assert BUILD_PULL_TEXT not in result.stdout

    def test_build_pull(self):
        # Make sure we have the latest busybox already
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/simple-dockerfile'
        self.dispatch(['build', 'simple'], None)

        result = self.dispatch(['build', '--pull', 'simple'])
        if not is_cluster(self.client):
            # If previous build happened on another node, cache won't be available
            assert BUILD_CACHE_TEXT in result.stdout
        assert BUILD_PULL_TEXT in result.stdout

    def test_build_no_cache_pull(self):
        # Make sure we have the latest busybox already
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/simple-dockerfile'
        self.dispatch(['build', 'simple'])

        result = self.dispatch(['build', '--no-cache', '--pull', 'simple'])
        assert BUILD_CACHE_TEXT not in result.stdout
        assert BUILD_PULL_TEXT in result.stdout

    def test_build_log_level(self):
        self.base_dir = 'tests/fixtures/simple-dockerfile'
        result = self.dispatch(['--log-level', 'warning', 'build', 'simple'])
        assert result.stderr == ''
        result = self.dispatch(['--log-level', 'debug', 'build', 'simple'])
        assert 'Building simple' in result.stderr
        assert 'Using configuration file' in result.stderr
        self.base_dir = 'tests/fixtures/simple-failing-dockerfile'
        result = self.dispatch(['--log-level', 'critical', 'build', 'simple'], returncode=1)
        assert result.stderr == ''
        result = self.dispatch(['--log-level', 'debug', 'build', 'simple'], returncode=1)
        assert 'Building simple' in result.stderr
        assert 'non-zero code' in result.stderr

    def test_build_failed(self):
        self.base_dir = 'tests/fixtures/simple-failing-dockerfile'
        self.dispatch(['build', 'simple'], returncode=1)

        labels = ["com.docker.compose.test_failing_image=true"]
        containers = [
            Container.from_ps(self.project.client, c)
            for c in self.project.client.containers(
                all=True,
                filters={"label": labels})
        ]
        assert len(containers) == 1

    def test_build_failed_forcerm(self):
        self.base_dir = 'tests/fixtures/simple-failing-dockerfile'
        self.dispatch(['build', '--force-rm', 'simple'], returncode=1)

        labels = ["com.docker.compose.test_failing_image=true"]

        containers = [
            Container.from_ps(self.project.client, c)
            for c in self.project.client.containers(
                all=True,
                filters={"label": labels})
        ]
        assert not containers

    def test_build_shm_size_build_option(self):
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/build-shm-size'
        result = self.dispatch(['build', '--no-cache'], None)
        assert 'shm_size: 96' in result.stdout

    def test_build_memory_build_option(self):
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/build-memory'
        result = self.dispatch(['build', '--no-cache', '--memory', '96m', 'service'], None)
        assert 'memory: 100663296' in result.stdout  # 96 * 1024 * 1024

    def test_build_with_buildarg_from_compose_file(self):
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/build-args'
        result = self.dispatch(['build'], None)
        assert 'Favorite Touhou Character: mariya.kirisame' in result.stdout

    def test_build_with_buildarg_cli_override(self):
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/build-args'
        result = self.dispatch(['build', '--build-arg', 'favorite_th_character=sakuya.izayoi'], None)
        assert 'Favorite Touhou Character: sakuya.izayoi' in result.stdout

    @mock.patch.dict(os.environ)
    def test_build_with_buildarg_old_api_version(self):
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/build-args'
        os.environ['COMPOSE_API_VERSION'] = '1.24'
        result = self.dispatch(
            ['build', '--build-arg', 'favorite_th_character=reimu.hakurei'], None, returncode=1
        )
        assert '--build-arg is only supported when services are specified' in result.stderr

        result = self.dispatch(
            ['build', '--build-arg', 'favorite_th_character=hong.meiling', 'web'], None
        )
        assert 'Favorite Touhou Character: hong.meiling' in result.stdout

    def test_bundle_with_digests(self):
        self.base_dir = 'tests/fixtures/bundle-with-digests/'
        tmpdir = pytest.ensuretemp('cli_test_bundle')
        self.addCleanup(tmpdir.remove)
        filename = str(tmpdir.join('example.dab'))

        self.dispatch(['bundle', '--output', filename])
        with open(filename, 'r') as fh:
            bundle = json.load(fh)

        assert bundle == {
            'Version': '0.1',
            'Services': {
                'web': {
                    'Image': ('dockercloud/hello-world@sha256:fe79a2cfbd17eefc3'
                              '44fb8419420808df95a1e22d93b7f621a7399fd1e9dca1d'),
                    'Networks': ['default'],
                },
                'redis': {
                    'Image': ('redis@sha256:a84cb8f53a70e19f61ff2e1d5e73fb7ae62d'
                              '374b2b7392de1e7d77be26ef8f7b'),
                    'Networks': ['default'],
                }
            },
        }

    def test_build_override_dir(self):
        self.base_dir = 'tests/fixtures/build-path-override-dir'
        self.override_dir = os.path.abspath('tests/fixtures')
        result = self.dispatch([
            '--project-directory', self.override_dir,
            'build'])

        assert 'Successfully built' in result.stdout

    def test_build_override_dir_invalid_path(self):
        config_path = os.path.abspath('tests/fixtures/build-path-override-dir/docker-compose.yml')
        result = self.dispatch([
            '-f', config_path,
            'build'], returncode=1)

        assert 'does not exist, is not accessible, or is not a valid URL' in result.stderr

    def test_build_parallel(self):
        self.base_dir = 'tests/fixtures/build-multiple-composefile'
        result = self.dispatch(['build', '--parallel'])
        assert 'Successfully tagged build-multiple-composefile_a:latest' in result.stdout
        assert 'Successfully tagged build-multiple-composefile_b:latest' in result.stdout
        assert 'Successfully built' in result.stdout

    def test_create(self):
        self.dispatch(['create'])
        service = self.project.get_service('simple')
        another = self.project.get_service('another')
        service_containers = service.containers(stopped=True)
        another_containers = another.containers(stopped=True)
        assert len(service_containers) == 1
        assert len(another_containers) == 1
        assert not service_containers[0].is_running
        assert not another_containers[0].is_running

    def test_create_with_force_recreate(self):
        self.dispatch(['create'], None)
        service = self.project.get_service('simple')
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

        old_ids = [c.id for c in service.containers(stopped=True)]

        self.dispatch(['create', '--force-recreate'], None)
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

        new_ids = [c.id for c in service_containers]

        assert old_ids != new_ids

    def test_create_with_no_recreate(self):
        self.dispatch(['create'], None)
        service = self.project.get_service('simple')
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

        old_ids = [c.id for c in service.containers(stopped=True)]

        self.dispatch(['create', '--no-recreate'], None)
        service_containers = service.containers(stopped=True)
        assert len(service_containers) == 1
        assert not service_containers[0].is_running

        new_ids = [c.id for c in service_containers]

        assert old_ids == new_ids

    def test_run_one_off_with_volume(self):
        self.base_dir = 'tests/fixtures/simple-composefile-volume-ready'
        volume_path = os.path.abspath(os.path.join(os.getcwd(), self.base_dir, 'files'))
        node = create_host_file(self.client, os.path.join(volume_path, 'example.txt'))

        self.dispatch([
            'run',
            '-v', '{}:/data'.format(volume_path),
            '-e', 'constraint:node=={}'.format(node if node is not None else '*'),
            'simple',
            'test', '-f', '/data/example.txt'
        ], returncode=0)

        service = self.project.get_service('simple')
        container_data = service.containers(one_off=OneOffFilter.only, stopped=True)[0]
        mount = container_data.get('Mounts')[0]
        assert mount['Source'] == volume_path
        assert mount['Destination'] == '/data'
        assert mount['Type'] == 'bind'

    def test_run_one_off_with_multiple_volumes(self):
        self.base_dir = 'tests/fixtures/simple-composefile-volume-ready'
        volume_path = os.path.abspath(os.path.join(os.getcwd(), self.base_dir, 'files'))
        node = create_host_file(self.client, os.path.join(volume_path, 'example.txt'))

        self.dispatch([
            'run',
            '-v', '{}:/data'.format(volume_path),
            '-v', '{}:/data1'.format(volume_path),
            '-e', 'constraint:node=={}'.format(node if node is not None else '*'),
            'simple',
            'test', '-f', '/data/example.txt'
        ], returncode=0)

        self.dispatch([
            'run',
            '-v', '{}:/data'.format(volume_path),
            '-v', '{}:/data1'.format(volume_path),
            '-e', 'constraint:node=={}'.format(node if node is not None else '*'),
            'simple',
            'test', '-f' '/data1/example.txt'
        ], returncode=0)

    def test_run_one_off_with_volume_merge(self):
        self.base_dir = 'tests/fixtures/simple-composefile-volume-ready'
        volume_path = os.path.abspath(os.path.join(os.getcwd(), self.base_dir, 'files'))
        node = create_host_file(self.client, os.path.join(volume_path, 'example.txt'))

        self.dispatch([
            '-f', 'docker-compose.merge.yml',
            'run',
            '-v', '{}:/data'.format(volume_path),
            '-e', 'constraint:node=={}'.format(node if node is not None else '*'),
            'simple',
            'test', '-f', '/data/example.txt'
        ], returncode=0)

        service = self.project.get_service('simple')
        container_data = service.containers(one_off=OneOffFilter.only, stopped=True)[0]
        mounts = container_data.get('Mounts')
        assert len(mounts) == 2
        config_mount = [m for m in mounts if m['Destination'] == '/data1'][0]
        override_mount = [m for m in mounts if m['Destination'] == '/data'][0]

        assert config_mount['Type'] == 'volume'
        assert override_mount['Source'] == volume_path
        assert override_mount['Type'] == 'bind'

    def test_create_with_force_recreate_and_no_recreate(self):
        self.dispatch(
            ['create', '--force-recreate', '--no-recreate'],
            returncode=1)

    def test_down_invalid_rmi_flag(self):
        result = self.dispatch(['down', '--rmi', 'bogus'], returncode=1)
        assert '--rmi flag must be' in result.stderr

    @v2_only()
    def test_down(self):
        self.base_dir = 'tests/fixtures/v2-full'

        self.dispatch(['up', '-d'])
        wait_on_condition(ContainerCountCondition(self.project, 2))

        self.dispatch(['run', 'web', 'true'])
        self.dispatch(['run', '-d', 'web', 'tail', '-f', '/dev/null'])
        assert len(self.project.containers(one_off=OneOffFilter.only, stopped=True)) == 2

        result = self.dispatch(['down', '--rmi=local', '--volumes'])
        assert 'Stopping v2-full_web_1' in result.stderr
        assert 'Stopping v2-full_other_1' in result.stderr
        assert 'Stopping v2-full_web_run_2' in result.stderr
        assert 'Removing v2-full_web_1' in result.stderr
        assert 'Removing v2-full_other_1' in result.stderr
        assert 'Removing v2-full_web_run_1' in result.stderr
        assert 'Removing v2-full_web_run_2' in result.stderr
        assert 'Removing volume v2-full_data' in result.stderr
        assert 'Removing image v2-full_web' in result.stderr
        assert 'Removing image busybox' not in result.stderr
        assert 'Removing network v2-full_default' in result.stderr
        assert 'Removing network v2-full_front' in result.stderr

    def test_down_timeout(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1
        assert service.containers()[0].is_running
        ""

        self.dispatch(['down', '-t', '1'], None)

        assert len(service.containers(stopped=True)) == 0

    def test_down_signal(self):
        self.base_dir = 'tests/fixtures/stop-signal-composefile'
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1
        assert service.containers()[0].is_running

        self.dispatch(['down', '-t', '1'], None)
        assert len(service.containers(stopped=True)) == 0

    def test_up_detached(self):
        self.dispatch(['up', '-d'])
        service = self.project.get_service('simple')
        another = self.project.get_service('another')
        assert len(service.containers()) == 1
        assert len(another.containers()) == 1

        # Ensure containers don't have stdin and stdout connected in -d mode
        container, = service.containers()
        assert not container.get('Config.AttachStderr')
        assert not container.get('Config.AttachStdout')
        assert not container.get('Config.AttachStdin')

    def test_up_detached_long_form(self):
        self.dispatch(['up', '--detach'])
        service = self.project.get_service('simple')
        another = self.project.get_service('another')
        assert len(service.containers()) == 1
        assert len(another.containers()) == 1

        # Ensure containers don't have stdin and stdout connected in -d mode
        container, = service.containers()
        assert not container.get('Config.AttachStderr')
        assert not container.get('Config.AttachStdout')
        assert not container.get('Config.AttachStdin')

    def test_up_attached(self):
        self.base_dir = 'tests/fixtures/echo-services'
        result = self.dispatch(['up', '--no-color'])
        simple_name = self.project.get_service('simple').containers(stopped=True)[0].name_without_project
        another_name = self.project.get_service('another').containers(
            stopped=True
        )[0].name_without_project

        assert '{} | simple'.format(simple_name) in result.stdout
        assert '{} | another'.format(another_name) in result.stdout
        assert '{} exited with code 0'.format(simple_name) in result.stdout
        assert '{} exited with code 0'.format(another_name) in result.stdout

    @v2_only()
    def test_up(self):
        self.base_dir = 'tests/fixtures/v2-simple'
        self.dispatch(['up', '-d'], None)

        services = self.project.get_services()

        network_name = self.project.networks.networks['default'].full_name
        networks = self.client.networks(names=[network_name])
        assert len(networks) == 1
        assert networks[0]['Driver'] == 'bridge' if not is_cluster(self.client) else 'overlay'
        assert 'com.docker.network.bridge.enable_icc' not in networks[0]['Options']

        network = self.client.inspect_network(networks[0]['Id'])

        for service in services:
            containers = service.containers()
            assert len(containers) == 1

            container = containers[0]
            assert container.id in network['Containers']

            networks = container.get('NetworkSettings.Networks')
            assert list(networks) == [network['Name']]

            assert sorted(networks[network['Name']]['Aliases']) == sorted(
                [service.name, container.short_id]
            )

            for service in services:
                assert self.lookup(container, service.name)

    @v2_only()
    def test_up_no_start(self):
        self.base_dir = 'tests/fixtures/v2-full'
        self.dispatch(['up', '--no-start'], None)

        services = self.project.get_services()

        default_network = self.project.networks.networks['default'].full_name
        front_network = self.project.networks.networks['front'].full_name
        networks = self.client.networks(names=[default_network, front_network])
        assert len(networks) == 2

        for service in services:
            containers = service.containers(stopped=True)
            assert len(containers) == 1

            container = containers[0]
            assert not container.is_running
            assert container.get('State.Status') == 'created'

        volumes = self.project.volumes.volumes
        assert 'data' in volumes
        volume = volumes['data']

        # The code below is a Swarm-compatible equivalent to volume.exists()
        remote_volumes = [
            v for v in self.client.volumes().get('Volumes', [])
            if v['Name'].split('/')[-1] == volume.full_name
        ]
        assert len(remote_volumes) > 0

    @v2_only()
    def test_up_no_ansi(self):
        self.base_dir = 'tests/fixtures/v2-simple'
        result = self.dispatch(['--no-ansi', 'up', '-d'], None)
        assert "%c[2K\r" % 27 not in result.stderr
        assert "%c[1A" % 27 not in result.stderr
        assert "%c[1B" % 27 not in result.stderr

    @v2_only()
    def test_up_with_default_network_config(self):
        filename = 'default-network-config.yml'

        self.base_dir = 'tests/fixtures/networks'
        self._project = get_project(self.base_dir, [filename])

        self.dispatch(['-f', filename, 'up', '-d'], None)

        network_name = self.project.networks.networks['default'].full_name
        networks = self.client.networks(names=[network_name])

        assert networks[0]['Options']['com.docker.network.bridge.enable_icc'] == 'false'

    @v2_only()
    def test_up_with_network_aliases(self):
        filename = 'network-aliases.yml'
        self.base_dir = 'tests/fixtures/networks'
        self.dispatch(['-f', filename, 'up', '-d'], None)
        back_name = '{}_back'.format(self.project.name)
        front_name = '{}_front'.format(self.project.name)

        networks = [
            n for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]

        # Two networks were created: back and front
        assert sorted(n['Name'].split('/')[-1] for n in networks) == [back_name, front_name]
        web_container = self.project.get_service('web').containers()[0]

        back_aliases = web_container.get(
            'NetworkSettings.Networks.{}.Aliases'.format(back_name)
        )
        assert 'web' in back_aliases
        front_aliases = web_container.get(
            'NetworkSettings.Networks.{}.Aliases'.format(front_name)
        )
        assert 'web' in front_aliases
        assert 'forward_facing' in front_aliases
        assert 'ahead' in front_aliases

    @v2_only()
    def test_up_with_network_internal(self):
        self.require_api_version('1.23')
        filename = 'network-internal.yml'
        self.base_dir = 'tests/fixtures/networks'
        self.dispatch(['-f', filename, 'up', '-d'], None)
        internal_net = '{}_internal'.format(self.project.name)

        networks = [
            n for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]

        # One network was created: internal
        assert sorted(n['Name'].split('/')[-1] for n in networks) == [internal_net]

        assert networks[0]['Internal'] is True

    @v2_only()
    def test_up_with_network_static_addresses(self):
        filename = 'network-static-addresses.yml'
        ipv4_address = '172.16.100.100'
        ipv6_address = 'fe80::1001:100'
        self.base_dir = 'tests/fixtures/networks'
        self.dispatch(['-f', filename, 'up', '-d'], None)
        static_net = '{}_static_test'.format(self.project.name)

        networks = [
            n for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]

        # One networks was created: front
        assert sorted(n['Name'].split('/')[-1] for n in networks) == [static_net]
        web_container = self.project.get_service('web').containers()[0]

        ipam_config = web_container.get(
            'NetworkSettings.Networks.{}.IPAMConfig'.format(static_net)
        )
        assert ipv4_address in ipam_config.values()
        assert ipv6_address in ipam_config.values()

    @v2_only()
    def test_up_with_networks(self):
        self.base_dir = 'tests/fixtures/networks'
        self.dispatch(['up', '-d'], None)

        back_name = '{}_back'.format(self.project.name)
        front_name = '{}_front'.format(self.project.name)

        networks = [
            n for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]

        # Two networks were created: back and front
        assert sorted(n['Name'].split('/')[-1] for n in networks) == [back_name, front_name]

        # lookup by ID instead of name in case of duplicates
        back_network = self.client.inspect_network(
            [n for n in networks if n['Name'] == back_name][0]['Id']
        )
        front_network = self.client.inspect_network(
            [n for n in networks if n['Name'] == front_name][0]['Id']
        )

        web_container = self.project.get_service('web').containers()[0]
        app_container = self.project.get_service('app').containers()[0]
        db_container = self.project.get_service('db').containers()[0]

        for net_name in [front_name, back_name]:
            links = app_container.get('NetworkSettings.Networks.{}.Links'.format(net_name))
            assert '{}:database'.format(db_container.name) in links

        # db and app joined the back network
        assert sorted(back_network['Containers']) == sorted([db_container.id, app_container.id])

        # web and app joined the front network
        assert sorted(front_network['Containers']) == sorted([web_container.id, app_container.id])

        # web can see app but not db
        assert self.lookup(web_container, "app")
        assert not self.lookup(web_container, "db")

        # app can see db
        assert self.lookup(app_container, "db")

        # app has aliased db to "database"
        assert self.lookup(app_container, "database")

    @v2_only()
    def test_up_missing_network(self):
        self.base_dir = 'tests/fixtures/networks'

        result = self.dispatch(
            ['-f', 'missing-network.yml', 'up', '-d'],
            returncode=1)

        assert 'Service "web" uses an undefined network "foo"' in result.stderr

    @v2_only()
    @no_cluster('container networks not supported in Swarm')
    def test_up_with_network_mode(self):
        c = self.client.create_container(
            'busybox', 'top', name='composetest_network_mode_container',
            host_config={}
        )
        self.addCleanup(self.client.remove_container, c, force=True)
        self.client.start(c)
        container_mode_source = 'container:{}'.format(c['Id'])

        filename = 'network-mode.yml'

        self.base_dir = 'tests/fixtures/networks'
        self._project = get_project(self.base_dir, [filename])

        self.dispatch(['-f', filename, 'up', '-d'], None)

        networks = [
            n for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]
        assert not networks

        for name in ['bridge', 'host', 'none']:
            container = self.project.get_service(name).containers()[0]
            assert list(container.get('NetworkSettings.Networks')) == [name]
            assert container.get('HostConfig.NetworkMode') == name

        service_mode_source = 'container:{}'.format(
            self.project.get_service('bridge').containers()[0].id)
        service_mode_container = self.project.get_service('service').containers()[0]
        assert not service_mode_container.get('NetworkSettings.Networks')
        assert service_mode_container.get('HostConfig.NetworkMode') == service_mode_source

        container_mode_container = self.project.get_service('container').containers()[0]
        assert not container_mode_container.get('NetworkSettings.Networks')
        assert container_mode_container.get('HostConfig.NetworkMode') == container_mode_source

    @v2_only()
    def test_up_external_networks(self):
        filename = 'external-networks.yml'

        self.base_dir = 'tests/fixtures/networks'
        self._project = get_project(self.base_dir, [filename])

        result = self.dispatch(['-f', filename, 'up', '-d'], returncode=1)
        assert 'declared as external, but could not be found' in result.stderr

        networks = [
            n['Name'] for n in self.client.networks()
            if n['Name'].startswith('{}_'.format(self.project.name))
        ]
        assert not networks

        network_names = ['{}_{}'.format(self.project.name, n) for n in ['foo', 'bar']]
        for name in network_names:
            self.client.create_network(name, attachable=True)

        self.dispatch(['-f', filename, 'up', '-d'])
        container = self.project.containers()[0]
        assert sorted(list(container.get('NetworkSettings.Networks'))) == sorted(network_names)

    @v2_only()
    def test_up_with_external_default_network(self):
        filename = 'external-default.yml'

        self.base_dir = 'tests/fixtures/networks'
        self._project = get_project(self.base_dir, [filename])

        result = self.dispatch(['-f', filename, 'up', '-d'], returncode=1)
        assert 'declared as external, but could not be found' in result.stderr

        networks = [
            n['Name'] for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]
        assert not networks

        network_name = 'composetest_external_network'
        self.client.create_network(network_name, attachable=True)

        self.dispatch(['-f', filename, 'up', '-d'])
        container = self.project.containers()[0]
        assert list(container.get('NetworkSettings.Networks')) == [network_name]

    @v2_1_only()
    def test_up_with_network_labels(self):
        filename = 'network-label.yml'

        self.base_dir = 'tests/fixtures/networks'
        self._project = get_project(self.base_dir, [filename])

        self.dispatch(['-f', filename, 'up', '-d'], returncode=0)

        network_with_label = '{}_network_with_label'.format(self.project.name)

        networks = [
            n for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]

        assert [n['Name'].split('/')[-1] for n in networks] == [network_with_label]
        assert 'label_key' in networks[0]['Labels']
        assert networks[0]['Labels']['label_key'] == 'label_val'

    @v2_1_only()
    def test_up_with_volume_labels(self):
        filename = 'volume-label.yml'

        self.base_dir = 'tests/fixtures/volumes'
        self._project = get_project(self.base_dir, [filename])

        self.dispatch(['-f', filename, 'up', '-d'], returncode=0)

        volume_with_label = '{}_volume_with_label'.format(self.project.name)

        volumes = [
            v for v in self.client.volumes().get('Volumes', [])
            if v['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]

        assert set([v['Name'].split('/')[-1] for v in volumes]) == set([volume_with_label])
        assert 'label_key' in volumes[0]['Labels']
        assert volumes[0]['Labels']['label_key'] == 'label_val'

    @v2_only()
    def test_up_no_services(self):
        self.base_dir = 'tests/fixtures/no-services'
        self.dispatch(['up', '-d'], None)

        network_names = [
            n['Name'] for n in self.client.networks()
            if n['Name'].split('/')[-1].startswith('{}_'.format(self.project.name))
        ]
        assert network_names == []

    def test_up_with_links_v1(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['up', '-d', 'web'], None)

        # No network was created
        network_name = self.project.networks.networks['default'].full_name
        networks = self.client.networks(names=[network_name])
        assert networks == []

        web = self.project.get_service('web')
        db = self.project.get_service('db')
        console = self.project.get_service('console')

        # console was not started
        assert len(web.containers()) == 1
        assert len(db.containers()) == 1
        assert len(console.containers()) == 0

        # web has links
        web_container = web.containers()[0]
        assert web_container.get('HostConfig.Links')

    def test_up_with_net_is_invalid(self):
        self.base_dir = 'tests/fixtures/net-container'

        result = self.dispatch(
            ['-f', 'v2-invalid.yml', 'up', '-d'],
            returncode=1)

        assert "Unsupported config option for services.bar: 'net'" in result.stderr

    @no_cluster("Legacy networking not supported on Swarm")
    def test_up_with_net_v1(self):
        self.base_dir = 'tests/fixtures/net-container'
        self.dispatch(['up', '-d'], None)

        bar = self.project.get_service('bar')
        bar_container = bar.containers()[0]

        foo = self.project.get_service('foo')
        foo_container = foo.containers()[0]

        assert foo_container.get('HostConfig.NetworkMode') == 'container:{}'.format(
            bar_container.id
        )

    @v3_only()
    def test_up_with_healthcheck(self):
        def wait_on_health_status(container, status):
            def condition():
                container.inspect()
                return container.get('State.Health.Status') == status

            return wait_on_condition(condition, delay=0.5)

        self.base_dir = 'tests/fixtures/healthcheck'
        self.dispatch(['up', '-d'], None)

        passes = self.project.get_service('passes')
        passes_container = passes.containers()[0]

        assert passes_container.get('Config.Healthcheck') == {
            "Test": ["CMD-SHELL", "/bin/true"],
            "Interval": nanoseconds_from_time_seconds(1),
            "Timeout": nanoseconds_from_time_seconds(30 * 60),
            "Retries": 1,
        }

        wait_on_health_status(passes_container, 'healthy')

        fails = self.project.get_service('fails')
        fails_container = fails.containers()[0]

        assert fails_container.get('Config.Healthcheck') == {
            "Test": ["CMD", "/bin/false"],
            "Interval": nanoseconds_from_time_seconds(2.5),
            "Retries": 2,
        }

        wait_on_health_status(fails_container, 'unhealthy')

        disabled = self.project.get_service('disabled')
        disabled_container = disabled.containers()[0]

        assert disabled_container.get('Config.Healthcheck') == {
            "Test": ["NONE"],
        }

        assert 'Health' not in disabled_container.get('State')

    def test_up_with_no_deps(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['up', '-d', '--no-deps', 'web'], None)
        web = self.project.get_service('web')
        db = self.project.get_service('db')
        console = self.project.get_service('console')
        assert len(web.containers()) == 1
        assert len(db.containers()) == 0
        assert len(console.containers()) == 0

    def test_up_with_force_recreate(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1

        old_ids = [c.id for c in service.containers()]

        self.dispatch(['up', '-d', '--force-recreate'], None)
        assert len(service.containers()) == 1

        new_ids = [c.id for c in service.containers()]

        assert old_ids != new_ids

    def test_up_with_no_recreate(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1

        old_ids = [c.id for c in service.containers()]

        self.dispatch(['up', '-d', '--no-recreate'], None)
        assert len(service.containers()) == 1

        new_ids = [c.id for c in service.containers()]

        assert old_ids == new_ids

    def test_up_with_force_recreate_and_no_recreate(self):
        self.dispatch(
            ['up', '-d', '--force-recreate', '--no-recreate'],
            returncode=1)

    def test_up_with_timeout(self):
        self.dispatch(['up', '-d', '-t', '1'])
        service = self.project.get_service('simple')
        another = self.project.get_service('another')
        assert len(service.containers()) == 1
        assert len(another.containers()) == 1

    @mock.patch.dict(os.environ)
    def test_up_with_ignore_remove_orphans(self):
        os.environ["COMPOSE_IGNORE_ORPHANS"] = "True"
        result = self.dispatch(['up', '-d', '--remove-orphans'], returncode=1)
        assert "COMPOSE_IGNORE_ORPHANS and --remove-orphans cannot be combined." in result.stderr

    def test_up_handles_sigint(self):
        proc = start_process(self.base_dir, ['up', '-t', '2'])
        wait_on_condition(ContainerCountCondition(self.project, 2))

        os.kill(proc.pid, signal.SIGINT)
        wait_on_condition(ContainerCountCondition(self.project, 0))

    def test_up_handles_sigterm(self):
        proc = start_process(self.base_dir, ['up', '-t', '2'])
        wait_on_condition(ContainerCountCondition(self.project, 2))

        os.kill(proc.pid, signal.SIGTERM)
        wait_on_condition(ContainerCountCondition(self.project, 0))

    @v2_only()
    def test_up_handles_force_shutdown(self):
        self.base_dir = 'tests/fixtures/sleeps-composefile'
        proc = start_process(self.base_dir, ['up', '-t', '200'])
        wait_on_condition(ContainerCountCondition(self.project, 2))

        os.kill(proc.pid, signal.SIGTERM)
        time.sleep(0.1)
        os.kill(proc.pid, signal.SIGTERM)
        wait_on_condition(ContainerCountCondition(self.project, 0))

    def test_up_handles_abort_on_container_exit(self):
        self.base_dir = 'tests/fixtures/abort-on-container-exit-0'
        proc = start_process(self.base_dir, ['up', '--abort-on-container-exit'])
        wait_on_condition(ContainerCountCondition(self.project, 0))
        proc.wait()
        assert proc.returncode == 0

    def test_up_handles_abort_on_container_exit_code(self):
        self.base_dir = 'tests/fixtures/abort-on-container-exit-1'
        proc = start_process(self.base_dir, ['up', '--abort-on-container-exit'])
        wait_on_condition(ContainerCountCondition(self.project, 0))
        proc.wait()
        assert proc.returncode == 1

    @v2_only()
    @no_cluster('Container PID mode does not work across clusters')
    def test_up_with_pid_mode(self):
        c = self.client.create_container(
            'busybox', 'top', name='composetest_pid_mode_container',
            host_config={}
        )
        self.addCleanup(self.client.remove_container, c, force=True)
        self.client.start(c)
        container_mode_source = 'container:{}'.format(c['Id'])

        self.base_dir = 'tests/fixtures/pid-mode'

        self.dispatch(['up', '-d'], None)

        service_mode_source = 'container:{}'.format(
            self.project.get_service('container').containers()[0].id)
        service_mode_container = self.project.get_service('service').containers()[0]
        assert service_mode_container.get('HostConfig.PidMode') == service_mode_source

        container_mode_container = self.project.get_service('container').containers()[0]
        assert container_mode_container.get('HostConfig.PidMode') == container_mode_source

        host_mode_container = self.project.get_service('host').containers()[0]
        assert host_mode_container.get('HostConfig.PidMode') == 'host'

    def test_exec_without_tty(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['up', '-d', 'console'])
        assert len(self.project.containers()) == 1

        stdout, stderr = self.dispatch(['exec', '-T', 'console', 'ls', '-1d', '/'])
        assert stderr == ""
        assert stdout == "/\n"

    def test_exec_detach_long_form(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['up', '--detach', 'console'])
        assert len(self.project.containers()) == 1

        stdout, stderr = self.dispatch(['exec', '-T', 'console', 'ls', '-1d', '/'])
        assert stderr == ""
        assert stdout == "/\n"

    def test_exec_custom_user(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['up', '-d', 'console'])
        assert len(self.project.containers()) == 1

        stdout, stderr = self.dispatch(['exec', '-T', '--user=operator', 'console', 'whoami'])
        assert stdout == "operator\n"
        assert stderr == ""

    @v3_only()
    def test_exec_workdir(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        os.environ['COMPOSE_API_VERSION'] = '1.35'
        self.dispatch(['up', '-d', 'console'])
        assert len(self.project.containers()) == 1

        stdout, stderr = self.dispatch(['exec', '-T', '--workdir', '/etc', 'console', 'ls'])
        assert 'passwd' in stdout

    @v2_2_only()
    def test_exec_service_with_environment_overridden(self):
        name = 'service'
        self.base_dir = 'tests/fixtures/environment-exec'
        self.dispatch(['up', '-d'])
        assert len(self.project.containers()) == 1

        stdout, stderr = self.dispatch([
            'exec',
            '-T',
            '-e', 'foo=notbar',
            '--env', 'alpha=beta',
            name,
            'env',
        ])

        # env overridden
        assert 'foo=notbar' in stdout
        # keep environment from yaml
        assert 'hello=world' in stdout
        # added option from command line
        assert 'alpha=beta' in stdout

        assert stderr == ''

    def test_run_service_without_links(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['run', 'console', '/bin/true'])
        assert len(self.project.containers()) == 0

        # Ensure stdin/out was open
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        config = container.inspect()['Config']
        assert config['AttachStderr']
        assert config['AttachStdout']
        assert config['AttachStdin']

    def test_run_service_with_links(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['run', 'web', '/bin/true'], None)
        db = self.project.get_service('db')
        console = self.project.get_service('console')
        assert len(db.containers()) == 1
        assert len(console.containers()) == 0

    @v2_only()
    def test_run_service_with_dependencies(self):
        self.base_dir = 'tests/fixtures/v2-dependencies'
        self.dispatch(['run', 'web', '/bin/true'], None)
        db = self.project.get_service('db')
        console = self.project.get_service('console')
        assert len(db.containers()) == 1
        assert len(console.containers()) == 0

    def test_run_service_with_scaled_dependencies(self):
        self.base_dir = 'tests/fixtures/v2-dependencies'
        self.dispatch(['up', '-d', '--scale', 'db=2', '--scale', 'console=0'])
        db = self.project.get_service('db')
        console = self.project.get_service('console')
        assert len(db.containers()) == 2
        assert len(console.containers()) == 0
        self.dispatch(['run', 'web', '/bin/true'], None)
        assert len(db.containers()) == 2
        assert len(console.containers()) == 0

    def test_run_with_no_deps(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['run', '--no-deps', 'web', '/bin/true'])
        db = self.project.get_service('db')
        assert len(db.containers()) == 0

    def test_run_does_not_recreate_linked_containers(self):
        self.base_dir = 'tests/fixtures/links-composefile'
        self.dispatch(['up', '-d', 'db'])
        db = self.project.get_service('db')
        assert len(db.containers()) == 1

        old_ids = [c.id for c in db.containers()]

        self.dispatch(['run', 'web', '/bin/true'], None)
        assert len(db.containers()) == 1

        new_ids = [c.id for c in db.containers()]

        assert old_ids == new_ids

    def test_run_without_command(self):
        self.base_dir = 'tests/fixtures/commands-composefile'
        self.check_build('tests/fixtures/simple-dockerfile', tag='composetest_test')

        self.dispatch(['run', 'implicit'])
        service = self.project.get_service('implicit')
        containers = service.containers(stopped=True, one_off=OneOffFilter.only)
        assert [c.human_readable_command for c in containers] == [u'/bin/sh -c echo "success"']

        self.dispatch(['run', 'explicit'])
        service = self.project.get_service('explicit')
        containers = service.containers(stopped=True, one_off=OneOffFilter.only)
        assert [c.human_readable_command for c in containers] == [u'/bin/true']

    @pytest.mark.skipif(SWARM_SKIP_RM_VOLUMES, reason='Swarm DELETE /containers/<id> bug')
    def test_run_rm(self):
        self.base_dir = 'tests/fixtures/volume'
        proc = start_process(self.base_dir, ['run', '--rm', 'test'])
        service = self.project.get_service('test')
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'volume_test_run_*',
            'running')
        )
        containers = service.containers(one_off=OneOffFilter.only)
        assert len(containers) == 1
        mounts = containers[0].get('Mounts')
        for mount in mounts:
            if mount['Destination'] == '/container-path':
                anonymous_name = mount['Name']
                break
        os.kill(proc.pid, signal.SIGINT)
        wait_on_process(proc, 1)

        assert len(service.containers(stopped=True, one_off=OneOffFilter.only)) == 0

        volumes = self.client.volumes()['Volumes']
        assert volumes is not None
        for volume in service.options.get('volumes'):
            if volume.internal == '/container-named-path':
                name = volume.external
                break
        volume_names = [v['Name'].split('/')[-1] for v in volumes]
        assert name in volume_names
        assert anonymous_name not in volume_names

    def test_run_service_with_dockerfile_entrypoint(self):
        self.base_dir = 'tests/fixtures/entrypoint-dockerfile'
        self.dispatch(['run', 'test'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['printf']
        assert container.get('Config.Cmd') == ['default', 'args']

    def test_run_service_with_unset_entrypoint(self):
        self.base_dir = 'tests/fixtures/entrypoint-dockerfile'
        self.dispatch(['run', '--entrypoint=""', 'test', 'true'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') is None
        assert container.get('Config.Cmd') == ['true']

        self.dispatch(['run', '--entrypoint', '""', 'test', 'true'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') is None
        assert container.get('Config.Cmd') == ['true']

    def test_run_service_with_dockerfile_entrypoint_overridden(self):
        self.base_dir = 'tests/fixtures/entrypoint-dockerfile'
        self.dispatch(['run', '--entrypoint', 'echo', 'test'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['echo']
        assert not container.get('Config.Cmd')

    def test_run_service_with_dockerfile_entrypoint_and_command_overridden(self):
        self.base_dir = 'tests/fixtures/entrypoint-dockerfile'
        self.dispatch(['run', '--entrypoint', 'echo', 'test', 'foo'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['echo']
        assert container.get('Config.Cmd') == ['foo']

    def test_run_service_with_compose_file_entrypoint(self):
        self.base_dir = 'tests/fixtures/entrypoint-composefile'
        self.dispatch(['run', 'test'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['printf']
        assert container.get('Config.Cmd') == ['default', 'args']

    def test_run_service_with_compose_file_entrypoint_overridden(self):
        self.base_dir = 'tests/fixtures/entrypoint-composefile'
        self.dispatch(['run', '--entrypoint', 'echo', 'test'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['echo']
        assert not container.get('Config.Cmd')

    def test_run_service_with_compose_file_entrypoint_and_command_overridden(self):
        self.base_dir = 'tests/fixtures/entrypoint-composefile'
        self.dispatch(['run', '--entrypoint', 'echo', 'test', 'foo'])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['echo']
        assert container.get('Config.Cmd') == ['foo']

    def test_run_service_with_compose_file_entrypoint_and_empty_string_command(self):
        self.base_dir = 'tests/fixtures/entrypoint-composefile'
        self.dispatch(['run', '--entrypoint', 'echo', 'test', ''])
        container = self.project.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert container.get('Config.Entrypoint') == ['echo']
        assert container.get('Config.Cmd') == ['']

    def test_run_service_with_user_overridden(self):
        self.base_dir = 'tests/fixtures/user-composefile'
        name = 'service'
        user = 'sshd'
        self.dispatch(['run', '--user={user}'.format(user=user), name], returncode=1)
        service = self.project.get_service(name)
        container = service.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert user == container.get('Config.User')

    def test_run_service_with_user_overridden_short_form(self):
        self.base_dir = 'tests/fixtures/user-composefile'
        name = 'service'
        user = 'sshd'
        self.dispatch(['run', '-u', user, name], returncode=1)
        service = self.project.get_service(name)
        container = service.containers(stopped=True, one_off=OneOffFilter.only)[0]
        assert user == container.get('Config.User')

    def test_run_service_with_environment_overridden(self):
        name = 'service'
        self.base_dir = 'tests/fixtures/environment-composefile'
        self.dispatch([
            'run', '-e', 'foo=notbar',
            '-e', 'allo=moto=bobo',
            '-e', 'alpha=beta',
            name,
            '/bin/true',
        ])
        service = self.project.get_service(name)
        container = service.containers(stopped=True, one_off=OneOffFilter.only)[0]
        # env overridden
        assert 'notbar' == container.environment['foo']
        # keep environment from yaml
        assert 'world' == container.environment['hello']
        # added option from command line
        assert 'beta' == container.environment['alpha']
        # make sure a value with a = don't crash out
        assert 'moto=bobo' == container.environment['allo']

    def test_run_service_without_map_ports(self):
        # create one off container
        self.base_dir = 'tests/fixtures/ports-composefile'
        self.dispatch(['run', '-d', 'simple'])
        container = self.project.get_service('simple').containers(one_off=OneOffFilter.only)[0]

        # get port information
        port_random = container.get_local_port(3000)
        port_assigned = container.get_local_port(3001)

        # close all one off containers we just created
        container.stop()

        # check the ports
        assert port_random is None
        assert port_assigned is None

    def test_run_service_with_map_ports(self):
        # create one off container
        self.base_dir = 'tests/fixtures/ports-composefile'
        self.dispatch(['run', '-d', '--service-ports', 'simple'])
        container = self.project.get_service('simple').containers(one_off=OneOffFilter.only)[0]

        # get port information
        port_random = container.get_local_port(3000)
        port_assigned = container.get_local_port(3001)
        port_range = container.get_local_port(3002), container.get_local_port(3003)

        # close all one off containers we just created
        container.stop()

        # check the ports
        assert port_random is not None
        assert port_assigned.endswith(':49152')
        assert port_range[0].endswith(':49153')
        assert port_range[1].endswith(':49154')

    def test_run_service_with_explicitly_mapped_ports(self):
        # create one off container
        self.base_dir = 'tests/fixtures/ports-composefile'
        self.dispatch(['run', '-d', '-p', '30000:3000', '--publish', '30001:3001', 'simple'])
        container = self.project.get_service('simple').containers(one_off=OneOffFilter.only)[0]

        # get port information
        port_short = container.get_local_port(3000)
        port_full = container.get_local_port(3001)

        # close all one off containers we just created
        container.stop()

        # check the ports
        assert port_short.endswith(':30000')
        assert port_full.endswith(':30001')

    def test_run_service_with_explicitly_mapped_ip_ports(self):
        # create one off container
        self.base_dir = 'tests/fixtures/ports-composefile'
        self.dispatch([
            'run', '-d',
            '-p', '127.0.0.1:30000:3000',
            '--publish', '127.0.0.1:30001:3001',
            'simple'
        ])
        container = self.project.get_service('simple').containers(one_off=OneOffFilter.only)[0]

        # get port information
        port_short = container.get_local_port(3000)
        port_full = container.get_local_port(3001)

        # close all one off containers we just created
        container.stop()

        # check the ports
        assert port_short == "127.0.0.1:30000"
        assert port_full == "127.0.0.1:30001"

    def test_run_with_expose_ports(self):
        # create one off container
        self.base_dir = 'tests/fixtures/expose-composefile'
        self.dispatch(['run', '-d', '--service-ports', 'simple'])
        container = self.project.get_service('simple').containers(one_off=OneOffFilter.only)[0]

        ports = container.ports
        assert len(ports) == 9
        # exposed ports are not mapped to host ports
        assert ports['3000/tcp'] is None
        assert ports['3001/tcp'] is None
        assert ports['3001/udp'] is None
        assert ports['3002/tcp'] is None
        assert ports['3003/tcp'] is None
        assert ports['3004/tcp'] is None
        assert ports['3005/tcp'] is None
        assert ports['3006/udp'] is None
        assert ports['3007/udp'] is None

        # close all one off containers we just created
        container.stop()

    def test_run_with_custom_name(self):
        self.base_dir = 'tests/fixtures/environment-composefile'
        name = 'the-container-name'
        self.dispatch(['run', '--name', name, 'service', '/bin/true'])

        service = self.project.get_service('service')
        container, = service.containers(stopped=True, one_off=OneOffFilter.only)
        assert container.name == name

    def test_run_service_with_workdir_overridden(self):
        self.base_dir = 'tests/fixtures/run-workdir'
        name = 'service'
        workdir = '/var'
        self.dispatch(['run', '--workdir={workdir}'.format(workdir=workdir), name])
        service = self.project.get_service(name)
        container = service.containers(stopped=True, one_off=True)[0]
        assert workdir == container.get('Config.WorkingDir')

    def test_run_service_with_workdir_overridden_short_form(self):
        self.base_dir = 'tests/fixtures/run-workdir'
        name = 'service'
        workdir = '/var'
        self.dispatch(['run', '-w', workdir, name])
        service = self.project.get_service(name)
        container = service.containers(stopped=True, one_off=True)[0]
        assert workdir == container.get('Config.WorkingDir')

    @v2_only()
    def test_run_service_with_use_aliases(self):
        filename = 'network-aliases.yml'
        self.base_dir = 'tests/fixtures/networks'
        self.dispatch(['-f', filename, 'run', '-d', '--use-aliases', 'web', 'top'])

        back_name = '{}_back'.format(self.project.name)
        front_name = '{}_front'.format(self.project.name)

        web_container = self.project.get_service('web').containers(one_off=OneOffFilter.only)[0]

        back_aliases = web_container.get(
            'NetworkSettings.Networks.{}.Aliases'.format(back_name)
        )
        assert 'web' in back_aliases
        front_aliases = web_container.get(
            'NetworkSettings.Networks.{}.Aliases'.format(front_name)
        )
        assert 'web' in front_aliases
        assert 'forward_facing' in front_aliases
        assert 'ahead' in front_aliases

    @v2_only()
    def test_run_interactive_connects_to_network(self):
        self.base_dir = 'tests/fixtures/networks'

        self.dispatch(['up', '-d'])
        self.dispatch(['run', 'app', 'nslookup', 'app'])
        self.dispatch(['run', 'app', 'nslookup', 'db'])

        containers = self.project.get_service('app').containers(
            stopped=True, one_off=OneOffFilter.only)
        assert len(containers) == 2

        for container in containers:
            networks = container.get('NetworkSettings.Networks')

            assert sorted(list(networks)) == [
                '{}_{}'.format(self.project.name, name)
                for name in ['back', 'front']
            ]

            for _, config in networks.items():
                # TODO: once we drop support for API <1.24, this can be changed to:
                # assert config['Aliases'] == [container.short_id]
                aliases = set(config['Aliases'] or []) - set([container.short_id])
                assert not aliases

    @v2_only()
    def test_run_detached_connects_to_network(self):
        self.base_dir = 'tests/fixtures/networks'
        self.dispatch(['up', '-d'])
        self.dispatch(['run', '-d', 'app', 'top'])

        container = self.project.get_service('app').containers(one_off=OneOffFilter.only)[0]
        networks = container.get('NetworkSettings.Networks')

        assert sorted(list(networks)) == [
            '{}_{}'.format(self.project.name, name)
            for name in ['back', 'front']
        ]

        for _, config in networks.items():
            # TODO: once we drop support for API <1.24, this can be changed to:
            # assert config['Aliases'] == [container.short_id]
            aliases = set(config['Aliases'] or []) - set([container.short_id])
            assert not aliases

        assert self.lookup(container, 'app')
        assert self.lookup(container, 'db')

    def test_run_handles_sigint(self):
        proc = start_process(self.base_dir, ['run', '-T', 'simple', 'top'])
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'simple-composefile_simple_run_*',
            'running'))

        os.kill(proc.pid, signal.SIGINT)
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'simple-composefile_simple_run_*',
            'exited'))

    def test_run_handles_sigterm(self):
        proc = start_process(self.base_dir, ['run', '-T', 'simple', 'top'])
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'simple-composefile_simple_run_*',
            'running'))

        os.kill(proc.pid, signal.SIGTERM)
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'simple-composefile_simple_run_*',
            'exited'))

    def test_run_handles_sighup(self):
        proc = start_process(self.base_dir, ['run', '-T', 'simple', 'top'])
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'simple-composefile_simple_run_*',
            'running'))

        os.kill(proc.pid, signal.SIGHUP)
        wait_on_condition(ContainerStateCondition(
            self.project.client,
            'simple-composefile_simple_run_*',
            'exited'))

    @mock.patch.dict(os.environ)
    def test_run_unicode_env_values_from_system(self):
        value = ', , , , , , , , '
        if six.PY2:  # os.environ doesn't support unicode values in Py2
            os.environ['BAR'] = value.encode('utf-8')
        else:  # ... and doesn't support byte values in Py3
            os.environ['BAR'] = value
        self.base_dir = 'tests/fixtures/unicode-environment'
        result = self.dispatch(['run', 'simple'])

        if six.PY2:  # Can't retrieve output on Py3. See issue #3670
            assert value in result.stdout.strip()

        container = self.project.containers(one_off=OneOffFilter.only, stopped=True)[0]
        environment = container.get('Config.Env')
        assert 'FOO={}'.format(value) in environment

    @mock.patch.dict(os.environ)
    def test_run_env_values_from_system(self):
        os.environ['FOO'] = 'bar'
        os.environ['BAR'] = 'baz'

        self.dispatch(['run', '-e', 'FOO', 'simple', 'true'], None)

        container = self.project.containers(one_off=OneOffFilter.only, stopped=True)[0]
        environment = container.get('Config.Env')
        assert 'FOO=bar' in environment
        assert 'BAR=baz' not in environment

    def test_run_label_flag(self):
        self.base_dir = 'tests/fixtures/run-labels'
        name = 'service'
        self.dispatch(['run', '-l', 'default', '--label', 'foo=baz', name, '/bin/true'])
        service = self.project.get_service(name)
        container, = service.containers(stopped=True, one_off=OneOffFilter.only)
        labels = container.labels
        assert labels['default'] == ''
        assert labels['foo'] == 'baz'
        assert labels['hello'] == 'world'

    def test_rm(self):
        service = self.project.get_service('simple')
        service.create_container()
        kill_service(service)
        assert len(service.containers(stopped=True)) == 1
        self.dispatch(['rm', '--force'], None)
        assert len(service.containers(stopped=True)) == 0
        service = self.project.get_service('simple')
        service.create_container()
        kill_service(service)
        assert len(service.containers(stopped=True)) == 1
        self.dispatch(['rm', '-f'], None)
        assert len(service.containers(stopped=True)) == 0
        service = self.project.get_service('simple')
        service.create_container()
        self.dispatch(['rm', '-fs'], None)
        assert len(service.containers(stopped=True)) == 0

    def test_rm_stop(self):
        self.dispatch(['up', '-d'], None)
        simple = self.project.get_service('simple')
        another = self.project.get_service('another')
        assert len(simple.containers()) == 1
        assert len(another.containers()) == 1
        self.dispatch(['rm', '-fs'], None)
        assert len(simple.containers(stopped=True)) == 0
        assert len(another.containers(stopped=True)) == 0

        self.dispatch(['up', '-d'], None)
        assert len(simple.containers()) == 1
        assert len(another.containers()) == 1
        self.dispatch(['rm', '-fs', 'another'], None)
        assert len(simple.containers()) == 1
        assert len(another.containers(stopped=True)) == 0

    def test_rm_all(self):
        service = self.project.get_service('simple')
        service.create_container(one_off=False)
        service.create_container(one_off=True)
        kill_service(service)
        assert len(service.containers(stopped=True)) == 1
        assert len(service.containers(stopped=True, one_off=OneOffFilter.only)) == 1
        self.dispatch(['rm', '-f'], None)
        assert len(service.containers(stopped=True)) == 0
        assert len(service.containers(stopped=True, one_off=OneOffFilter.only)) == 0

        service.create_container(one_off=False)
        service.create_container(one_off=True)
        kill_service(service)
        assert len(service.containers(stopped=True)) == 1
        assert len(service.containers(stopped=True, one_off=OneOffFilter.only)) == 1
        self.dispatch(['rm', '-f', '--all'], None)
        assert len(service.containers(stopped=True)) == 0
        assert len(service.containers(stopped=True, one_off=OneOffFilter.only)) == 0

    def test_stop(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1
        assert service.containers()[0].is_running

        self.dispatch(['stop', '-t', '1'], None)

        assert len(service.containers(stopped=True)) == 1
        assert not service.containers(stopped=True)[0].is_running

    def test_stop_signal(self):
        self.base_dir = 'tests/fixtures/stop-signal-composefile'
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1
        assert service.containers()[0].is_running

        self.dispatch(['stop', '-t', '1'], None)
        assert len(service.containers(stopped=True)) == 1
        assert not service.containers(stopped=True)[0].is_running
        assert service.containers(stopped=True)[0].exit_code == 0

    def test_start_no_containers(self):
        result = self.dispatch(['start'], returncode=1)
        assert 'No containers to start' in result.stderr

    @v2_only()
    def test_up_logging(self):
        self.base_dir = 'tests/fixtures/logging-composefile'
        self.dispatch(['up', '-d'])
        simple = self.project.get_service('simple').containers()[0]
        log_config = simple.get('HostConfig.LogConfig')
        assert log_config
        assert log_config.get('Type') == 'none'

        another = self.project.get_service('another').containers()[0]
        log_config = another.get('HostConfig.LogConfig')
        assert log_config
        assert log_config.get('Type') == 'json-file'
        assert log_config.get('Config')['max-size'] == '10m'

    def test_up_logging_legacy(self):
        self.base_dir = 'tests/fixtures/logging-composefile-legacy'
        self.dispatch(['up', '-d'])
        simple = self.project.get_service('simple').containers()[0]
        log_config = simple.get('HostConfig.LogConfig')
        assert log_config
        assert log_config.get('Type') == 'none'

        another = self.project.get_service('another').containers()[0]
        log_config = another.get('HostConfig.LogConfig')
        assert log_config
        assert log_config.get('Type') == 'json-file'
        assert log_config.get('Config')['max-size'] == '10m'

    def test_pause_unpause(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert not service.containers()[0].is_paused

        self.dispatch(['pause'], None)
        assert service.containers()[0].is_paused

        self.dispatch(['unpause'], None)
        assert not service.containers()[0].is_paused

    def test_pause_no_containers(self):
        result = self.dispatch(['pause'], returncode=1)
        assert 'No containers to pause' in result.stderr

    def test_unpause_no_containers(self):
        result = self.dispatch(['unpause'], returncode=1)
        assert 'No containers to unpause' in result.stderr

    def test_logs_invalid_service_name(self):
        self.dispatch(['logs', 'madeupname'], returncode=1)

    def test_logs_follow(self):
        self.base_dir = 'tests/fixtures/echo-services'
        self.dispatch(['up', '-d'])

        result = self.dispatch(['logs', '-f'])

        if not is_cluster(self.client):
            assert result.stdout.count('\n') == 5
        else:
            # Sometimes logs are picked up from old containers that haven't yet
            # been removed (removal in Swarm is async)
            assert result.stdout.count('\n') >= 5

        assert 'simple' in result.stdout
        assert 'another' in result.stdout
        assert 'exited with code 0' in result.stdout

    def test_logs_follow_logs_from_new_containers(self):
        self.base_dir = 'tests/fixtures/logs-composefile'
        self.dispatch(['up', '-d', 'simple'])

        proc = start_process(self.base_dir, ['logs', '-f'])

        self.dispatch(['up', '-d', 'another'])
        another_name = self.project.get_service('another').get_container().name_without_project
        wait_on_condition(
            ContainerStateCondition(
                self.project.client,
                'logs-composefile_another_*',
                'exited'
            )
        )

        simple_name = self.project.get_service('simple').get_container().name_without_project
        self.dispatch(['kill', 'simple'])

        result = wait_on_process(proc)

        assert 'hello' in result.stdout
        assert 'test' in result.stdout
        assert '{} exited with code 0'.format(another_name) in result.stdout
        assert '{} exited with code 137'.format(simple_name) in result.stdout

    def test_logs_follow_logs_from_restarted_containers(self):
        self.base_dir = 'tests/fixtures/logs-restart-composefile'
        proc = start_process(self.base_dir, ['up'])

        wait_on_condition(
            ContainerStateCondition(
                self.project.client,
                'logs-restart-composefile_another_*',
                'exited'
            )
        )
        self.dispatch(['kill', 'simple'])

        result = wait_on_process(proc)

        assert len(re.findall(
            r'logs-restart-composefile_another_1_[a-f0-9]{12} exited with code 1',
            result.stdout
        )) == 3
        assert result.stdout.count('world') == 3

    def test_logs_default(self):
        self.base_dir = 'tests/fixtures/logs-composefile'
        self.dispatch(['up', '-d'])

        result = self.dispatch(['logs'])
        assert 'hello' in result.stdout
        assert 'test' in result.stdout
        assert 'exited with' not in result.stdout

    def test_logs_on_stopped_containers_exits(self):
        self.base_dir = 'tests/fixtures/echo-services'
        self.dispatch(['up'])

        result = self.dispatch(['logs'])
        assert 'simple' in result.stdout
        assert 'another' in result.stdout
        assert 'exited with' not in result.stdout

    def test_logs_timestamps(self):
        self.base_dir = 'tests/fixtures/echo-services'
        self.dispatch(['up', '-d'])

        result = self.dispatch(['logs', '-f', '-t'])
        assert re.search(r'(\d{4})-(\d{2})-(\d{2})T(\d{2})\:(\d{2})\:(\d{2})', result.stdout)

    def test_logs_tail(self):
        self.base_dir = 'tests/fixtures/logs-tail-composefile'
        self.dispatch(['up'])

        result = self.dispatch(['logs', '--tail', '2'])
        assert 'y\n' in result.stdout
        assert 'z\n' in result.stdout
        assert 'w\n' not in result.stdout
        assert 'x\n' not in result.stdout

    def test_kill(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1
        assert service.containers()[0].is_running

        self.dispatch(['kill'], None)

        assert len(service.containers(stopped=True)) == 1
        assert not service.containers(stopped=True)[0].is_running

    def test_kill_signal_sigstop(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        assert len(service.containers()) == 1
        assert service.containers()[0].is_running

        self.dispatch(['kill', '-s', 'SIGSTOP'], None)

        assert len(service.containers()) == 1
        # The container is still running. It has only been paused
        assert service.containers()[0].is_running

    def test_kill_stopped_service(self):
        self.dispatch(['up', '-d'], None)
        service = self.project.get_service('simple')
        self.dispatch(['kill', '-s', 'SIGSTOP'], None)
        assert service.containers()[0].is_running

        self.dispatch(['kill', '-s', 'SIGKILL'], None)

        assert len(service.containers(stopped=True)) == 1
        assert not service.containers(stopped=True)[0].is_running

    def test_restart(self):
        service = self.project.get_service('simple')
        container = service.create_container()
        service.start_container(container)
        started_at = container.dictionary['State']['StartedAt']
        self.dispatch(['restart', '-t', '1'], None)
        container.inspect()
        assert container.dictionary['State']['FinishedAt'] != '0001-01-01T00:00:00Z'
        assert container.dictionary['State']['StartedAt'] != started_at

    def test_restart_stopped_container(self):
        service = self.project.get_service('simple')
        container = service.create_container()
        container.start()
        container.kill()
        assert len(service.containers(stopped=True)) == 1
        self.dispatch(['restart', '-t', '1'], None)
        assert len(service.containers(stopped=False)) == 1

    def test_restart_no_containers(self):
        result = self.dispatch(['restart'], returncode=1)
        assert 'No containers to restart' in result.stderr

    def test_scale(self):
        project = self.project

        self.dispatch(['scale', 'simple=1'])
        assert len(project.get_service('simple').containers()) == 1

        self.dispatch(['scale', 'simple=3', 'another=2'])
        assert len(project.get_service('simple').containers()) == 3
        assert len(project.get_service('another').containers()) == 2

        self.dispatch(['scale', 'simple=1', 'another=1'])
        assert len(project.get_service('simple').containers()) == 1
        assert len(project.get_service('another').containers()) == 1

        self.dispatch(['scale', 'simple=1', 'another=1'])
        assert len(project.get_service('simple').containers()) == 1
        assert len(project.get_service('another').containers()) == 1

        self.dispatch(['scale', 'simple=0', 'another=0'])
        assert len(project.get_service('simple').containers()) == 0
        assert len(project.get_service('another').containers()) == 0

    def test_scale_v2_2(self):
        self.base_dir = 'tests/fixtures/scale'
        result = self.dispatch(['scale', 'web=1'], returncode=1)
        assert 'incompatible with the v2.2 format' in result.stderr

    def test_up_scale_scale_up(self):
        self.base_dir = 'tests/fixtures/scale'
        project = self.project

        self.dispatch(['up', '-d'])
        assert len(project.get_service('web').containers()) == 2
        assert len(project.get_service('db').containers()) == 1

        self.dispatch(['up', '-d', '--scale', 'web=3'])
        assert len(project.get_service('web').containers()) == 3
        assert len(project.get_service('db').containers()) == 1

    def test_up_scale_scale_down(self):
        self.base_dir = 'tests/fixtures/scale'
        project = self.project

        self.dispatch(['up', '-d'])
        assert len(project.get_service('web').containers()) == 2
        assert len(project.get_service('db').containers()) == 1

        self.dispatch(['up', '-d', '--scale', 'web=1'])
        assert len(project.get_service('web').containers()) == 1
        assert len(project.get_service('db').containers()) == 1

    def test_up_scale_reset(self):
        self.base_dir = 'tests/fixtures/scale'
        project = self.project

        self.dispatch(['up', '-d', '--scale', 'web=3', '--scale', 'db=3'])
        assert len(project.get_service('web').containers()) == 3
        assert len(project.get_service('db').containers()) == 3

        self.dispatch(['up', '-d'])
        assert len(project.get_service('web').containers()) == 2
        assert len(project.get_service('db').containers()) == 1

    def test_up_scale_to_zero(self):
        self.base_dir = 'tests/fixtures/scale'
        project = self.project

        self.dispatch(['up', '-d'])
        assert len(project.get_service('web').containers()) == 2
        assert len(project.get_service('db').containers()) == 1

        self.dispatch(['up', '-d', '--scale', 'web=0', '--scale', 'db=0'])
        assert len(project.get_service('web').containers()) == 0
        assert len(project.get_service('db').containers()) == 0

    def test_port(self):
        self.base_dir = 'tests/fixtures/ports-composefile'
        self.dispatch(['up', '-d'], None)
        container = self.project.get_service('simple').get_container()

        def get_port(number):
            result = self.dispatch(['port', 'simple', str(number)])
            return result.stdout.rstrip()

        assert get_port(3000) == container.get_local_port(3000)
        assert ':49152' in get_port(3001)
        assert ':49153' in get_port(3002)

    def test_expanded_port(self):
        self.base_dir = 'tests/fixtures/ports-composefile'
        self.dispatch(['-f', 'expanded-notation.yml', 'up', '-d'])
        container = self.project.get_service('simple').get_container()

        def get_port(number):
            result = self.dispatch(['port', 'simple', str(number)])
            return result.stdout.rstrip()

        assert get_port(3000) == container.get_local_port(3000)
        assert ':53222' in get_port(3001)
        assert ':53223' in get_port(3002)

    def test_port_with_scale(self):
        self.base_dir = 'tests/fixtures/ports-composefile-scale'
        self.dispatch(['scale', 'simple=2'], None)
        containers = sorted(
            self.project.containers(service_names=['simple']),
            key=attrgetter('name'))

        def get_port(number, index=None):
            if index is None:
                result = self.dispatch(['port', 'simple', str(number)])
            else:
                result = self.dispatch(['port', '--index=' + str(index), 'simple', str(number)])
            return result.stdout.rstrip()

        assert get_port(3000) in (containers[0].get_local_port(3000), containers[1].get_local_port(3000))
        assert get_port(3000, index=containers[0].number) == containers[0].get_local_port(3000)
        assert get_port(3000, index=containers[1].number) == containers[1].get_local_port(3000)
        assert get_port(3002) == ""

    def test_events_json(self):
        events_proc = start_process(self.base_dir, ['events', '--json'])
        self.dispatch(['up', '-d'])
        wait_on_condition(ContainerCountCondition(self.project, 2))

        os.kill(events_proc.pid, signal.SIGINT)
        result = wait_on_process(events_proc, returncode=1)
        lines = [json.loads(line) for line in result.stdout.rstrip().split('\n')]
        assert Counter(e['action'] for e in lines) == {'create': 2, 'start': 2}

    def test_events_human_readable(self):

        def has_timestamp(string):
            str_iso_date, str_iso_time, container_info = string.split(' ', 2)
            try:
                return isinstance(datetime.datetime.strptime(
                    '%s %s' % (str_iso_date, str_iso_time),
                    '%Y-%m-%d %H:%M:%S.%f'),
                    datetime.datetime)
            except ValueError:
                return False

        events_proc = start_process(self.base_dir, ['events'])
        self.dispatch(['up', '-d', 'simple'])
        wait_on_condition(ContainerCountCondition(self.project, 1))

        os.kill(events_proc.pid, signal.SIGINT)
        result = wait_on_process(events_proc, returncode=1)
        lines = result.stdout.rstrip().split('\n')
        assert len(lines) == 2

        container, = self.project.containers()
        expected_template = ' container {} {}'
        expected_meta_info = ['image=busybox:latest', 'name=simple-composefile_simple_']

        assert expected_template.format('create', container.id) in lines[0]
        assert expected_template.format('start', container.id) in lines[1]
        for line in lines:
            for info in expected_meta_info:
                assert info in line

        assert has_timestamp(lines[0])

    def test_env_file_relative_to_compose_file(self):
        config_path = os.path.abspath('tests/fixtures/env-file/docker-compose.yml')
        self.dispatch(['-f', config_path, 'up', '-d'], None)
        self._project = get_project(self.base_dir, [config_path])

        containers = self.project.containers(stopped=True)
        assert len(containers) == 1
        assert "FOO=1" in containers[0].get('Config.Env')

    @mock.patch.dict(os.environ)
    def test_home_and_env_var_in_volume_path(self):
        os.environ['VOLUME_NAME'] = 'my-volume'
        os.environ['HOME'] = '/tmp/home-dir'

        self.base_dir = 'tests/fixtures/volume-path-interpolation'
        self.dispatch(['up', '-d'], None)

        container = self.project.containers(stopped=True)[0]
        actual_host_path = container.get_mount('/container-path')['Source']
        components = actual_host_path.split('/')
        assert components[-2:] == ['home-dir', 'my-volume']

    def test_up_with_default_override_file(self):
        self.base_dir = 'tests/fixtures/override-files'
        self.dispatch(['up', '-d'], None)

        containers = self.project.containers()
        assert len(containers) == 2

        web, db = containers
        assert web.human_readable_command == 'top'
        assert db.human_readable_command == 'top'

    def test_up_with_multiple_files(self):
        self.base_dir = 'tests/fixtures/override-files'
        config_paths = [
            'docker-compose.yml',
            'docker-compose.override.yml',
            'extra.yml',
        ]
        self._project = get_project(self.base_dir, config_paths)
        self.dispatch(
            [
                '-f', config_paths[0],
                '-f', config_paths[1],
                '-f', config_paths[2],
                'up', '-d',
            ],
            None)

        containers = self.project.containers()
        assert len(containers) == 3

        web, other, db = containers
        assert web.human_readable_command == 'top'
        assert db.human_readable_command == 'top'
        assert other.human_readable_command == 'top'

    def test_up_with_extends(self):
        self.base_dir = 'tests/fixtures/extends'
        self.dispatch(['up', '-d'], None)

        assert set([s.name for s in self.project.services]) == set(['mydb', 'myweb'])

        # Sort by name so we get [db, web]
        containers = sorted(
            self.project.containers(stopped=True),
            key=lambda c: c.name,
        )

        assert len(containers) == 2
        web = containers[1]
        db_name = containers[0].name_without_project

        assert set(get_links(web)) == set(
            ['db', db_name, 'extends_{}'.format(db_name)]
        )

        expected_env = set([
            "FOO=1",
            "BAR=2",
            "BAZ=2",
        ])
        assert expected_env <= set(web.get('Config.Env'))

    def test_top_services_not_running(self):
        self.base_dir = 'tests/fixtures/top'
        result = self.dispatch(['top'])
        assert len(result.stdout) == 0

    def test_top_services_running(self):
        self.base_dir = 'tests/fixtures/top'
        self.dispatch(['up', '-d'])
        result = self.dispatch(['top'])

        assert 'top_service_a' in result.stdout
        assert 'top_service_b' in result.stdout
        assert 'top_not_a_service' not in result.stdout

    def test_top_processes_running(self):
        self.base_dir = 'tests/fixtures/top'
        self.dispatch(['up', '-d'])
        result = self.dispatch(['top'])
        assert result.stdout.count("top") == 4

    def test_forward_exitval(self):
        self.base_dir = 'tests/fixtures/exit-code-from'
        proc = start_process(
            self.base_dir,
            ['up', '--abort-on-container-exit', '--exit-code-from', 'another']
        )

        result = wait_on_process(proc, returncode=1)
        assert re.findall(r'exit-code-from_another_1_[a-f0-9]{12} exited with code 1', result.stdout)

    def test_exit_code_from_signal_stop(self):
        self.base_dir = 'tests/fixtures/exit-code-from'
        proc = start_process(
            self.base_dir,
            ['up', '--abort-on-container-exit', '--exit-code-from', 'simple']
        )
        result = wait_on_process(proc, returncode=137)  # SIGKILL
        name = self.project.get_service('another').containers(stopped=True)[0].name_without_project
        assert '{} exited with code 1'.format(name) in result.stdout

    def test_images(self):
        self.project.get_service('simple').create_container()
        result = self.dispatch(['images'])
        assert 'busybox' in result.stdout
        assert 'simple-composefile_simple_' in result.stdout

    def test_images_default_composefile(self):
        self.base_dir = 'tests/fixtures/multiple-composefiles'
        self.dispatch(['up', '-d'])
        result = self.dispatch(['images'])

        assert 'busybox' in result.stdout
        assert 'multiple-composefiles_another_1' in result.stdout
        assert 'multiple-composefiles_simple_1' in result.stdout

    @mock.patch.dict(os.environ)
    def test_images_tagless_image(self):
        self.base_dir = 'tests/fixtures/tagless-image'
        stream = self.client.build(self.base_dir, decode=True)
        img_id = None
        for data in stream:
            if 'aux' in data:
                img_id = data['aux']['ID']
                break
            if 'stream' in data and 'Successfully built' in data['stream']:
                img_id = self.client.inspect_image(data['stream'].split(' ')[2].strip())['Id']

        assert img_id

        os.environ['IMAGE_ID'] = img_id
        self.project.get_service('foo').create_container()
        result = self.dispatch(['images'])
        assert '<none>' in result.stdout
        assert 'tagless-image_foo_1' in result.stdout

    def test_up_with_override_yaml(self):
        self.base_dir = 'tests/fixtures/override-yaml-files'
        self._project = get_project(self.base_dir, [])
        self.dispatch(['up', '-d'], None)

        containers = self.project.containers()
        assert len(containers) == 2

        web, db = containers
        assert web.human_readable_command == 'sleep 100'
        assert db.human_readable_command == 'top'

    def test_up_with_duplicate_override_yaml_files(self):
        self.base_dir = 'tests/fixtures/duplicate-override-yaml-files'
        with pytest.raises(DuplicateOverrideFileFound):
            get_project(self.base_dir, [])
        self.base_dir = None

    def test_images_use_service_tag(self):
        pull_busybox(self.client)
        self.base_dir = 'tests/fixtures/images-service-tag'
        self.dispatch(['up', '-d', '--build'])
        result = self.dispatch(['images'])

        assert re.search(r'foo1.+test[ \t]+dev', result.stdout) is not None
        assert re.search(r'foo2.+test[ \t]+prod', result.stdout) is not None
        assert re.search(r'foo3.+_foo3[ \t]+latest', result.stdout) is not None
<EOF>
<BOF>
#!/usr/bin/env python
"""
Query the github API for the git tags of a project, and return a list of
version tags for recent releases, or the default release.

The default release is the most recent non-RC version.

Recent is a list of unique major.minor versions, where each is the most
recent version in the series.

For example, if the list of versions is:

    1.8.0-rc2
    1.8.0-rc1
    1.7.1
    1.7.0
    1.7.0-rc1
    1.6.2
    1.6.1

`default` would return `1.7.1` and
`recent -n 3` would return `1.8.0-rc2 1.7.1 1.6.2`
"""
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import argparse
import itertools
import operator
import sys
from collections import namedtuple

import requests


GITHUB_API = 'https://api.github.com/repos'

STAGES = ['tp', 'beta', 'rc']


class Version(namedtuple('_Version', 'major minor patch stage edition')):

    @classmethod
    def parse(cls, version):
        edition = None
        version = version.lstrip('v')
        version, _, stage = version.partition('-')
        if stage:
            if not any(marker in stage for marker in STAGES):
                edition = stage
                stage = None
            elif '-' in stage:
                edition, stage = stage.split('-')
        major, minor, patch = version.split('.', 3)
        return cls(major, minor, patch, stage, edition)

    @property
    def major_minor(self):
        return self.major, self.minor

    @property
    def order(self):
        """Return a representation that allows this object to be sorted
        correctly with the default comparator.
        """
        # non-GA releases should appear before GA releases
        # Order: tp -> beta -> rc -> GA
        if self.stage:
            for st in STAGES:
                if st in self.stage:
                    stage = (STAGES.index(st), self.stage)
                    break
        else:
            stage = (len(STAGES),)

        return (int(self.major), int(self.minor), int(self.patch)) + stage

    def __str__(self):
        stage = '-{}'.format(self.stage) if self.stage else ''
        edition = '-{}'.format(self.edition) if self.edition else ''
        return '.'.join(map(str, self[:3])) + edition + stage


BLACKLIST = [  # List of versions known to be broken and should not be used
    Version.parse('18.03.0-ce-rc2'),
]


def group_versions(versions):
    """Group versions by `major.minor` releases.

    Example:

        >>> group_versions([
                Version(1, 0, 0),
                Version(2, 0, 0, 'rc1'),
                Version(2, 0, 0),
                Version(2, 1, 0),
            ])

        [
            [Version(1, 0, 0)],
            [Version(2, 0, 0), Version(2, 0, 0, 'rc1')],
            [Version(2, 1, 0)],
        ]
    """
    return list(
        list(releases)
        for _, releases
        in itertools.groupby(versions, operator.attrgetter('major_minor'))
    )


def get_latest_versions(versions, num=1):
    """Return a list of the most recent versions for each major.minor version
    group.
    """
    versions = group_versions(versions)
    num = min(len(versions), num)
    return [versions[index][0] for index in range(num)]


def get_default(versions):
    """Return a :class:`Version` for the latest GA version."""
    for version in versions:
        if not version.stage:
            return version


def get_versions(tags):
    for tag in tags:
        try:
            v = Version.parse(tag['name'])
            if v in BLACKLIST:
                continue
            yield v
        except ValueError:
            print("Skipping invalid tag: {name}".format(**tag), file=sys.stderr)


def get_github_releases(projects):
    """Query the Github API for a list of version tags and return them in
    sorted order.

    See https://developer.github.com/v3/repos/#list-tags
    """
    versions = []
    for project in projects:
        url = '{}/{}/tags'.format(GITHUB_API, project)
        response = requests.get(url)
        response.raise_for_status()
        versions.extend(get_versions(response.json()))
    return sorted(versions, reverse=True, key=operator.attrgetter('order'))


def parse_args(argv):
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('project', help="Github project name (ex: docker/docker)")
    parser.add_argument('command', choices=['recent', 'default'])
    parser.add_argument('-n', '--num', type=int, default=2,
                        help="Number of versions to return from `recent`")
    return parser.parse_args(argv)


def main(argv=None):
    args = parse_args(argv)
    versions = get_github_releases(args.project.split(','))

    if args.command == 'recent':
        print(' '.join(map(str, get_latest_versions(versions, args.num))))
    elif args.command == 'default':
        print(get_default(versions))
    else:
        raise ValueError("Unknown command {}".format(args.command))


if __name__ == "__main__":
    main()
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import argparse
import os
import shutil
import sys
import time
from distutils.core import run_setup

import pypandoc
from jinja2 import Template
from release.bintray import BintrayAPI
from release.const import BINTRAY_ORG
from release.const import NAME
from release.const import REPO_ROOT
from release.downloader import BinaryDownloader
from release.images import ImageManager
from release.pypi import check_pypirc
from release.pypi import pypi_upload
from release.repository import delete_assets
from release.repository import get_contributors
from release.repository import Repository
from release.repository import upload_assets
from release.utils import branch_name
from release.utils import compatibility_matrix
from release.utils import read_release_notes_from_changelog
from release.utils import ScriptError
from release.utils import update_init_py_version
from release.utils import update_run_sh_version
from release.utils import yesno


def create_initial_branch(repository, args):
    release_branch = repository.create_release_branch(args.release, args.base)
    if args.base and args.cherries:
        print('Detected patch version.')
        cherries = input('Indicate (space-separated) PR numbers to cherry-pick then press Enter:\n')
        repository.cherry_pick_prs(release_branch, cherries.split())

    return create_bump_commit(repository, release_branch, args.bintray_user, args.bintray_org)


def create_bump_commit(repository, release_branch, bintray_user, bintray_org):
    with release_branch.config_reader() as cfg:
        release = cfg.get('release')
    print('Updating version info in __init__.py and run.sh')
    update_run_sh_version(release)
    update_init_py_version(release)

    input('Please add the release notes to the CHANGELOG.md file, then press Enter to continue.')
    proceed = None
    while not proceed:
        print(repository.diff())
        proceed = yesno('Are these changes ok? y/N ', default=False)

    if repository.diff():
        repository.create_bump_commit(release_branch, release)
    repository.push_branch_to_remote(release_branch)

    bintray_api = BintrayAPI(os.environ['BINTRAY_TOKEN'], bintray_user)
    if not bintray_api.repository_exists(bintray_org, release_branch.name):
        print('Creating data repository {} on bintray'.format(release_branch.name))
        bintray_api.create_repository(bintray_org, release_branch.name, 'generic')
    else:
        print('Bintray repository {} already exists. Skipping'.format(release_branch.name))


def monitor_pr_status(pr_data):
    print('Waiting for CI to complete...')
    last_commit = pr_data.get_commits().reversed[0]
    while True:
        status = last_commit.get_combined_status()
        if status.state == 'pending' or status.state == 'failure':
            summary = {
                'pending': 0,
                'success': 0,
                'failure': 0,
                'error': 0,
            }
            for detail in status.statuses:
                if detail.context == 'dco-signed':
                    # dco-signed check breaks on merge remote-tracking ; ignore it
                    continue
                if detail.state in summary:
                    summary[detail.state] += 1
            print(
                '{pending} pending, {success} successes, {failure} failures, '
                '{error} errors'.format(**summary)
            )
            if summary['failure'] > 0 or summary['error'] > 0:
                raise ScriptError('CI failures detected!')
            elif summary['pending'] == 0 and summary['success'] > 0:
                # This check assumes at least 1 non-DCO CI check to avoid race conditions.
                # If testing on a repo without CI, use --skip-ci-check to avoid looping eternally
                return True
            time.sleep(30)
        elif status.state == 'success':
            print('{} successes: all clear!'.format(status.total_count))
            return True


def check_pr_mergeable(pr_data):
    if pr_data.mergeable is False:
        # mergeable can also be null, in which case the warning would be a false positive.
        print(
            'WARNING!! PR #{} can not currently be merged. You will need to '
            'resolve the conflicts manually before finalizing the release.'.format(pr_data.number)
        )

    return pr_data.mergeable is True


def create_release_draft(repository, version, pr_data, files):
    print('Creating Github release draft')
    with open(os.path.join(os.path.dirname(__file__), 'release.md.tmpl'), 'r') as f:
        template = Template(f.read())
    print('Rendering release notes based on template')
    release_notes = template.render(
        version=version,
        compat_matrix=compatibility_matrix(),
        integrity=files,
        contributors=get_contributors(pr_data),
        changelog=read_release_notes_from_changelog(),
    )
    gh_release = repository.create_release(
        version, release_notes, draft=True, prerelease='-rc' in version,
        target_commitish='release'
    )
    print('Release draft initialized')
    return gh_release


def print_final_instructions(args):
    print(
        "You're almost done! Please verify that everything is in order and "
        "you are ready to make the release public, then run the following "
        "command:\n{exe} -b {user} finalize {version}".format(
            exe='./script/release/release.sh', user=args.bintray_user, version=args.release
        )
    )


def distclean():
    print('Running distclean...')
    dirs = [
        os.path.join(REPO_ROOT, 'build'), os.path.join(REPO_ROOT, 'dist'),
        os.path.join(REPO_ROOT, 'docker-compose.egg-info')
    ]
    files = []
    for base, dirnames, fnames in os.walk(REPO_ROOT):
        for fname in fnames:
            path = os.path.normpath(os.path.join(base, fname))
            if fname.endswith('.pyc'):
                files.append(path)
            elif fname.startswith('.coverage.'):
                files.append(path)
        for dirname in dirnames:
            path = os.path.normpath(os.path.join(base, dirname))
            if dirname == '__pycache__':
                dirs.append(path)
            elif dirname == '.coverage-binfiles':
                dirs.append(path)

    for file in files:
        os.unlink(file)

    for folder in dirs:
        shutil.rmtree(folder, ignore_errors=True)


def resume(args):
    try:
        distclean()
        repository = Repository(REPO_ROOT, args.repo)
        br_name = branch_name(args.release)
        if not repository.branch_exists(br_name):
            raise ScriptError('No local branch exists for this release.')
        gh_release = repository.find_release(args.release)
        if gh_release and not gh_release.draft:
            print('WARNING!! Found non-draft (public) release for this version!')
            proceed = yesno(
                'Are you sure you wish to proceed? Modifying an already '
                'released version is dangerous! y/N ', default=False
            )
            if proceed.lower() is not True:
                raise ScriptError('Aborting release')

        release_branch = repository.checkout_branch(br_name)
        if args.cherries:
            cherries = input('Indicate (space-separated) PR numbers to cherry-pick then press Enter:\n')
            repository.cherry_pick_prs(release_branch, cherries.split())

        create_bump_commit(repository, release_branch, args.bintray_user, args.bintray_org)
        pr_data = repository.find_release_pr(args.release)
        if not pr_data:
            pr_data = repository.create_release_pull_request(args.release)
        check_pr_mergeable(pr_data)
        if not args.skip_ci:
            monitor_pr_status(pr_data)
        downloader = BinaryDownloader(args.destination)
        files = downloader.download_all(args.release)
        if not gh_release:
            gh_release = create_release_draft(repository, args.release, pr_data, files)
        delete_assets(gh_release)
        upload_assets(gh_release, files)
        img_manager = ImageManager(args.release)
        img_manager.build_images(repository, files)
    except ScriptError as e:
        print(e)
        return 1

    print_final_instructions(args)
    return 0


def cancel(args):
    try:
        repository = Repository(REPO_ROOT, args.repo)
        repository.close_release_pr(args.release)
        repository.remove_release(args.release)
        repository.remove_bump_branch(args.release)
        bintray_api = BintrayAPI(os.environ['BINTRAY_TOKEN'], args.bintray_user)
        print('Removing Bintray data repository for {}'.format(args.release))
        bintray_api.delete_repository(args.bintray_org, branch_name(args.release))
        distclean()
    except ScriptError as e:
        print(e)
        return 1
    print('Release cancellation complete.')
    return 0


def start(args):
    distclean()
    try:
        repository = Repository(REPO_ROOT, args.repo)
        create_initial_branch(repository, args)
        pr_data = repository.create_release_pull_request(args.release)
        check_pr_mergeable(pr_data)
        if not args.skip_ci:
            monitor_pr_status(pr_data)
        downloader = BinaryDownloader(args.destination)
        files = downloader.download_all(args.release)
        gh_release = create_release_draft(repository, args.release, pr_data, files)
        upload_assets(gh_release, files)
        img_manager = ImageManager(args.release)
        img_manager.build_images(repository, files)
    except ScriptError as e:
        print(e)
        return 1

    print_final_instructions(args)
    return 0


def finalize(args):
    distclean()
    try:
        check_pypirc()
        repository = Repository(REPO_ROOT, args.repo)
        img_manager = ImageManager(args.release)
        pr_data = repository.find_release_pr(args.release)
        if not pr_data:
            raise ScriptError('No PR found for {}'.format(args.release))
        if not check_pr_mergeable(pr_data):
            raise ScriptError('Can not finalize release with an unmergeable PR')
        if not img_manager.check_images():
            raise ScriptError('Missing release image')
        br_name = branch_name(args.release)
        if not repository.branch_exists(br_name):
            raise ScriptError('No local branch exists for this release.')
        gh_release = repository.find_release(args.release)
        if not gh_release:
            raise ScriptError('No Github release draft for this version')

        repository.checkout_branch(br_name)

        pypandoc.convert_file(
            os.path.join(REPO_ROOT, 'README.md'), 'rst', outputfile=os.path.join(REPO_ROOT, 'README.rst')
        )
        run_setup(os.path.join(REPO_ROOT, 'setup.py'), script_args=['sdist', 'bdist_wheel'])

        merge_status = pr_data.merge()
        if not merge_status.merged and not args.finalize_resume:
            raise ScriptError(
                'Unable to merge PR #{}: {}'.format(pr_data.number, merge_status.message)
            )

        pypi_upload(args)

        img_manager.push_images()
        repository.publish_release(gh_release)
    except ScriptError as e:
        print(e)
        return 1

    return 0


ACTIONS = [
    'start',
    'cancel',
    'resume',
    'finalize',
]

EPILOG = '''Example uses:
    * Start a new feature release (includes all changes currently in master)
        release.sh -b user start 1.23.0
    * Start a new patch release
        release.sh -b user --patch 1.21.0 start 1.21.1
    * Cancel / rollback an existing release draft
        release.sh -b user cancel 1.23.0
    * Restart a previously aborted patch release
        release.sh -b user -p 1.21.0 resume 1.21.1
'''


def main():
    if 'GITHUB_TOKEN' not in os.environ:
        print('GITHUB_TOKEN environment variable must be set')
        return 1

    if 'BINTRAY_TOKEN' not in os.environ:
        print('BINTRAY_TOKEN environment variable must be set')
        return 1

    parser = argparse.ArgumentParser(
        description='Orchestrate a new release of docker/compose. This tool assumes that you have '
                    'obtained a Github API token and Bintray API key and set the GITHUB_TOKEN and '
                    'BINTRAY_TOKEN environment variables accordingly.',
        epilog=EPILOG, formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument(
        'action', choices=ACTIONS, help='The action to be performed for this release'
    )
    parser.add_argument('release', help='Release number, e.g. 1.9.0-rc1, 2.1.1')
    parser.add_argument(
        '--patch', '-p', dest='base',
        help='Which version is being patched by this release'
    )
    parser.add_argument(
        '--repo', '-r', dest='repo', default=NAME,
        help='Start a release for the given repo (default: {})'.format(NAME)
    )
    parser.add_argument(
        '-b', dest='bintray_user', required=True, metavar='USER',
        help='Username associated with the Bintray API key'
    )
    parser.add_argument(
        '--bintray-org', dest='bintray_org', metavar='ORG', default=BINTRAY_ORG,
        help='Organization name on bintray where the data repository will be created.'
    )
    parser.add_argument(
        '--destination', '-o', metavar='DIR', default='binaries',
        help='Directory where release binaries will be downloaded relative to the project root'
    )
    parser.add_argument(
        '--no-cherries', '-C', dest='cherries', action='store_false',
        help='If set, the program will not prompt the user for PR numbers to cherry-pick'
    )
    parser.add_argument(
        '--skip-ci-checks', dest='skip_ci', action='store_true',
        help='If set, the program will not wait for CI jobs to complete'
    )
    parser.add_argument(
        '--finalize-resume', dest='finalize_resume', action='store_true',
        help='If set, finalize will continue through steps that have already been completed.'
    )
    args = parser.parse_args()

    if args.action == 'start':
        return start(args)
    elif args.action == 'resume':
        return resume(args)
    elif args.action == 'cancel':
        return cancel(args)
    elif args.action == 'finalize':
        return finalize(args)

    print('Unexpected action "{}"'.format(args.action), file=sys.stderr)
    return 1


if __name__ == '__main__':
    sys.exit(main())
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import os
import tempfile

import requests
from git import GitCommandError
from git import Repo
from github import Github

from .const import NAME
from .const import REPO_ROOT
from .utils import branch_name
from .utils import read_release_notes_from_changelog
from .utils import ScriptError


class Repository(object):
    def __init__(self, root=None, gh_name=None):
        if root is None:
            root = REPO_ROOT
        if gh_name is None:
            gh_name = NAME
        self.git_repo = Repo(root)
        self.gh_client = Github(os.environ['GITHUB_TOKEN'])
        self.gh_repo = self.gh_client.get_repo(gh_name)

    def create_release_branch(self, version, base=None):
        print('Creating release branch {} based on {}...'.format(version, base or 'master'))
        remote = self.find_remote(self.gh_repo.full_name)
        br_name = branch_name(version)
        remote.fetch()
        if self.branch_exists(br_name):
            raise ScriptError(
                "Branch {} already exists locally. Please remove it before "
                "running the release script, or use `resume` instead.".format(
                    br_name
                )
            )
        if base is not None:
            base = self.git_repo.tag('refs/tags/{}'.format(base))
        else:
            base = 'refs/remotes/{}/master'.format(remote.name)
        release_branch = self.git_repo.create_head(br_name, commit=base)
        release_branch.checkout()
        self.git_repo.git.merge('--strategy=ours', '--no-edit', '{}/release'.format(remote.name))
        with release_branch.config_writer() as cfg:
            cfg.set_value('release', version)
        return release_branch

    def find_remote(self, remote_name=None):
        if not remote_name:
            remote_name = self.gh_repo.full_name
        for remote in self.git_repo.remotes:
            for url in remote.urls:
                if remote_name in url:
                    return remote
        return None

    def create_bump_commit(self, bump_branch, version):
        print('Creating bump commit...')
        bump_branch.checkout()
        self.git_repo.git.commit('-a', '-s', '-m "Bump {}"'.format(version), '--no-verify')

    def diff(self):
        return self.git_repo.git.diff()

    def checkout_branch(self, name):
        return self.git_repo.branches[name].checkout()

    def push_branch_to_remote(self, branch, remote_name=None):
        print('Pushing branch {} to remote...'.format(branch.name))
        remote = self.find_remote(remote_name)
        remote.push(refspec=branch, force=True)

    def branch_exists(self, name):
        return name in [h.name for h in self.git_repo.heads]

    def create_release_pull_request(self, version):
        return self.gh_repo.create_pull(
            title='Bump {}'.format(version),
            body='Automated release for docker-compose {}\n\n{}'.format(
                version, read_release_notes_from_changelog()
            ),
            base='release',
            head=branch_name(version),
        )

    def create_release(self, version, release_notes, **kwargs):
        return self.gh_repo.create_git_release(
            tag=version, name=version, message=release_notes, **kwargs
        )

    def find_release(self, version):
        print('Retrieving release draft for {}'.format(version))
        releases = self.gh_repo.get_releases()
        for release in releases:
            if release.tag_name == version and release.title == version:
                return release
        return None

    def publish_release(self, release):
        release.update_release(
            name=release.title,
            message=release.body,
            draft=False,
            prerelease=release.prerelease
        )

    def remove_release(self, version):
        print('Removing release draft for {}'.format(version))
        releases = self.gh_repo.get_releases()
        for release in releases:
            if release.tag_name == version and release.title == version:
                if not release.draft:
                    print(
                        'The release at {} is no longer a draft. If you TRULY intend '
                        'to remove it, please do so manually.'.format(release.url)
                    )
                    continue
                release.delete_release()

    def remove_bump_branch(self, version, remote_name=None):
        name = branch_name(version)
        if not self.branch_exists(name):
            return False
        print('Removing local branch "{}"'.format(name))
        if self.git_repo.active_branch.name == name:
            print('Active branch is about to be deleted. Checking out to master...')
            try:
                self.checkout_branch('master')
            except GitCommandError:
                raise ScriptError(
                    'Unable to checkout master. Try stashing local changes before proceeding.'
                )
        self.git_repo.branches[name].delete(self.git_repo, name, force=True)
        print('Removing remote branch "{}"'.format(name))
        remote = self.find_remote(remote_name)
        try:
            remote.push(name, delete=True)
        except GitCommandError as e:
            if 'remote ref does not exist' in str(e):
                return False
            raise ScriptError(
                'Error trying to remove remote branch: {}'.format(e)
            )
        return True

    def find_release_pr(self, version):
        print('Retrieving release PR for {}'.format(version))
        name = branch_name(version)
        open_prs = self.gh_repo.get_pulls(state='open')
        for pr in open_prs:
            if pr.head.ref == name:
                print('Found matching PR #{}'.format(pr.number))
                return pr
        print('No open PR for this release branch.')
        return None

    def close_release_pr(self, version):
        print('Retrieving and closing release PR for {}'.format(version))
        name = branch_name(version)
        open_prs = self.gh_repo.get_pulls(state='open')
        count = 0
        for pr in open_prs:
            if pr.head.ref == name:
                print('Found matching PR #{}'.format(pr.number))
                pr.edit(state='closed')
                count += 1
        if count == 0:
            print('No open PR for this release branch.')
        return count

    def write_git_sha(self):
        with open(os.path.join(REPO_ROOT, 'compose', 'GITSHA'), 'w') as f:
            f.write(self.git_repo.head.commit.hexsha[:7])

    def cherry_pick_prs(self, release_branch, ids):
        if not ids:
            return
        release_branch.checkout()
        for i in ids:
            try:
                i = int(i)
            except ValueError as e:
                raise ScriptError('Invalid PR id: {}'.format(e))
            print('Retrieving PR#{}'.format(i))
            pr = self.gh_repo.get_pull(i)
            patch_data = requests.get(pr.patch_url).text
            self.apply_patch(patch_data)

    def apply_patch(self, patch_data):
        with tempfile.NamedTemporaryFile(mode='w', prefix='_compose_cherry', encoding='utf-8') as f:
            f.write(patch_data)
            f.flush()
            self.git_repo.git.am('--3way', f.name)

    def get_prs_in_milestone(self, version):
        milestones = self.gh_repo.get_milestones(state='open')
        milestone = None
        for ms in milestones:
            if ms.title == version:
                milestone = ms
                break
        if not milestone:
            print('Didn\'t find a milestone matching "{}"'.format(version))
            return None

        issues = self.gh_repo.get_issues(milestone=milestone, state='all')
        prs = []
        for issue in issues:
            if issue.pull_request is not None:
                prs.append(issue.number)
        return sorted(prs)


def get_contributors(pr_data):
    commits = pr_data.get_commits()
    authors = {}
    for commit in commits:
        author = commit.author.login
        authors[author] = authors.get(author, 0) + 1
    return [x[0] for x in sorted(list(authors.items()), key=lambda x: x[1])]


def upload_assets(gh_release, files):
    print('Uploading binaries and hash sums')
    for filename, filedata in files.items():
        print('Uploading {}...'.format(filename))
        gh_release.upload_asset(filedata[0], content_type='application/octet-stream')
        gh_release.upload_asset('{}.sha256'.format(filedata[0]), content_type='text/plain')
    print('Uploading run.sh...')
    gh_release.upload_asset(
        os.path.join(REPO_ROOT, 'script', 'run', 'run.sh'), content_type='text/plain'
    )


def delete_assets(gh_release):
    print('Removing previously uploaded assets')
    for asset in gh_release.get_assets():
        print('Deleting asset {}'.format(asset.name))
        asset.delete_asset()
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import os
import re

from .const import REPO_ROOT
from compose import const as compose_const

section_header_re = re.compile(r'^[0-9]+\.[0-9]+\.[0-9]+ \([0-9]{4}-[01][0-9]-[0-3][0-9]\)$')


class ScriptError(Exception):
    pass


def branch_name(version):
    return 'bump-{}'.format(version)


def read_release_notes_from_changelog():
    with open(os.path.join(REPO_ROOT, 'CHANGELOG.md'), 'r') as f:
        lines = f.readlines()
    i = 0
    while i < len(lines):
        if section_header_re.match(lines[i]):
            break
        i += 1

    j = i + 1
    while j < len(lines):
        if section_header_re.match(lines[j]):
            break
        j += 1

    return ''.join(lines[i + 2:j - 1])


def update_init_py_version(version):
    path = os.path.join(REPO_ROOT, 'compose', '__init__.py')
    with open(path, 'r') as f:
        contents = f.read()
    contents = re.sub(r"__version__ = '[0-9a-z.-]+'", "__version__ = '{}'".format(version), contents)
    with open(path, 'w') as f:
        f.write(contents)


def update_run_sh_version(version):
    path = os.path.join(REPO_ROOT, 'script', 'run', 'run.sh')
    with open(path, 'r') as f:
        contents = f.read()
    contents = re.sub(r'VERSION="[0-9a-z.-]+"', 'VERSION="{}"'.format(version), contents)
    with open(path, 'w') as f:
        f.write(contents)


def compatibility_matrix():
    result = {}
    for engine_version in compose_const.API_VERSION_TO_ENGINE_VERSION.values():
        result[engine_version] = []
    for fmt, api_version in compose_const.API_VERSIONS.items():
        result[compose_const.API_VERSION_TO_ENGINE_VERSION[api_version]].append(fmt.vstring)
    return result


def yesno(prompt, default=None):
    """
    Prompt the user for a yes or no.

    Can optionally specify a default value, which will only be
    used if they enter a blank line.

    Unrecognised input (anything other than "y", "n", "yes",
    "no" or "") will return None.
    """
    answer = input(prompt).strip().lower()

    if answer == "y" or answer == "yes":
        return True
    elif answer == "n" or answer == "no":
        return False
    elif answer == "":
        return default
    else:
        return None
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import base64
import json
import os
import shutil

import docker

from .const import REPO_ROOT
from .utils import ScriptError


class ImageManager(object):
    def __init__(self, version):
        self.docker_client = docker.APIClient(**docker.utils.kwargs_from_env())
        self.version = version
        if 'HUB_CREDENTIALS' in os.environ:
            print('HUB_CREDENTIALS found in environment, issuing login')
            credentials = json.loads(base64.urlsafe_b64decode(os.environ['HUB_CREDENTIALS']))
            self.docker_client.login(
                username=credentials['Username'], password=credentials['Password']
            )

    def build_images(self, repository, files):
        print("Building release images...")
        repository.write_git_sha()
        distdir = os.path.join(REPO_ROOT, 'dist')
        os.makedirs(distdir, exist_ok=True)
        shutil.copy(files['docker-compose-Linux-x86_64'][0], distdir)
        os.chmod(os.path.join(distdir, 'docker-compose-Linux-x86_64'), 0o755)
        print('Building docker/compose image')
        logstream = self.docker_client.build(
            REPO_ROOT, tag='docker/compose:{}'.format(self.version), dockerfile='Dockerfile.run',
            decode=True
        )
        for chunk in logstream:
            if 'error' in chunk:
                raise ScriptError('Build error: {}'.format(chunk['error']))
            if 'stream' in chunk:
                print(chunk['stream'], end='')

        print('Building test image (for UCP e2e)')
        logstream = self.docker_client.build(
            REPO_ROOT, tag='docker-compose-tests:tmp', decode=True
        )
        for chunk in logstream:
            if 'error' in chunk:
                raise ScriptError('Build error: {}'.format(chunk['error']))
            if 'stream' in chunk:
                print(chunk['stream'], end='')

        container = self.docker_client.create_container(
            'docker-compose-tests:tmp', entrypoint='tox'
        )
        self.docker_client.commit(container, 'docker/compose-tests', 'latest')
        self.docker_client.tag(
            'docker/compose-tests:latest', 'docker/compose-tests:{}'.format(self.version)
        )
        self.docker_client.remove_container(container, force=True)
        self.docker_client.remove_image('docker-compose-tests:tmp', force=True)

    @property
    def image_names(self):
        return [
            'docker/compose-tests:latest',
            'docker/compose-tests:{}'.format(self.version),
            'docker/compose:{}'.format(self.version)
        ]

    def check_images(self):
        for name in self.image_names:
            try:
                self.docker_client.inspect_image(name)
            except docker.errors.ImageNotFound:
                print('Expected image {} was not found'.format(name))
                return False
        return True

    def push_images(self):
        for name in self.image_names:
            print('Pushing {} to Docker Hub'.format(name))
            logstream = self.docker_client.push(name, stream=True, decode=True)
            for chunk in logstream:
                if 'status' in chunk:
                    print(chunk['status'])
                if 'error' in chunk:
                    raise ScriptError(
                        'Error pushing {name}: {err}'.format(name=name, err=chunk['error'])
                    )
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import os


REPO_ROOT = os.path.join(os.path.dirname(__file__), '..', '..', '..')
NAME = 'docker/compose'
BINTRAY_ORG = 'docker-compose'
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

from configparser import Error
from requests.exceptions import HTTPError
from twine.commands.upload import main as twine_upload
from twine.utils import get_config

from .utils import ScriptError


def pypi_upload(args):
    print('Uploading to PyPi')
    try:
        rel = args.release.replace('-rc', 'rc')
        twine_upload([
            'dist/docker_compose-{}*.whl'.format(rel),
            'dist/docker-compose-{}*.tar.gz'.format(rel)
        ])
    except HTTPError as e:
        if e.response.status_code == 400 and 'File already exists' in e.message:
            if not args.finalize_resume:
                raise ScriptError(
                    'Package already uploaded on PyPi.'
                )
            print('Skipping PyPi upload - package already uploaded')
        else:
            raise ScriptError('Unexpected HTTP error uploading package to PyPi: {}'.format(e))


def check_pypirc():
    try:
        config = get_config()
    except Error as e:
        raise ScriptError('Failed to parse .pypirc file: {}'.format(e))

    if config is None:
        raise ScriptError('Failed to parse .pypirc file')

    if 'pypi' not in config:
        raise ScriptError('Missing [pypi] section in .pypirc file')

    if not (config['pypi'].get('username') and config['pypi'].get('password')):
        raise ScriptError('Missing login/password pair for pypi repo')
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import hashlib
import os

import requests

from .const import BINTRAY_ORG
from .const import NAME
from .const import REPO_ROOT
from .utils import branch_name


class BinaryDownloader(requests.Session):
    base_bintray_url = 'https://dl.bintray.com/{}'.format(BINTRAY_ORG)
    base_appveyor_url = 'https://ci.appveyor.com/api/projects/{}/artifacts/'.format(NAME)

    def __init__(self, destination, *args, **kwargs):
        super(BinaryDownloader, self).__init__(*args, **kwargs)
        self.destination = destination
        os.makedirs(self.destination, exist_ok=True)

    def download_from_bintray(self, repo_name, filename):
        print('Downloading {} from bintray'.format(filename))
        url = '{base}/{repo_name}/{filename}'.format(
            base=self.base_bintray_url, repo_name=repo_name, filename=filename
        )
        full_dest = os.path.join(REPO_ROOT, self.destination, filename)
        return self._download(url, full_dest)

    def download_from_appveyor(self, branch_name, filename):
        print('Downloading {} from appveyor'.format(filename))
        url = '{base}/dist%2F{filename}?branch={branch_name}'.format(
            base=self.base_appveyor_url, filename=filename, branch_name=branch_name
        )
        full_dest = os.path.join(REPO_ROOT, self.destination, filename)
        return self._download(url, full_dest)

    def _download(self, url, full_dest):
        m = hashlib.sha256()
        with open(full_dest, 'wb') as f:
            r = self.get(url, stream=True)
            for chunk in r.iter_content(chunk_size=1024 * 600, decode_unicode=False):
                print('.', end='', flush=True)
                m.update(chunk)
                f.write(chunk)

        print(' download complete')
        hex_digest = m.hexdigest()
        with open(full_dest + '.sha256', 'w') as f:
            f.write('{}  {}\n'.format(hex_digest, os.path.basename(full_dest)))
        return full_dest, hex_digest

    def download_all(self, version):
        files = {
            'docker-compose-Darwin-x86_64': None,
            'docker-compose-Linux-x86_64': None,
            'docker-compose-Windows-x86_64.exe': None,
        }

        for filename in files.keys():
            if 'Windows' in filename:
                files[filename] = self.download_from_appveyor(
                    branch_name(version), filename
                )
            else:
                files[filename] = self.download_from_bintray(
                    branch_name(version), filename
                )
        return files
<EOF>
<BOF>
from __future__ import absolute_import
from __future__ import unicode_literals

import json

import requests

from .const import NAME


class BintrayAPI(requests.Session):
    def __init__(self, api_key, user, *args, **kwargs):
        super(BintrayAPI, self).__init__(*args, **kwargs)
        self.auth = (user, api_key)
        self.base_url = 'https://api.bintray.com/'

    def create_repository(self, subject, repo_name, repo_type='generic'):
        url = '{base}repos/{subject}/{repo_name}'.format(
            base=self.base_url, subject=subject, repo_name=repo_name,
        )
        data = {
            'name': repo_name,
            'type': repo_type,
            'private': False,
            'desc': 'Automated release for {}: {}'.format(NAME, repo_name),
            'labels': ['docker-compose', 'docker', 'release-bot'],
        }
        return self.post_json(url, data)

    def repository_exists(self, subject, repo_name):
        url = '{base}/repos/{subject}/{repo_name}'.format(
            base=self.base_url, subject=subject, repo_name=repo_name,
        )
        result = self.get(url)
        if result.status_code == 404:
            return False
        result.raise_for_status()
        return True

    def delete_repository(self, subject, repo_name):
        url = '{base}repos/{subject}/{repo_name}'.format(
            base=self.base_url, subject=subject, repo_name=repo_name,
        )
        return self.delete(url)

    def post_json(self, url, data, **kwargs):
        if 'headers' not in kwargs:
            kwargs['headers'] = {}
        kwargs['headers']['Content-Type'] = 'application/json'
        return self.post(url, data=json.dumps(data), **kwargs)
<EOF>
