<BOF>
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
This is a tiny Flask app used for a couple of self-serve ad tracking
mechanisms. The URLs it provides are:

/click

    Promoted links have their URL replaced with a /click URL by the JS
    (after a call to /fetch-trackers). Redirect to the actual URL after logging
    the click. This must be run in a place whose logs are stored for traffic
    analysis.

For convenience, the script can compile itself into a Zip archive suitable for
use on Amazon Elastic Beanstalk (and possibly other systems).

"""


import cStringIO
import os
import hashlib
import hmac
import time
import urllib
from urlparse import parse_qsl, urlparse, urlunparse

from ConfigParser import RawConfigParser
from wsgiref.handlers import format_date_time

from flask import Flask, request, json, make_response, abort, redirect


application = Flask(__name__)
REQUIRED_PACKAGES = [
    "flask",
]


class ApplicationConfig(object):
    """A thin wrapper around ConfigParser that remembers what we read.

    The remembered settings can then be written out to a minimal config file
    when building the Elastic Beanstalk zipfile.

    """
    def __init__(self):
        self.input = RawConfigParser()
        config_filename = os.environ.get("CONFIG", "production.ini")
        with open(config_filename) as f:
            self.input.readfp(f)
        self.output = RawConfigParser()

    def get(self, section, key):
        value = self.input.get(section, key)

        # remember that we needed this configuration value
        if (section.upper() != "DEFAULT" and
            not self.output.has_section(section)):
            self.output.add_section(section)
        self.output.set(section, key, value)

        return value

    def to_config(self):
        io = cStringIO.StringIO()
        self.output.write(io)
        return io.getvalue()


config = ApplicationConfig()
tracking_secret = config.get('DEFAULT', 'tracking_secret')
reddit_domain = config.get('DEFAULT', 'domain')
reddit_domain_prefix = config.get('DEFAULT', 'domain_prefix')


@application.route("/")
def healthcheck():
    return "I am healthy."


@application.route('/click')
def click_redirect():
    destination = request.args['url'].encode('utf-8')
    fullname = request.args['id'].encode('utf-8')
    observed_mac = request.args['hash']

    expected_hashable = ''.join((destination, fullname))
    expected_mac = hmac.new(
            tracking_secret, expected_hashable, hashlib.sha1).hexdigest()

    if not constant_time_compare(expected_mac, observed_mac):
        abort(403)

    # fix encoding in the query string of the destination
    u = urlparse(destination)
    if u.query:
        u = _fix_query_encoding(u)
        destination = u.geturl()

    return _redirect_nocache(destination)


@application.route('/event_redirect')
def event_redirect():
    destination = request.args['url'].encode('utf-8')

    # Parse and avoid open redirects
    netloc = "%s.%s" % (reddit_domain_prefix, reddit_domain)
    u = urlparse(destination)._replace(netloc=netloc, scheme="https")

    if u.query:
        u = _fix_query_encoding(u)
        destination = u.geturl()

    return _redirect_nocache(destination)


@application.route('/event_click')
def event_click():
    """Take in an evented request, append session data to payload, and redirect.

    This is only useful for situations in which we're navigating from a request
    that does not have session information - i.e. served from redditmedia.com.
    If we want to track a click and the user that did so from these pages,
    we need to identify the user before sending the payload.

    Note: If we add hmac validation, this will need verify and resign before
    redirecting. We can also probably drop a redirect here once we're not
    relying on log files for event tracking and have a proper events endpoint.
    """
    try:
        session_str = urllib.unquote(request.cookies.get('reddit_session', ''))
        user_id = int(session_str.split(',')[0])
    except ValueError:
        user_id = None

    args = request.args.to_dict()
    if user_id:
        payload = args.get('data').encode('utf-8')
        try:
            payload_json = json.loads(payload)
        except ValueError:
            # if we fail to load the JSON, continue on to the redirect to not
            # block the user - ETL can deal with/report the malformed data.
            pass
        else:
            payload_json['user_id'] = user_id
            args['data'] = json.dumps(payload_json)

    return _redirect_nocache('/event_redirect?%s' % urllib.urlencode(args))


def _fix_query_encoding(parse_result):
    "Fix encoding in the query string."
    query_params = parse_qsl(parse_result.query, keep_blank_values=True)

    # this effectively calls urllib.quote_plus on every query value
    return parse_result._replace(query=urllib.urlencode(query_params))


def _redirect_nocache(destination):
    now = format_date_time(time.time())
    response = redirect(destination)
    response.headers['Cache-control'] = 'no-cache'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Date'] = now
    response.headers['Expires'] = now
    return response


# copied from r2.lib.utils
def constant_time_compare(actual, expected):
    """
    Returns True if the two strings are equal, False otherwise

    The time taken is dependent on the number of characters provided
    instead of the number of characters that match.
    """
    actual_len   = len(actual)
    expected_len = len(expected)
    result = actual_len ^ expected_len
    if expected_len > 0:
        for i in xrange(actual_len):
            result |= ord(actual[i]) ^ ord(expected[i % expected_len])
    return result == 0


if __name__ == "__main__":
    # package up for elastic beanstalk
    import zipfile

    with zipfile.ZipFile("/tmp/tracker.zip", "w", zipfile.ZIP_DEFLATED) as zip:
        zip.write(__file__, "application.py")
        zip.writestr("production.ini", config.to_config())
        zip.writestr("requirements.txt", "\n".join(REQUIRED_PACKAGES) + "\n")
<EOF>
<BOF>
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
############################################################################### 
"""
This is a tiny Flask app used for geoip lookups against a maxmind database.

If you are using this service be sure to set `geoip_location` in your ini file.

"""

import json

import GeoIP
from flask import Flask, make_response

application = Flask(__name__)

# SET THESE PATHS TO YOUR MAXMIND GEOIP LEGACY DATABASES
# http://dev.maxmind.com/geoip/legacy/geolite/
COUNTRY_DB_PATH = '/usr/share/GeoIP/GeoIP.dat'
CITY_DB_PATH = '/var/lib/GeoIP/GeoIPCity.dat'
ORG_DB_PATH = '/var/lib/GeoIP/GeoIPOrg.dat'


try:
    gc = GeoIP.open(COUNTRY_DB_PATH, GeoIP.GEOIP_MEMORY_CACHE)
except:
    gc = None

try:
    gi = GeoIP.open(CITY_DB_PATH, GeoIP.GEOIP_MEMORY_CACHE)
except:
    gi = None

try:
    go = GeoIP.open(ORG_DB_PATH, GeoIP.GEOIP_MEMORY_CACHE)
except:
    go = None


def json_response(result):
    json_output = json.dumps(result, ensure_ascii=False, encoding='iso-8859-1')
    response = make_response(json_output.encode('utf-8'), 200)
    response.headers['Content-Type'] = 'application/json; charset=utf-8'
    return response


@application.route('/geoip/<ips>')
def get_record(ips):
    if gi:
        result = {ip: gi.record_by_addr(ip) for ip in ips.split('+')}
    elif gc:
        result = {
            ip : {
                'country_code': gc.country_code_by_addr(ip),
                'country_name': gc.country_name_by_addr(ip),
            } for ip in ips.split('+')
        }
    else:
        result = {}

    return json_response(result)


@application.route('/org/<ips>')
def get_organizations(ips):
    if go:
        return json_response({ip: go.org_by_addr(ip) for ip in ips.split('+')})
    else:
        return json_response({})


if __name__ == "__main__":
    application.run()
<EOF>
<BOF>
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


import os
import argparse
import mimetypes
import urlparse

import boto


NEVER = 'Thu, 31 Dec 2037 23:59:59 GMT'


def upload(static_root, bucket_url):
    s3 = boto.connect_s3()
    bucket = s3.get_bucket(bucket_url.netloc, validate=False)

    # build a list of files already in the bucket
    print "checking existing files on s3..."
    remote_files = {key.name : key.etag.strip('"') for key in bucket.list()}

    # upload local files not already in the bucket
    for root, dirs, files in os.walk(static_root):
        for file in files:
            absolute_path = os.path.join(root, file)
            relative_path = os.path.relpath(absolute_path, start=static_root)
            key_name = os.path.join(bucket_url.path, relative_path).lstrip("/")

            type, encoding = mimetypes.guess_type(file)
            if not type:
                continue
            headers = {}
            headers['Expires'] = NEVER
            headers['Content-Type'] = type
            if encoding:
                headers['Content-Encoding'] = encoding

            key = bucket.new_key(key_name)
            with open(absolute_path, 'rb') as f:
                etag, base64_tag = key.compute_md5(f)

                # don't upload the file if it already exists unmodified in the bucket
                if remote_files.get(key_name, None) == etag:
                    continue

                print "uploading", key_name, "to S3..."
                key.set_contents_from_file(
                    f,
                    headers=headers,
                    policy='public-read',
                    md5=(etag, base64_tag),
                )

    print "all done"


def s3_url(text):
    parsed = urlparse.urlparse(text)
    if parsed.scheme != "s3":
        raise ValueError("not an s3 url")
    if parsed.params or parsed.query or parsed.fragment:
        raise ValueError("params, query, and fragment not supported")
    return parsed


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("root")
    parser.add_argument("bucket", type=s3_url)
    args = parser.parse_args()
    upload(args.root, args.bucket)


if __name__ == "__main__":
    main()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from __future__ import division

import collections
import HTMLParser
import itertools
import random
import string
import time

import requests

from pylons import app_globals as g

from r2.lib.db import queries
from r2.lib import amqp
from r2.lib.utils import weighted_lottery, get_requests_resp_json
from r2.lib.voting import cast_vote
from r2.models import (
    Account,
    Comment,
    Link,
    LocalizedDefaultSubreddits,
    LocalizedFeaturedSubreddits,
    NotFound,
    register,
    Subreddit,
    Vote,
)


unescape_htmlentities = HTMLParser.HTMLParser().unescape


class TextGenerator(object):
    """A Markov Chain based text mimicker."""

    def __init__(self, order=8):
        self.order = order
        self.starts = collections.Counter()
        self.start_lengths = collections.defaultdict(collections.Counter)
        self.models = [
            collections.defaultdict(collections.Counter)
            for i in xrange(self.order)]

    @staticmethod
    def _in_groups(input_iterable, n):
        iterables = itertools.tee(input_iterable, n)
        for offset, iterable in enumerate(iterables):
            for _ in xrange(offset):
                next(iterable, None)
        return itertools.izip(*iterables)

    def add_sample(self, sample):
        """Add a sample to the model of text for this generator."""

        if len(sample) <= self.order:
            return

        start = sample[:self.order]
        self.starts[start] += 1
        self.start_lengths[start][len(sample)] += 1
        for order, model in enumerate(self.models, 1):
            for chars in self._in_groups(sample, order+1):
                prefix = "".join(chars[:-1])
                next_char = chars[-1]
                model[prefix][next_char] += 1

    def generate(self):
        """Generate a string similar to samples previously fed in."""

        start = weighted_lottery(self.starts)
        desired_length = weighted_lottery(self.start_lengths[start])
        desired_length = max(desired_length, self.order)

        generated = []
        generated.extend(start)
        while len(generated) < desired_length:
            # try each model, from highest order down, til we find
            # something
            for order, model in reversed(list(enumerate(self.models, 1))):
                current_prefix = "".join(generated[-order:])
                frequencies = model[current_prefix]
                if frequencies:
                    generated.append(weighted_lottery(frequencies))
                    break
            else:
                generated.append(random.choice(string.lowercase))

        return "".join(generated)


def fetch_listing(path, limit=1000, batch_size=100):
    """Fetch a reddit listing from reddit.com."""

    session = requests.Session()
    session.headers.update({
        "User-Agent": "reddit-test-data-generator/1.0",
    })

    base_url = "https://api.reddit.com" + path

    after = None
    count = 0
    while count < limit:
        params = {"limit": batch_size, "count": count}
        if after:
            params["after"] = after

        print "> {}-{}".format(count, count+batch_size)
        response = session.get(base_url, params=params)
        response.raise_for_status()

        listing = get_requests_resp_json(response)["data"]
        for child in listing["children"]:
            yield child["data"]
            count += 1

        after = listing["after"]
        if not after:
            break

        # obey reddit.com's ratelimits
        # see: https://github.com/reddit/reddit/wiki/API#rules
        time.sleep(2)


class Modeler(object):
    def __init__(self):
        self.usernames = TextGenerator(order=2)

    def model_subreddit(self, subreddit_name):
        """Return a model of links and comments in a given subreddit."""

        subreddit_path = "/r/{}".format(subreddit_name)
        print ">>>", subreddit_path

        print ">> Links"
        titles = TextGenerator(order=5)
        selfposts = TextGenerator(order=8)
        link_count = self_count = 0
        urls = set()
        for link in fetch_listing(subreddit_path, limit=500):
            self.usernames.add_sample(link["author"])
            titles.add_sample(unescape_htmlentities(link["title"]))
            if link["is_self"]:
                self_count += 1
                selfposts.add_sample(unescape_htmlentities(link["selftext"]))
            else:
                urls.add(link["url"])
            link_count += 1
        self_frequency = self_count / link_count

        print ">> Comments"
        comments = TextGenerator(order=8)
        for comment in fetch_listing(subreddit_path + "/comments"):
            self.usernames.add_sample(comment["author"])
            comments.add_sample(unescape_htmlentities(comment["body"]))

        return SubredditModel(
            subreddit_name, titles, selfposts, urls, comments, self_frequency)

    def generate_username(self):
        """Generate and return a username like those seen on reddit.com."""
        return self.usernames.generate()


class SubredditModel(object):
    """A snapshot of a subreddit's links and comments."""

    def __init__(self, name, titles, selfposts, urls, comments, self_frequency):
        self.name = name
        self.titles = titles
        self.selfposts = selfposts
        self.urls = list(urls)
        self.comments = comments
        self.selfpost_frequency = self_frequency

    def generate_link_title(self):
        """Generate and return a title like those seen in the subreddit."""
        return self.titles.generate()

    def generate_link_url(self):
        """Generate and return a URL from one seen in the subreddit.

        The URL returned may be "self" indicating a self post. This should
        happen with the same frequency it is seen in the modeled subreddit.

        """
        if random.random() < self.selfpost_frequency:
            return "self"
        else:
            return random.choice(self.urls)

    def generate_selfpost_body(self):
        """Generate and return a self-post body like seen in the subreddit."""
        return self.selfposts.generate()

    def generate_comment_body(self):
        """Generate and return a comment body like seen in the subreddit."""
        return self.comments.generate()


def fuzz_number(number):
    return int(random.betavariate(2, 8) * 5 * number)


def ensure_account(name):
    """Look up or register an account and return it."""
    try:
        account = Account._by_name(name)
        print ">> found /u/{}".format(name)
        return account
    except NotFound:
        print ">> registering /u/{}".format(name)
        return register(name, "password", "127.0.0.1")


def ensure_subreddit(name, author):
    """Look up or create a subreddit and return it."""
    try:
        sr = Subreddit._by_name(name)
        print ">> found /r/{}".format(name)
        return sr
    except NotFound:
        print ">> creating /r/{}".format(name)
        sr = Subreddit._new(
            name=name,
            title="/r/{}".format(name),
            author_id=author._id,
            lang="en",
            ip="127.0.0.1",
        )
        sr._commit()
        return sr


def inject_test_data(num_links=25, num_comments=25, num_votes=5):
    """Flood your reddit install with test data based on reddit.com."""

    print ">>>> Ensuring configured objects exist"
    system_user = ensure_account(g.system_user)
    ensure_account(g.automoderator_account)
    ensure_subreddit(g.default_sr, system_user)
    ensure_subreddit(g.takedown_sr, system_user)
    ensure_subreddit(g.beta_sr, system_user)
    ensure_subreddit(g.promo_sr_name, system_user)

    print
    print

    print ">>>> Fetching real data from reddit.com"
    modeler = Modeler()
    subreddits = [
        modeler.model_subreddit("pics"),
        modeler.model_subreddit("videos"),
        modeler.model_subreddit("askhistorians"),
    ]
    extra_settings = {
        "pics": {
            "show_media": True,
        },
        "videos": {
            "show_media": True,
        },
    }

    print
    print

    print ">>>> Generating test data"
    print ">>> Accounts"
    account_query = Account._query(sort="_date", limit=500, data=True)
    accounts = [a for a in account_query if a.name != g.system_user]
    accounts.extend(
        ensure_account(modeler.generate_username())
        for i in xrange(50 - len(accounts)))

    print ">>> Content"
    things = []
    for sr_model in subreddits:
        sr_author = random.choice(accounts)
        sr = ensure_subreddit(sr_model.name, sr_author)

        # make the system user subscribed for easier testing
        if sr.add_subscriber(system_user):
            sr._incr("_ups", 1)

        # apply any custom config we need for this sr
        for setting, value in extra_settings.get(sr.name, {}).iteritems():
            setattr(sr, setting, value)
        sr._commit()

        for i in xrange(num_links):
            link_author = random.choice(accounts)
            url = sr_model.generate_link_url()
            is_self = (url == "self")
            content = sr_model.generate_selfpost_body() if is_self else url
            link = Link._submit(
                is_self=is_self,
                title=sr_model.generate_link_title(),
                content=content,
                author=link_author,
                sr=sr,
                ip="127.0.0.1",
            )
            queries.new_link(link)
            things.append(link)

            comments = [None]
            for i in xrange(fuzz_number(num_comments)):
                comment_author = random.choice(accounts)
                comment, inbox_rel = Comment._new(
                    comment_author,
                    link,
                    parent=random.choice(comments),
                    body=sr_model.generate_comment_body(),
                    ip="127.0.0.1",
                )
                queries.new_comment(comment, inbox_rel)
                comments.append(comment)
                things.append(comment)

    for thing in things:
        for i in xrange(fuzz_number(num_votes)):
            direction = random.choice([
                Vote.DIRECTIONS.up,
                Vote.DIRECTIONS.unvote,
                Vote.DIRECTIONS.down,
            ])
            voter = random.choice(accounts)

            cast_vote(voter, thing, direction)

    amqp.worker.join()

    srs = [Subreddit._by_name(n) for n in ("pics", "videos", "askhistorians")]
    LocalizedDefaultSubreddits.set_global_srs(srs)
    LocalizedFeaturedSubreddits.set_global_srs([Subreddit._by_name('pics')])
<EOF>
<BOF>
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Tools for evaluating promoted link distribution."""

from collections import defaultdict
import datetime
from math import sqrt

from pylons import app_globals as g
from sqlalchemy.sql.functions import sum as sa_sum

from r2.lib import promote
from r2.lib.db.operators import and_, or_
from r2.lib.utils import to36, weighted_lottery
from r2.models.traffic import (
    Session,
    TargetedImpressionsByCodename,
    PageviewsBySubredditAndPath,
)
from r2.models.bidding import PromotionWeights
from r2.models import (
    Link,
    PromoCampaign,
    DefaultSR,
)

LINK_PREFIX = Link._type_prefix + str(Link._type_id)
PC_PREFIX = PromoCampaign._type_prefix + str(PromoCampaign._type_id)


def error_statistics(errors):
    mean_error = sum(errors) / len(errors)
    min_error = min([abs(i) for i in errors])
    max_error = max([abs(i) for i in errors])
    stdev_error = sqrt(
        (sum([i ** 2 for i in errors]) / len(errors))
        - mean_error ** 2)
    return (mean_error, min_error, max_error, stdev_error)


def get_scheduled(date, sr_name=''):
    campaign_ids = PromotionWeights.get_campaign_ids(date, sr_names=[sr_name])
    campaigns = PromoCampaign._byID(campaign_ids, return_dict=False, data=True)
    links = Link._by_fullname({camp.link_id for camp in campaigns},
                              return_dict=False, data=True)
    links = {l._id: l for l in links}
    kept = []
    for camp in campaigns:
        if camp.trans_id == 0:
            continue

        link = links[camp.link_id]
        if link._spam or not promote.is_accepted(link):
            continue

        kept.append(camp._id)

    return [(camp._fullname, camp.link_id, camp.total_budget_dollars)
        for camp in kept]


def get_campaign_pageviews(date, sr_name=''):
    # ads go live at hour=5
    start = datetime.datetime(date.year, date.month, date.day, 5, 0)
    hours = [start + datetime.timedelta(hours=i) for i in xrange(24)]

    traffic_cls = TargetedImpressionsByCodename
    codename_string = PC_PREFIX + '_%'
    q = (Session.query(traffic_cls.codename,
                       sa_sum(traffic_cls.pageview_count).label('daily'))
            .filter(traffic_cls.subreddit == sr_name)
            .filter(traffic_cls.codename.like(codename_string))
            .filter(traffic_cls.interval == 'hour')
            .filter(traffic_cls.date.in_(hours))
            .group_by(traffic_cls.codename))

    pageviews = dict(q)
    return pageviews


def filter_campaigns(date, fullnames):
    campaigns = PromoCampaign._by_fullname(fullnames, data=True,
                                           return_dict=False)

    # filter out campaigns that shouldn't be live
    pc_date = datetime.datetime(date.year, date.month, date.day, 0, 0,
                                tzinfo=g.tz)

    campaigns = [camp for camp in campaigns
                 if camp.start_date <= pc_date <= camp.end_date]

    # check for links with targeted campaigns - we can't handle them now
    has_targeted = [camp.link_id for camp in campaigns if camp.sr_name != '']
    return [camp for camp in campaigns if camp.link_id not in has_targeted]


def get_frontpage_pageviews(date):
    sr_name = DefaultSR.name
    traffic_cls = PageviewsBySubredditAndPath
    q = (Session.query(traffic_cls.srpath, traffic_cls.pageview_count)
           .filter(traffic_cls.interval == 'day')
           .filter(traffic_cls.date == date)
           .filter(traffic_cls.srpath == '%s-GET_listing' % sr_name))
    r = list(q)
    return r[0][1]


def compare_pageviews(daysago=0, verbose=False):
    """Evaluate past delivery for promoted links.

    Check frontpage promoted links for their actual delivery compared to what
    would be expected based on their bids.

    """

    date = (datetime.datetime.now(g.tz) -
            datetime.timedelta(days=daysago)).date()

    scheduled = get_scheduled(date)
    pageviews_by_camp = get_campaign_pageviews(date)
    campaigns = filter_campaigns(date, pageviews_by_camp.keys())
    actual = []
    for camp in campaigns:
        link_fullname = '%s_%s' % (LINK_PREFIX, to36(camp.link_id))
        i = (camp._fullname, link_fullname, pageviews_by_camp[camp._fullname])
        actual.append(i)

    scheduled_links = {link for camp, link, pageviews in scheduled}
    actual_links = {link for camp, link, pageviews in actual}

    bid_by_link = defaultdict(int)
    total_bid = 0

    pageviews_by_link = defaultdict(int)
    total_pageviews = 0

    for camp, link, bid in scheduled:
        if link not in actual_links:
            if verbose:
                print '%s not found in actual, skipping' % link
            continue

        bid_by_link[link] += bid
        total_bid += bid

    for camp, link, pageviews in actual:
        # not ideal: links shouldn't be here
        if link not in scheduled_links:
            if verbose:
                print '%s not found in schedule, skipping' % link
            continue

        pageviews_by_link[link] += pageviews
        total_pageviews += pageviews

    errors = []
    for link, bid in sorted(bid_by_link.items(), key=lambda t: t[1]):
        pageviews = pageviews_by_link.get(link, 0)
        expected = bid / total_bid
        realized = float(pageviews) / total_pageviews
        difference = (realized - expected) / expected
        errors.append(difference)
        if verbose:
            print '%s - %s - %s - %s' % (link, expected, realized, difference)

    mean_error, min_error, max_error, stdev_error = error_statistics(errors)

    print '%s' % date
    print ('error %s max, %s min, %s +- %s' %
           (max_error, min_error, mean_error, stdev_error))
    print 'total bid %s' % total_bid
    print ('pageviews for promoted links targeted only to frontpage %s' %
           total_pageviews)
    print ('frontpage pageviews for all promoted links %s' %
           sum(pageviews_by_camp.values()))
    print 'promoted eligible pageviews %s' % get_frontpage_pageviews(date)


PROMOS = [('promo_%s' % i, i + 1) for i in xrange(100)]


def select_subset(n, weighted=False):
    promos = copy(PROMOS)
    selected = []

    if weighted:
        d = {(name, weight): weight for name, weight in promos}
        while len(selected) < n and d:
            i = weighted_lottery(d)
            del d[i]
            selected.append(i)
    else:
        # Sample without replacement
        if n > len(promos):
            return promos
        else:
            return random.sample(promos, n)
    return selected


def pick(subset, weighted=False):
    if weighted:
        d = {(name, weight): weight for name, weight in subset}
        picked = weighted_lottery(d)
    else:
        picked = random.choice(subset)
    return picked


def benchmark(subsets=1440, picks=6945, weighted_subset=False,
              weighted_pick=True, subset_size=10, verbose=False):
    """Test 2 stage randomization.

    First stage picks a subset of promoted links, second stage picks a single
    promoted link. This is to simulate the server side subset plus client side
    randomization of promoted link display.

    """

    counts = {(name, weight): 0 for name, weight in PROMOS}

    for i in xrange(subsets):
        subset = select_subset(subset_size, weighted=weighted_subset)

        for j in xrange(picks):
            name, weight = pick(subset, weighted=weighted_pick)
            counts[(name, weight)] += 1

    total_weight = sum(counts.values())
    errors = []
    for name, weight in sorted(counts.keys(), key=lambda t: t[1]):
        count = counts[(name, weight)]
        actual = float(count) / (subsets * picks)
        expected = float(weight) / total_weight
        error = (actual - expected) / expected
        errors.append(error)
        if verbose:
            print ('%s - expected: %s - actual: %s - error %s' %
                   (name, expected, actual, error))

    mean_error, min_error, max_error, stdev_error = error_statistics(errors)

    if verbose:
        print ('Error %s max, %s min, %s +- %s' %
               (max_error, min_error, mean_error, stdev_error))

    return (max_error, min_error, mean_error, stdev_error)
<EOF>
<BOF>
#!/usr/bin/env python2.7

from Queue import Queue
import argparse
import logging
import multiprocessing
import os
import re
import string
import subprocess
import sys
import threading


def parse_size(s):
    def mult(multiplier):
        return int(s[:-1])*multiplier

    if all(x in string.digits for x in s):
        return int(s)
    if s.endswith('b'):
        return mult(1)
    if s.endswith('k'):
        return mult(1024)
    if s.endswith('m'):
        return mult(1024*1024)
    if s.endswith('g'):
        return mult(1024*1024*1024)
    raise Exception("Can't parse %r" % (s,))


class JobInputter(threading.Thread):
    """
    Takes input originally from stdin through iq and sends it to the job
    """
    def __init__(self, job_name, popen, iq):
        self.job_name = job_name
        self.popen = popen
        self.iq = iq
        super(JobInputter, self).__init__()

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, self.job_name)

    def run(self):
        while True:
            item = self.iq.get()
            logging.debug("%r got item %r", self, item)
            if item is None:
                logging.debug("%r closing %r", self, self.popen.stdin)
                self.popen.stdin.close()
                self.iq.task_done()
                break

            try:
                self.popen.stdin.write(item)
                self.popen.stdin.flush()
                self.iq.task_done()
            except IOError:
                logging.exception("exception writing to popen %r", self.popen)
                return os._exit(1)


class JobOutputter(threading.Thread):
    """
    Takes output from the job and sends it to stdout
    """
    def __init__(self, job_name, popen, out_fd, lock):
        self.job_name = job_name
        self.popen = popen
        self.out_fd = out_fd
        self.lock = lock
        super(JobOutputter, self).__init__()

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, self.job_name)

    def run(self):
        for line in self.popen.stdout:
            logging.debug("%r read %d bytes", self, len(line))
            with self.lock:
                try:
                    self.out_fd.write(line)
                except IOError as e:
                    if e.errno != errno.EPIPE:
                        logging.exception("exception writing to output %r", self.out_fd)
                    return os._exit(1)

            logging.debug("Got eof on %r", self)


def hash_select(key, choices):
    return choices[hash(key) % len(choices)]


def main():
    try:
        return _main()
    except KeyboardInterrupt:
        # because we mess with threads a lot, we need to make sure that ^C is
        # actually a nuclear kill
        os._exit(1)

def _main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-n', metavar='N', type=int,
                        default=multiprocessing.cpu_count(), dest='nprocs')
    parser.add_argument('-b', '--buffer', metavar='N', type=parse_size,
                        help="size (in lines) of input buffer for each process",
                        default=1024,
                        dest='bufsize')
    parser.add_argument('-f', metavar='FIELDSEP', type=str, default='\t',
                        dest='field_sep')
    parser.add_argument('-r', metavar='FIELDRE', type=str, default=None,
                        dest='field_re')
    parser.add_argument('--logging', help=argparse.SUPPRESS, default='error')
    parser.add_argument('cmd', nargs='+')

    args = parser.parse_args()

    if args.field_re and args.field_sep:
        args.print_usage()
        return sys.exit(1)

    if args.nprocs == 1:
        # if you only want one, what do you need me for?
        os.execvp(args.cmd[0], args.cmd)
        return sys.exit(1) # will never get here

    if args.field_re:
        first_field_re = re.compile(args.field_re)
    else:
        first_field_re = re.compile('^([^'+re.escape(args.field_sep)+']+)')

    logging.basicConfig(level=getattr(logging, args.logging.upper()))

    stdout_mutex = threading.Lock()
    processes = []

    for x in range(args.nprocs):
        logging.debug("Starting %r (%d)", args.cmd, x)
        ps = subprocess.Popen(args.cmd,
                              stdin=subprocess.PIPE,
                              stdout=subprocess.PIPE)
        psi = JobInputter(x, ps, Queue(maxsize=args.bufsize))
        pso = JobOutputter(x, ps, sys.stdout, stdout_mutex)
        psi.start()
        pso.start()
        processes.append((psi, pso))

    for line in sys.stdin:
        if not line:
            continue

        logging.debug("Read %d bytes from stdin", len(line))

        first_field_m = first_field_re.match(line)
        first_field = first_field_m.group(0)
        psi, _pso = hash_select(first_field, processes)
        logging.debug("Writing %d bytes to %r (%r)", len(line), psi, first_field)
        psi.iq.put(line)

    logging.debug("Hit eof on stdin")

    for x, (psi, pso) in enumerate(processes):
        logging.debug("Sending terminator to %d (%r)", x, psi)
        psi.iq.put(None)

    for x, (psi, pso) in enumerate(processes):
        logging.debug("Waiting for q %d (%r)", x, psi)
        psi.iq.join()
        logging.debug("Waiting for psi %d (%r)", x, psi)
        psi.join()
        logging.debug("Waiting for pso %d (%r)", x, psi)
        pso.join()

    return sys.exit(0)


if __name__ == '__main__':
    main()
<EOF>
<BOF>
#!/usr/bin/python

from org.apache.pig.scripting import Pig


SCRIPT_ROOT = "udfs/dist/lib/"
INPUT_ROOT = "input/"
OUTPUT_ROOT = "output"

relations = {"savehide": ("UserQueryCache", "link"),
             "inbox_account_comment": ("UserQueryCache", "comment"),
             "inbox_account_message": ("UserQueryCache", "message"),
             "moderatorinbox": ("SubredditQueryCache", "message"),
             "vote_account_link": ("UserQueryCache", "link"),
            }

####### Pig script fragments
load_things = """
things =
LOAD '$THINGS'
    USING PigStorage()
    AS (id:long,
        ups:int,
        downs:int,
        deleted:chararray,
        spam:chararray,
        timestamp:double);
"""

make_things_items = """
items =
FOREACH things GENERATE *;
"""

load_rels = """
items =
LOAD '$RELS'
    USING PigStorage()
    AS (id:long,
        thing1_id:long,
        thing2_id:long,
        name:chararray,
        timestamp:double);
"""


load_and_map_data = """
data =
LOAD '$DATA'
    USING PigStorage()
    AS (id:long,
        key:chararray,
        value);

grouped_with_data =
COGROUP items BY id, data BY id;

items_with_data =
FOREACH grouped_with_data
    GENERATE FLATTEN(items),
             com.reddit.pig.MAKE_MAP(data.(key, value)) AS data;
"""

add_unread = """
SPLIT items_with_data
    INTO inbox IF 1 == 1,
         unread IF (chararray)data#'new' == 't';

inbox_with_relname =
FOREACH inbox GENERATE '$RELATION' AS relation, *;

unread_with_relname =
FOREACH unread GENERATE '$RELATION:unread' AS relation, *;

rels_with_relname =
UNION ONSCHEMA inbox_with_relname,
               unread_with_relname;
"""

add_relname = """
rels_with_relname =
FOREACH items GENERATE '$RELATION' AS relation, *;
"""

generate_rel_items = """
minimal_things =
FOREACH things GENERATE id, deleted;

joined =
JOIN rels_with_relname BY thing2_id LEFT OUTER,
     minimal_things BY id;

only_valid =
FILTER joined BY minimal_things::id IS NOT NULL AND
                 deleted == 'f';

potential_columns =
FOREACH only_valid
    GENERATE com.reddit.pig.MAKE_ROWKEY(relation, name, thing1_id) AS rowkey,
             com.reddit.pig.MAKE_THING2_FULLNAME(relation, thing2_id) AS colkey,
             timestamp AS value;
"""

store_top_1000_per_rowkey = """
non_null =
FILTER potential_columns BY rowkey IS NOT NULL AND colkey IS NOT NULL;

grouped =
GROUP non_null BY rowkey;

limited =
FOREACH grouped {
    sorted = ORDER non_null BY value DESC;
    limited = LIMIT sorted 1000;
    GENERATE group AS rowkey, FLATTEN(limited.(colkey, value));
};

jsonified =
FOREACH limited GENERATE rowkey,
                         colkey,
                         com.reddit.pig.TO_JSON(value);

STORE jsonified INTO '$OUTPUT' USING PigStorage();
"""

###### run the jobs
# register the reddit udfs
Pig.registerJar(SCRIPT_ROOT + "reddit-pig-udfs.jar")

# process rels
for rel, (cf, thing2_type) in relations.iteritems():
    # build source for a script
    script = "SET default_parallel 10;"
    script += load_rels
    if "inbox" in rel:
        script += load_and_map_data
        script += add_unread
    else:
        script += add_relname
    script += load_things
    script += generate_rel_items
    script += store_top_1000_per_rowkey

    # run it
    compiled = Pig.compile(script)
    bound = compiled.bind({
        "RELS": INPUT_ROOT + rel + ".dump",
        "DATA": INPUT_ROOT + rel + "-data.dump",
        "THINGS": INPUT_ROOT + thing2_type + ".dump",
        "RELATION": rel,
        "OUTPUT": "/".join((OUTPUT_ROOT, cf, rel)),
    })
    bound.runSingle()

# rebuild message-based queries (just get_sent right now)
if False:
    script = "SET default_parallel 10;"
    script += load_things
    script += make_things_items
    script += load_and_map_data
    script += """
    non_null =
    FILTER items_with_data BY data#'author_id' IS NOT NULL;

    potential_columns =
    FOREACH non_null
    GENERATE
        CONCAT('sent.', com.reddit.pig.TO_36(data#'author_id')) AS rowkey,
        com.reddit.pig.MAKE_FULLNAME('message', id) AS colkey,
        timestamp AS value;
    """
    script += store_top_1000_per_rowkey
    compiled = Pig.compile(script)
    bound = compiled.bind({
        "THINGS": INPUT_ROOT + "message.dump",
        "DATA": INPUT_ROOT + "message-data.dump",
        "OUTPUT": "/".join((OUTPUT_ROOT, "UserQueryCache", "sent")),
    })
    result = bound.runSingle()

# rebuild comment-based queries
if True:
    script = "SET default_parallel 10;"
    script += load_things
    script += make_things_items
    script += load_and_map_data
    script += """
    SPLIT items_with_data INTO
        spam_comments IF spam == 't',
        ham_comments IF spam == 'f';

    ham_comments_with_name =
    FOREACH ham_comments GENERATE 'sr_comments' AS name, *;

    reported_comments =
    FILTER ham_comments BY (int)data#'reported' > 0;

    reported_comments_with_name =
    FOREACH reported_comments GENERATE 'reported_comments' AS name, *;

    spam_comments_with_name =
    FOREACH spam_comments GENERATE 'spam_comments' AS name, *;

    comments_with_name =
    UNION ONSCHEMA ham_comments_with_name,
                   reported_comments_with_name,
                   spam_comments_with_name;

    potential_columns =
    FOREACH comments_with_name GENERATE
        CONCAT(name, CONCAT('.', com.reddit.pig.TO_36(data#'sr_id'))) AS rowkey,
        com.reddit.pig.MAKE_FULLNAME('comment', id) AS colkey,
        timestamp AS value;
    """
    script += store_top_1000_per_rowkey
    compiled = Pig.compile(script)
    bound = compiled.bind({
        "THINGS": INPUT_ROOT + "comment.dump",
        "DATA": INPUT_ROOT + "comment-data.dump",
        "OUTPUT": "/".join((OUTPUT_ROOT, "SubredditQueryCache", "comment")),
    })
    result = bound.runSingle()
<EOF>
<BOF>
#!/usr/bin/jython
"""A jython script that takes as input a set of tuples meant to be
bulk-loaded into Cassandra, and outputs a set of sstables usable
by Cassandra's sstableloader.

The Cassandra jars and configuration must be on the classpath for this
to function properly.
"""

import time
import os
import os.path
import logging

from org.apache.cassandra.utils.ByteBufferUtil import bytes
from java.nio import ByteBuffer


def utf8(val):
    return bytes(val)

def datetime(val):
    milliseconds = long(float(val) * 1e3)
    return ByteBuffer.allocate(8).putLong(0, milliseconds)

COERCERS = dict(utf8=utf8,
                datetime=datetime)


def convert_to_sstables(input_files, column_family,
                        output_dir_name, keyspace, timestamp, buffer_size,
                        data_type, verbose=False):
    import fileinput
    from java.io import File
    from org.apache.cassandra.io.sstable import SSTableSimpleUnsortedWriter
    from org.apache.cassandra.db.marshal import AsciiType
    from org.apache.cassandra.service import StorageService
    from org.apache.cassandra.io.compress import CompressionParameters

    partitioner = StorageService.getPartitioner()

    try:
        coercer = COERCERS[data_type]
    except KeyError:
        raise ValueError("invalid data type")

    output_dir = File(output_dir_name)

    if not output_dir.exists():
        output_dir.mkdir()

    compression_options = CompressionParameters.create({
        'sstable_compression': 'org.apache.cassandra.io.compress.SnappyCompressor',
        'chunk_length_kb': '64'
    })

    writer = SSTableSimpleUnsortedWriter(output_dir,
                                         partitioner,
                                         keyspace,
                                         column_family,
                                         AsciiType.instance,
                                         None,
                                         buffer_size,
                                         compression_options)


    try:
        previous_rowkey = None
        for line in fileinput.input(input_files):
            ttl = None

            t_columns = line.rstrip("\n").split("\t")
            if len(t_columns) == 3:
                rowkey, colkey, value = t_columns
            elif len(t_columns) == 4:
                rowkey, colkey, value, ttl = t_columns
                ttl = int(ttl)
            else:
                raise Exception("unknown data format for %r" % (t_columns,))

            if rowkey != previous_rowkey:
                writer.newRow(bytes(rowkey))

            coerced = coercer(value)

            if ttl is None:
                writer.addColumn(bytes(colkey), coerced, timestamp)
            else:
                # see
                # https://svn.apache.org/repos/asf/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java addExpiringColumn:expirationTimestampMS
                # for explanation
                expirationTimestampMS = (timestamp / 1000) + (ttl * 1000)
                writer.addExpiringColumn(bytes(colkey), coerced,
                                         timestamp, ttl, expirationTimestampMS)

            if verbose and fileinput.lineno() % 10000 == 0:
                print "%d items processed (%s)" % (fileinput.lineno(),
                                                   fileinput.filename())
    except:
        # it's common that whatever causes us to fail also cases the finally
        # clause below to fail, which masks the original exception
        logging.exception("Failed")
        raise
    finally:
        writer.close()


def main():
    import os
    import optparse

    parser = optparse.OptionParser(
        usage="USAGE: tuples_to_sstables [options] COLUMN_FAMILY INPUT [...]")
    parser.add_option("--timestamp",
                      type="long",
                      nargs=1, dest="timestamp",
                      default=int(time.time()*1000000),
                      help="timestamp to use for each column")
    parser.add_option("--buffer-size",
                      type="int",
                      nargs=1, dest="buffer_size", default=128,
                      help="size in MB to buffer before writing SSTables")
    parser.add_option("--data-type",
                      nargs=1, dest="data_type", default="utf8",
                      help="type to coerce data into for column values")
    parser.add_option("-k", "--keyspace",
                      nargs=1, dest="keyspace", default="reddit",
                      help="the name of the keyspace the data is for")
    parser.add_option("-o", "--output-root",
                      nargs=1, dest="output_root", default=".",
                      help="the root directory to write the SSTables into")
    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="print status messages to stdout")

    options, args = parser.parse_args()
    options = dict(options.__dict__) # in jython, __dict__ is a StringMap
    options["output_dir_name"] = os.path.join(options.pop("output_root"))
    options["column_family"], input_files = args[0], args[1:]

    return convert_to_sstables(input_files, **options)


if __name__ == "__main__":
    import sys
    sys.exit(main())
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict

import time

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2, to36
from r2.models.subreddit import SRMember, SubscribedSubredditsByAccount


def get_query(after_user_id):
    q = SRMember._query(
        SRMember.c._name == "subscriber",
        SRMember.c._thing2_id < after_user_id,
        sort=desc("_thing2_id"),
    )
    return q


def get_srmembers(after_user_id):
    previous_user_id = None

    while True:
        # there isn't a good index on rel_id so we need to get a new query
        # for each batch rather than relying solely on fetch_things2
        q = get_query(after_user_id)
        users_seen = 0

        for rel in fetch_things2(q):
            user_id = rel._thing2_id

            if user_id != previous_user_id:
                if users_seen >= 20:
                    # set after_user_id to the previous id so we will pick up
                    # the query at this same point
                    after_user_id = previous_user_id
                    break

                users_seen += 1
                previous_user_id = user_id

            yield rel


def migrate_srmember_subscribers(after_user_id=39566712):
    columns = {}
    rowkey = None
    proc_time = time.time()

    for i, rel in enumerate(get_srmembers(after_user_id)):
        sr_id = rel._thing1_id
        user_id = rel._thing2_id
        action_date = rel._date
        new_rowkey = to36(user_id)

        if new_rowkey != rowkey and columns:
            SubscribedSubredditsByAccount._cf.insert(
                rowkey, columns, timestamp=1434403336829573)
            columns = {}

        columns[to36(sr_id)] = action_date
        rowkey = new_rowkey

        if i % 1000 == 0:
            new_proc_time = time.time()
            duration = new_proc_time - proc_time
            print "%s (%.3f): %s - %s" % (i, duration, user_id, action_date)
            proc_time = new_proc_time
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from r2.lib.db.operators import asc
from r2.lib.utils import fetch_things2
from r2.models import ModAction, ModActionBySRActionMod, Subreddit

def backfill(after=None):
    q = Subreddit._query(sort=asc('_date'))
    if after:
        sr = Subreddit._by_name(after)
        q = q._after(sr)

    for sr in fetch_things2(q):
        backfill_sr(sr)


def backfill_sr(sr):
    print "processing %s" % sr.name
    after = None
    count = 100
    q = ModAction.get_actions(sr, after=after, count=count)
    actions = list(q)
    while actions:
        for ma in actions:
            ModActionBySRActionMod.add_object(ma)
        q = ModAction.get_actions(sr, after=actions[-1], count=count)
        actions = list(q)
<EOF>
<BOF>

# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import urllib2

from pylons import app_globals as g

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2
from r2.lib.media import upload_media
from r2.models.subreddit import Subreddit
from r2.models.wiki import WikiPage, ImagesByWikiPage


all_subreddits = Subreddit._query(sort=desc("_date"))
for sr in fetch_things2(all_subreddits):
    images = sr.images.copy()
    images.pop("/empties/", None)

    if not images:
        continue

    print 'Processing /r/%s (id36: %s)' % (sr.name, sr._id36)

    # upgrade old-style image ids to urls
    for name, image_url in images.items():
        if not isinstance(image_url, int):
            continue

        print "  upgrading image %r" % image_url
        url = "http://%s/%s_%d.png" % (g.s3_old_thumb_bucket,
                                       sr._fullname, image_url)
        image_data = urllib2.urlopen(url).read()
        new_url = upload_media(image_data, file_type=".png")
        images[name] = new_url

    # use a timestamp of zero to make sure that we don't overwrite any changes
    # from live dual-writes.
    rowkey = WikiPage.id_for(sr, "config/stylesheet")
    ImagesByWikiPage._cf.insert(rowkey, images, timestamp=0)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the num_gildings for users

This is used to determine which gilding trophy level they should have.
"""
from pylons import app_globals as g

from r2.models import Account
from r2.models.gold import gold_table, ENGINE
from r2admin.lib.trophies import add_to_trophy_queue
from sqlalchemy.sql.expression import select
from sqlalchemy.sql.functions import count as sa_count


def update_num_gildings(update_trophy=True, user_id=None):
    """Returns total number of link, comment, and user gildings"""
    query = (select([gold_table.c.paying_id, sa_count(gold_table.c.trans_id)])
        .where(gold_table.c.trans_id.like('X%'))
        .group_by(gold_table.c.paying_id)
        .order_by(sa_count(gold_table.c.trans_id).desc())
    )
    if user_id:
        query = query.where(gold_table.c.paying_id == str(user_id))

    rows = ENGINE.execute(query)
    total_updated = 0
    for paying_id, count in rows:
        try:
            a = Account._byID(int(paying_id), data=True)
            a.num_gildings = count
            a._commit()
            total_updated += 1
            #if 'server seconds paid' for are public, update gilding trophies
            if update_trophy and a.pref_public_server_seconds:
                add_to_trophy_queue(a, "gilding")
        except:
            g.log.debug("update_num_gildings: paying_id %s is invalid" % paying_id)

    g.log.debug("update_num_gildings: updated %s accounts" % total_updated)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Ensure modmsgtime is properly set on all accounts.

See the comment in Account.is_moderator_somewhere for possible values of this
attribute now.

"""

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2, progress
from r2.models import Account, Subreddit


all_accounts = Account._query(sort=desc("_date"))
for account in progress(fetch_things2(all_accounts)):
    is_moderator_somewhere = bool(Subreddit.reverse_moderator_ids(account))
    if is_moderator_somewhere:
        if not account.modmsgtime:
            account.modmsgtime = False
        else:
            # the account already has a date for modmsgtime meaning unread mail
            pass
    else:
        account.modmsgtime = None
    account._commit()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Converts msgtime for users to inbox_count, for inbox count tracking."""

import sys

from r2.lib.db import queries
from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2, progress
from r2.models import Account, Message

from pylons import app_globals as g


def _keep(msg, account):
    """Adapted from listingcontroller.MessageController's keep_fn."""
    if msg._deleted:
        return False

    if msg._spam and msg.author_id != account._id:
        return False

    if msg.author_id in account.enemies:
        return False

    # do not keep messages which were deleted on recipient
    if (isinstance(msg, Message) and
            msg.to_id == account._id and msg.del_on_recipient):
        return False

    # don't show user their own unread stuff
    if msg.author_id == account._id:
        return False

    return True

resume_id = long(sys.argv[1]) if len(sys.argv) > 1 else None

msg_accounts = Account._query(sort=desc("_date"), data=True)

if resume_id:
    msg_accounts._filter(Account.c._id < resume_id)

for account in progress(fetch_things2(msg_accounts), estimate=resume_id):
    current_inbox_count = account.inbox_count
    unread_messages = list(queries.get_unread_inbox(account))

    if account._id % 100000 == 0:
        g.reset_caches()

    if not len(unread_messages):
        if current_inbox_count:
            account._incr('inbox_count', -current_inbox_count)
    else:
        msgs = Message._by_fullname(
            unread_messages,
            data=True,
            return_dict=False,
            ignore_missing=True,
        )
        kept_msgs = sum(1 for msg in msgs if _keep(msg, account))

        if kept_msgs or current_inbox_count:
            account._incr('inbox_count', kept_msgs - current_inbox_count)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the gildings listing for users.

This listing is stored in get_user_gildings and seen on
/user/<username>/gildings.

"""

import datetime

from pylons import app_globals as g

from r2.lib.db.queries import get_user_gildings
from r2.lib.utils import Storage
from r2.models import GildingsByDay, Thing, Comment
from r2.models.query_cache import CachedQueryMutator


date = datetime.datetime.now(g.tz)
earliest_date = datetime.datetime(2012, 10, 01, tzinfo=g.tz)

already_seen = set()

with CachedQueryMutator() as m:
    while date > earliest_date:
        gildings = GildingsByDay.get_gildings(date)
        fullnames = [x["thing"] for x in gildings]
        things = Thing._by_fullname(fullnames, data=True, return_dict=False)
        comments = {t._fullname: t for t in things if isinstance(t, Comment)}

        for gilding in gildings:
            fullname = gilding["thing"]
            if fullname in comments and fullname not in already_seen:
                thing = gilding["thing"] = comments[fullname]
                gilding_object = Storage(gilding)
                m.insert(get_user_gildings(gilding["user"]), [gilding_object])
                already_seen.add(fullname)
        date -= datetime.timedelta(days=1)
<EOF>
<BOF>
from collections import defaultdict
from datetime import datetime

from pylons import app_globals as g

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2
from r2.models import (
    calculate_server_seconds,
    Comment,
    Link,
    Subreddit,
)

LINK_GILDING_START = datetime(2014, 2, 1, 0, 0, tzinfo=g.tz)
COMMENT_GILDING_START = datetime(2012, 10, 1, 0, 0, tzinfo=g.tz)

queries = [
    Link._query(
        Link.c.gildings != 0, Link.c._date > LINK_GILDING_START, data=True,
        sort=desc('_date'),
    ),
    Comment._query(
        Comment.c.gildings != 0, Comment.c._date > COMMENT_GILDING_START,
        data=True, sort=desc('_date'),
    ),
]

seconds_by_srid = defaultdict(int)
gilding_price = g.gold_month_price.pennies

for q in queries:
    for things in fetch_things2(q, chunks=True, chunk_size=100):
        print things[0]._fullname

        for thing in things:
            seconds_per_gilding = calculate_server_seconds(gilding_price, thing._date)
            seconds_by_srid[thing.sr_id] += int(thing.gildings * seconds_per_gilding)

for sr_id, seconds in seconds_by_srid:
    sr = Subreddit._byID(sr_id, data=True)
    print "%s: %s seconds" % (sr.name, seconds)
    sr._incr("gilding_server_seconds", seconds)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the gilded comment listing for users.

This listing is stored in get_gilded_user_comments and seen on
/user/<username>/gilded.

"""

import datetime

from pylons import app_globals as g

from r2.lib.db.queries import get_gilded_user_comments
from r2.lib.utils import Storage
from r2.models import GildingsByDay, Thing, Comment
from r2.models.query_cache import CachedQueryMutator


date = datetime.datetime.now(g.tz)
earliest_date = datetime.datetime(2012, 10, 01, tzinfo=g.tz)

already_seen = set()

with CachedQueryMutator() as m:
    while date > earliest_date:
        gildings = GildingsByDay.get_gildings(date)
        fullnames = [x["thing"] for x in gildings]
        things = Thing._by_fullname(fullnames, data=True, return_dict=False)
        comments = {t._fullname: t for t in things if isinstance(t, Comment)}

        for gilding in gildings:
            fullname = gilding["thing"]
            if fullname in comments and fullname not in already_seen:
                thing = gilding["thing"] = comments[fullname]
                gilding_object = Storage(gilding)
                m.insert(get_gilded_user_comments(thing.author_id),
                         [gilding_object])
                already_seen.add(fullname)
        date -= datetime.timedelta(days=1)
<EOF>
<BOF>
from datetime import datetime

from pylons import app_globals as g

from r2.models import (
    CommentSortsCache,
    CommentScoresByLink,
)

# estimate 93,233,408 rows in CommentSortsCache


def run():
    start_time = datetime.now(g.tz)
    epoch_micro_seconds = int(start_time.strftime("%s")) * 1000000
    count = 0

    for rowkey, columns in CommentSortsCache._cf.get_range(column_count=1000):
        # CommentSortsCache rowkey is "{linkid36}{sort}"
        # CommentScoresByLink rowkey is the same

        if len(columns) == 1000:
            columns = CommentSortsCache._cf.xget(rowkey)
            # convert str column values to floats
            float_columns = {k: float(v) for k, v in columns}
        else:
            # convert str column values to floats
            float_columns = {k: float(v) for k, v in columns.iteritems()}

        # write with a timestamp to not overwrite any writes since our read
        CommentScoresByLink._cf.insert(
            rowkey, float_columns, timestamp=epoch_micro_seconds)

        count += 1
        if count % 1000 == 0:
            print "processed %s rows, last seen was %s" % (count, rowkey)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the gilded comment listing.

This listing is stored in get_gilded_comments and seen on /comments/gilded.

"""

import datetime

from pylons import app_globals as g

from r2.lib.db.queries import get_gilded_comments, get_all_gilded_comments
from r2.lib.utils import Storage
from r2.models import GildingsByDay, Thing, Comment
from r2.models.query_cache import CachedQueryMutator


date = datetime.datetime.now(g.tz)
earliest_date = datetime.datetime(2012, 10, 01, tzinfo=g.tz)

already_seen = set()

with CachedQueryMutator() as m:
    while date > earliest_date:
        gildings = GildingsByDay.get_gildings(date)
        fullnames = [x["thing"] for x in gildings]
        things = Thing._by_fullname(fullnames, data=True, return_dict=False)
        comments = {t._fullname: t for t in things if isinstance(t, Comment)}

        for gilding in gildings:
            fullname = gilding["thing"]
            if fullname in comments and fullname not in already_seen:
                thing = gilding["thing"] = comments[fullname]
                gilding_object = Storage(gilding)
                m.insert(get_gilded_comments(thing.sr_id), [gilding_object])
                m.insert(get_all_gilded_comments(), [gilding_object])
                already_seen.add(fullname)
        date -= datetime.timedelta(days=1)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Script for backunfilling data from deleted users.

You might want to change `run_changed()` to `run_changed(use_safe_get=True)`
in `reddit-consumer-cloudsearch_q.conf` unless you're sure *everything* in
`LinksByAccount` is a valid `Link`. Otherwise, you're gonna back up the
cloudsearch queue.
"""

import time
import sys

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2, progress
from r2.lib import amqp
from r2.models import Account


def get_queue_length(name):
    # https://stackoverflow.com/questions/1038318/check-rabbitmq-queue-size-from-client
    chan = amqp.connection_manager.get_channel()
    queue_response = chan.queue_declare(name, passive=True)
    return queue_response[1]


def backfill_deleted_accounts(resume_id=None):
    del_accts = Account._query(Account.c._deleted == True, sort=desc('_date'))
    if resume_id:
        del_accts._filter(Account.c._id < resume_id)

    for i, account in enumerate(progress(fetch_things2(del_accts))):
        # Don't kill the rabbit! Wait for the relevant queues to calm down.
        if i % 1000 == 0:
            del_len = get_queue_length('del_account_q')
            cs_len = get_queue_length('cloudsearch_changes')
            while (del_len > 1000 or
                    cs_len > 10000):
                sys.stderr.write(("CS: %d, DEL: %d" % (cs_len, del_len)) + "\n")
                sys.stderr.flush()
                time.sleep(1)
                del_len = get_queue_length('del_account_q')
                cs_len = get_queue_length('cloudsearch_changes')
        amqp.add_item('account_deleted', account._fullname)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Fix the urls of previously-uploaded preview images so they all work.
"""

import sys

import boto
import pycassa

from boto.s3.key import Key
from pylons import app_globals as g

from r2.lib.media import _get_scrape_url
from r2.lib.providers.media.s3 import S3MediaProvider
from r2.lib.utils import UrlParser
from r2.models.link import Link, LinksByImage
from r2.models.media_cache import MediaByURL

def good_preview_object(preview_object):
    if not preview_object or not 'url' in preview_object:
        print '  aborting - bad preview object: %s' % preview_object
        return False
    if not preview_object['url']:
        print '  aborting - bad preview url: %s' % preview_object['url']
        return False
    return True

s3 = boto.connect_s3(g.S3KEY_ID or None, g.S3SECRET_KEY or None)

for uid, columns in LinksByImage._cf.get_range():
# When resuming, use:
#for uid, columns in LinksByImage._cf.get_range(start='<uid>'):
    print 'Looking at image %s' % uid
    link_ids = columns.keys()
    links = Link._byID36(link_ids, return_dict=False, data=True)
    if not links:
        continue

    # Pull information about the image from the first link (they *should* all
    # be the same).
    link = links[0]
    preview_object = link.preview_object
    if not good_preview_object(preview_object):
        continue

    u = UrlParser(preview_object['url'])
    if preview_object['url'].startswith(g.media_fs_base_url_http):
        # Uploaded to the local filesystem instead of s3.  Should only be in
        # dev.
        print '  non-s3 image'
        continue
    elif u.hostname == 's3.amazonaws.com':
        parts = u.path.lstrip('/').split('/')

        bucket = parts.pop(0)
        filename = '/'.join(parts)
    else:
        bucket = u.hostname
        filename = u.path.lstrip('/')

    print '  bucket: %s' % bucket
    print '  filename: %s' % filename

    if bucket in g.s3_image_buckets:
        print '  skipping - already in correct place'
        continue

    k = Key(s3.get_bucket(bucket))
    k.key = filename
    k.copy(s3.get_bucket(g.s3_image_buckets[0]), filename)
    url = 'http://s3.amazonaws.com/%s/%s' % (g.s3_image_buckets[0], filename)
    print '  new url: %s' % url
    for link in links:
        print '  altering Link %s' % link
        if not good_preview_object(link.preview_object):
            continue
        if not link.preview_object == preview_object:
            print "  aborting - preview objects don't match"
            print '    first: %s' % preview_object
            print '    ours:  %s' % link.preview_object
            continue

        link.preview_object['url'] = url
        link._commit()
        # Guess at the key that'll contain the (now-incorrect) cache of the
        # preview object so we can delete it and not end up inserting old info
        # into new Links.
        #
        # These parameters are what's used in most of the code; the only place
        # they're overridden is for promoted links, where they could be
        # anything.  We'll just have to deal with those as they come up.
        image_url = _get_scrape_url(link)
        cache_key = MediaByURL._rowkey(image_url, autoplay=False, maxwidth=600)
        print '  deleting cache with key %s' % cache_key
        cache = MediaByURL(_id=cache_key)
        cache._committed = True
        try:
            cache._destroy()
        except pycassa.cassandra.ttypes.InvalidRequestException as e:
            print '    skipping cache deletion (%s)' % e.why
            continue
    # Delete *after* we've updated all the Links so they'll continue to work
    # while we're in the migration process.
    k.delete()
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import fnmatch
import sys
from setuptools import setup, find_packages, Extension


commands = {}


try:
    from Cython.Build import cythonize
except ImportError:
    print "Cannot find Cython. Skipping Cython build."
    pyx_extensions = []
else:
    pyx_files = []
    for root, directories, files in os.walk('.'):
        for f in fnmatch.filter(files, '*.pyx'):
            pyx_files.append(os.path.join(root, f))
    pyx_extensions = cythonize(pyx_files)


# guard against import errors in case this is the first run of setup.py and we
# don't have any dependencies (including baseplate) yet
try:
    from baseplate.integration.thrift.command import ThriftBuildPyCommand
except ImportError:
    print "Cannot find Baseplate. Skipping Thrift build."
else:
    commands["build_py"] = ThriftBuildPyCommand


setup(
    name="r2",
    version="",
    install_requires=[
        "Pylons",
        "Routes",
        "mako>=0.5",
        "boto >= 2.0",
        "pytz",
        "pycrypto",
        "Babel>=1.0",
        "cython>=0.14",
        "SQLAlchemy",
        "BeautifulSoup",
        "chardet",
        "psycopg2",
        "pycassa>=1.7.0",
        "pycaptcha",
        "amqplib",
        "py-bcrypt",
        "snudown>=1.1.0",
        "l2cs>=2.0.2",
        "lxml",
        "kazoo",
        "stripe",
        "requests",
        "tinycss2",
        "unidecode",
        "PyYAML",
        "Pillow",
        "pylibmc==1.2.2",
        "webob",
        "webtest",
        "python-snappy",
        "httpagentparser==1.7.8",
        "raven",
    ],
    # setup tests (allowing for "python setup.py test")
    tests_require=['mock', 'nose', 'coverage'],
    test_suite="nose.collector",
    dependency_links=[
        "https://github.com/reddit/snudown/archive/v1.1.3.tar.gz#egg=snudown-1.1.3",
        "https://s3.amazonaws.com/code.reddit.com/pycaptcha-0.4.tar.gz#egg=pycaptcha-0.4",
    ],
    packages=find_packages(exclude=["ez_setup"]),
    cmdclass=commands,
    ext_modules=pyx_extensions + [
        Extension(
            "Cfilters",
            sources=[
                "r2/lib/c/filters.c",
            ]
        ),
    ],
    entry_points="""
    [paste.app_factory]
    main=r2:make_app
    [paste.paster_command]
    run = r2.commands:RunCommand
    shell = pylons.commands:ShellCommand
    [paste.filter_app_factory]
    gzip = r2.lib.gzipper:make_gzip_middleware
    [r2.provider.media]
    s3 = r2.lib.providers.media.s3:S3MediaProvider
    filesystem = r2.lib.providers.media.filesystem:FileSystemMediaProvider
    [r2.provider.cdn]
    fastly = r2.lib.providers.cdn.fastly:FastlyCdnProvider
    cloudflare = r2.lib.providers.cdn.cloudflare:CloudFlareCdnProvider
    null = r2.lib.providers.cdn.null:NullCdnProvider
    [r2.provider.auth]
    cookie = r2.lib.providers.auth.cookie:CookieAuthenticationProvider
    http = r2.lib.providers.auth.http:HttpAuthenticationProvider
    [r2.provider.support]
    zendesk = r2.lib.providers.support.zendesk:ZenDeskProvider
    [r2.provider.search]
    cloudsearch = r2.lib.providers.search.cloudsearch:CloudSearchProvider
    solr = r2.lib.providers.search.solr:SolrSearchProvider
    [r2.provider.image_resizing]
    imgix = r2.lib.providers.image_resizing.imgix:ImgixImageResizingProvider
    no_op = r2.lib.providers.image_resizing.no_op:NoOpImageResizingProvider
    unsplashit = r2.lib.providers.image_resizing.unsplashit:UnsplashitImageResizingProvider
    [r2.provider.email]
    null = r2.lib.providers.email.null:NullEmailProvider
    mailgun = r2.lib.providers.email.mailgun:MailgunEmailProvider
    """,
)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import os

from r2.lib.translation import I18N_PATH
from r2.lib.plugin import PluginLoader
from r2.lib import js

print 'POTFILE := ' + os.path.join(I18N_PATH, 'r2.pot')

plugins = PluginLoader()
print 'PLUGINS := ' + ' '.join(plugin.name for plugin in plugins
                               if plugin.needs_static_build)

print 'PLUGIN_I18N_PATHS := ' + ','.join(os.path.relpath(plugin.path)
                                         for plugin in plugins
                                         if plugin.needs_translation)

import sys
for plugin in plugins:
    print 'PLUGIN_PATH_%s := %s' % (plugin.name, plugin.path)

js.load_plugin_modules(plugins)
modules = dict((k, m) for k, m in js.module.iteritems())
print 'JS_MODULES := ' + ' '.join(modules.iterkeys())
outputs = []
for name, module in modules.iteritems():
    outputs.extend(module.outputs)
    print 'JS_MODULE_OUTPUTS_%s := %s' % (name, ' '.join(module.outputs))
    print 'JS_MODULE_DEPS_%s := %s' % (name, ' '.join(module.dependencies))

print 'JS_OUTPUTS := ' + ' '.join(outputs)
print 'DEFS_SUCCESS := 1'
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from ConfigParser import MissingSectionHeaderError
from StringIO import StringIO
import sys

from r2.lib.utils import parse_ini_file

HEADER = '''
# YOU DO NOT NEED TO EDIT THIS FILE
# This is a generated file. To update the configuration,
# edit the *.update file of the same name, and then
# run 'make ini'
# Configuration settings in the *.update file will override
# or be added to the base 'example.ini' file.
'''

def main(source_ini, update_ini):
    with open(source_ini) as source:
        parser = parse_ini_file(source)
    with open(update_ini) as f:
        updates = f.read()
    try:
        # Existing *.update files don't include section
        # headers; inject a [DEFAULT] header if the parsing
        # fails
        parser.readfp(StringIO(updates))
    except MissingSectionHeaderError:
        updates = "[DEFAULT]\n" + updates
        parser.readfp(StringIO(updates))
    print HEADER
    parser.write(sys.stdout)

if __name__ == '__main__':
    args = sys.argv
    if len(args) != 3:
        print 'usage: %s [source] [update]' % sys.argv[0]
        sys.exit(1)
    else:
        main(sys.argv[1], sys.argv[2])
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os, sys

import paste.fixture
from paste.script import command
from paste.deploy import loadapp

from r2.lib.log import RavenErrorReporter


class RunCommand(command.Command):
    max_args = 2
    min_args = 1

    usage = "CONFIGFILE CMDFILE.py"
    summary = "Executed CMDFILE with pylons support"
    group_name = "Reddit"


    parser = command.Command.standard_parser(verbose=True)
    parser.add_option('-c', '--command',
                      dest='command',
                      help="execute command in module")
    parser.add_option("", "--proctitle",
                      dest="proctitle",
                      help="set the title seen by ps and top")

    def command(self):
        try:
            if self.options.proctitle:
                import setproctitle
                setproctitle.setproctitle("paster " + self.options.proctitle)
        except ImportError:
            pass

        config_file = self.args[0]
        config_name = 'config:%s' % config_file

        report_to_sentry = "REDDIT_ERRORS_TO_SENTRY" in os.environ

        here_dir = os.getcwd()

        # Load locals and populate with objects for use in shell
        sys.path.insert(0, here_dir)

        # Load the wsgi app first so that everything is initialized right
        global_conf = {
            'running_as_script': "true",
        }
        wsgiapp = loadapp(
            config_name, relative_to=here_dir, global_conf=global_conf)
        test_app = paste.fixture.TestApp(wsgiapp)

        # Query the test app to setup the environment
        tresponse = test_app.get('/_test_vars')
        request_id = int(tresponse.body)

        # Disable restoration during test_app requests
        test_app.pre_request_hook = lambda self: \
            paste.registry.restorer.restoration_end()
        test_app.post_request_hook = lambda self: \
            paste.registry.restorer.restoration_begin(request_id)

        # Restore the state of the Pylons special objects
        # (StackedObjectProxies)
        paste.registry.restorer.restoration_begin(request_id)

        loaded_namespace = {}

        try:
            if self.args[1:]:
                execfile(self.args[1], loaded_namespace)

            if self.options.command:
                exec self.options.command in loaded_namespace
        except Exception:
            if report_to_sentry:
                exc_info = sys.exc_info()
                RavenErrorReporter.capture_exception(exc_info=exc_info)
            raise
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""r2

This file loads the finished app from r2.config.middleware.
"""

# _strptime is imported with PyImport_ImportModuleNoBlock which can fail
# miserably when multiple threads try to import it simultaneously.
# import this here to get it over with
# see "Non Blocking Module Imports" in:
# http://code.google.com/p/modwsgi/wiki/ApplicationIssues
import _strptime

# defer the (hefty) import until it's actually needed. this allows
# modules below r2 to be imported before cython files are built, also
# provides a hefty speed boost to said imports when they don't need
# the app initialization.
def make_app(*args, **kwargs):
    from r2.config.middleware import make_app as real_make_app
    return real_make_app(*args, **kwargs)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import sys
import Queue
from unittest import TestCase
from mock import patch, MagicMock
from collections import defaultdict

import pylons
from pylons.i18n.translation import _get_translator
from routes.util import URLGenerator
from pylons import url
import baseplate.events
import pkg_resources
import paste.fixture
import paste.script.appinstall
from paste.deploy import loadapp

from routes.util import url_for
from r2.lib.utils import query_string
from r2.lib import eventcollector


__all__ = ['RedditTestCase', 'RedditControllerTestCase']

here_dir = os.path.dirname(os.path.abspath(__file__))
conf_dir = os.path.dirname(os.path.dirname(here_dir))

sys.path.insert(0, conf_dir)
pkg_resources.working_set.add_entry(conf_dir)
pkg_resources.require('Paste')
pkg_resources.require('PasteScript')


# on case-insensitive file systems, Captcha gets masked by
# r2.lib.captcha which is also in sys.path.  This import ensures
# that the subsequent import is already in sys.modules, sidestepping
# the issue
try:
    from Captcha import Base
except ImportError:
    with patch.object(
        sys, "path",
        [x for x in sys.path if not x.endswith("r2/r2/lib")]
    ):
        from Captcha import Base

from pylons import app_globals as g
from r2.config.middleware import RedditApp

# unfortunately, because of the deep intertwinded dependency we have in the
# orm with app_globals, we unfortunately have to do some pylons-setup
# at import time
baseplate.events.EventQueue = Queue.Queue
wsgiapp = loadapp('config:test.ini', relative_to=conf_dir)
pylons.app_globals._push_object(wsgiapp.config['pylons.app_globals'])
pylons.config._push_object(wsgiapp.config)

# Initialize a translator for tests that utilize i18n
translator = _get_translator(pylons.config.get('lang'))
pylons.translator._push_object(translator)

url._push_object(URLGenerator(pylons.config['routes.map'], {}))


def diff_dicts(d, expected, prefix=None):
    """Given 2 dicts, return a summary of their differences

    :param dict d: dict to match from ("got")
    :param dict expected: dict to match against ("want")
    :param prefix: key prefix (used for recursion)
    :type prefix: list or None
    :rtype: dict
    :returns: mapping of flattened keys to 2-ples of (got, want)
    """
    prefix = prefix or []
    diffs = {}
    for k in set(d.keys() + expected.keys()):
        current_prefix = prefix + [k]
        want = d.get(k)
        got = expected.get(k)
        if isinstance(want, dict) and isinstance(got, dict):
            diffs.update(diff_dicts(got, want, prefix=current_prefix))
        elif got != want:
            key = ".".join(current_prefix)
            diffs[key] = (got, want)
    return diffs


class DiffAssertionError(AssertionError):
    def __init__(self, diffs):
        s = "\n".join(
            "\t{key}: {want} != {got}".format(
                key=key, want=repr(want), got=repr(got),
            ) for key, (want, got) in sorted(diffs.iteritems())
        )
        super(DiffAssertionError, self).__init__(
            "Mismatched ditionaries:\n%s" % s
        )


def assert_same_dict(data, expected_data):
    """Asserts two dicts are the same (recursively)

    :param dict data: dictionary to be compared from
    :param dict expected_data: expected dictionary

    :raises: :py:class:`DiffAssertionError`
    """
    diffs = diff_dicts(data, expected_data)
    if diffs:
        raise DiffAssertionError(diffs)


class MockAmqp(object):
    """An amqp replacement, suitable for unit tests.
    Besides providing a mock `queue` for storing all received events, this
    class provides a set of handy assert-style functions for checking what
    was previously queued.
    """
    def __init__(self, test_cls):
        self.queue = defaultdict(list)
        self.test_cls = test_cls

    def add_item(self, name, body, **kw):
        self.queue[name].append((body, kw))

    def assert_item_count(self, name, count=None):
        """Assert that `count` items have been queued in queue `name`.

        If count is none, just asserts that at least one item has been added
        to that queue
        """
        if count is None:
            self.test_cls.assertTrue(bool(self.queue.get(name)))
        else:
            err = "expected %d events in queue, saw %d" % (
                count, len(self.queue)
            )
            assert len(self.queue) == count, err

    def assert_event_item(
        self, expected_data, expected_num=1, name="event_collector"
    ):
        candidates = []

        queue = self.queue[name]

        # find candidate events that are of the same topic as the provided
        # error.
        for data, _ in queue:
            data = data.copy()
            # and do they have a timestamp, uuid, and payload?
            assert data.pop("event_ts", None) is not None, \
                "event_ts is missing"
            assert data.pop("uuid", None) is not None, "uuid is missing"
            # there is some variability, but this should at least be present
            assert "event_topic" in data, "event_topic is missing"

            if data['event_topic'] == expected_data['event_topic']:
                candidates.append(data)

        # No candidates when expecting some, fail early.
        if not candidates and expected_num > 0:
            raise AssertionError(
                "No %r events found" % expected_data['event_topic']
            )

        # for each candidate, look for an exact dictionary match
        diffs = []
        nmatches = 0
        for candidate in candidates:
            diff = diff_dicts(candidate, expected_data)
            if not diff:
                nmatches += 1
            diffs.append(diff)

        # if no matches, present the closest match.
        if nmatches == 0 and expected_num > 0:
            raise DiffAssertionError(min(diffs, key=len))

        # raise if we found matches, but not the correct number
        if nmatches != expected_num:
            raise AssertionError("Expected %d event, got %d of %r" % (
                expected_num, nmatches, expected_data,
            ))


class RedditTestCase(TestCase):
    """Base Test Case for tests that require the app environment to run.

    App startup does take time, so try to use unittest.TestCase directly when
    this isn't necessary as it'll save time.

    """
    def setUp(self):

        # disable controllers for this type of test, and make sure to set
        # things back to where they started.
        def reset_test_mode(orig_value=RedditApp.test_mode):
            RedditApp.test_mode = orig_value
        self.addCleanup(reset_test_mode)
        RedditApp.test_mode = True

        self.app = paste.fixture.TestApp(wsgiapp)
        test_response = self.app.get("/_test_vars")
        request_id = int(test_response.body)
        self.app.pre_request_hook = lambda self: \
            paste.registry.restorer.restoration_end()
        self.app.post_request_hook = lambda self: \
            paste.registry.restorer.restoration_begin(request_id)
        paste.registry.restorer.restoration_begin(request_id)

    def assert_same_dict(self, data, expected_data, prefix=None):
        prefix = prefix or []
        for k in set(data.keys() + expected_data.keys()):
            current_prefix = prefix + [k]
            want = expected_data.get(k)
            got = data.get(k)
            if isinstance(want, dict) and isinstance(got, dict):
                self.assert_same_dict(got, want, prefix=current_prefix)
            else:
                self.assertEqual(
                    got, want,
                    "Mismatch for %s: %r != %r" % (
                        ".".join(current_prefix), got, want
                    )
                )

    def mock_eventcollector(self):
        """Mock out the parts of the event collector which write to the queue.

        Also mocks `domain` and `to_epoch_milliseconds` as it makes writing
        tests easier since we pass in mock data to the events as well.
        """
        p = patch.object(eventcollector.json, "dumps", lambda x: x)
        p.start()
        self.addCleanup(p.stop)

        amqp = MockAmqp(self)
        self.amqp = self.autopatch(g.events, "queue", amqp)

        self.domain_mock = self.autopatch(eventcollector, "domain")

        self.created_ts_mock = MagicMock(name="created_ts")
        self.to_epoch_milliseconds = self.autopatch(
            eventcollector, "to_epoch_milliseconds",
            return_value=self.created_ts_mock)

    def autopatch(self, obj, attr, *a, **kw):
        """Helper method to patch an object and automatically cleanup."""
        p = patch.object(obj, attr, *a, **kw)
        m = p.start()
        self.addCleanup(p.stop)
        return m

    def patch_g(self, **kw):
        """Helper method to patch attrs on pylons.g.

        Since we do this all the time.  autpatch g with the provided kw.
        """
        for k, v in kw.iteritems():
            self.autopatch(g, k, v, create=not hasattr(g, k))

    def patch_liveconfig(self, k, v):
        """Helper method to patch g.live_config (with cleanup)."""
        def cleanup(orig=g.live_config[k]):
            g.live_config[k] = orig
        g.live_config[k] = v
        self.addCleanup(cleanup)


class NonCache(object):
    def get(self, *a, **kw):
        return

    def get_multi(self, *a, **kw):
        return {}

    def set(self, *a, **kw):
        return

    def set_multi(self, *a, **kw):
        return

    def add(self, *a, **kw):
        return

    def incr(self, *a, **kw):
        return


class RedditControllerTestCase(RedditTestCase):
    CONTROLLER = None
    ACTIONS = {}

    def setUp(self):
        super(RedditControllerTestCase, self).setUp()
        from r2.models import Link, Subreddit, Account
        # unfortunately, these classes' _type attrs are used as import
        # side effects for some controllers, and need to be set for things
        # to work properly
        for i, _cls in enumerate((Link, Subreddit, Account)):
            if not hasattr(_cls, "_type_id"):
                self.autopatch(_cls, "_type_id", i + 1000, create=True)
            if not hasattr(_cls, "_type_name"):
                self.autopatch(
                    _cls, "_type_name", _cls.__name__.lower(),
                    create=True)
        # The same is true for _by_name on Subreddit and Account
        self.subreddit_by_name = self.autopatch(Subreddit, "_by_name")
        self.account_by_name = self.autopatch(Account, "_by_name")

        # mock out any Memcached side effects
        self.patch_g(
            rendercache=NonCache(),
            ratelimitcache=NonCache(),
            commentpanecache=NonCache(),
            gencache=NonCache(),
            memoizecache=NonCache(),
        )

        self.mock_eventcollector()

        self.simple_event = self.autopatch(g.stats, "simple_event")

        self.user_agent = "Hacky McBrowser/1.0"
        self.device_id = None

        # Lastly, pull the app out of test mode so it'll load controllers on
        # first use
        RedditApp.test_mode = False

    def do_post(self, action, params, headers=None, expect_errors=False):

        assert self.CONTROLLER is not None

        body = self.make_qs(**params)

        headers = headers or {}
        headers.setdefault('User-Agent', self.user_agent)
        if self.device_id:
            headers.setdefault('Client-Vendor-ID', self.device_id)
        for k, v in self.additional_headers(headers, body).iteritems():
            headers.setdefault(k, v)
        headers = {k: v for k, v in headers.iteritems() if v is not None}
        return self.app.post(
            url_for(controller=self.CONTROLLER,
                    action=self.ACTIONS.get(action, action)),
            extra_environ={"REMOTE_ADDR": "1.2.3.4"},
            headers=headers,
            params=body,
            expect_errors=expect_errors,
        )

    def make_qs(self, **kw):
        """Convert the provided kw into a kw string suitable for app.post."""
        return query_string(kw).lstrip("?")

    def additional_headers(self, headers, body):
        """Additional generated headers to be added to the request."""
        return {}
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import contextlib

from r2.tests import RedditControllerTestCase
from mock import patch, MagicMock
from r2.lib.validator import VByName, VUser, VModhash

from r2.models import Link, Message, Account

from pylons import app_globals as g


class DelMsgTest(RedditControllerTestCase):
    CONTROLLER = "api"

    def setUp(self):
        super(DelMsgTest, self).setUp()

        self.id = 1

    def test_del_msg_success(self):
        """Del_msg succeeds: Returns 200 and sets del_on_recipient."""
        message = MagicMock(spec=Message)
        message.name = "msg_1"
        message.to_id = self.id
        message.del_on_recipient = False

        with self.mock_del_msg(message):
            res = self.do_del_msg(message.name)

            self.assertEqual(res.status, 200)
            self.assertTrue(message.del_on_recipient)

    def test_del_msg_failure_with_link(self):
        """Del_msg fails: Returns 200 and does not set del_on_recipient."""
        link = MagicMock(spec=Link)
        link.del_on_recipient = False
        link.name = "msg_2"

        with self.mock_del_msg(link):
            res = self.do_del_msg(link.name)

            self.assertEqual(res.status, 200)
            self.assertFalse(link.del_on_recipient)

    def test_del_msg_failure_with_null_msg(self):
        """Del_msg fails: Returns 200 and does not set del_on_recipient."""
        message = MagicMock(spec=Message)
        message.name = "msg_3"
        message.to_id = self.id
        message.del_on_recipient = False

        with self.mock_del_msg(message, False):
            res = self.do_del_msg(message.name)

            self.assertEqual(res.status, 200)
            self.assertFalse(message.del_on_recipient)

    def test_del_msg_failure_with_sender(self):
        """Del_msg fails: Returns 200 and does not set del_on_recipient."""
        message = MagicMock(spec=Message)
        message.name = "msg_3"
        message.to_id = self.id + 1
        message.del_on_recipient = False

        with self.mock_del_msg(message):
            res = self.do_del_msg(message.name)

            self.assertEqual(res.status, 200)
            self.assertFalse(message.del_on_recipient)

    def mock_del_msg(self, thing, ret=True):
        """Context manager for mocking del_msg."""

        return contextlib.nested(
            patch.object(VByName, "run", return_value=thing if ret else None),
            patch.object(VModhash, "run", side_effect=None),
            patch.object(VUser, "run", side_effect=None),
            patch.object(thing, "_commit", side_effect=None),
            patch.object(Account, "_id", self.id, create=True),
            patch.object(g.events, "message_event", side_effect=None),
        )

    def do_del_msg(self, name, **kw):
        return self.do_post("del_msg", {"id": name}, **kw)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from r2.tests import RedditControllerTestCase
from common import LoginRegBase


class LoginRegTests(LoginRegBase, RedditControllerTestCase):
    CONTROLLER = "api"

    def assert_success(self, res):
        self.assertEqual(res.status, 200)
        self.assertTrue("error" not in res)

    def assert_failure(self, res, code=None):
        self.assertEqual(res.status, 200)
        self.assertTrue("error" in res)
        self.assertTrue(code in res)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import contextlib
import unittest
import json
from mock import patch, MagicMock

from r2.lib import signing
from r2.tests import RedditControllerTestCase
from r2.lib.validator import VThrottledLogin, VUname
from common import LoginRegBase


class APIV1LoginTests(LoginRegBase, RedditControllerTestCase):
    CONTROLLER = "apiv1login"

    def setUp(self):
        super(APIV1LoginTests, self).setUp()
        self.device_id = "dead-beef"

    def make_ua_signature(self, platform="test", version=1):
        payload = "User-Agent:{}|Client-Vendor-ID:{}".format(
            self.user_agent, self.device_id,
        )
        return self.sign(payload, platform, version)

    def sign(self, payload, platform="test", version=1):
        return signing.sign_v1_message(payload, platform, version)

    def additional_headers(self, headers, body):
        return {
            signing.SIGNATURE_UA_HEADER: self.make_ua_signature(),
            signing.SIGNATURE_BODY_HEADER: self.sign("Body:" + body),
        }

    def assert_success(self, res):
        self.assertEqual(res.status, 200)
        body = res.body
        body = json.loads(body)
        self.assertTrue("json" in body)
        errors = body['json'].get("errors")
        self.assertEqual(len(errors), 0)
        data = body['json'].get("data")
        self.assertTrue(bool(data))
        self.assertTrue("modhash" in data)
        self.assertTrue("cookie" in data)

    def assert_failure(self, res, code=None):
        self.assertEqual(res.status, 200)
        body = res.body
        body = json.loads(body)
        self.assertTrue("json" in body)
        errors = body['json'].get("errors")
        self.assertTrue(code in [x[0] for x in errors])
        data = body['json'].get("data")
        self.assertFalse(bool(data))

    def assert_403_response(self, res, calling):
        self.assertEqual(res.status, 403)
        self.simple_event.assert_any_call(calling)
        self.assert_headers(
            res,
            "content-type",
            "application/json; charset=UTF-8",
        )

    def test_nosigning_login(self):
        res = self.do_login(
            headers={
                signing.SIGNATURE_UA_HEADER: None,
                signing.SIGNATURE_BODY_HEADER: None,
            },
            expect_errors=True,
        )
        self.assert_403_response(res, "signing.ua.invalid.invalid_format")

    def test_no_body_signing_login(self):
        res = self.do_login(
            headers={
                signing.SIGNATURE_BODY_HEADER: None,
            },
            expect_errors=True,
        )
        self.assert_403_response(res, "signing.body.invalid.invalid_format")

    def test_nosigning_register(self):
        res = self.do_register(
            headers={
                signing.SIGNATURE_UA_HEADER: None,
                signing.SIGNATURE_BODY_HEADER: None,
            },
            expect_errors=True,
        )
        self.assert_403_response(res, "signing.ua.invalid.invalid_format")

    def test_no_body_signing_register(self):
        res = self.do_login(
            headers={
                signing.SIGNATURE_BODY_HEADER: None,
            },
            expect_errors=True,
        )
        self.assert_403_response(res, "signing.body.invalid.invalid_format")

    @unittest.skip("registration captcha is unfinished")
    def test_captcha_blocking(self):
        with contextlib.nested(
            self.mock_register(),
            self.failed_captcha()
        ):
            res = self.do_register()
            self.assert_success(res)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################
import contextlib
import unittest
from mock import patch, MagicMock

from pylons import app_globals as g

from r2.lib.validator import VThrottledLogin, VUname, validator
from r2.models import Account, NotFound


class LoginRegBase(object):
    """Mixin for login-centered controller tests.

    This class is (purposely) not a test case that'll be picked up by nose
    but rather should be added as a mixin on a RedditControllerTestCase
    subclass. The subclass needs to implement

     * assert_success - passed a result of do_post, and invoked in places
       where we expect the request to have succeeded
     * assert_failure - same, for failed and error cases from the server.

    Included are base test cases that should be common to all controllers
    which use r2.lib.controlers.login as part of the flow.
    """
    def do_login(self, user="test", passwd="test123", **kw):
        return self.do_post("login", {"user": user, "passwd": passwd}, **kw)

    def do_register(
        self, user="test", passwd="test123", passwd2="test123", **kw
    ):
        return self.do_post("register", {
            "user": user,
            "passwd": passwd,
            "passwd2": passwd2,
        }, **kw)

    def mock_login(self, name="test", cookie="cookievaluehere"):
        """Context manager for mocking login.

        Patches VThrottledLogin to always return a mock with the provided
        name and cookie value
        """
        account = MagicMock()
        account.name = name
        account.make_cookie.return_value = cookie
        return patch.object(VThrottledLogin, "run", return_value=account)

    def mock_register(self):
        """Context manager for mocking out registration.

        Within this context, new users can be registered but they will
        be mock objects.  Also all usernames can be registered as the account
        lookup is bypassed and Account._by_name always raises NotFound.
        """
        from r2.controllers import login
        return contextlib.nested(
            patch.object(login, "register"),
            patch.object(VUname, "run", return_value="test"),
            # ensure this user does not currently exist
            patch.object(Account, "_by_name", side_effect=NotFound),
        )

    def failed_captcha(self):
        """Context manager for mocking a failed captcha."""
        return contextlib.nested(
            # ensure that a captcha is needed
            patch.object(
                validator,
                "need_provider_captcha",
                return_value=True,
            ),
            # ensure that the captcha is invalid
            patch.object(
                g.captcha_provider,
                "validate_captcha",
                return_value=False,
            ),
        )

    def disabled_captcha(self):
        """Context manager for mocking a disabled captcha.

        Will raise an AssertionError if the captcha code is called.
        """
        return contextlib.nested(
            # ensure that a captcha is not needed
            patch.object(
                validator,
                "need_provider_captcha",
                return_value=False,
            ),
            # ensure that the captcha is unused
            patch.object(
                g.captcha_provider,
                "validate_captcha",
                side_effect=AssertionError,
            ),
        )

    def find_headers(self, res, name):
        """Find header in res"""
        for k, v in res.headers:
            if k == name.lower():
                yield v

    def assert_headers(self, res, name, test):
        """Assert header value with test (lambda function or value)"""
        for value in self.find_headers(res, name):
            if callable(test) and test(value):
                return
            elif value == test:
                return
        raise AssertionError("No matching %s header found" % name)

    def assert_success(self, res):
        """Test that is run when we expect the post to succeed."""
        raise NotImplementedError

    def assert_failure(self, res, code=None):
        """Test that is run when we expect the post to fail."""
        raise NotImplementedError

    def test_login(self):
        with self.mock_login():
            res = self.do_login()
            self.assert_success(res)

    def test_login_wrong_password(self):
        with patch.object(Account, "_by_name", side_effect=NotFound):
            res = self.do_login()
            self.assert_failure(res, "WRONG_PASSWORD")

    def test_register(self):
        with self.mock_register():
            res = self.do_register()
            self.assert_success(res)

    def test_register_username_taken(self):
        with patch.object(
            Account, "_by_name", return_value=MagicMock(_deleted=False)
        ):
            res = self.do_register()
            self.assert_failure(res, "USERNAME_TAKEN")

    @unittest.skip("registration captcha is unfinished")
    def test_captcha_blocking(self):
        with contextlib.nested(
            self.mock_register(),
            self.failed_captcha()
        ):
            res = self.do_register()
            self.assert_failure(res, "BAD_CAPTCHA")

    @unittest.skip("registration captcha is unfinished")
    def test_captcha_disabling(self):
        with contextlib.nested(
            self.mock_register(),
            self.disabled_captcha()
        ):
            res = self.do_register()
            self.assert_success(res)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from r2.tests import RedditControllerTestCase
from r2.lib.errors import error_list
from r2.lib.unicode import _force_unicode
from r2.models import Subreddit
from common import LoginRegBase


class PostLoginRegTests(LoginRegBase, RedditControllerTestCase):
    CONTROLLER = "post"
    ACTIONS = {
        "register": "reg",
    }

    def setUp(self):
        super(PostLoginRegTests, self).setUp()
        self.autopatch(Subreddit, "_byID", return_value=[])
        self.dest = "/foo"

    def assert_success(self, res):
        # On sucess, we redirect the user to the provided "dest" parameter
        # that has been added in make_qs
        self.assertEqual(res.status, 302)
        self.assert_headers(
            res,
            "Location",
            lambda value: value.endswith(self.dest)
        )
        self.assert_headers(
            res,
            "Set-Cookie",
            lambda value: value.startswith("reddit_session=")
        )

    def assert_failure(self, res, code=None):
        # counterintuitively, failure to login will return a 200
        # (compared to a redirect).
        self.assertEqual(res.status, 200)
        # recaptcha is done entirely in JS
        if code != "BAD_CAPTCHA":
            self.assertTrue(error_list[code] in _force_unicode(res.body))

    def make_qs(self, **kw):
        kw['dest'] = self.dest
        return super(PostLoginRegTests, self).make_qs(**kw)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################
import unittest
from mock import patch

import pylibmc
from pylons import app_globals as g

from r2.lib import ratelimit
from r2.lib.cache import LocalCache


class RateLimitStandaloneFunctionsTest(unittest.TestCase):
    def setUp(self):
        self.patch('ratelimit.time.time', lambda: self.now)

        self.cache = LocalCache()
        self.patch('ratelimit.g.ratelimitcache', self.cache)

    def patch(self, *a, **kw):
        p = patch(*a,  **kw)
        p.start()
        self.addCleanup(p.stop)

    def test_get_timeslice(self):
        self.now = 125
        ts = ratelimit.get_timeslice(60)
        self.assertEquals(120, ts.beginning)
        self.assertEquals(180, ts.end)
        self.assertEquals(55, ts.remaining)

    def test_make_ratelimit_cache_key_1s(self):
        self.now = 14
        ts = ratelimit.get_timeslice(1)
        key = ratelimit._make_ratelimit_cache_key('a', ts)
        self.assertEquals('rl:a-000014', key)

    def test_make_ratelimit_cache_key_1m(self):
        self.now = 65
        ts = ratelimit.get_timeslice(60)
        key = ratelimit._make_ratelimit_cache_key('a', ts)
        self.assertEquals('rl:a-000100', key)

    def test_make_ratelimit_cache_key_1h(self):
        self.now = 3650
        ts = ratelimit.get_timeslice(3600)
        key = ratelimit._make_ratelimit_cache_key('a', ts)
        self.assertEquals('rl:a-010000', key)

    def test_make_ratelimit_cache_key_1d(self):
        self.now = 24 * 3600 + 5
        ts = ratelimit.get_timeslice(24 * 3600)
        key = ratelimit._make_ratelimit_cache_key('a', ts)
        self.assertEquals('rl:a-@86400', key)

    def test_make_ratelimit_cache_key_1w(self):
        self.now = 7 * 24 * 3600 + 5
        ts = ratelimit.get_timeslice(24 * 3600)
        key = ratelimit._make_ratelimit_cache_key('a', ts)
        self.assertEquals('rl:a-@604800', key)

    def test_record_usage(self):
        self.now = 24 * 3600 + 5
        ts = ratelimit.get_timeslice(3600)
        ratelimit.record_usage('a', ts)
        self.assertEquals(1, self.cache['rl:a-000000'])
        ratelimit.record_usage('a', ts)
        self.assertEquals(2, self.cache['rl:a-000000'])

        self.now = 24 * 3600 + 5 * 3600
        ts = ratelimit.get_timeslice(3600)
        ratelimit.record_usage('a', ts)
        self.assertEquals(1, self.cache['rl:a-050000'])

    def test_record_usage_across_slice_expiration(self):
        self.now = 24 * 3600 + 5
        ts = ratelimit.get_timeslice(3600)
        real_incr = self.cache.incr
        evicted = False

        def fake_incr(key):
            if evicted:
                del self.cache[key]
                raise pylibmc.NotFound()
            return real_incr(key)

        with patch.object(self.cache, 'incr', fake_incr):
            # Forcibly evict the key before incr() is called, but after the
            # initial add() call inside record_usage().
            evicted = True
            ratelimit.record_usage('a', ts)
            self.assertEquals(1, self.cache['rl:a-000000'])

    def test_get_usage(self):
        self.now = 24 * 3600 + 5 * 3600
        ts = ratelimit.get_timeslice(3600)
        self.assertEquals(None, ratelimit.get_usage('a', ts))
        ratelimit.record_usage('a', ts)
        self.assertEquals(1, ratelimit.get_usage('a', ts))


class RateLimitTest(unittest.TestCase):
    class TestRateLimit(ratelimit.RateLimit):
        event_name = 'TestRateLimit'
        event_type = 'tests'
        key = 'tests'
        limit = 1
        seconds = 3600

    def setUp(self):
        self.patch('ratelimit.time.time', lambda: self.now)

        self.cache = LocalCache()
        self.patch('ratelimit.g.ratelimitcache', self.cache)

    def patch(self, *a, **kw):
        p = patch(*a,  **kw)
        p.start()
        self.addCleanup(p.stop)

    def test_record_usage(self):
        rl = self.TestRateLimit()

        self.now = 24 * 3600 + 5
        rl.record_usage()
        self.assertEquals(1, self.cache['rl:tests-000000'])
        rl.record_usage()
        self.assertEquals(2, self.cache['rl:tests-000000'])

        self.now = 24 * 3600 + 5 * 3600
        rl.record_usage()
        self.assertEquals(1, self.cache['rl:tests-050000'])

    def test_get_usage(self):
        rl = self.TestRateLimit()
        self.now = 24 * 3600 + 5 * 3600
        self.assertTrue(rl.check())
        rl.record_usage()
        self.assertFalse(rl.check())


class LiveConfigRateLimitTest(unittest.TestCase):
    class TestRateLimit(ratelimit.LiveConfigRateLimit):
        event_name = 'TestRateLimit'
        event_type = 'tests'
        key = 'rl-tests'
        limit_live_key = 'RL_TESTS'
        seconds_live_key = 'RL_TESTS_RESET_SECS'

    def patch_liveconfig(self, k, v):
        """Helper method to patch g.live_config (with cleanup)."""
        def cleanup(orig=g.live_config.get(k), has=k in g.live_config):
            if has:
                g.live_config[k] = orig
            else:
                del g.live_config[k]
        g.live_config[k] = v
        self.addCleanup(cleanup)

    def configure_rate_limit(self, num, per_unit):
        self.patch_liveconfig('RL_TESTS', num)
        self.patch_liveconfig('RL_TESTS_RESET_SECS', per_unit)

    def test_limit(self):
        self.configure_rate_limit(1, 3600)
        rl = self.TestRateLimit()
        self.assertEquals(1, rl.limit)

        self.configure_rate_limit(2, 3600)
        self.assertEquals(2, rl.limit)

    def test_seconds(self):
        self.configure_rate_limit(1, 3600)
        rl = self.TestRateLimit()
        self.assertEquals(3600, rl.seconds)

        self.configure_rate_limit(1, 300)
        self.assertEquals(300, rl.seconds)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from mock import MagicMock, patch

from r2.models.link import Link, Comment

TINY_COMMENT = 'rekt'
SHORT_COMMENT = 'What is your favorite car from a rival brand?'
MEDIUM_COMMENT = '''I'm humbled by how many of you were interested in talking
to me and hearing about what we're doing with autonomous driving. When I signed
off last night, I never thought there would be so many more questions than what
I was able to answer after I left. I wish I had had more time last night, but
I'm answering more questions today.'''
LONG_COMMENT = '''That was a really good question then and still is and gets
back to something I said earlier about the difference between autonomous drive
vehicles, in which the driver remains in control, and self-driving cars, in
which a driver is not even required.
The focus of our R&D right now is on autonomous drive vehicles that enhance the
driving experience, by giving the driver the option of letting the car handle
some functions and by improving the car's ability to avoid accidents. That
technology exists already and will be rolled out in phases over the next
several years.
We are not dismissive of self-driving vehicles, but the reality is the timeline
for them is much further out into the future.'''

class CommentMock(Comment):
    """This class exists to allow us to call the _qa() method on Comments
    without having to dick around with everything else they support."""
    _nodb = True

    def __init__(self, ups=1, downs=0, body=TINY_COMMENT, author_id=None):
        self._ups = ups
        self._downs = downs
        self.body = body
        self.author_id = author_id

    def __setattr__(self, attr, val):
        self.__dict__[attr] = val

    def __getattr__(self, attr):
        return self.attr


class TestCommentQaSort(unittest.TestCase):
    def test_simple_upvotes(self):
        """All else equal, do upvoted comments score better?"""
        no_upvotes = CommentMock(ups=1)
        one_upvote = CommentMock(ups=2)
        many_upvotes = CommentMock(ups=50)
        no_upvotes_score = no_upvotes._qa((), ())
        one_upvote_score = one_upvote._qa((), ())
        many_upvotes_score = many_upvotes._qa((), ())

        self.assertLess(no_upvotes_score, one_upvote_score)
        self.assertLess(one_upvote_score, many_upvotes_score)

    def test_simple_downvotes(self):
        """All else equal, do downvoted comments score worse?"""
        no_downvotes = CommentMock(downs=0)
        one_downvote = CommentMock(downs=1)
        many_downvotes = CommentMock(downs=50)
        no_downvotes_score = no_downvotes._qa((), ())
        one_downvote_score = one_downvote._qa((), ())
        many_downvotes_score = many_downvotes._qa((), ())

        self.assertGreater(no_downvotes_score, one_downvote_score)
        self.assertGreater(one_downvote_score, many_downvotes_score)

    def test_simple_length(self):
        """All else equal, do longer comments score better?"""
        tiny = CommentMock(body=TINY_COMMENT)
        short = CommentMock(body=SHORT_COMMENT)
        medium = CommentMock(body=MEDIUM_COMMENT)
        long = CommentMock(body=LONG_COMMENT)
        tiny_score = tiny._qa((), ())
        short_score = short._qa((), ())
        medium_score = medium._qa((), ())
        long_score = long._qa((), ())

        self.assertLess(tiny_score, short_score)
        self.assertLess(short_score, medium_score)
        self.assertLess(medium_score, long_score)

    def test_simple_op_responses(self):
        """All else equal, do OP answers bump up the score of comments?"""
        question = CommentMock()
        answer = CommentMock(author_id=1)
        no_answer_score = question._qa((), ())
        no_op_answer_score = question._qa((answer,), (2,))
        with_op_answer_score = question._qa((answer,), (1,))

        self.assertEqual(no_answer_score, no_op_answer_score)
        self.assertLess(no_op_answer_score, with_op_answer_score)

    def test_multiple_op_responses(self):
        """What effect do multiple OP responses have on a comment's score?"""
        question = CommentMock()
        op_answer = CommentMock(author_id=1)
        another_op_answer = CommentMock(author_id=1)
        one_answer_score = question._qa((op_answer,), (1,))
        two_answers_score = question._qa((op_answer, another_op_answer), (1,))

        self.assertEqual(one_answer_score, two_answers_score)

        bad_op_answer = CommentMock(ups=0, author_id=1)
        good_op_answer = CommentMock(ups=30, author_id=1)
        good_op_answer_score = question._qa((good_op_answer,), (1,))
        op_answers = (bad_op_answer, good_op_answer, op_answer)
        three_op_answers_score = question._qa(op_answers, (1,))

        # Are we basing the score on the highest-scoring OP answer?
        self.assertEqual(good_op_answer_score, three_op_answers_score)

    def test_simple_op_comments(self):
        """All else equal, do comments from OP score better?"""
        comment = CommentMock(author_id=1)
        op_score = comment._qa((), (1,))
        non_op_score = comment._qa((), (2,))

        self.assertLess(non_op_score, op_score)


ID = 0
def _mock_id(instance):
    if not getattr(instance, "__id", False):
        instance.__id = ID + 1
    return instance.__id


class LinkMock(Link):
    _nodb = True

    def __init__(self, **kwargs):
        for key, value in kwargs.iteritems():
            setattr(self, key, value)

    def __setattr__(self, attr, val):
        self.__dict__[attr] = val

    def __getattr__(self, attr):
        return getattr(
            self.__dict__,
            attr,
            getattr(self._defaults, attr, None),
        )

    @property
    def _id(self):
        return _mock_id(self)

    @property
    def subreddit_slow(self):
        return SubredditMock()

    def _commit(self):
        pass

    @classmethod
    @patch('r2.lib.voting.cast_vote')
    def _submit(cls, cast_vote, *args, **kwargs):
        """A _submit that mocks calls we don't care about testing."""
        return super(LinkMock, cls)._submit(*args, **kwargs)


class ThingMock():
    _nodb = True

    @property
    def _id(self):
        return _mock_id(self)


class AccountMock(ThingMock):
    @property
    def _spam(self):
        return False

    def _commit(self, *a, **kw):
        pass


class SubredditMock(ThingMock):
    @property
    def lang(self):
        return "en"

    @property
    def name(self):
        return "linktests"

    @property
    def spam_selfposts(self):
        return "high"

    @property
    def spam_links(self):
        return "low"


class TestSubmit(unittest.TestCase):
    def setUp(self):
        from r2.models import (
            LinksByAccount,
            LinksByUrlAndSubreddit,
            SubredditParticipationByAccount,
            SubredditsActiveForFrontPage,
        )

        LinksByAccount.add_link = MagicMock()
        SubredditParticipationByAccount.mark_participated = MagicMock()
        SubredditsActiveForFrontPage.mark_new_post = MagicMock()

        self.links_by_url_add_link = MagicMock()
        LinksByUrlAndSubreddit.add_link = self.links_by_url_add_link
        self.links_by_url_remove_link = MagicMock()
        LinksByUrlAndSubreddit.remove_link = self.links_by_url_remove_link

    def test_new_self_post_has_url(self):
        l = LinkMock._submit(
            is_self=True,
            content="this is a self post",
            title="test post",
            ip="127.0.0.1",
            sr=SubredditMock(),
            author=AccountMock()
        )

        self.assertEqual(l.url, u"/r/linktests/comments/%s/test_post/" % l._id36)

    def test_new_self_post_doesnt_modify_links_by_url(self):
        l = LinkMock._submit(
            is_self=True,
            content="this is a self post",
            title="test post",
            ip="127.0.0.1",
            sr=SubredditMock(),
            author=AccountMock()
        )

        self.assertEqual(self.links_by_url_add_link.call_count, 0)
        self.assertEqual(self.links_by_url_remove_link.call_count, 0)

    def test_changing_non_promo_fails(self):
        l = LinkMock._submit(
            is_self=True,
            content="this is a self post",
            title="test post",
            ip="127.0.0.1",
            sr=SubredditMock(),
            author=AccountMock()
        )

        with self.assertRaises(ValueError):
            l.set_content(False, "http://test.com/1")

    def test_changing_from_self_doesnt_remove_links_by_url(self):
        l = LinkMock._submit(
            is_self=True,
            content="this is a self post",
            title="test post",
            ip="127.0.0.1",
            sr=SubredditMock(),
            author=AccountMock()
        )
        l.promoted = True

        url = "http://test.com/1"

        self.assertEqual(self.links_by_url_add_link.call_count, 0)
        self.assertEqual(self.links_by_url_remove_link.call_count, 0)

        l.set_content(False, url)

        self.assertEqual(l.url, url)
        self.assertEqual(self.links_by_url_remove_link.call_count, 0)

    def test_changing_url_adds_links_by_url(self):
        url1 = "http://test.com/1"
        url2 = "http://test.com/2"
        l = LinkMock._submit(
            is_self=False,
            content=url1,
            title="test post",
            ip="127.0.0.1",
            sr=SubredditMock(),
            author=AccountMock()
        )
        l.promoted = True

        self.assertEqual(self.links_by_url_add_link.call_count, 1)
        self.assertEqual(self.links_by_url_remove_link.call_count, 0)

        l.set_content(False, url2)

        self.assertEqual(self.links_by_url_add_link.call_count, 2)
        self.assertEqual(self.links_by_url_remove_link.call_count, 1)

    def test_changing_to_self_removes_links_by_url(self):
        url = "http://test.com/1"
        l = LinkMock._submit(
            is_self=False,
            content=url,
            title="test post",
            ip="127.0.0.1",
            sr=SubredditMock(),
            author=AccountMock()
        )
        l.promoted = True

        self.assertEqual(self.links_by_url_add_link.call_count, 1)

        l.set_content(True, "change to self post")

        self.assertEqual(self.links_by_url_remove_link.call_count, 1)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import namedtuple, defaultdict
from mock import MagicMock

from r2.lib.utils.comment_tree_utils import get_tree_details, calc_num_children
from r2.lib.db import operators
from r2.models import builder
from r2.models import Comment
from r2.models.builder import CommentBuilder
from r2.models.comment_tree import CommentTree
from r2.tests import RedditTestCase


CommentTreeElement = namedtuple(
    "CommentTreeElement", ["id", "score", "children"])


TREE = [
    CommentTreeElement(id=100, score=100, children=[
        CommentTreeElement(id=101, score=90, children=[]),
        CommentTreeElement(id=102, score=80, children=[
            CommentTreeElement(id=104, score=95, children=[]),
            CommentTreeElement(id=105, score=85, children=[]),
            CommentTreeElement(id=106, score=75, children = []),
        ]),
        CommentTreeElement(id=103, score=70, children=[]),
    ]),
    CommentTreeElement(id=107, score=60, children=[]),
    CommentTreeElement(id=108, score=55, children=[
        CommentTreeElement(id=110, score=110, children=[]),
    ]),
    CommentTreeElement(id=109, score=50, children=[]),
]


DISTINGUISHES = {
    104: "yes",
}

AUTHOR_IDS = {
    100: "a",
    101: "b",
    102: "c",
    103: "d",
    104: "e",
    105: "f",
    106: "g",
    107: "h",
    108: "i",
    109: "j",
    110: "k",
}


def make_comment_tree(link):
    tree = {}

    def _add_comment(comment, parent):
        tree[comment.id] = [child.id for child in comment.children]
        for child in comment.children:
            _add_comment(child, parent=comment)

    tree[None] = [comment.id for comment in TREE]

    for comment in TREE:
        _add_comment(comment, parent=None)

    cids, depth, parents = get_tree_details(tree)
    num_children = calc_num_children(tree)
    num_children = defaultdict(int, num_children)

    return CommentTree(link, cids, tree, depth, parents, num_children)


def make_comment_scores():
    scores_by_id = {}

    def _add_comment(comment):
        scores_by_id[comment.id] = comment.score
        for child in comment.children:
            _add_comment(child)

    for comment in TREE:
        _add_comment(comment)

    return scores_by_id


FakeComment = namedtuple(
    "Comment", ["parent_id", "author_id", "distinguished"])

def comments_by_id():
    comment_tree = make_comment_tree(None)
    ret = {}

    for comment_id in comment_tree.cids:
        parent_id = comment_tree.parents[comment_id]
        author_id = AUTHOR_IDS[comment_id]
        distinguished = DISTINGUISHES.get(comment_id, "no")
        ret[comment_id] = FakeComment(parent_id, author_id, distinguished)

    return ret


class CommentOrderTest(RedditTestCase):
    def setUp(self):
        self.link = MagicMock()
        self.link._id = 1000
        self.link.sticky_comment_id = None
        self.link.precomputed_sorts = None

        comment_scores = make_comment_scores()
        self.autopatch(
            builder, "get_comment_scores", return_value=comment_scores)

        comment_tree_for_link = make_comment_tree(self.link)
        self.autopatch(
            CommentTree, "by_link", return_value=comment_tree_for_link)

        fake_comments = comments_by_id()
        self.autopatch(
            Comment, "_byID", return_value=fake_comments)

    def tearDown(self):
        self.link = None

    def test_comment_order_full(self):
        sort = operators.desc("_confidence")
        builder = CommentBuilder(self.link, sort, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order,
            [100, 101, 102, 104, 105, 106, 103, 107, 108, 110, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_full_asc(self):
        sort = operators.asc("_confidence")
        builder = CommentBuilder(self.link, sort, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order,
            [109, 108, 107, 100, 103, 102, 106, 105, 101, 104, 110])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_limit(self):
        sort = operators.desc("_confidence")
        builder = CommentBuilder(self.link, sort, num=5)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [100, 101, 102, 104, 105])
        self.assertEqual(builder.missing_root_comments, {107, 108, 109})
        self.assertEqual(builder.missing_root_count, 4)

    def test_comment_order_depth(self):
        sort = operators.desc("_confidence")
        builder = CommentBuilder(self.link, sort, num=1500, max_depth=1)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [100, 107, 108, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_sticky(self):
        self.link.sticky_comment_id = 100
        sort = operators.desc("_confidence")
        builder = CommentBuilder(self.link, sort, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [100, 107, 108, 110, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_invalid_sticky(self):
        self.link.sticky_comment_id = 101
        sort = operators.desc("_confidence")
        builder = CommentBuilder(self.link, sort, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order,
            [100, 101, 102, 104, 105, 106, 103, 107, 108, 110, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_permalink(self):
        sort = operators.desc("_confidence")
        comment = MagicMock()
        comment._id = 100
        builder = CommentBuilder(self.link, sort, comment=comment, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [100, 101, 102, 104, 105, 106, 103])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_permalink_context(self):
        sort = operators.desc("_confidence")
        comment = MagicMock()
        comment._id = 104
        builder = CommentBuilder(
            self.link, sort, comment=comment, context=3, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [100, 102, 104])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_invalid_permalink_defocus(self):
        sort = operators.desc("_confidence")
        comment = MagicMock()
        comment._id = 999999
        builder = CommentBuilder(self.link, sort, comment=comment, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order,
            [100, 101, 102, 104, 105, 106, 103, 107, 108, 110, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_children(self):
        sort = operators.desc("_confidence")
        builder = CommentBuilder(
            self.link, sort, children=[101, 102, 103], num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [101, 102, 104, 105, 106, 103])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_children_limit(self):
        sort = operators.desc("_confidence")
        builder = CommentBuilder(
            self.link, sort, children=[107, 108, 109], num=3)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [107, 108, 110])
        self.assertEqual(builder.missing_root_comments, {109})
        self.assertEqual(builder.missing_root_count, 1)

    def test_comment_order_children_limit_bug(self):
        sort = operators.desc("_confidence")
        builder = CommentBuilder(
            self.link, sort, children=[101, 102, 103], num=3)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order, [101, 102, 104])
        # missing_root_comments SHOULD be {103}, but there's a bug here.
        # if the requested children are not root level but we don't show some
        # of them we should add a MoreChildren to allow a subsequent request
        # to get the missing comments.
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_qa(self):
        self.link.responder_ids = ("c",)
        sort = operators.desc("_qa")
        builder = CommentBuilder(self.link, sort, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order,
            [100, 102, 104, 105, 106, 107, 108, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)

    def test_comment_order_qa_multiple_responders(self):
        self.link.responder_ids = ("c", "d", "e")
        sort = operators.desc("_qa")
        builder = CommentBuilder(self.link, sort, num=1500)
        builder.load_comment_order()
        comment_order = [
            comment_tuple.comment_id
            for comment_tuple in builder.ordered_comment_tuples
        ]
        self.assertEqual(comment_order,
            [100, 102, 104, 105, 106, 103, 107, 108, 109])
        self.assertEqual(builder.missing_root_comments, set())
        self.assertEqual(builder.missing_root_count, 0)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from mock import MagicMock, patch

from r2.lib.db.thing import (
    CreationError,
    hooks,
    NotFound,
    tdb,
    Thing,
)
from r2.lib.lock import TimeoutExpired
from r2.tests import RedditTestCase


class SimpleThing(Thing):
    _nodb = True
    _type_name = "simplething"
    _type_id = 100
    _cache = MagicMock()
    _data_int_props = ("prop_for_data",)
    _defaults = {
        "prop_for_data": 0,
    }


class TestThingReadCaching(RedditTestCase):
    def setUp(self):
        self.get_things_from_cache = self.autopatch(Thing, "get_things_from_cache")
        self.get_things_from_db = self.autopatch(Thing, "get_things_from_db")
        self.write_things_to_cache = self.autopatch(Thing, "write_things_to_cache")

    def test_not_found(self):
        things_by_id = {}

        self.get_things_from_cache.return_value = {}
        self.get_things_from_db.return_value = things_by_id

        with self.assertRaises(NotFound):
            SimpleThing._byID([1, 2, 3], stale=False)

        self.get_things_from_cache.assert_called_once_with([1, 2, 3], stale=False)
        self.get_things_from_db.assert_called_once_with([1, 2, 3])
        self.write_things_to_cache.assert_not_called()

    def test_partial_not_found(self):
        things_by_id = {
            1: "one",
        }

        self.get_things_from_cache.return_value = {}
        self.get_things_from_db.return_value = things_by_id

        with self.assertRaises(NotFound):
            SimpleThing._byID([1, 2, 3], stale=False)

        self.get_things_from_cache.assert_called_once_with([1, 2, 3], stale=False)
        self.get_things_from_db.assert_called_once_with([1, 2, 3])
        self.write_things_to_cache.assert_called_once_with(things_by_id)

    def test_partial_not_found_ignore(self):
        things_by_id = {
            1: "one",
        }

        self.get_things_from_cache.return_value = {}
        self.get_things_from_db.return_value = things_by_id

        ret = SimpleThing._byID([1, 2, 3], stale=False, ignore_missing=True)
        self.get_things_from_cache.assert_called_once_with([1, 2, 3], stale=False)
        self.get_things_from_db.assert_called_once_with([1, 2, 3])
        self.write_things_to_cache.assert_called_once_with(things_by_id)
        self.assertEqual(ret, things_by_id)

    def test_cache_miss(self):
        things_by_id = {
            1: "one",
            2: "two",
            3: "three",
        }

        self.get_things_from_cache.return_value = {}
        self.get_things_from_db.return_value = things_by_id

        ret = SimpleThing._byID([1, 2, 3], stale=False)
        self.get_things_from_cache.assert_called_once_with(
            [1, 2, 3], stale=False)
        self.get_things_from_db.assert_called_once_with([1, 2, 3])
        self.write_things_to_cache.assert_called_once_with(things_by_id)
        self.assertEqual(ret, things_by_id)

    def test_cache_hit(self):
        things_by_id = {
            1: "one",
            2: "two",
            3: "three",
        }

        self.get_things_from_cache.return_value = things_by_id

        ret = SimpleThing._byID([1, 2, 3], stale=False)
        self.get_things_from_cache.assert_called_once_with([1, 2, 3], stale=False)
        self.get_things_from_db.assert_not_called()
        self.write_things_to_cache.assert_not_called()
        self.assertEqual(ret, things_by_id)

    def test_partial_hit(self):
        things_by_id = {
            1: "one",
            2: "two",
            3: "three",
        }

        self.get_things_from_cache.return_value = {1: "one"}
        self.get_things_from_db.return_value = {2: "two", 3: "three"}

        ret = SimpleThing._byID([1, 2, 3], stale=False)
        self.get_things_from_cache.assert_called_once_with([1, 2, 3], stale=False)
        self.get_things_from_db.assert_called_once_with([2, 3])
        self.write_things_to_cache.assert_called_once_with({2: "two", 3: "three"})
        self.assertEqual(ret, things_by_id)

    def test_return_list(self):
        things_by_id = {
            1: "one",
            2: "two",
            3: "three",
        }

        self.get_things_from_cache.return_value = things_by_id
        self.get_things_from_db.return_value = things_by_id

        ret = SimpleThing._byID([1, 2, 3], stale=False, return_dict=False)
        self.get_things_from_cache.assert_called_once_with([1, 2, 3], stale=False)
        self.get_things_from_db.assert_not_called()
        self.write_things_to_cache.assert_not_called()
        self.assertEqual(ret, ["one", "two", "three"])

    def test_single_not_found(self):
        things_by_id = {}

        self.get_things_from_cache.return_value = {}
        self.get_things_from_db.return_value = {}

        with self.assertRaises(NotFound):
            SimpleThing._byID(1, stale=False)

        self.get_things_from_cache.assert_called_once_with((1,), stale=False)
        self.get_things_from_db.assert_called_once_with([1])
        self.write_things_to_cache.assert_not_called()

    def test_single_miss(self):
        things_by_id = {
            1: "one",
        }

        self.get_things_from_cache.return_value = {}
        self.get_things_from_db.return_value = things_by_id

        ret = SimpleThing._byID(1, stale=False)
        self.get_things_from_cache.assert_called_once_with((1,), stale=False)
        self.get_things_from_db.assert_called_once_with([1])
        self.write_things_to_cache.assert_called_once_with(things_by_id)
        self.assertEqual(ret, "one")

    def test_single_hit(self):
        things_by_id = {
            1: "one",
        }

        self.get_things_from_cache.return_value = things_by_id
        self.get_things_from_db.return_value = things_by_id

        ret = SimpleThing._byID(1, stale=False)
        self.get_things_from_cache.assert_called_once_with((1,), stale=False)
        self.get_things_from_db.assert_not_called()
        self.write_things_to_cache.assert_not_called()
        self.assertEqual(ret, "one")


class FakeLock(object):
    def __init__(self):
        self.have_lock = True

    def acquire(self):
        return

    def release(self):
        return

    def __enter__(self):
        return self

    def __exit__(self, *args):
        return


class TestThingWrite(RedditTestCase):
    def setUp(self):
        self.lock = FakeLock()
        self.thing_id = 333

        self.autopatch(tdb, "transactions")
        self.autopatch(hooks, "get_hook")
        self.autopatch(Thing, "write_new_thing_to_db", return_value=self.thing_id)
        self.autopatch(Thing, "get_read_modify_write_lock", return_value=self.lock)
        self.autopatch(Thing, "write_props_to_db")
        self.autopatch(Thing, "write_thing_to_cache")
        self.autopatch(Thing, "update_from_cache")

    def reset_mocks(self):
        SimpleThing.write_new_thing_to_db.reset_mock()
        SimpleThing.get_read_modify_write_lock.reset_mock()
        SimpleThing.update_from_cache.reset_mock()
        SimpleThing.write_props_to_db.reset_mock()
        SimpleThing.write_thing_to_cache.reset_mock()

    def test_create(self):
        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing.other_prop = 100
        thing._commit()

        SimpleThing.write_new_thing_to_db.assert_called_once_with()
        SimpleThing.update_from_cache.assert_not_called()
        SimpleThing.write_props_to_db.assert_called_once_with({}, {'other_prop': 100}, True)
        SimpleThing.write_thing_to_cache.assert_called_once_with(lock=None, brand_new_thing=True)

    def test_modify(self):
        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing.other_prop = 100
        thing._commit()

        self.reset_mocks()

        thing.other_prop = 101
        thing._ups = 12
        thing._commit()

        SimpleThing.write_new_thing_to_db.assert_not_called()
        SimpleThing.get_read_modify_write_lock.assert_called_once_with()
        SimpleThing.update_from_cache.assert_called_once_with(self.lock)
        SimpleThing.write_props_to_db.assert_called_once_with({'ups': 12}, {'other_prop': 101}, False)
        SimpleThing.write_thing_to_cache.assert_called_once_with(self.lock)


class TestThingIncr(RedditTestCase):
    def setUp(self):
        self.lock = FakeLock()
        self.thing_id = 333

        self.autopatch(tdb, "transactions")
        self.autopatch(hooks, "get_hook")
        self.autopatch(Thing, "write_new_thing_to_db", return_value=self.thing_id)
        self.autopatch(Thing, "get_read_modify_write_lock", return_value=self.lock)
        self.autopatch(Thing, "write_props_to_db")
        self.autopatch(Thing, "write_thing_to_cache")
        self.autopatch(Thing, "update_from_cache")

    def reset_mocks(self):
        SimpleThing.write_new_thing_to_db.reset_mock()
        SimpleThing.get_read_modify_write_lock.reset_mock()
        SimpleThing.update_from_cache.reset_mock()
        SimpleThing.write_props_to_db.reset_mock()
        SimpleThing.write_thing_to_cache.reset_mock()

    @patch("r2.lib.db.tdb_sql.incr_thing_prop")
    def test_incr_base_prop(self, incr_thing_prop):
        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing._commit()

        self.reset_mocks()

        thing._incr("_ups")
        incr_thing_prop.assert_called_once_with(
            type_id=SimpleThing._type_id,
            thing_id=thing._id,
            prop="ups",
            amount=1,
        )

    @patch("r2.lib.db.tdb_sql.incr_thing_data")
    def test_incr_data_prop(self, incr_thing_data):
        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing.prop_for_data = 100
        thing._commit()

        self.reset_mocks()

        thing._incr("prop_for_data")
        incr_thing_data.assert_called_once_with(
            type_id=SimpleThing._type_id,
            thing_id=thing._id,
            prop="prop_for_data",
            amount=1,
        )

    @patch("r2.lib.db.tdb_sql.set_thing_data")
    def test_incr_unset_data_prop(self, set_thing_data):
        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing._commit()

        self.reset_mocks()

        thing._incr("prop_for_data")
        set_thing_data.assert_called_once_with(
            type_id=SimpleThing._type_id,
            thing_id=thing._id,
            brand_new_thing=False,
            prop_for_data=1,
        )

    def test_incr_dirty(self):
        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing._commit()

        self.reset_mocks()

        thing.other_prop = 100

        with self.assertRaises(AssertionError):
            thing._incr("_ups")


class TestThingWriteConflict(RedditTestCase):
    def setUp(self):
        self.lock = FakeLock()
        self.thing_id = 333

        self.autopatch(tdb, "transactions")
        self.autopatch(hooks, "get_hook")
        self.autopatch(Thing, "write_new_thing_to_db", return_value=self.thing_id)
        self.autopatch(Thing, "get_read_modify_write_lock", return_value=self.lock)
        self.autopatch(Thing, "write_props_to_db")
        self.autopatch(Thing, "write_thing_to_cache")

    def reset_mocks(self):
        SimpleThing.write_new_thing_to_db.reset_mock()
        SimpleThing.get_read_modify_write_lock.reset_mock()
        SimpleThing.write_props_to_db.reset_mock()
        SimpleThing.write_thing_to_cache.reset_mock()

    @patch("r2.lib.db.thing.Thing.get_things_from_cache")
    def test_dont_overwrite(self, get_things_from_cache):
        other_thing = SimpleThing(
            ups=2,
            downs=0,
            spam=False,
            deleted=False,
            id=self.thing_id,
        )
        other_thing.__setattr__("another_prop", 3, make_dirty=False)
        get_things_from_cache.return_value = {self.thing_id: other_thing}

        thing = SimpleThing(
            ups=1,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing.other_prop = 100
        thing._commit()

        self.reset_mocks()

        thing._downs = 1
        thing.other_prop = 102
        thing._commit()

        SimpleThing.write_new_thing_to_db.assert_not_called()
        SimpleThing.get_read_modify_write_lock.assert_called_once_with()
        get_things_from_cache.assert_called_once_with([self.thing_id], allow_local=False)
        SimpleThing.write_props_to_db.assert_called_once_with({'downs': 1}, {'other_prop': 102}, False)
        SimpleThing.write_thing_to_cache.assert_called_once_with(self.lock)
        self.assertEqual(thing.another_prop, 3)

    @patch("r2.lib.db.thing.Thing.get_read_modify_write_lock")
    def test_lock_fail(self, get_read_modify_write_lock):
        get_read_modify_write_lock.side_effect = TimeoutExpired()

        thing = SimpleThing(
            ups=2,
            downs=0,
            spam=False,
            deleted=False,
            id=self.thing_id,
        )

        self.reset_mocks()

        thing._ups = 3

        with self.assertRaises(TimeoutExpired):
            thing._commit()

        tdb.transactions.rollback.assert_not_called()

    @patch("r2.lib.db.thing.Thing.write_new_thing_to_db")
    def test_create_fail(self, write_new_thing_to_db):
        write_new_thing_to_db.side_effect = CreationError()

        thing = SimpleThing(
            ups=2,
            downs=0,
            spam=False,
            deleted=False,
        )
        thing.other_prop = 13

        with self.assertRaises(CreationError):
            thing._commit()

        tdb.transactions.rollback.assert_called_once_with()

    @patch("r2.lib.db.thing.Thing.write_changes_to_db")
    def test_modify_fail(self, write_changes_to_db):
        write_changes_to_db.side_effect = CreationError()

        thing = SimpleThing(
            ups=2,
            downs=0,
            spam=False,
            deleted=False,
            id=self.thing_id,
        )

        self.reset_mocks()

        thing._ups = 3

        with self.assertRaises(CreationError):
            thing._commit()

        tdb.transactions.rollback.assert_called_once_with()
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from random import shuffle
from mock import MagicMock, Mock

from r2.models import Collection, CollectionStorage
from r2.tests import RedditTestCase


class CollectionStorageTest(RedditTestCase):

    def setUp(self):
        self.name = 'fake name'

    def test_set_attributes(self):
        """Assert that _set_attributes properly handles invalid attributes"""
        attribute_error_message = 'No attribute on %s called %s'
        invalid_attribute = {'invalid_attribute': None}
        valid_attribute = {'is_spotlight': None}

        collection = Collection(name=self.name, sr_names=[])

        Collection.by_name = Mock()
        Collection.by_name.return_value = collection

        # Assert that a bad attribute will raise NotFoundException
        with self.assertRaises(AttributeError) as e:
            CollectionStorage._set_attributes(self.name, invalid_attribute)
        self.assertEqual(e.exception.message, attribute_error_message %
            (self.name, invalid_attribute.keys()[0]))

        # Should throw even if there's a bad attribute AND valid attribute
        with self.assertRaises(AttributeError) as e:
            CollectionStorage._set_attributes(self.name,
            dict(invalid_attribute, **valid_attribute))
        self.assertEqual(e.exception.message, attribute_error_message %
            (self.name, invalid_attribute.keys()[0]))

        CollectionStorage._set_values = MagicMock()
        CollectionStorage._set_attributes(self.name,
            valid_attribute)

        # Assert that no exception thrown if valid attributes passed
        CollectionStorage._set_values.assert_called_once_with(self.name,
            valid_attribute)

    def test_set_over_18(self):
        """Assert that set_over_18 invokes _set_attributes"""
        CollectionStorage._set_attributes = MagicMock()
        CollectionStorage.set_over_18(self.name, True)

        CollectionStorage._set_attributes.assert_called_once_with(self.name,
            {'over_18': 'True'})

    def test_set_is_spotlight(self):
        """Assert that set_is_spotlight invokes _set_attributes"""
        CollectionStorage._set_attributes = MagicMock()
        CollectionStorage.set_is_spotlight(self.name, True)

        CollectionStorage._set_attributes.assert_called_once_with(self.name,
            {'is_spotlight': 'True'})


class CollectionTest(RedditTestCase):

    def test_is_spotlight_default(self):
        """Assert that is_spotlight defaults to False"""
        collection = Collection(name='fake name', sr_names=[])
        self.assertFalse(collection.is_spotlight)

        setattr(collection, 'is_spotlight', True)
        self.assertTrue(collection.is_spotlight)


class CollectionOrderTest(RedditTestCase):
    """
    Assert that Collection.get_all() sorts in the following sequence:
    1. SFW/NSFW
    2. Spotlighted
    3. Alphabetical
    """

    def setUp(self):
        # Setup all collections
        self.spotlight_a = Collection(name='spotlight_a', sr_names=[],
            is_spotlight=True)
        self.spotlight_z = Collection(name='spotlight_z', sr_names=[],
            is_spotlight=True)
        self.sfw_a = Collection(name='sfw_a', sr_names=[])
        self.sfw_b = Collection(name='sfw_B', sr_names=[])
        self.sfw_z = Collection(name='sfw_z', sr_names=[])
        self.nsfw_spotlight = Collection(name='nsfw_spotlight', sr_names=[],
            over_18=True, is_spotlight=True)
        self.nsfw_non_spotlight = Collection(name='nsfw_non_spotlight',
            sr_names=[], over_18=True)

        self.correct_order = [
            'spotlight_a',
            'spotlight_z',
            'sfw_a',
            'sfw_B',
            'sfw_z',
            'nsfw_spotlight',
            'nsfw_non_spotlight',
        ]

        # Mock the get_all method on CollectionStorage,
        # which returns an unordered list of collections
        CollectionStorage.get_all = MagicMock()

    def _assert_scenario(self, unordered_collections):
        CollectionStorage.get_all.return_value = unordered_collections
        self.assertEqual(
            [collection.name for collection in Collection.get_all()],
            self.correct_order)

    def test_scenario_reversed(self):
        """Assert that reversed order will order correctly"""
        unordered_collections = [
            self.nsfw_spotlight,
            self.nsfw_non_spotlight,
            self.sfw_z,
            self.sfw_b,
            self.sfw_a,
            self.spotlight_z,
            self.spotlight_a,
        ]
        self._assert_scenario(unordered_collections)

    def test_scenario_semi_sorted(self):
        """
        Assert that SFW and spotlight sorted list that is
        unordered alphabetically will order correctly
        """
        unordered_collections = [
            self.spotlight_z,
            self.spotlight_a,
            self.sfw_z,
            self.sfw_b,
            self.sfw_a,
            self.nsfw_spotlight,
            self.nsfw_non_spotlight,
        ]
        self._assert_scenario(unordered_collections)

    def test_scenario_random(self):
        """Assert that totally random list will order correctly"""
        unordered_collections = [
            self.sfw_z,
            self.nsfw_non_spotlight,
            self.sfw_a,
            self.spotlight_a,
            self.nsfw_spotlight,
            self.spotlight_z,
            self.sfw_b,
        ]
        self._assert_scenario(unordered_collections)

    def test_scenario_casing(self):
        """Assert that ordering is case-insensitive"""
        unordered_collections = [
            self.sfw_b,
            self.sfw_a,
            self.sfw_z,
            self.spotlight_a,
            self.spotlight_z,
            self.nsfw_spotlight,
            self.nsfw_non_spotlight,
        ]
        self._assert_scenario(unordered_collections)
<EOF>
<BOF>
from mock import patch, MagicMock
from datetime import datetime

import pytz

from r2.lib.utils import tup
from r2.models.vote import Vote
from r2.tests import RedditTestCase


class TestVoteValidator(RedditTestCase):

    def setUp(self):
        self.user = MagicMock(name="user")
        self.user._id36 = 'userid36'
        self.thing = MagicMock(name="thing")
        self.vote_data = {}
        super(RedditTestCase, self).setUp()

    def cast_vote(self, **kw):
        kw.setdefault("date", datetime.now(pytz.UTC))
        kw.setdefault("direction", Vote.DIRECTIONS.up)
        kw.setdefault("get_previous_vote", False)
        kw.setdefault("data", self.vote_data)
        return Vote(
            user=self.user,
            thing=self.thing,
            **kw
        )

    def assert_vote_effects(
        self, vote,
        affects_score=True,
        affects_karma=True,
        affected_thing_attr="_ups",
        notes=None,
    ):
        notes = set(tup(notes) if notes else [])
        self.assertEqual(vote.effects.affects_score, affects_score)
        self.assertEqual(vote.effects.affects_karma, affects_karma)
        self.assertEqual(vote.affected_thing_attr, affected_thing_attr)
        self.assertEqual(set(vote.effects.notes), notes)
        return vote

    def test_upvote_effects(self):
        vote = self.cast_vote()
        self.assertTrue(vote.is_upvote)
        self.assertFalse(vote.is_downvote)
        self.assertFalse(vote.is_self_vote)
        self.assert_vote_effects(vote)

    def test_downvote_effects(self):
        vote = self.cast_vote(direction=Vote.DIRECTIONS.down)
        self.assertFalse(vote.is_upvote)
        self.assertTrue(vote.is_downvote)
        self.assertFalse(vote.is_self_vote)
        self.assert_vote_effects(vote, affected_thing_attr="_downs")
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from mock import MagicMock
from pylons import app_globals as g

from r2.lib.permissions import PermissionSet

from r2.models import NotFound
from r2.models.account import Account
from r2.models.subreddit import SRMember, Subreddit

class TestPermissionSet(PermissionSet):
    info = dict(x={}, y={})


class SRMemberTest(unittest.TestCase):
    def setUp(self):
        a = Account()
        a._id = 1
        sr = Subreddit()
        sr._id = 2
        self.rel = SRMember(sr, a, 'test')

    def test_get_permissions(self):
        self.assertRaises(NotImplementedError, self.rel.get_permissions)
        self.rel._permission_class = TestPermissionSet
        self.assertEquals('', self.rel.get_permissions().dumps())
        self.rel.encoded_permissions = '+x,-y'
        self.assertEquals('+x,-y', self.rel.get_permissions().dumps())

    def test_has_permission(self):
        self.assertRaises(NotImplementedError, self.rel.has_permission, 'x')
        self.rel._permission_class = TestPermissionSet
        self.assertFalse(self.rel.has_permission('x'))
        self.rel.encoded_permissions = '+x,-y'
        self.assertTrue(self.rel.has_permission('x'))
        self.assertFalse(self.rel.has_permission('y'))
        self.rel.encoded_permissions = '+all'
        self.assertTrue(self.rel.has_permission('x'))
        self.assertTrue(self.rel.has_permission('y'))
        self.assertFalse(self.rel.has_permission('z'))

    def test_update_permissions(self):
        self.assertRaises(NotImplementedError,
                          self.rel.update_permissions, x=True)
        self.rel._permission_class = TestPermissionSet
        self.rel.update_permissions(x=True, y=False)
        self.assertEquals('+x,-y', self.rel.encoded_permissions)
        self.rel.update_permissions(x=None)
        self.assertEquals('-y', self.rel.encoded_permissions)
        self.rel.update_permissions(y=None, z=None)
        self.assertEquals('', self.rel.encoded_permissions)
        self.rel.update_permissions(x=True, y=False, all=True)
        self.assertEquals('+all', self.rel.encoded_permissions)

    def test_set_permissions(self):
        self.rel.set_permissions(PermissionSet(x=True, y=False))
        self.assertEquals('+x,-y', self.rel.encoded_permissions)

    def test_is_superuser(self):
        self.assertRaises(NotImplementedError, self.rel.is_superuser)
        self.rel._permission_class = TestPermissionSet
        self.assertFalse(self.rel.is_superuser())
        self.rel.encoded_permissions = '+all'
        self.assertTrue(self.rel.is_superuser())


class IsValidNameTest(unittest.TestCase):
    def test_empty(self):
        self.assertFalse(Subreddit.is_valid_name(None))

    def test_short(self):
        self.assertTrue(Subreddit.is_valid_name('aaa'))

    def test_too_short(self):
        self.assertFalse(Subreddit.is_valid_name('aa'))

    def test_long(self):
        self.assertTrue(Subreddit.is_valid_name('aaaaaaaaaaaaaaaaaaaaa'))

    def test_too_long(self):
        self.assertFalse(Subreddit.is_valid_name('aaaaaaaaaaaaaaaaaaaaaa'))

    def test_underscore(self):
        self.assertTrue(Subreddit.is_valid_name('a_a'))

    def test_leading_underscore(self):
        self.assertFalse(Subreddit.is_valid_name('_aa'))

    def test_capitals(self):
        self.assertTrue(Subreddit.is_valid_name('AZA'))

    def test_numerics(self):
        self.assertTrue(Subreddit.is_valid_name('090'))


class ByNameTest(unittest.TestCase):
    def setUp(self):
        self.cache = MagicMock()
        g.gencache = self.cache

        self.subreddit_byID = MagicMock()
        Subreddit._byID = self.subreddit_byID

        self.subreddit_query = MagicMock()
        Subreddit._query = self.subreddit_query

    def testSingleCached(self):
        subreddit = Subreddit(id=1, name="exists")
        self.cache.get_multi.return_value = {"exists": subreddit._id}
        self.subreddit_byID.return_value = [subreddit]

        ret = Subreddit._by_name("exists")

        self.assertEqual(ret, subreddit)
        self.assertEqual(self.subreddit_query.call_count, 0)

    def testSingleFromDB(self):
        subreddit = Subreddit(id=1, name="exists")
        self.cache.get_multi.return_value = {}
        self.subreddit_query.return_value = [subreddit]
        self.subreddit_byID.return_value = [subreddit]

        ret = Subreddit._by_name("exists")

        self.assertEqual(ret, subreddit)
        self.assertEqual(self.cache.set_multi.call_count, 1)

    def testSingleNotFound(self):
        self.cache.get_multi.return_value = {}
        self.subreddit_query.return_value = []

        with self.assertRaises(NotFound):
            Subreddit._by_name("doesnotexist")

    def testSingleInvalid(self):
        with self.assertRaises(NotFound):
            Subreddit._by_name("_illegalunderscore")

        self.assertEqual(self.cache.get_multi.call_count, 0)
        self.assertEqual(self.subreddit_query.call_count, 0)

    def testMultiCached(self):
        srs = [
            Subreddit(id=1, name="exists"),
            Subreddit(id=2, name="also"),
        ]
        self.cache.get_multi.return_value = {sr.name: sr._id for sr in srs}
        self.subreddit_byID.return_value = srs

        ret = Subreddit._by_name(["exists", "also"])

        self.assertEqual(ret, {sr.name: sr for sr in srs})
        self.assertEqual(self.subreddit_query.call_count, 0)

    def testMultiCacheMissesAllExist(self):
        srs = [
            Subreddit(id=1, name="exists"),
            Subreddit(id=2, name="also"),
        ]

        self.cache.get_multi.return_value = {}
        self.subreddit_query.return_value = srs
        self.subreddit_byID.return_value = srs

        ret = Subreddit._by_name(["exists", "also"])

        self.assertEqual(ret, {sr.name: sr for sr in srs})
        self.assertEqual(self.cache.get_multi.call_count, 1)
        self.assertEqual(self.subreddit_query.call_count, 1)

    def testMultiSomeDontExist(self):
        sr = Subreddit(id=1, name="exists")
        self.cache.get_multi.return_value = {sr.name: sr._id}
        self.subreddit_query.return_value = []
        self.subreddit_byID.return_value = [sr]

        ret = Subreddit._by_name(["exists", "doesnt"])

        self.assertEqual(ret, {sr.name: sr})
        self.assertEqual(self.cache.get_multi.call_count, 1)
        self.assertEqual(self.subreddit_query.call_count, 1)

    def testMultiSomeInvalid(self):
        sr = Subreddit(id=1, name="exists")
        self.cache.get_multi.return_value = {sr.name: sr._id}
        self.subreddit_query.return_value = []
        self.subreddit_byID.return_value = [sr]

        ret = Subreddit._by_name(["exists", "_illegalunderscore"])

        self.assertEqual(ret, {sr.name: sr})
        self.assertEqual(self.cache.get_multi.call_count, 1)
        self.assertEqual(self.subreddit_query.call_count, 0)

    def testForceUpdate(self):
        sr = Subreddit(id=1, name="exists")
        self.cache.get_multi.return_value = {sr.name: sr._id}
        self.subreddit_query.return_value = [sr]
        self.subreddit_byID.return_value = [sr]

        ret = Subreddit._by_name("exists", _update=True)

        self.assertEqual(ret, sr)
        self.cache.set_multi.assert_called_once_with(
            keys={sr.name: sr._id},
            prefix="srid:",
            time=43200,
        )

    def testCacheNegativeResults(self):
        self.cache.get_multi.return_value = {}
        self.subreddit_query.return_value = []
        self.subreddit_byID.return_value = []

        with self.assertRaises(NotFound):
            Subreddit._by_name("doesnotexist")

        self.cache.set_multi.assert_called_once_with(
            keys={"doesnotexist": Subreddit.SRNAME_NOTFOUND},
            prefix="srid:",
            time=43200,
        )

    def testExcludeNegativeLookups(self):
        self.cache.get_multi.return_value = {"doesnotexist": Subreddit.SRNAME_NOTFOUND}

        with self.assertRaises(NotFound):
            Subreddit._by_name("doesnotexist")
        self.assertEqual(self.subreddit_query.call_count, 0)
        self.assertEqual(self.subreddit_byID.call_count, 0)
        self.assertEqual(self.cache.set_multi.call_count, 0)


if __name__ == '__main__':
    unittest.main()
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import contextlib

from r2.tests import RedditTestCase

from mock import patch, MagicMock

from r2.models import Message
from r2.models.builder import UserMessageBuilder, MessageBuilder

from pylons import tmpl_context as c


class UserMessageBuilderTest(RedditTestCase):
    def setUp(self):
        super(UserMessageBuilderTest, self).setUp()
        self.user = MagicMock(name="user")
        self.message = MagicMock(spec=Message)

    def test_view_message_on_receiver_side_and_spam(self):
        user = MagicMock(name="user")
        userMessageBuilder = UserMessageBuilder(user)

        self.user._id = 1
        self.message.author_id = 2
        self.message._spam = True

        with self.mock_preparation():
            self.assertFalse(
                    userMessageBuilder._viewable_message(self.message))

    def test_view_message_on_receiver_side_and_del(self):
        user = MagicMock(name="user")
        userMessageBuilder = UserMessageBuilder(user)

        self.user._id = 1
        self.message.author_id = 2
        self.message.to_id = self.user._id
        self.message._spam = False
        self.message.del_on_recipient = True

        with self.mock_preparation():
            self.assertFalse(
                    userMessageBuilder._viewable_message(self.message))

    def test_view_message_on_receiver_side(self):
        user = MagicMock(name="user")
        userMessageBuilder = UserMessageBuilder(user)

        self.user._id = 1
        self.message.author_id = 2
        self.message.to_id = self.user._id
        self.message._spam = False
        self.message.del_on_recipient = False

        with self.mock_preparation():
            self.assertTrue(
                userMessageBuilder._viewable_message(self.message))

    def test_view_message_on_sender_side_and_del(self):
        user = MagicMock(name="user")
        userMessageBuilder = UserMessageBuilder(user)

        self.message.to_id = 1
        self.user._id = 2
        self.message.author_id = self.user._id
        self.message._spam = False
        self.message.del_on_recipient = True

        with self.mock_preparation():
            self.assertTrue(
                userMessageBuilder._viewable_message(self.message))

    def test_view_message_on_admin_and_del(self):
        user = MagicMock(name="user")
        userMessageBuilder = UserMessageBuilder(user)

        self.user._id = 1
        self.message.author_id = 2
        self.message.to_id = self.user._id
        self.message._spam = False
        self.message.del_on_recipient = True

        with self.mock_preparation(True):
            self.assertTrue(
                userMessageBuilder._viewable_message(self.message))

    def mock_preparation(self, is_admin=False):
        """ Context manager for mocking function calls. """

        return contextlib.nested(
            patch.object(c, "user", self.user, create=True),
            patch.object(c, "user_is_admin", is_admin, create=True),
            patch.object(MessageBuilder,
                         "_viewable_message", return_value=True)
        )

<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import random
import string
import unittest

import mock

from r2.config.feature.state import FeatureState
from r2.config.feature.world import World
from r2.tests import RedditTestCase


class MockAccount(object):
    def __init__(self, name, _fullname):
        self.name = name
        self._fullname = _fullname
        _, _, _id = _fullname.partition("_")
        self._id = int(_id, 36)

gary = MockAccount(name='gary', _fullname='t2_beef')
all_uppercase = MockAccount(name='ALL_UPPERCASE', _fullname='t2_f00d')

class MockWorld(World):
    def _make_state(self, config):
        # Mock by hand because _parse_config is called in __init__, so we
        # can't instantiate then update.
        class MockState(FeatureState):
            def _parse_config(*args, **kwargs):
                return config
        return MockState('test_state', self)

class TestFeatureBase(RedditTestCase):
    _world = None
    # Append user-supplied error messages to the default output, rather than
    # overwriting it.
    longMessage = True

class TestFeatureBase(RedditTestCase):
    # Append user-supplied error messages to the default output, rather than
    # overwriting it.
    longMessage = True

    def setUp(self):
        self.world = MockWorld()
        self.world.current_user = mock.Mock(return_value='')
        self.world.current_subreddit = mock.Mock(return_value='')
        self.world.current_loid = mock.Mock(return_value='')


class TestFeatureBase(RedditTestCase):
    # Append user-supplied error messages to the default output, rather than
    # overwriting it.
    longMessage = True

    def setUp(self):
        super(TestFeatureBase, self).setUp()
        self.world = MockWorld()
        self.world.current_user = mock.Mock(return_value='')
        self.world.current_subreddit = mock.Mock(return_value='')
        self.world.current_loid = mock.Mock(return_value='')

    @classmethod
    def generate_loid(cls):
        return ''.join(random.sample(string.letters + string.digits, 16))


class TestFeature(TestFeatureBase):

    def _assert_fuzzy_percent_true(self, results, percent):
        stats = collections.Counter(results)
        total = sum(stats.values())
        # _roughly_ `percent` should have been `True`
        diff = abs((float(stats[True]) / total) - (percent / 100.0))
        self.assertTrue(diff < 0.1)

    def test_enabled(self):
        cfg = {'enabled': 'on'}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled())
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_disabled(self):
        cfg = {'enabled': 'off'}
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled())
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_admin_enabled(self):
        cfg = {'admin': True}
        self.world.is_admin = mock.Mock(return_value=True)
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_admin_disabled(self):
        cfg = {'admin': True}
        self.world.is_admin = mock.Mock(return_value=False)
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_employee_enabled(self):
        cfg = {'employee': True}
        self.world.is_employee = mock.Mock(return_value=True)
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_employee_disabled(self):
        cfg = {'employee': True}
        self.world.is_employee = mock.Mock(return_value=False)
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_beta_enabled(self):
        cfg = {'beta': True}
        self.world.user_has_beta_enabled = mock.Mock(return_value=True)
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_beta_disabled(self):
        cfg = {'beta': True}
        self.world.user_has_beta_enabled = mock.Mock(return_value=False)
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_gold_enabled(self):
        cfg = {'gold': True}
        self.world.has_gold = mock.Mock(return_value=True)
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_gold_disabled(self):
        cfg = {'gold': True}
        self.world.has_gold = mock.Mock(return_value=False)
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_loggedin_enabled(self):
        cfg = {'loggedin': True}
        self.world.is_user_loggedin = mock.Mock(return_value=True)
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_loggedin_disabled(self):
        cfg = {'loggedin': False}
        self.world.is_user_loggedin = mock.Mock(return_value=True)
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_loggedout_enabled(self):
        cfg = {'loggedout': True}
        self.world.is_user_loggedin = mock.Mock(return_value=False)
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_loggedout_disabled(self):
        cfg = {'loggedout': False}
        self.world.is_user_loggedin = mock.Mock(return_value=False)
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(user=gary))

    def test_percent_loggedin(self):
        num_users = 2000
        users = []
        for i in xrange(num_users):
            users.append(MockAccount(name=str(i), _fullname="t2_%s" % str(i)))

        def simulate_percent_loggedin(wanted_percent):
            cfg = {'percent_loggedin': wanted_percent}
            self.world.is_user_loggedin = mock.Mock(return_value=True)
            feature_state = self.world._make_state(cfg)
            return (feature_state.is_enabled(x) for x in users)

        self.assertFalse(any(simulate_percent_loggedin(0)))
        self.assertTrue(all(simulate_percent_loggedin(100)))
        self._assert_fuzzy_percent_true(simulate_percent_loggedin(25), 25)
        self._assert_fuzzy_percent_true(simulate_percent_loggedin(10), 10)
        self._assert_fuzzy_percent_true(simulate_percent_loggedin(50), 50)
        self._assert_fuzzy_percent_true(simulate_percent_loggedin(99), 99)

    def test_percent_loggedout(self):
        num_users = 2000

        def simulate_percent_loggedout(wanted_percent):
            cfg = {'percent_loggedout': wanted_percent}
            for i in xrange(num_users):
                loid = self.generate_loid()
                self.world.current_loid = mock.Mock(return_value=loid)
                self.world.is_user_loggedin = mock.Mock(return_value=False)
                feature_state = self.world._make_state(cfg)
                yield feature_state.is_enabled()

        self.assertFalse(any(simulate_percent_loggedout(0)))
        self.assertTrue(all(simulate_percent_loggedout(100)))
        self._assert_fuzzy_percent_true(simulate_percent_loggedout(25), 25)
        self._assert_fuzzy_percent_true(simulate_percent_loggedout(10), 10)
        self._assert_fuzzy_percent_true(simulate_percent_loggedout(50), 50)
        self._assert_fuzzy_percent_true(simulate_percent_loggedout(99), 99)


    def test_url_enabled(self):

        cfg = {'url': 'test_state'}
        self.world.url_features = mock.Mock(return_value={'test_state'})
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled())
        self.assertTrue(feature_state.is_enabled(user=gary))

        cfg = {'url': 'test_state'}
        self.world.url_features = mock.Mock(return_value={'x', 'test_state'})
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled())
        self.assertTrue(feature_state.is_enabled(user=gary))

        cfg = {'url': {'test_state_a': 'a', 'test_state_b': 'b'}}
        self.world.url_features = mock.Mock(return_value={'x', 'test_state_b'})
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled())
        self.assertEqual(feature_state.variant(user=gary), 'b')

    def test_url_disabled(self):

        cfg = {'url': 'test_state'}
        self.world.url_features = mock.Mock(return_value={})
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled())
        self.assertFalse(feature_state.is_enabled(user=gary))

        cfg = {'url': 'test_state'}
        self.world.url_features = mock.Mock(return_value={'x'})
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled())
        self.assertFalse(feature_state.is_enabled(user=gary))

        cfg = {'url': {'test_state_a': 'a', 'test_state_b': 'b'}}
        self.world.url_features = mock.Mock(return_value={'x'})
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled())

        cfg = {'url': {'test_state_c1': 'control_1', 'test_state_c2': 'control_2'}}
        self.world.url_features = mock.Mock(return_value={'x', 'test_state_c2'})
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled())

    def test_user_in(self):
        cfg = {'users': ['Gary']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

        cfg = {'users': ['ALL_UPPERCASE']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=all_uppercase))

        cfg = {'users': ['dave', 'gary']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(user=gary))

    def test_user_not_in(self):
        cfg = {'users': ['']}
        featurestate = self.world._make_state(cfg)
        self.assertFalse(featurestate.is_enabled(user=gary))

        cfg = {'users': ['dave', 'joe']}
        featurestate = self.world._make_state(cfg)
        self.assertFalse(featurestate.is_enabled(user=gary))

    def test_subreddit_in(self):
        cfg = {'subreddits': ['WTF']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(subreddit='wtf'))

        cfg = {'subreddits': ['wtf']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(subreddit='WTF'))

        cfg = {'subreddits': ['aww', 'wtf']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(subreddit='wtf'))

    def test_subreddit_not_in(self):
        cfg = {'subreddits': []}
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(subreddit='wtf'))

        cfg = {'subreddits': ['aww', 'wtfoobar']}
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(subreddit='wtf'))

    def test_subdomain_in(self):
        cfg = {'subdomains': ['BETA']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(subdomain='beta'))

        cfg = {'subdomains': ['beta']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(subdomain='BETA'))

        cfg = {'subdomains': ['www', 'beta']}
        feature_state = self.world._make_state(cfg)
        self.assertTrue(feature_state.is_enabled(subdomain='beta'))

    def test_subdomain_not_in(self):
        cfg = {'subdomains': []}
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(subdomain='beta'))
        self.assertFalse(feature_state.is_enabled(subdomain=''))

        cfg = {'subdomains': ['www', 'betanauts']}
        feature_state = self.world._make_state(cfg)
        self.assertFalse(feature_state.is_enabled(subdomain='beta'))

    def test_multiple(self):
        # is_admin, globally off should still be False
        cfg = {'enabled': 'off', 'admin': True}
        self.world.is_admin = mock.Mock(return_value=True)
        featurestate = self.world._make_state(cfg)
        self.assertFalse(featurestate.is_enabled(user=gary))

        # globally on but not admin should still be True
        cfg = {'enabled': 'on', 'admin': True}
        self.world.is_admin = mock.Mock(return_value=False)
        featurestate = self.world._make_state(cfg)
        self.assertTrue(featurestate.is_enabled(user=gary))
        self.assertTrue(featurestate.is_enabled())

        # no URL but admin should still be True
        cfg = {'url': 'test_featurestate', 'admin': True}
        self.world.url_features = mock.Mock(return_value={})
        self.world.is_admin = mock.Mock(return_value=True)
        featurestate = self.world._make_state(cfg)
        self.assertTrue(featurestate.is_enabled(user=gary))
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import collections
import itertools
import math
from mock import MagicMock

from pylons import app_globals as g

from r2.config.feature.state import FeatureState
from . feature_test import TestFeatureBase, MockAccount


class TestExperiment(TestFeatureBase):
    _world = None
    # Append user-supplied error messages to the default output, rather than
    # overwriting it.
    longMessage = True

    def setUp(self):
        super(TestExperiment, self).setUp()
        # for the purposes of this test, logged in users will be generated as
        # MockAccount objects and logged out users will be None.  This is in
        # keeping with how c.user is treated in the default unlogged-in case.
        self.world.is_user_loggedin = bool
        self.mock_eventcollector()
        # test by default with the logged out functionality enabled.
        self.patch_g(enable_loggedout_experiments=True)

    def get_loggedin_users(self, num_users):
        users = []
        for i in xrange(num_users):
            users.append(MockAccount(name=str(i), _fullname="t2_%s" % str(i)))
        return users

    @staticmethod
    def get_loggedout_users(num_users):
        return [None for _ in xrange(num_users)]

    def test_calculate_bucket(self):
        """Test FeatureState's _calculate_bucket function."""
        feature_state = self.world._make_state(config={})

        # Give ourselves enough users that we can get some reasonable amount of
        # precision when checking amounts per bucket.
        NUM_USERS = FeatureState.NUM_BUCKETS * 2000
        fullnames = []
        for i in xrange(NUM_USERS):
            fullnames.append("t2_%s" % str(i))

        counter = collections.Counter()
        for fullname in fullnames:
            bucket = feature_state._calculate_bucket(fullname)
            counter[bucket] += 1
            # Ensure bucketing is deterministic.
            self.assertEqual(bucket, feature_state._calculate_bucket(fullname))

        for bucket in xrange(FeatureState.NUM_BUCKETS):
            # We want an even distribution across buckets.
            expected = NUM_USERS / FeatureState.NUM_BUCKETS
            actual = counter[bucket]
            # Calculating the percentage difference instead of looking at the
            # raw difference scales better as we change NUM_USERS.
            percent_equal = float(actual) / expected
            self.assertAlmostEqual(percent_equal, 1.0, delta=.10,
                                   msg='bucket: %s' % bucket)

    def test_choose_variant(self):
        """Test FeatureState's _choose_variant function."""
        no_variants = {}
        three_variants = {
            'remove_vote_counters': 5,
            'control_1': 10,
            'control_2': 5,
        }
        three_variants_more = {
            'remove_vote_counters': 15.6,
            'control_1': 10,
            'control_2': 20,
        }

        counters = collections.defaultdict(collections.Counter)
        for bucket in xrange(FeatureState.NUM_BUCKETS):
            variant = FeatureState._choose_variant(bucket, no_variants)
            if variant:
                counters['no_variants'][variant] += 1
            # Ensure variant-choosing is deterministic.
            self.assertEqual(
                variant,
                FeatureState._choose_variant(bucket, no_variants))

            variant = FeatureState._choose_variant(bucket, three_variants)
            if variant:
                counters['three_variants'][variant] += 1
            # Ensure variant-choosing is deterministic.
            self.assertEqual(
                variant,
                FeatureState._choose_variant(bucket, three_variants))

            previous_variant = variant
            variant = FeatureState._choose_variant(bucket, three_variants_more)
            if variant:
                counters['three_variants_more'][variant] += 1
            # Ensure variant-choosing is deterministic.
            self.assertEqual(
                variant,
                FeatureState._choose_variant(bucket, three_variants_more))
            # If previously we had a variant, we should still have the same one
            # now.
            if previous_variant:
                self.assertEqual(variant, previous_variant)

        # Only controls chosen in the no-variant case.
        for variant, percentage in FeatureState.DEFAULT_CONTROL_GROUPS.items():
            count = counters['no_variants'][variant]
            # The variant percentage is expressed as a part of 100, so we need
            # to calculate the fraction-of-1 percentage and scale it
            # accordingly.
            scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
            self.assertEqual(scaled_percentage, percentage)
        for variant, percentage in three_variants.items():
            count = counters['three_variants'][variant]
            scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
            self.assertEqual(scaled_percentage, percentage)
        for variant, percentage in three_variants_more.items():
            count = counters['three_variants_more'][variant]
            scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
            self.assertEqual(scaled_percentage, percentage)

        # Test boundary conditions around the maximum percentage allowed for
        # variants.
        fifty_fifty = {
            'control_1': 50,
            'control_2': 50,
        }
        almost_fifty_fifty = {
            'control_1': 49,
            'control_2': 51,
        }
        for bucket in xrange(FeatureState.NUM_BUCKETS):
            variant = FeatureState._choose_variant(bucket, fifty_fifty)
            counters['fifty_fifty'][variant] += 1
            variant = FeatureState._choose_variant(bucket, almost_fifty_fifty)
            counters['almost_fifty_fifty'][variant] += 1
        count = counters['fifty_fifty']['control_1']
        scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
        self.assertEqual(scaled_percentage, 50)

        count = counters['fifty_fifty']['control_2']
        scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
        self.assertEqual(scaled_percentage, 50)

        count = counters['almost_fifty_fifty']['control_1']
        scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
        self.assertEqual(scaled_percentage, 49)

        count = counters['almost_fifty_fifty']['control_2']
        scaled_percentage = float(count) / (FeatureState.NUM_BUCKETS / 100)
        self.assertEqual(scaled_percentage, 50)

    def do_experiment_simulation(self, users, loid_generator=None, **cfg):
        num_users = len(users)
        if loid_generator is None:
            loid_generator = iter(self.generate_loid, None)

        feature_state = self.world._make_state(cfg)
        counter = collections.Counter()
        for user, loid in zip(users, loid_generator):
            # on every loop, we have a distinct user and a possibly distinct
            # loid which will be used multiple times.
            self.world.current_loid.return_value = loid
            # is_enabled() and variant() are related but independent code
            # paths so check both are set together.
            variant = feature_state.variant(user)
            if feature_state.is_enabled(user):
                self.assertIsNotNone(
                    variant, "an enabled experiment should have a variant!")
                counter[variant] += 1

        # this test will still probabilistically fail, but we can mitigate
        # the likeliness of that happening
        error_bar_percent = 100. / math.sqrt(num_users)
        for variant, percent in cfg['experiment']['variants'].items():
            # Our actual percentage should be within our expected percent
            # (expressed as a part of 100 rather than a fraction of 1)
            # +- 1%.
            measured_percent = (float(counter[variant]) / num_users) * 100
            self.assertAlmostEqual(
                measured_percent, percent, delta=error_bar_percent
            )

    def assert_no_experiment(self, users, **cfg):
        feature_state = self.world._make_state(cfg)
        for user in users:
            self.assertFalse(feature_state.is_enabled(user))

    def test_loggedin_experiment(self, num_users=2000):
        """Test variant distn for logged in users."""
        self.do_experiment_simulation(
            self.get_loggedin_users(num_users),
            experiment={
                'loggedin': True,
                'variants': {'larger': 5, 'smaller': 10},
            }
        )

    def test_loggedin_experiment_explicit_enable(self, num_users=2000):
        """Test variant distn for logged in users with explicit enable."""
        self.do_experiment_simulation(
            self.get_loggedin_users(num_users),
            experiment={
                'loggedin': True,
                'variants': {'larger': 5, 'smaller': 10},
                'enabled': True,
            },
        )

    def test_loggedin_experiment_explicit_disable(self, num_users=2000):
        """Test explicit disable for logged in users actually disables."""
        self.assert_no_experiment(
            self.get_loggedin_users(num_users),
            experiment={
                'loggedin': True,
                'variants': {'larger': 5, 'smaller': 10},
                'enabled': False,
            },
        )

    def test_loggedout_experiment(self, num_users=2000):
        """Test variant distn for logged out users."""
        self.do_experiment_simulation(
            self.get_loggedout_users(num_users),
            experiment={
                "loggedout": True,
                'variants': {'larger': 5, 'smaller': 10},
            },
        )

    def test_loggedout_experiment_missing_loids(self, num_users=2000):
        """Ensure logged out experiments with no loids do not bucket."""
        self.assert_no_experiment(
            self.get_loggedout_users(num_users),
            loid_generator=itertools.repeat(None),
            experiment={
                'loggedout': True,
                'variants': {'larger': 5, 'smaller': 10},
            },
        )

    def test_loggedout_experiment_explicit_enable(self, num_users=2000):
        """Test variant distn for logged out users with explicit enable."""
        self.do_experiment_simulation(
            self.get_loggedout_users(num_users),
            experiment={
                'loggedout': True,
                'variants': {'larger': 5, 'smaller': 10},
                'enabled': True,
            },
        )

    def test_loggedout_experiment_explicit_disable(self, num_users=2000):
        """Test explicit disable for logged in users actually disables."""
        self.assert_no_experiment(
            self.get_loggedout_users(num_users),
            experiment={
                'loggedout': True,
                'variants': {'larger': 5, 'smaller': 10},
                'enabled': False,
            }
        )

    def test_loggedout_experiment_global_disable(self, num_users=2000):
        """Test we can disable loid-experiments via configuration."""
        # we already patch this attr in setUp, so we can just explicitly change
        # it and rely on *that* cleanup
        g.enable_loggedout_experiments = False
        self.assert_no_experiment(
            self.get_loggedout_users(num_users),
            experiment={
                'loggedout': True,
                'variants': {'larger': 5, 'smaller': 10},
                'enabled': True,
            }
        )

    def test_mixed_experiment(self, num_users=2000):
        """Test a combination of loggedin/out users balances variants."""
        self.do_experiment_simulation(
            (
                self.get_loggedin_users(num_users / 2) +
                self.get_loggedout_users(num_users / 2)
            ),
            experiment={
                'loggedin': True,
                'loggedout': True,
                'variants': {'larger': 5, 'smaller': 10},
            }
        )

    def test_mixed_experiment_disable(self, num_users=2000):
        """Test a combination of loggedin/out users disables properly."""
        self.assert_no_experiment(
            (
                self.get_loggedin_users(num_users / 2) +
                self.get_loggedout_users(num_users / 2)
            ),
            experiment={
                'loggedin': True,
                'loggedout': True,
                'variants': {'larger': 5, 'smaller': 10},
                'enabled': False,
            }
        )
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################


from r2.lib.utils.reddit_agent_parser import (
    AlienBlueDetector,
    BaconReaderDetector,
    detect,
    McRedditDetector,
    NarwhalForRedditDetector,
    ReaditDetector,
    RedditAndroidDetector,
    RedditIsFunDetector,
    RedditIOSDetector,
    RedditSyncDetector,
    RelayForRedditDetector)
from r2.tests import RedditTestCase


class AgentDetectorTest(RedditTestCase):
    def test_reddit_is_fun_detector(self):
        user_agent = 'reddit is fun (Android) 4.1.15'
        agent_parsed = {}
        result = RedditIsFunDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'], 'reddit is fun')
        self.assertEqual(agent_parsed['browser']['version'], '4.1.15')
        self.assertEqual(agent_parsed['platform']['name'], 'Android')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])

    def test_reddit_android_detector(self):
        user_agent = 'RedditAndroid 1.1.5'
        agent_parsed = {}
        result = RedditAndroidDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         RedditAndroidDetector.name)
        self.assertEqual(agent_parsed['browser']['version'], '1.1.5')
        self.assertTrue(agent_parsed['app_name'],
                        agent_parsed['browser']['name'])

    def test_reddit_ios_detector(self):
        user_agent = ('Reddit/Version 1.1/Build 1106/iOS Version 9.3.2 '
                      '(Build 13F69)')
        agent_parsed = {}
        result = RedditIOSDetector().detect(user_agent, agent_parsed)
        self.assertEqual(agent_parsed['browser']['name'],
                         RedditIOSDetector.name)
        self.assertEqual(agent_parsed['browser']['version'], '1.1')
        self.assertEqual(agent_parsed['platform']['name'], 'iOS')
        self.assertEqual(agent_parsed['platform']['version'], '9.3.2')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])

    def test_alian_blue_detector(self):
        user_agent = 'AlienBlue/2.9.10.0.2 CFNetwork/758.4.3 Darwin/15.5.0'
        agent_parsed = {}
        result = AlienBlueDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         AlienBlueDetector.name)
        self.assertEqual(agent_parsed['browser']['version'], '2.9.10.0.2')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['app_name'])

    def test_relay_for_reddit_detector(self):
        user_agent = 'Relay by /u/DBrady v7.9.32'
        agent_parsed = {}
        result = RelayForRedditDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         RelayForRedditDetector.name)
        self.assertEqual(agent_parsed['browser']['version'], '7.9.32')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])

    def test_reddit_sync_detector(self):
        user_agent = ('android:com.laurencedawson.reddit_sync:v11.4 '
                      '(by /u/ljdawson)')
        agent_parsed = {}
        result = RedditSyncDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         RedditSyncDetector.name)
        self.assertEqual(agent_parsed['browser']['version'], '11.4')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])

    def test_narwhal_detector(self):
        user_agent = 'narwhal-iOS/2306 by det0ur'
        agent_parsed = {}
        result = NarwhalForRedditDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         NarwhalForRedditDetector.name)
        self.assertEqual(agent_parsed['platform']['name'], 'iOS')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])

    def test_mcreddit_detector(self):
        user_agent = 'McReddit - Reddit Client for iOS'
        agent_parsed = {}
        result = McRedditDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         McRedditDetector.name)
        self.assertEqual(agent_parsed['platform']['name'], 'iOS')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])

    def test_readit_detector(self):
        user_agent = (
            '(Readit for WP /u/MessageAcrossStudios) (Readit for WP '
            '/u/MessageAcrossStudios)')
        agent_parsed = {}
        result = ReaditDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'], ReaditDetector.name)
        self.assertIsNone(agent_parsed.get('app_name'))

    def test_bacon_reader_detector(self):
        user_agent = 'BaconReader/3.0 (iPhone; iOS 9.3.2; Scale/2.00)'
        agent_parsed = {}
        result = BaconReaderDetector().detect(user_agent, agent_parsed)
        self.assertTrue(result)
        self.assertEqual(agent_parsed['browser']['name'],
                         BaconReaderDetector.name)
        self.assertEqual(agent_parsed['browser']['version'], '3.0')
        self.assertEqual(agent_parsed['platform']['name'], 'iOS')
        self.assertEqual(agent_parsed['platform']['version'], '9.3.2')
        self.assertEqual(agent_parsed['app_name'],
                         agent_parsed['browser']['name'])


class HAPIntegrationTests(RedditTestCase):
    """Tests to ensure that parsers don't confilct with existing onex."""
    # TODO (katie.atkinson): Add tests to ensure reddit parsers don't conflict
    # with httpagentparser detectors.
    def test_reddit_is_fun_integration(self):
        user_agent = 'reddit is fun (Android) 4.1.15'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], 'reddit is fun')
        self.assertEqual(outs['dist']['name'], 'Android')

    def test_reddit_android_integration(self):
        user_agent = 'RedditAndroid 1.1.5'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], 'Reddit: The Official App')
        self.assertEqual(outs['dist']['name'], 'Android')

    def test_reddit_ios_integration(self):
        user_agent = ('Reddit/Version 1.1/Build 1106/iOS Version 9.3.2 '
                      '(Build 13F69)')
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], RedditIOSDetector.name)

    def test_alien_blue_detector(self):
        user_agent = 'AlienBlue/2.9.10.0.2 CFNetwork/758.4.3 Darwin/15.5.0'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], AlienBlueDetector.name)

    def test_relay_for_reddit_detector(self):
        user_agent = '  Relay by /u/DBrady v7.9.32'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], RelayForRedditDetector.name)

    def test_reddit_sync_detector(self):
        user_agent = ('android:com.laurencedawson.reddit_sync:v11.4 '
                      '(by /u/ljdawson)')
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], RedditSyncDetector.name)

    def test_narwhal_detector(self):
        user_agent = 'narwhal-iOS/2306 by det0ur'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'],
                         NarwhalForRedditDetector.name)

    def test_mcreddit_detector(self):
        user_agent = 'McReddit - Reddit Client for iOS'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], McRedditDetector.name)

    def test_readit_detector(self):
        user_agent = (
            '(Readit for WP /u/MessageAcrossStudios) '
            '(Readit for WP /u/MessageAcrossStudios)')
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], ReaditDetector.name)

    def test_bacon_reader_detector(self):
        user_agent = 'BaconReader/3.0 (iPhone; iOS 9.3.2; Scale/2.00)'
        outs = detect(user_agent)
        self.assertEqual(outs['browser']['name'], BaconReaderDetector.name)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import unittest
import contextlib
import math
import functools
import traceback
import sys

from mock import MagicMock, patch
from pylons import request
from pylons import app_globals as g

from r2.lib import utils


class CrappyQuery(object):
    """Helper class for testing.
    It satisfies the methods fetch_things2 will call on a query
    and also generator interface.

    It is expected that fetch_things2 will set _after on
    instance of this class."""

    def __init__(self,
                 start,
                 end,
                 failures_between_chunks=0,
                 chunk_num_to_fail_on=None
                 ):
        """
        :param int start: integer to stat yielding from.
        :param int end: integer to end yielding at (exclusive).
        :param int failure_between_chunks: number of times to
            fail before starting to yield numbers from next chunk.
        :param int chunk_num_to_fail_on:  If not None, fail only
            after this chunk.  Do not fail between other chunks.
        """
        self.start = start
        self.end = end
        self.failures_between_chunks = failures_between_chunks
        self.chunk_num_to_fail_on = chunk_num_to_fail_on

        self._sort = "ascending"
        self._rules = []

        self.current_chunk = 0
        self.num_after_was_called = 0
        self.total_num_failed = 0
        self.num_failed = 0
        self.should_fail = False
        self._reset_state()

    def _reset_state(self):
        self.i = self.start

    def __iter__(self):
        return self

    def _after(self, num):
        self.num_after_was_called += 1
        self.start = num + 1

    def __next__(self):
        if (self.i >= self.start + self._limit or
                self.i >= self.end):
            # quit iterating if we reach end or end of chunk
            self.current_chunk += 1
            raise StopIteration()
        if self.i == self.start:
            # if we are at a start of a chunk, do some failing
            if (self.num_failed < self.failures_between_chunks and
                (self.chunk_num_to_fail_on is None or
                    self.chunk_num_to_fail_on == self.current_chunk)):
                self.should_fail = True
            else:
                self.should_fail = False
                self.num_failed = 0
        if self.should_fail:
            self.num_failed += 1
            self.total_num_failed += 1
            raise ValueError("FOO %d %d %d" %
                             (self.i,
                              self.current_chunk,
                              self.num_failed))
        ret = self.i
        self.i += 1
        return ret

    def next(self):
        return self.__next__()

    def __call__(self):
        self._reset_state()
        return self


class UtilsTest(unittest.TestCase):
    def test_weighted_lottery_errors(self):
        self.assertRaises(ValueError, utils.weighted_lottery, {})
        self.assertRaises(ValueError, utils.weighted_lottery, {'x': 0})
        self.assertRaises(
            ValueError, utils.weighted_lottery,
            collections.OrderedDict([('x', -1), ('y', 1)]))

    @contextlib.contextmanager
    def check_exponential_backoff_sleep_times(self,
                                              start,
                                              num):
        sleepy_times = []

        def record_sleep_times(sec):
            sleepy_times.append(sec)

        try:
            with patch('time.sleep', new=record_sleep_times):
                yield
        finally:
            self.assertEquals(len(sleepy_times), num)
            walker = start / 1000.0
            for i in sleepy_times:
                self.assertEquals(i, walker)
                walker *= 2

    def test_exponential_retrier(self):

        num_retries = 5

        def make_crappy_function(fail_start=1, fail_end=5):
            """Make a function that iterates from zero to infinity.
            However when it is iterating in the range [fail_start,fail_end]
            it will throw a ValueError exception but still increment
            """
            side_effects = [0]

            def ret():
                ret = side_effects[0]
                side_effects[0] += 1
                if ret >= fail_start and ret <= fail_end:
                    raise ValueError("foo %d" % ret)
                return ret

            return ret

        crappy_function = make_crappy_function()

        with self.check_exponential_backoff_sleep_times(500, 0):
            # first call to crappy_function should return zero without
            # any retrying
            self.assertEquals(0,
                              utils.exponential_retrier(
                                  crappy_function,
                                  max_retries=num_retries))

        with self.check_exponential_backoff_sleep_times(500, num_retries):
            # this should return 6 as we will retry 5 times
            self.assertEquals(6,
                              utils.exponential_retrier(
                                  crappy_function,
                                  max_retries=num_retries))

        with self.check_exponential_backoff_sleep_times(500, 0):
            self.assertEqual(7,
                             utils.exponential_retrier(
                                 crappy_function,
                                 max_retries=num_retries))

        with self.check_exponential_backoff_sleep_times(1, num_retries):
            # make sure this will fail in exponential_retrier and
            # test that last exception is re_thrown
            crappy_function = make_crappy_function(fail_start=0,
                                                   fail_end=1000)

            error = None
            try:
                utils.exponential_retrier(crappy_function,
                                          retry_min_wait_ms=1,
                                          max_retries=num_retries)
            except ValueError as e:
                # test that exception caught here has proper stack trace
                self.assertTrue(any(map(
                    lambda x: x[3] == "raise ValueError(\"foo %d\" % ret)",
                    traceback.extract_tb(sys.exc_traceback))))
                error = e

            self.assertEquals(error.message, "foo %d" % num_retries)

        with patch('time.sleep'):
            # check that exception is rethrown if we pass
            # exception filter that returns False if exception
            # is ValueError
            crappy_function = make_crappy_function(fail_start=0,
                                                   fail_end=0)

            def exception_filter(exception):
                return type(exception) is not ValueError

            error = None
            try:
                utils.exponential_retrier(crappy_function,
                                          retry_min_wait_ms=1,
                                          max_retries=100000,
                                          exception_filter=exception_filter)
            except ValueError as e:
                error = e

            self.assertEquals(error.message, "foo 0")

    def test_retriable_fetch_things_passthrough(self):
        # test simple pass through case

        num_retries = 5
        chunk_size = 5
        end = 20
        num_chunks = int(math.ceil(end / float(chunk_size)))

        fetch_things_with_retry = functools.partial(
            utils.fetch_things_with_retry,
            chunk_size=chunk_size,
            max_retries=num_retries,
            retry_min_wait_ms=1)

        crappy_query = CrappyQuery(0, end, 0)
        generated = list(fetch_things_with_retry(crappy_query))

        self.assertEquals(generated, range(0, end))
        self.assertEquals(crappy_query.total_num_failed, 0)
        self.assertEquals(crappy_query.num_after_was_called, num_chunks)

    def test_retriable_fetch_things_exception_rethrow(self):
        # test that exception is rethrown if we run out of retries

        num_retries = 5
        chunk_size = 5
        end = 20
        num_chunks = int(math.ceil(end / float(chunk_size)))

        fetch_things_with_retry = functools.partial(
            utils.fetch_things_with_retry,
            chunk_size=chunk_size,
            max_retries=num_retries,
            retry_min_wait_ms=1)

        with self.check_exponential_backoff_sleep_times(1, num_retries):
            error = None
            crappy_query = CrappyQuery(0, end, num_retries + 1)
            try:
                list(fetch_things_with_retry(crappy_query))
            except ValueError as e:
                error = e

            # after should not have ever been called because
            # getting the first chunk should have failed
            self.assertEquals(0, crappy_query.num_after_was_called)
            self.assertEquals("FOO %d %d %d" % (0, 0, num_retries + 1),
                              error.message)

        # test same thing but failing in subsequent chunk
        with self.check_exponential_backoff_sleep_times(1, num_retries):
            crappy_query = CrappyQuery(0, end, num_retries + 1, 2)
            generated = []
            try:
                # cant use list here as it wont get cbunks that succeeded
                for i in fetch_things_with_retry(crappy_query):
                    generated.append(i)
            except ValueError as e:
                error = e

            # we should have generated some partial results
            self.assertEquals(generated, range(0, chunk_size * 2))
            self.assertEquals("FOO %d %d %d" % (10, 2, num_retries + 1),
                              error.message)
            self.assertEquals(2, crappy_query.num_after_was_called)

    def test_retriable_fetch_things_recover_from_fail(self):
        # test that we get all of the numbers in the range
        # if we the number of failures is less than number of retries

        num_retries = 5
        chunk_size = 5
        end = 20
        num_chunks = int(math.ceil(end / float(chunk_size)))

        fetch_things_with_retry = functools.partial(
            utils.fetch_things_with_retry,
            chunk_size=chunk_size,
            max_retries=num_retries,
            retry_min_wait_ms=1)

        with patch('time.sleep'):
            crappy_query = CrappyQuery(0, end, num_retries - 1)
            generated = list(fetch_things_with_retry(crappy_query))

            self.assertEquals(generated, range(0, end))
            self.assertEqual(num_chunks,
                             crappy_query.num_after_was_called)

            # same thing but fail in the subsequent chunk
            crappy_query = CrappyQuery(0, end, num_retries - 1, 2)
            generated = list(fetch_things_with_retry(crappy_query))

            self.assertEquals(generated, range(0, end))
            self.assertEquals(num_chunks,
                              crappy_query.num_after_was_called)

        # test same thing as above but with chunks=True
        with patch('time.sleep'):
            expected = []
            for i in range(0, num_chunks):
                expected.append(range(i * chunk_size,
                                      i * chunk_size + chunk_size))

            crappy_query = CrappyQuery(0, end, num_retries - 1)
            generated = list(fetch_things_with_retry(crappy_query,
                                                     chunks=True))

            self.assertEquals(generated, expected)
            self.assertEqual(num_chunks,
                             crappy_query.num_after_was_called)

            # same thing but fail in the subsequent chunk
            crappy_query = CrappyQuery(0, end, num_retries - 1, 2)
            generated = list(fetch_things_with_retry(crappy_query,
                                                     chunks=True))

            self.assertEquals(generated, expected)
            self.assertEquals(num_chunks,
                              crappy_query.num_after_was_called)

    def test_weighted_lottery(self):
        weights = collections.OrderedDict(
            [('x', 2), (None, 0), (None, 0), ('y', 3), ('z', 1)])

        def expect(result, random_value):
            scaled_r = float(random_value) / sum(weights.itervalues())
            self.assertEquals(
                result,
                utils.weighted_lottery(weights, _random=lambda: scaled_r))

        expect('x', 0)
        expect('x', 1)
        expect('y', 2)
        expect('y', 3)
        expect('y', 4)
        expect('z', 5)
        self.assertRaises(ValueError, expect, None, 6)

    def test_extract_subdomain(self):
        self.assertEquals(
            utils.extract_subdomain('beta.reddit.com', 'reddit.com'),
            'beta')

        self.assertEquals(
            utils.extract_subdomain('beta.reddit.local:8000', 'reddit.local'),
            'beta')

        self.assertEquals(
            utils.extract_subdomain('reddit.com', 'reddit.com'),
            '')

        self.assertEquals(
            utils.extract_subdomain('internet-frontpage.com', 'reddit.com'),
            '')

    def test_coerce_url_to_protocol(self):
        self.assertEquals(
            utils.coerce_url_to_protocol('http://example.com/foo'),
            'http://example.com/foo')

        self.assertEquals(
            utils.coerce_url_to_protocol('https://example.com/foo'),
            'http://example.com/foo')

        self.assertEquals(
            utils.coerce_url_to_protocol('//example.com/foo'),
            'http://example.com/foo')

        self.assertEquals(
            utils.coerce_url_to_protocol('http://example.com/foo', 'https'),
            'https://example.com/foo')

        self.assertEquals(
            utils.coerce_url_to_protocol('https://example.com/foo', 'https'),
            'https://example.com/foo')

        self.assertEquals(
            utils.coerce_url_to_protocol('//example.com/foo', 'https'),
            'https://example.com/foo')

    def test_sanitize_url(self):
        self.assertEquals(
            utils.sanitize_url('http://dk./'),
            'http://dk/'
        )

        self.assertEquals(
            utils.sanitize_url('http://google.com./'),
            'http://google.com/'
        )

        self.assertEquals(
            utils.sanitize_url('http://google.com/'),
            'http://google.com/'
        )

        self.assertEquals(
            utils.sanitize_url('https://github.com/reddit/reddit/pull/1302'),
            'https://github.com/reddit/reddit/pull/1302'
        )

        self.assertEquals(
            utils.sanitize_url('http://dk../'),
            None
        )


class TestCanonicalizeEmail(unittest.TestCase):
    def test_empty_string(self):
        canonical = utils.canonicalize_email("")
        self.assertEquals(canonical, "")

    def test_unicode(self):
        canonical = utils.canonicalize_email(u"\u2713@example.com")
        self.assertEquals(canonical, "\xe2\x9c\x93@example.com")

    def test_localonly(self):
        canonical = utils.canonicalize_email("invalid")
        self.assertEquals(canonical, "")

    def test_multiple_ats(self):
        canonical = utils.canonicalize_email("invalid@invalid@invalid")
        self.assertEquals(canonical, "")

    def test_remove_dots(self):
        canonical = utils.canonicalize_email("d.o.t.s@example.com")
        self.assertEquals(canonical, "dots@example.com")

    def test_remove_plus_address(self):
        canonical = utils.canonicalize_email("fork+nork@example.com")
        self.assertEquals(canonical, "fork@example.com")

    def test_unicode_in_byte_str(self):
        # this shouldn't ever happen, but some entries in postgres appear
        # to be byte strings with non-ascii in 'em.
        canonical = utils.canonicalize_email("\xe2\x9c\x93@example.com")
        self.assertEquals(canonical, "\xe2\x9c\x93@example.com")


class TestTruncString(unittest.TestCase):
    def test_empty_string(self):
        truncated = utils.trunc_string('', 80)
        self.assertEqual(truncated, '')

    def test_short_enough(self):
        truncated = utils.trunc_string('short string', 80)
        self.assertEqual(truncated, 'short string')

    def test_word_breaks(self):
        truncated = utils.trunc_string('two words', 6)
        self.assertEqual(truncated, 'two...')

    def test_suffix(self):
        truncated = utils.trunc_string('two words', 6, '')
        self.assertEqual(truncated, 'two')

    def test_really_long_words(self):
        truncated = utils.trunc_string('ThisIsALongWord', 10)
        self.assertEqual(truncated, 'ThisIsA...')


class TestUrlToThing(unittest.TestCase):

    def test_subreddit_noslash(self):
        with patch('r2.models.Subreddit') as MockSubreddit:
            MockSubreddit._by_name.return_value = s.Subreddit
            self.assertEqual(
                utils.url_to_thing('http://reddit.local/r/pics'),
                s.Subreddit,
            )

    def test_subreddit(self):
        with patch('r2.models.Subreddit') as MockSubreddit:
            MockSubreddit._by_name.return_value = s.Subreddit
            self.assertEqual(
                utils.url_to_thing('http://reddit.local/r/pics/'),
                s.Subreddit,
            )

    def test_frontpage(self):
        self.assertEqual(
            utils.url_to_thing('http://reddit.local/'),
            None,
        )
<EOF>
<BOF>
from mock import MagicMock, patch

from pylons import app_globals as g

from r2.tests import RedditTestCase
from r2.lib import signing


class SigningTests(RedditTestCase):
    def setUp(self):
        super(RedditTestCase, self).setUp()
        g.secrets['request_signature_secret'] = "super_secret_do_not_share"

    def test_get_token(self):
        # since the secret is static, so are these, so we're testing backward
        # compat of the algo as well as verifying it varies with all inputs!
        self.assertEqual(
            signing.get_secret_token("test", 1, 1),
            "008c42a8952d949b9c95109eea5016bb00a5a0ac141b35a0691fe6a01f084241",
        )
        self.assertEqual(
            signing.get_secret_token("test", 2, 1),
            "5081cd2623e0391da6b81d9590e9272e00bd17c29b4e3fb9b0044ff999cf5ae2",
        )
        self.assertRaises(
            AssertionError,
            lambda: signing.get_secret_token("test", 1, 2),
        )
        self.assertEqual(
            signing.get_secret_token("test2", 1, 1),
            "07e87fdff4b8300b5282993cf30f8d652383854bf37a96da018354f7f5481832",
        )

    def make_sig_header(self, body, platform="test", version=1, epoch=None):
        return signing.sign_v1_message(
            body,
            platform=platform,
            version=version,
            epoch=epoch,
        )

    def _assert_validity(self, body, header, success, error, **expected):
        request = MagicMock(body=body, headers={})
        if header:
            request.headers[signing.SIGNATURE_BODY_HEADER] = header
        signature = signing.valid_post_signature(request)
        self.assertEqual(signature.is_valid(), bool(success))
        if error:
            self.assertIn(error.code, [code for code, _ in signature.errors])
        else:
            self.assertEqual(len(signature.errors), 0)
        has_mac = expected.pop("has_mac", False)
        for k, v in expected.iteritems():
            got = getattr(signature, k)
            self.assertEqual(got, v, "signature.%s: %s != %s" % (k, got, v))

        if has_mac:
            self.assertTrue(bool(signature.mac))
        else:
            self.assertIsNone(signature.mac)

    def assert_valid(self, body, header, **expected):
        expected['success'] = True
        expected['error'] = None
        expected['has_mac'] = True
        return self._assert_validity(body, header, **expected)

    def assert_invalid(self, body, header, error, **expected):
        expected.setdefault("global_version", -1)
        expected.setdefault("version", -1)
        expected.setdefault("platform", None)
        expected.setdefault('has_mac', False)
        expected['success'] = False
        expected['error'] = error
        return self._assert_validity(body, header, **expected)

    def test_signing(self):
        epoch_time = 1234567890
        header = self.make_sig_header(
            '{"user": "reddit", "password": "hunter2"}',
            epoch=epoch_time,
        )
        self.assertEqual(
            header,
            "1:test:1:1234567890:"
            "0fc3d90d83ac7433a5376c17f2aea9b470c368740c91c513e819e3a4980349de"
        )

    def test_valid_header(self):
        body = '{"user": "reddit", "password": "hunter2"}'
        platform = "something"
        version = 2
        header = self.make_sig_header(
            "Body:{}".format(body),
            platform=platform,
            version=version,
        )
        self.assert_valid(
            body,
            header,
            version=version,
            platform=platform,
            global_version=1,
        )

    def test_no_header(self):
        body = '{"user": "reddit", "password": "hunter2"}'
        self.assert_invalid(body, "", signing.ERRORS.INVALID_FORMAT)

    def test_garbage_header(self):
        body = '{"user": "reddit", "password": "hunter2"}'
        self.assert_invalid(
            body,
            header="idontneednosignature",
            error=signing.ERRORS.INVALID_FORMAT,
        )

    def test_future_header(self):
        body = '{"user": "reddit", "password": "hunter2"}'
        self.assert_invalid(
            body,
            header="2:awesomefuturespec",
            error=signing.ERRORS.UNKOWN_GLOBAL_VERSION,
            global_version=2,
        )

    @patch.object(signing, "is_invalid_token", return_value=True)
    def test_invalid(self, _):
        body = '{"user": "reddit", "password": "hunter2"}'
        platform = "something"
        version = 2
        # this is a perfectly valid signature (from `test_valid_header`)
        # and a properly constructed request, but we've patched
        # is_invalid_token
        header = self.make_sig_header(body, platform=platform, version=version)
        self.assert_invalid(
            body,
            header=header,
            error=signing.ERRORS.INVALIDATED_TOKEN,
            global_version=1,
            version=version,
            platform=platform,
            has_mac=True,
        )

    def test_invalid_header(self):
        body = '{"user": "reddit", "password": "hunter2"}'
        platform = "test"
        version = 1
        header = "1:%s:%s:deadbeef" % (platform, version)
        self.assert_invalid(
            body,
            header=header,
            error=signing.ERRORS.UNPARSEABLE,
            global_version=1,
        )

    def test_expired_header(self):
        body = '{"user": "reddit", "password": "hunter2"}'
        platform = "test"
        version = 1
        header = "1:%s:%s:0:deadbeef" % (platform, version)
        self.assert_invalid(
            body,
            header=header,
            error=signing.ERRORS.EXPIRED_TOKEN,
            global_version=1,
            platform=platform,
            version=version,
            has_mac=True,
        )
<EOF>
<BOF>
import datetime
import unittest

from mock import MagicMock, Mock, patch

from r2.lib.promote import (
    get_nsfw_collections_srnames,
    get_refund_amount,
    refund_campaign,
    srnames_from_site,
)
from r2.models import (
    Account,
    Collection,
    FakeAccount,
    Frontpage,
    PromoCampaign,
    Subreddit,
    MultiReddit,
)
from r2.tests import RedditTestCase, NonCache


subscriptions_srnames = ["foo", "bar"]
subscriptions = map(lambda srname: Subreddit(name=srname), subscriptions_srnames)
multi_srnames = ["bing", "bat"]
multi_subreddits = map(lambda srname: Subreddit(name=srname), multi_srnames)
nice_srname = "mylittlepony"
nsfw_srname = "pr0n"
questionably_nsfw = "sexstories"
quarantined_srname = "croontown"
naughty_subscriptions = [
    Subreddit(name=nice_srname),
    Subreddit(name=nsfw_srname, over_18=True),
    Subreddit(name=quarantined_srname, quarantine=True),
]
nsfw_collection_srnames = [questionably_nsfw, nsfw_srname]
nsfw_collection = Collection(
    name="after dark",
    sr_names=nsfw_collection_srnames,
    over_18=True
)

class TestSRNamesFromSite(RedditTestCase):
    def setUp(self):
        self.logged_in = Account(name="test")
        self.logged_out = FakeAccount()

        self.patch_g(memoizecache=NonCache())

    def test_frontpage_logged_out(self):
        srnames = srnames_from_site(self.logged_out, Frontpage)

        self.assertEqual(srnames, {Frontpage.name})

    @patch("r2.models.Subreddit.user_subreddits")
    def test_frontpage_logged_in(self, user_subreddits):
        user_subreddits.return_value = subscriptions
        srnames = srnames_from_site(self.logged_in, Frontpage)

        self.assertEqual(srnames, set(subscriptions_srnames) | {Frontpage.name})

    def test_multi_logged_out(self):
        multi = MultiReddit(path="/user/test/m/multi_test", srs=multi_subreddits)
        srnames = srnames_from_site(self.logged_out, multi)

        self.assertEqual(srnames, set(multi_srnames))

    @patch("r2.models.Subreddit.user_subreddits")
    def test_multi_logged_in(self, user_subreddits):
        user_subreddits.return_value = subscriptions
        multi = MultiReddit(path="/user/test/m/multi_test", srs=multi_subreddits)
        srnames = srnames_from_site(self.logged_in, multi)

        self.assertEqual(srnames, set(multi_srnames))

    def test_subreddit_logged_out(self):
        srname = "test1"
        subreddit = Subreddit(name=srname)
        srnames = srnames_from_site(self.logged_out, subreddit)

        self.assertEqual(srnames, {srname})

    @patch("r2.models.Subreddit.user_subreddits")
    def test_subreddit_logged_in(self, user_subreddits):
        user_subreddits.return_value = subscriptions
        srname = "test1"
        subreddit = Subreddit(name=srname)
        srnames = srnames_from_site(self.logged_in, subreddit)

        self.assertEqual(srnames, {srname})

    @patch("r2.models.Subreddit.user_subreddits")
    def test_quarantined_subscriptions_are_never_included(self, user_subreddits):
        user_subreddits.return_value = naughty_subscriptions
        subreddit = Frontpage
        srnames = srnames_from_site(self.logged_in, subreddit)

        self.assertEqual(srnames, {subreddit.name} | {nice_srname})
        self.assertTrue(len(srnames & {quarantined_srname}) == 0)

    @patch("r2.models.Subreddit.user_subreddits")
    def test_nsfw_subscriptions_arent_included_when_viewing_frontpage(self, user_subreddits):
        user_subreddits.return_value = naughty_subscriptions
        srnames = srnames_from_site(self.logged_in, Frontpage)

        self.assertEqual(srnames, {Frontpage.name} | {nice_srname})
        self.assertTrue(len(srnames & {nsfw_srname}) == 0)

    @patch("r2.models.Collection.get_all")
    def test_get_nsfw_collections_srnames(self, get_all):
        get_all.return_value = [nsfw_collection]
        srnames = get_nsfw_collections_srnames()

        self.assertEqual(srnames, set(nsfw_collection_srnames))

    @patch("r2.lib.promote.get_nsfw_collections_srnames")
    def test_remove_nsfw_collection_srnames_on_frontpage(self, get_nsfw_collections_srnames):
        get_nsfw_collections_srnames.return_value = set(nsfw_collection.sr_names)
        srname = "test1"
        subreddit = Subreddit(name=srname)
        Subreddit.user_subreddits = MagicMock(return_value=[
            Subreddit(name=nice_srname),
            Subreddit(name=questionably_nsfw),
        ])

        frontpage_srnames = srnames_from_site(self.logged_in, Frontpage)
        swf_srnames = srnames_from_site(self.logged_in, subreddit)

        self.assertEqual(frontpage_srnames, {Frontpage.name, nice_srname})
        self.assertTrue(len(frontpage_srnames & {questionably_nsfw}) == 0)


class TestPromoteRefunds(unittest.TestCase):
    def setUp(self):
        self.link = Mock()
        self.campaign = MagicMock(spec=PromoCampaign)
        self.campaign._id = 1
        self.campaign.owner_id = 1
        self.campaign.trans_id = 1
        self.campaign.bid_pennies = 1
        self.campaign.start_date = datetime.datetime.now()
        self.campaign.end_date = (datetime.datetime.now() +
            datetime.timedelta(days=1))
        self.campaign.total_budget_dollars = 200.
        self.refund_amount = 100.
        self.billable_amount = 100.
        self.billable_impressions = 1000

    @patch('r2.lib.promote.authorize.refund_transaction')
    @patch('r2.lib.promote.PromotionLog.add')
    @patch('r2.lib.promote.queries.unset_underdelivered_campaigns')
    @patch('r2.lib.promote.emailer.refunded_promo')
    def test_refund_campaign_success(self, emailer_refunded_promo,
            queries_unset, promotion_log_add, refund_transaction):
        """Assert return value and that correct calls are made on success."""
        refund_transaction.return_value = (True, None)

        # the refund process attemtps a db lookup. We don't need it for the
        # purpose of the test.
        with patch.object(Account, "_byID"):
            success = refund_campaign(
                link=self.link,
                camp=self.campaign,
                refund_amount=self.refund_amount,
                billable_amount=self.billable_amount,
                billable_impressions=self.billable_impressions,
            )

        self.assertTrue(refund_transaction.called)
        self.assertTrue(promotion_log_add.called)
        queries_unset.assert_called_once_with(self.campaign)
        emailer_refunded_promo.assert_called_once_with(self.link)
        self.assertTrue(success)

    @patch('r2.lib.promote.authorize.refund_transaction')
    @patch('r2.lib.promote.PromotionLog.add')
    def test_refund_campaign_failed(self, promotion_log_add,
            refund_transaction):
        """Assert return value and that correct calls are made on failure."""
        refund_transaction.return_value = (False, None)

        # the refund process attemtps a db lookup. We don't need it for the
        # purpose of the test.
        with patch.object(Account, "_byID"):
            success = refund_campaign(
                link=self.link,
                camp=self.campaign,
                refund_amount=self.refund_amount,
                billable_amount=self.billable_amount,
                billable_impressions=self.billable_impressions,
            )

        self.assertTrue(refund_transaction.called)
        self.assertTrue(promotion_log_add.called)
        self.assertFalse(success)

    def test_get_refund_amount_when_zero(self):
        """
        Assert that correct value is returned when existing refund_amount is
        zero.
        """
        campaign = MagicMock(spec=('total_budget_dollars',))
        campaign.total_budget_dollars = 200.
        refund_amount = get_refund_amount(campaign, self.billable_amount)
        self.assertEquals(refund_amount,
            campaign.total_budget_dollars - self.billable_amount)

    def test_get_refund_amount_rounding(self):
        """Assert that inputs are correctly rounded up to the nearest penny."""
        # If campaign.refund_amount is less than a fraction of a penny,
        # the refund_amount should be campaign.total_budget_dollars.
        self.campaign.refund_amount = 0.00000001
        refund_amount = get_refund_amount(self.campaign, self.billable_amount)
        self.assertEquals(refund_amount, self.billable_amount)

        self.campaign.refund_amount = 0.00999999
        refund_amount = get_refund_amount(self.campaign, self.billable_amount)
        self.assertEquals(refund_amount, self.billable_amount)

        # If campaign.refund_amount is just slightly more than a penny,
        # the refund amount should be campaign.total_budget_dollars - 0.01.
        self.campaign.refund_amount = 0.01000001
        refund_amount = get_refund_amount(self.campaign, self.billable_amount)
        self.assertEquals(refund_amount, self.billable_amount - 0.01)

        # Even if campaign.refund_amount is just barely short of two pennies,
        # the refund amount should be campaign.total_budget_dollars - 0.01.
        self.campaign.refund_amount = 0.01999999
        refund_amount = get_refund_amount(self.campaign, self.billable_amount)
        self.assertEquals(refund_amount, self.billable_amount - 0.01)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib import js


def concat_sources(sources):
    return ";".join(sources)


class TestFileSource(js.FileSource):
    def get_source(self, *args, **kwargs):
        return self.name


class TestModule(js.Module):
    def get_default_source(self, source):
        return TestFileSource(source)

    def build(self, *args, **kwargs):
        sources = self.get_flattened_sources([])
        sources = [s.get_source() for s in sources]
        return concat_sources(sources)


class TestModuleGetFlattenedSources(unittest.TestCase):
    def test_flat_modules_include_all_sources(self):
        test_files = ["foo.js", "bar.js", "baz.js", "qux.js"]
        test_module = TestModule("test_module", *test_files)
        self.assertEqual(test_module.build(), concat_sources(test_files))

    def test_nested_modules_include_all_sources(self):
        test_files_a = ["foo.js", "bar.js"]
        test_module_a = TestModule("test_module_a", *test_files_a)
        test_files_b = ["baz.js", "qux.js"]
        test_module_b = TestModule("test_module_b", *test_files_b)
        test_module = TestModule("test_mobule", test_module_a, test_module_b)
        self.assertEqual(test_module.build(), concat_sources(test_files_a + test_files_b))

    def test_flat_modules_only_include_sources_once(self):
        test_files = ["foo.js", "bar.js", "baz.js", "qux.js"]
        test_files_dup = test_files * 2
        test_module = TestModule("test_module", *test_files_dup)
        self.assertEqual(test_module.build(), concat_sources(test_files))

    def test_nested_modules_only_include_sources_once(self):
        test_files = ["foo.js", "bar.js", "baz.js", "qux.js"]
        test_module_a = TestModule("test_module_a", *test_files)
        test_module_b = TestModule("test_module_b", *test_files)
        test_module = TestModule("test_mobule", test_module_a, test_module_b)
        self.assertEqual(test_module.build(), concat_sources(test_files))

    def test_filtered_modules_do_not_include_filtered_sources(self):
        test_files = ["foo.js", "bar.js"]
        filtered_files = ["baz.js", "qux.js"]
        all_files = test_files + filtered_files
        filter_module = TestModule("filter_module", *filtered_files)
        test_module = TestModule("test_module", filter_module=filter_module, *all_files)
        self.assertEqual(test_module.build(), concat_sources(test_files))
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib import stats

class TimingStatBufferTest(unittest.TestCase):
    def test_tsb(self):
        tsb = stats.TimingStatBuffer()
        self.assertEquals([], list(tsb.flush()))

        for i in xrange(1, 4):
            for j in xrange(i):
                tsb.record(str(i), 0, 0.1 * (j + 1))
        self.assertEquals(
            set([('1', '100.0|ms'),
                 ('2', '150.0|ms'),  # (0.1 + 0.2) / 2
                 ('3', '200.0|ms'),  # (0.1 + 0.2 + 0.3) / 3
                ]), set(tsb.flush()))

class CountingStatBufferTest(unittest.TestCase):
    def test_csb(self):
        csb = stats.CountingStatBuffer()
        self.assertEquals([], list(csb.flush()))

        for i in xrange(1, 4):
            for j in xrange(i):
                csb.record(str(i), j + 1)
        self.assertEquals(
            set([('1', '1|c'),
                 ('2', '3|c'),
                 ('3', '6|c')]),
            set(csb.flush()))

class StringCountBufferTest(unittest.TestCase):
    def test_encode_string(self):
        enc = stats.StringCountBuffer._encode_string
        self.assertEquals('test', enc('test'))
        self.assertEquals('\\n\\&\\\\&', enc('\n|\\&'))

    def test_scb(self):
        scb = stats.StringCountBuffer()
        self.assertEquals([], list(scb.flush()))

        for i in xrange(1, 4):
            for j in xrange(i):
                for k in xrange(j + 1):
                    scb.record(str(i), str(j))
        self.assertEquals(
            set([('1', '1|s|0'),
                 ('2', '1|s|0'),
                 ('2', '2|s|1'),
                 ('3', '1|s|0'),
                 ('3', '2|s|1'),
                 ('3', '3|s|2')]),
            set(scb.flush()))

class FakeUdpSocket:
    def __init__(self, *ignored_args):
        self.host = None
        self.port = None
        self.datagrams = []

    def sendto(self, datagram, host_port):
        self.datagrams.append(datagram)

class StatsdConnectionUnderTest(stats.StatsdConnection):
    _make_socket = FakeUdpSocket

class StatsdConnectionTest(unittest.TestCase):
    @staticmethod
    def connect(compress=False):
         return StatsdConnectionUnderTest('host:1000', compress=compress)

    def test_parse_addr(self):
        self.assertEquals(
            ('1:2', 3), stats.StatsdConnection._parse_addr('1:2:3'))

    def test_send(self):
        conn = self.connect()
        conn.send((i, i) for i in xrange(1, 6))
        self.assertEquals(
            ['1:1\n2:2\n3:3\n4:4\n5:5'],
            conn.sock.datagrams)

        # verify compression
        data = [('a.b.c.w', 1), ('a.b.c.x', 2), ('a.b.c.y', 3), ('a.b.z', 4),
                ('bbb', 5), ('bbc', 6)]
        conn = self.connect(compress=True)
        conn.send(reversed(data))
        self.assertEquals(
            ['a.b.c.w:1\n^06x:2\n^06y:3\n^04z:4\nbbb:5\nbbc:6'],
            conn.sock.datagrams)
        conn = self.connect(compress=False)
        conn.send(reversed(data))
        self.assertEquals(
            ['bbc:6\nbbb:5\na.b.z:4\na.b.c.y:3\na.b.c.x:2\na.b.c.w:1'],
            conn.sock.datagrams)

        # ensure send is a no-op when not connected
        conn.sock = None
        conn.send((i, i) for i in xrange(1, 6))

class StatsdClientUnderTest(stats.StatsdClient):
    @classmethod
    def _data_iterator(cls, x):
       return sorted(iter(x))

    @classmethod
    def _make_conn(cls, addr):
        return StatsdConnectionUnderTest(addr, compress=False)

class StatsdClientTest(unittest.TestCase):
    def test_flush(self):
        client = StatsdClientUnderTest('host:1000')
        client.timing_stats.record('t', 0, 1)
        client.counting_stats.record('c', 1)
        client.flush()
        self.assertEquals(
            ['c:1|c\nt:1000.0|ms'],
            client.conn.sock.datagrams)

class CounterAndTimerTest(unittest.TestCase):
    @staticmethod
    def client():
        return StatsdClientUnderTest('host:1000')

    def test_get_stat_name(self):
        self.assertEquals(
            'a.b.c',
            stats._get_stat_name('a', '', u'b', None, 'c', 0))

    def test_counter(self):
        c = stats.Counter(self.client(), 'c')
        c.increment('a')
        c.increment('b', 2)
        c.decrement('c')
        c.decrement('d', 2)
        c += 1
        c -= 2
        self.assertEquals(
            set([('c.a', '1|c'),
                 ('c.b', '2|c'),
                 ('c.c', '-1|c'),
                 ('c.d', '-2|c'),
                 ('c', '-1|c')]),
            set(c.client.counting_stats.flush()))
        self.assertEquals(set(), set(c.client.counting_stats.flush()))

    def test_timer(self):
        t = stats.Timer(self.client(), 't')
        t._time = iter(i / 10.0 for i in xrange(10)).next
        self.assertRaises(AssertionError, t.intermediate, 'fail')
        self.assertRaises(AssertionError, t.stop)

        t.start()
        t.intermediate('a')
        t.intermediate('b')
        t.intermediate('c')
        t.stop(subname='t')

        self.assertRaises(AssertionError, t.intermediate, 'fail')
        self.assertRaises(AssertionError, t.stop)
        t.send('x', 0, 0.5)

        self.assertEquals(
            set([('t.a', '100.0|ms'),
                 ('t.b', '100.0|ms'),
                 ('t.c', '100.0|ms'),
                 ('t.t', '400.0|ms'),
                 ('t.x', '500.0|ms')]),
            set(t.client.timing_stats.flush()))
        self.assertEquals(set(), set(t.client.timing_stats.flush()))
<EOF>
<BOF>
#!/usr/bin/env python
# coding=utf-8
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import unittest

from r2.lib.utils import UrlParser
from r2.tests import RedditTestCase
from pylons import app_globals as g


class TestIsRedditURL(RedditTestCase):

    def setUp(self):
        self.patch_g(offsite_subdomains=['blog'])

    def _is_safe_reddit_url(self, url, subreddit=None):
        web_safe = UrlParser(url).is_web_safe_url()
        return web_safe and UrlParser(url).is_reddit_url(subreddit)

    def assertIsSafeRedditUrl(self, url, subreddit=None):
        self.assertTrue(self._is_safe_reddit_url(url, subreddit))

    def assertIsNotSafeRedditUrl(self, url, subreddit=None):
        self.assertFalse(self._is_safe_reddit_url(url, subreddit))

    def test_normal_urls(self):
        self.assertIsSafeRedditUrl("https://%s/" % g.domain)
        self.assertIsSafeRedditUrl("https://en.%s/" % g.domain)
        self.assertIsSafeRedditUrl("https://foobar.baz.%s/quux/?a" % g.domain)
        self.assertIsSafeRedditUrl("#anchorage")
        self.assertIsSafeRedditUrl("?path_relative_queries")
        self.assertIsSafeRedditUrl("/")
        self.assertIsSafeRedditUrl("/cats")
        self.assertIsSafeRedditUrl("/cats/")
        self.assertIsSafeRedditUrl("/cats/#maru")
        self.assertIsSafeRedditUrl("//foobaz.%s/aa/baz#quux" % g.domain)
        # XXX: This is technically a legal relative URL, are there any UAs
        # stupid enough to treat this as absolute?
        self.assertIsSafeRedditUrl("path_relative_subpath.com")
        # "blog.reddit.com" is not a reddit URL.
        self.assertIsNotSafeRedditUrl("http://blog.%s/" % g.domain)
        self.assertIsNotSafeRedditUrl("http://foo.blog.%s/" % g.domain)

    def test_incorrect_anchoring(self):
        self.assertIsNotSafeRedditUrl("http://www.%s.whatever.com/" % g.domain)

    def test_protocol_relative(self):
        self.assertIsNotSafeRedditUrl("//foobaz.example.com/aa/baz#quux")

    def test_weird_protocols(self):
        self.assertIsNotSafeRedditUrl(
            "javascript://%s/%%0d%%0aalert(1)" % g.domain
        )
        self.assertIsNotSafeRedditUrl("hackery:whatever")

    def test_http_auth(self):
        # There's no legitimate reason to include HTTP auth details in the URL,
        # they only serve to confuse everyone involved.
        # For example, this used to be the behaviour of `UrlParser`, oops!
        # > UrlParser("http://everyoneforgets:aboutthese@/baz.com/").unparse()
        # 'http:///baz.com/'
        self.assertIsNotSafeRedditUrl("http://foo:bar@/example.com/")

    def test_browser_quirks(self):
        # Some browsers try to be helpful and ignore characters in URLs that
        # they think might have been accidental (I guess due to things like:
        # `<a href=" http://badathtml.com/ ">`. We need to ignore those when
        # determining if a URL is local.
        self.assertIsNotSafeRedditUrl("/\x00/example.com")
        self.assertIsNotSafeRedditUrl("\x09//example.com")
        self.assertIsNotSafeRedditUrl(" http://example.com/")

        # This is makes sure we're not vulnerable to a bug in
        # urlparse / urlunparse.
        # urlunparse(urlparse("////foo.com")) == "//foo.com"! screwy!
        self.assertIsNotSafeRedditUrl("////example.com/")
        self.assertIsNotSafeRedditUrl("//////example.com/")
        # Similar, but with a scheme
        self.assertIsNotSafeRedditUrl(r"http:///example.com/")
        # Webkit and co like to treat backslashes as equivalent to slashes in
        # different places, maybe to make OCD Windows users happy.
        self.assertIsNotSafeRedditUrl(r"/\example.com/")
        # On chrome this goes to example.com, not a subdomain of reddit.com!
        self.assertIsNotSafeRedditUrl(
            r"http://\\example.com\a.%s/foo" % g.domain
        )

        # Combo attacks!
        self.assertIsNotSafeRedditUrl(r"///\example.com/")
        self.assertIsNotSafeRedditUrl(r"\\example.com")
        self.assertIsNotSafeRedditUrl("/\x00//\\example.com/")
        self.assertIsNotSafeRedditUrl(
            "\x09javascript://%s/%%0d%%0aalert(1)" % g.domain
        )
        self.assertIsNotSafeRedditUrl(
            "http://\x09example.com\\%s/foo" % g.domain
        )

    def test_url_mutation(self):
        u = UrlParser("http://example.com/")
        u.hostname = g.domain
        self.assertTrue(u.is_reddit_url())

        u = UrlParser("http://%s/" % g.domain)
        u.hostname = "example.com"
        self.assertFalse(u.is_reddit_url())

    def test_nbsp_allowances(self):
        # We have to allow nbsps in URLs, let's just allow them where they can't
        # do any damage.
        self.assertIsNotSafeRedditUrl("http://\xa0.%s/" % g.domain)
        self.assertIsNotSafeRedditUrl("\xa0http://%s/" % g.domain)
        self.assertIsSafeRedditUrl("http://%s/\xa0" % g.domain)
        self.assertIsSafeRedditUrl("/foo/bar/\xa0baz")
        # Make sure this works if the URL is unicode
        self.assertIsNotSafeRedditUrl(u"http://\xa0.%s/" % g.domain)
        self.assertIsNotSafeRedditUrl(u"\xa0http://%s/" % g.domain)
        self.assertIsSafeRedditUrl(u"http://%s/\xa0" % g.domain)
        self.assertIsSafeRedditUrl(u"/foo/bar/\xa0baz")


class TestSwitchSubdomainByExtension(RedditTestCase):
    def setUp(self):
        self.patch_g(
            domain='reddit.com',
            domain_prefix='www',
        )

    def test_normal_urls(self):
        u = UrlParser('http://www.reddit.com/r/redditdev')
        u.switch_subdomain_by_extension('compact')
        result = u.unparse()
        self.assertEquals('http://i.reddit.com/r/redditdev', result)

        u = UrlParser(result)
        u.switch_subdomain_by_extension('mobile')
        result = u.unparse()
        self.assertEquals('http://simple.reddit.com/r/redditdev', result)

    def test_default_prefix(self):
        u = UrlParser('http://i.reddit.com/r/redditdev')
        u.switch_subdomain_by_extension()
        self.assertEquals('http://www.reddit.com/r/redditdev', u.unparse())

        u = UrlParser('http://i.reddit.com/r/redditdev')
        u.switch_subdomain_by_extension('does-not-exist')
        self.assertEquals('http://www.reddit.com/r/redditdev', u.unparse())


class TestPathExtension(unittest.TestCase):
    def test_no_path(self):
        u = UrlParser('http://example.com')
        self.assertEquals('', u.path_extension())

    def test_directory(self):
        u = UrlParser('http://example.com/')
        self.assertEquals('', u.path_extension())

        u = UrlParser('http://example.com/foo/')
        self.assertEquals('', u.path_extension())

    def test_no_extension(self):
        u = UrlParser('http://example.com/a')
        self.assertEquals('', u.path_extension())

    def test_root_file(self):
        u = UrlParser('http://example.com/a.jpg')
        self.assertEquals('jpg', u.path_extension())

    def test_nested_file(self):
        u = UrlParser('http://example.com/foo/a.jpg')
        self.assertEquals('jpg', u.path_extension())

    def test_empty_extension(self):
        u = UrlParser('http://example.com/a.')
        self.assertEquals('', u.path_extension())

    def test_two_extensions(self):
        u = UrlParser('http://example.com/a.jpg.exe')
        self.assertEquals('exe', u.path_extension())

    def test_only_extension(self):
        u = UrlParser('http://example.com/.bashrc')
        self.assertEquals('bashrc', u.path_extension())


class TestEquality(unittest.TestCase):
    def test_different_objects(self):
        u = UrlParser('http://example.com')
        self.assertNotEquals(u, None)

    def test_different_protocols(self):
        u = UrlParser('http://example.com')
        u2 = UrlParser('https://example.com')
        self.assertNotEquals(u, u2)

    def test_different_domains(self):
        u = UrlParser('http://example.com')
        u2 = UrlParser('http://example.org')
        self.assertNotEquals(u, u2)

    def test_different_ports(self):
        u = UrlParser('http://example.com')
        u2 = UrlParser('http://example.com:8000')
        u3 = UrlParser('http://example.com:8008')
        self.assertNotEquals(u, u2)
        self.assertNotEquals(u2, u3)

    def test_different_paths(self):
        u = UrlParser('http://example.com')
        u2 = UrlParser('http://example.com/a')
        u3 = UrlParser('http://example.com/b')
        self.assertNotEquals(u, u2)
        self.assertNotEquals(u2, u3)

    def test_different_params(self):
        u = UrlParser('http://example.com/')
        u2 = UrlParser('http://example.com/;foo')
        u3 = UrlParser('http://example.com/;bar')
        self.assertNotEquals(u, u2)
        self.assertNotEquals(u2, u3)

    def test_different_queries(self):
        u = UrlParser('http://example.com/')
        u2 = UrlParser('http://example.com/?foo')
        u3 = UrlParser('http://example.com/?foo=bar')
        self.assertNotEquals(u, u2)
        self.assertNotEquals(u2, u3)

    def test_different_fragments(self):
        u = UrlParser('http://example.com/')
        u2 = UrlParser('http://example.com/#foo')
        u3 = UrlParser('http://example.com/#bar')
        self.assertNotEquals(u, u2)
        self.assertNotEquals(u2, u3)

    def test_same_url(self):
        u = UrlParser('http://example.com:8000/a;b?foo=bar&bar=baz#spam')
        u2 = UrlParser('http://example.com:8000/a;b?bar=baz&foo=bar#spam')
        self.assertEquals(u, u2)

        u3 = UrlParser('')
        u3.scheme = 'http'
        u3.hostname = 'example.com'
        u3.port = 8000
        u3.path = '/a'
        u3.params = 'b'
        u3.update_query(foo='bar', bar='baz')
        u3.fragment = 'spam'
        self.assertEquals(u, u3)

    def test_integer_query_params(self):
        u = UrlParser('http://example.com/?page=1234')
        u2 = UrlParser('http://example.com/')
        u2.update_query(page=1234)
        self.assertEquals(u, u2)

    def test_unicode_query_params(self):
        u = UrlParser(u'http://example.com/?page=ｕｎｉｃｏｄｅ：（')
        u2 = UrlParser('http://example.com/')
        u2.update_query(page=u'ｕｎｉｃｏｄｅ：（')
        self.assertEquals(u, u2)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest


MESSAGE = "the quick brown fox jumped over..."
BLOCK_O_PADDING = ("\x10\x10\x10\x10\x10\x10\x10\x10"
                   "\x10\x10\x10\x10\x10\x10\x10\x10")
SECRET = "abcdefghijklmnopqrstuvwxyz"
ENCRYPTED = ("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaIbzth1QTzJxzHbHGnJywG5V1uR3tWtSB"
             "8hTyIcfg6rUZC4Wo0pT8jkEt9o1c%2FkTn")


class TestPadding(unittest.TestCase):
    def test_pad_empty_string(self):
        from r2.lib.tracking import _pad_message
        padded = _pad_message("")
        self.assertEquals(padded, BLOCK_O_PADDING)

    def test_pad_round_string(self):
        from r2.lib.tracking import _pad_message, KEY_SIZE
        padded = _pad_message("x" * KEY_SIZE)
        self.assertEquals(len(padded), KEY_SIZE * 2)
        self.assertEquals(padded[KEY_SIZE:], BLOCK_O_PADDING)

    def test_unpad_empty_message(self):
        from r2.lib.tracking import _unpad_message
        unpadded = _unpad_message("")
        self.assertEquals(unpadded, "")

    def test_unpad_evil_message(self):
        from r2.lib.tracking import _unpad_message
        evil = ("a" * 88) + chr(57)
        result = _unpad_message(evil)
        self.assertEquals(result, "")

    def test_padding_roundtrip(self):
        from r2.lib.tracking import _unpad_message, _pad_message
        tested = _unpad_message(_pad_message(MESSAGE))
        self.assertEquals(MESSAGE, tested)


class TestEncryption(unittest.TestCase):
    def test_salt(self):
        from r2.lib.tracking import _make_salt, SALT_SIZE
        self.assertEquals(len(_make_salt()), SALT_SIZE)

    def test_encrypt(self):
        from r2.lib.tracking import _encrypt, SALT_SIZE
        encrypted = _encrypt(
            "a" * SALT_SIZE,
            MESSAGE,
            SECRET,
        )
        self.assertEquals(encrypted, ENCRYPTED)

    def test_decrypt(self):
        from r2.lib.tracking import _decrypt
        decrypted = _decrypt(ENCRYPTED, SECRET)
        self.assertEquals(MESSAGE, decrypted)
<EOF>
<BOF>
#!/usr/bin/env python
# coding=utf-8
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime

import pytz
import json
from pylons import app_globals as g
from mock import MagicMock, patch

from r2.tests import RedditTestCase
from r2.models import Link
from r2.lib import hooks
from r2 import models


FAKE_DATE = datetime.datetime(2005, 6, 23, 3, 14, 0, tzinfo=pytz.UTC)


class TestEventCollector(RedditTestCase):

    def setUp(self):
        super(TestEventCollector, self).setUp()
        self.mock_eventcollector()
        self.autopatch(hooks, "get_hook")

    def test_vote_event(self):
        self.patch_liveconfig("events_collector_vote_sample_rate", 1.0)
        enum_name = "foo"
        enum_note = "bar"
        notes = "%s(%s)" % (enum_name, enum_note)
        initial_vote = MagicMock(is_upvote=True, is_downvote=False,
                                 is_automatic_initial_vote=True,
                                 previous_vote=None,
                                 data={"rank": MagicMock()},
                                 name="initial_vote",
                                 effects=MagicMock(
                                     note_codes=[enum_name],
                                     serializable_data={"notes": notes}))
        g.events.vote_event(initial_vote)

        self.amqp.assert_event_item(
            dict(
                event_topic="vote_server",
                event_type="server_vote",
                payload={
                    'vote_direction': 'up',
                    'target_type': 'magicmock',
                    'target_age_seconds': initial_vote.thing._age.total_seconds(),
                    'target_rank': initial_vote.data['rank'],
                    'sr_id': initial_vote.thing.subreddit_slow._id,
                    'sr_name': initial_vote.thing.subreddit_slow.name,
                    'target_fullname': initial_vote.thing._fullname,
                    'target_name': initial_vote.thing.name,
                    'target_id': initial_vote.thing._id,
                    'details_text': notes,
                    'process_notes': enum_name,
                    'auto_self_vote': True,
                }
            )
        )

    def test_vote_event_with_prev(self):
        self.patch_liveconfig("events_collector_vote_sample_rate", 1.0)
        upvote = MagicMock(name="upvote",
                           is_automatic_initial_vote=False,
                           data={"rank": MagicMock()})
        upvote.previous_vote = MagicMock(name="previous_vote",
                                         is_upvote=False, is_downvote=True)
        g.events.vote_event(upvote)

        self.amqp.assert_event_item(
            dict(
                event_topic="vote_server",
                event_type="server_vote",
                payload={
                    'vote_direction': 'up',
                    'target_type': 'magicmock',
                    'target_age_seconds': upvote.thing._age.total_seconds(),
                    'target_rank': upvote.data['rank'],
                    'sr_id': upvote.thing.subreddit_slow._id,
                    'sr_name': upvote.thing.subreddit_slow.name,
                    'target_fullname': upvote.thing._fullname,
                    'target_name': upvote.thing.name,
                    'target_id': upvote.thing._id,
                    'prev_vote_ts': self.created_ts_mock,
                    'prev_vote_direction': 'down',
                }
            )
        )

    def test_submit_event(self):
        self.patch_liveconfig("events_collector_submit_sample_rate", 1.0)
        new_link = MagicMock(name="new_link")
        context = MagicMock(name="context")
        request = MagicMock(name="request")
        request.ip = "1.2.3.4"
        g.events.submit_event(new_link, context=context, request=request)

        self.amqp.assert_event_item(
            dict(
                event_topic="submit_events",
                event_type="ss.submit",
                payload={
                    'domain': request.host,
                    'user_id': context.user._id,
                    'user_name': context.user.name,
                    'user_neutered': new_link.author_slow._spam,
                    'post_id': new_link._id,
                    'post_fullname': new_link._fullname,
                    'post_title': new_link.title,
                    'post_type': "self",
                    'post_body': new_link.selftext,
                    'sr_id': new_link.subreddit_slow._id,
                    'sr_name': new_link.subreddit_slow.name,
                    'geoip_country': context.location,
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'referrer_domain': self.domain_mock(),
                    'referrer_url': request.headers.get(),
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'obfuscated_data': {
                        'client_ip': request.ip,
                        'client_ipv4_24': "1.2.3",
                        'client_ipv4_16': "1.2",
                    }
                }
            )
        )

    def test_report_event_link(self):
        self.patch_liveconfig("events_collector_report_sample_rate", 1.0)

        target = MagicMock(name="target")
        target.__class__ = Link
        target._deleted = False
        target.author_slow._deleted = False

        context = MagicMock(name="context")
        request = MagicMock(name="request")
        request.ip = "1.2.3.4"
        g.events.report_event(
            target=target, context=context, request=request
        )

        self.amqp.assert_event_item(
            {
                'event_type': "ss.report",
                'event_topic': 'report_events',
                'payload': {
                    'process_notes': "CUSTOM",
                    'target_fullname': target._fullname,
                    'target_name': target.name,
                    'target_title': target.title,
                    'target_type': "self",
                    'target_author_id': target.author_slow._id,
                    'target_author_name': target.author_slow.name,
                    'target_id': target._id,
                    'target_age_seconds': target._age.total_seconds(),
                    'target_created_ts': self.created_ts_mock,
                    'domain': request.host,
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'referrer_url': request.headers.get(),
                    'user_id': context.user._id,
                    'user_name': context.user.name,
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'referrer_domain': self.domain_mock(),
                    'geoip_country': context.location,
                    'obfuscated_data': {
                        'client_ip': request.ip,
                        'client_ipv4_24': "1.2.3",
                        'client_ipv4_16': "1.2",
                    }
                }
            }
        )

    def test_mod_event(self):
        self.patch_liveconfig("events_collector_mod_sample_rate", 1.0)
        mod = MagicMock(name="mod")
        modaction = MagicMock(name="modaction")
        subreddit = MagicMock(name="subreddit")
        context = MagicMock(name="context")
        request = MagicMock(name="request")
        request.ip = "1.2.3.4"
        g.events.mod_event(
            modaction, subreddit, mod, context=context, request=request
        )

        self.amqp.assert_event_item(
            {
                'event_type': modaction.action,
                'event_topic': 'mod_events',
                'payload': {
                    'sr_id': subreddit._id,
                    'sr_name': subreddit.name,
                    'domain': request.host,
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'referrer_url': request.headers.get(),
                    'user_id': context.user._id,
                    'user_name': context.user.name,
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'referrer_domain': self.domain_mock(),
                    'details_text': modaction.details_text,
                    'geoip_country': context.location,
                    'obfuscated_data': {
                        'client_ip': request.ip,
                        'client_ipv4_24': "1.2.3",
                        'client_ipv4_16': "1.2",
                    }
                }
            }
        )

    def test_quarantine_event(self):
        self.patch_liveconfig("events_collector_quarantine_sample_rate", 1.0)
        event_type = MagicMock(name="event_type")
        subreddit = MagicMock(name="subreddit")
        context = MagicMock(name="context")
        request = MagicMock(name="request")
        request.ip = "1.2.3.4"
        g.events.quarantine_event(
            event_type, subreddit, context=context, request=request
        )

        self.amqp.assert_event_item(
            {
                'event_type': event_type,
                'event_topic': 'quarantine',
                "payload": {
                    'domain': request.host,
                    'referrer_domain': self.domain_mock(),
                    'verified_email': context.user.email_verified,
                    'user_id': context.user._id,
                    'sr_name': subreddit.name,
                    'referrer_url': request.headers.get(),
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'sr_id': subreddit._id,
                    'user_name': context.user.name,
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'geoip_country': context.location,
                    'obfuscated_data': {
                        'client_ip': request.ip,
                        'client_ipv4_24': "1.2.3",
                        'client_ipv4_16': "1.2",
                    }
                }
            }
        )

    def test_modmail_event(self):
        self.patch_liveconfig("events_collector_modmail_sample_rate", 1.0)
        message = MagicMock(name="message", _date=FAKE_DATE)
        first_message = MagicMock(name="first_message")
        message_cls = self.autopatch(models, "Message")
        message_cls._byID.return_value = first_message
        context = MagicMock(name="context")
        request = MagicMock(name="request")
        request.ip = "1.2.3.4"
        g.events.modmail_event(
            message, context=context, request=request
        )

        self.amqp.assert_event_item(
            {
                'event_type': "ss.send_message",
                'event_topic': "message_events",
                "payload": {
                    'domain': request.host,
                    'referrer_domain': self.domain_mock(),
                    'user_id': message.author_slow._id,
                    'user_name': message.author_slow.name,
                    'message_id': message._id,
                    'message_fullname': message._fullname,
                    'message_kind': "modmail",
                    'message_body': message.body,
                    'message_subject': message.subject,
                    'first_message_fullname': first_message._fullname,
                    'first_message_id': first_message._id,
                    'sender_type': "moderator",
                    'is_third_party': True,
                    'third_party_metadata': "mailgun",
                    'referrer_url': request.headers.get(),
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'sr_id': message.subreddit_slow._id,
                    'sr_name': message.subreddit_slow.name,
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'geoip_country': context.location,
                    'obfuscated_data': {
                        'client_ip': request.ip,
                        'client_ipv4_24': "1.2.3",
                        'client_ipv4_16': "1.2",
                    },
                },
            }
        )

    def test_message_event(self):
        self.patch_liveconfig("events_collector_modmail_sample_rate", 1.0)
        message = MagicMock(name="message", _date=FAKE_DATE)
        first_message = MagicMock(name="first_message")
        message_cls = self.autopatch(models, "Message")
        message_cls._byID.return_value = first_message
        context = MagicMock(name="context")
        request = MagicMock(name="request")
        request.ip = "1.2.3.4"
        g.events.message_event(
            message, context=context, request=request
        )

        self.amqp.assert_event_item(
            {
                'event_type': "ss.send_message",
                'event_topic': "message_events",
                "payload": {
                    'domain': request.host,
                    'referrer_domain': self.domain_mock(),
                    'user_id': message.author_slow._id,
                    'user_name': message.author_slow.name,
                    'message_id': message._id,
                    'message_fullname': message._fullname,
                    'message_kind': "message",
                    'message_body': message.body,
                    'message_subject': message.subject,
                    'first_message_fullname': first_message._fullname,
                    'first_message_id': first_message._id,
                    'sender_type': "user",
                    'is_third_party': True,
                    'third_party_metadata': "mailgun",
                    'referrer_url': request.headers.get(),
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'geoip_country': context.location,
                    'obfuscated_data': {
                        'client_ip': request.ip,
                        'client_ipv4_24': "1.2.3",
                        'client_ipv4_16': "1.2",
                    },
                },
            }
        )
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.souptest import (
    souptest_fragment,
    SoupDetectedCrasherError,
    SoupError,
    SoupSyntaxError,
    SoupUnexpectedCDataSectionError,
    SoupUnexpectedCommentError,
    SoupUnsupportedAttrError,
    SoupUnsupportedEntityError,
    SoupUnsupportedNodeError,
    SoupUnsupportedSchemeError,
    SoupUnsupportedTagError,
)


class TestSoupTest(unittest.TestCase):
    def assertFragmentRaises(self, fragment, error):
        self.assertRaises(error, souptest_fragment, fragment)

    def assertFragmentValid(self, fragment):
        souptest_fragment(fragment)

    def test_benign(self):
        """A typical example of what we might get out of `safemarkdown()`"""
        testcase = """
            <!-- SC_OFF -->
            <div class="md"><a href="http://zombo.com/">Welcome</a></div>
            <!-- SC_ON -->
        """
        self.assertFragmentValid(testcase)

    def test_unbalanced(self):
        self.assertFragmentRaises("<div></div></div>", SoupSyntaxError)

    def test_unclosed_comment(self):
        self.assertFragmentRaises("<!--", SoupSyntaxError)

    def test_invalid_comment(self):
        testcase = "<!--[if IE 6]>WHAT YEAR IS IT?<![endif]-->"
        self.assertFragmentRaises(testcase, SoupUnexpectedCommentError)

    def test_quoting(self):
        self.assertFragmentRaises("<div class=`poor IE`></div>",
                                  SoupSyntaxError)

    def test_processing_instruction(self):
        self.assertFragmentRaises("<?php not even once ?>",
                                  SoupUnsupportedNodeError)

    def test_doctype(self):
        self.assertFragmentRaises('<!DOCTYPE VRML>', SoupSyntaxError)

    def test_entity_declarations(self):
        testcase = '<!ENTITY lol "bad things">'
        self.assertFragmentRaises(testcase, SoupSyntaxError)
        testcase = '<!DOCTYPE div- [<!ENTITY lol "bad things">]>'
        self.assertFragmentRaises(testcase, SoupSyntaxError)

    def test_cdata_section(self):
        testcase = '<![CDATA[If only XHTML 2 went anywhere]]>'
        self.assertFragmentRaises(testcase, SoupUnexpectedCDataSectionError)

    def test_entities(self):
        self.assertFragmentRaises('&xml:what;', SoupError)
        self.assertFragmentRaises('&foo,bar;', SoupError)
        self.assertFragmentRaises('&#999999999999;', SoupUnsupportedEntityError)
        self.assertFragmentRaises('&#00;', SoupUnsupportedEntityError)
        self.assertFragmentRaises('&foo-bar;', SoupUnsupportedEntityError)
        self.assertFragmentRaises('&foobar;', SoupUnsupportedEntityError)
        self.assertFragmentValid('&nbsp;')
        self.assertFragmentValid('&Omicron;')

    def test_tag_whitelist(self):
        testcase = "<div><a><a><script>alert(1)</script></a></a></div>"
        self.assertFragmentRaises(testcase, SoupUnsupportedTagError)

    def test_attr_whitelist(self):
        testcase = '<div><a><a><em onclick="alert(1)">FOO!</em></a></a></div>'
        self.assertFragmentRaises(testcase, SoupUnsupportedAttrError)

    def test_tag_xmlns(self):
        self.assertFragmentRaises('<xml:div></xml:div>',
                                  SoupUnsupportedTagError)
        self.assertFragmentRaises('<div xmlns="http://zombo.com/foo"></div>',
                                  SoupError)

    def test_attr_xmlns(self):
        self.assertFragmentRaises('<div xml:class="baz"></div>',
                                  SoupUnsupportedAttrError)

    def test_schemes(self):
        self.assertFragmentValid('<a href="http://google.com">a</a>')
        self.assertFragmentValid('<a href="Http://google.com">a</a>')
        self.assertFragmentValid('<a href="/google.com">a</a>')
        self.assertFragmentRaises('<a href="javascript://google.com">a</a>',
                                  SoupUnsupportedSchemeError)

    def test_crashers(self):
        # Chrome crashes on weirdly encoded nulls.
        self.assertFragmentRaises('<a href="http://example.com/%%30%30">foo</a>',
                                  SoupDetectedCrasherError)
        self.assertFragmentRaises('<a href="http://example.com/%0%30">foo</a>',
                                  SoupDetectedCrasherError)
        self.assertFragmentRaises('<a href="http://example.com/%%300">foo</a>',
                                  SoupDetectedCrasherError)
        # Chrome crashes on extremely long hostnames
        self.assertFragmentRaises('<a href="http://%s.com">foo</a>' % ("x" * 300),
                                  SoupDetectedCrasherError)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from mock import patch

from r2.lib.media import _get_scrape_url
from r2.models import Link

class TestGetScrapeUrl(unittest.TestCase):
    @patch('r2.lib.media.Link')
    def test_link_post(self, Link):
        post = Link()
        post.url = 'https://example.com'
        post.is_self = False
        url = _get_scrape_url(post)
        self.assertEqual(url, 'https://example.com')

    def test_simple_self_post(self):
        post = Link(is_self=True, selftext='''
Some text here.
https://example.com
https://reddit.com''')
        url = _get_scrape_url(post)
        self.assertEqual(url, 'https://example.com')

    def test_imgur_link(self):
        post = Link(is_self=True, selftext='''
Some text here.
https://example.com
https://imgur.com''')
        url = _get_scrape_url(post)
        self.assertEqual(url, 'https://imgur.com')

    def test_image_link(self):
        post = Link(is_self=True, selftext='''
Some text here.
https://example.com
https://reddit.com/a.jpg''')
        url = _get_scrape_url(post)
        self.assertEqual(url, 'https://reddit.com/a.jpg')

        post = Link(is_self=True, selftext='''
Some text here.
https://example.com
https://reddit.com/a.PNG''')
        url = _get_scrape_url(post)
        self.assertEqual(url, 'https://reddit.com/a.PNG')

        post = Link(is_self=True, selftext='''
Some text here.
https://example.com
https://reddit.com/a.jpg/b''')
        url = _get_scrape_url(post)
        self.assertEqual(url, 'https://example.com')
<EOF>
<BOF>
from mock import MagicMock, ANY, call
from urllib import quote
from r2.tests import RedditTestCase
from r2.lib import hooks
from r2.lib.loid import LoId, LOID_COOKIE, LOID_CREATED_COOKIE, isodate
from r2.lib.utils import to_epoch_milliseconds


class LoidTests(RedditTestCase):

    def setUp(self):
        super(LoidTests, self).setUp()
        self.mock_eventcollector()

    def test_ftue_autocreate(self):
        request = MagicMock()
        context = MagicMock()
        request.cookies = {}
        loid = LoId.load(request, context, create=True)
        self.assertIsNotNone(loid.loid)
        self.assertIsNotNone(loid.created)
        self.assertTrue(loid.new)

        loid.save()

        context.cookies.add.assert_has_calls([
            call(
                LOID_COOKIE,
                quote(loid.loid),
                expires=ANY,
            ),
            call(
                LOID_CREATED_COOKIE,
                isodate(loid.created),
                expires=ANY,
            )
        ])
        self.amqp.assert_event_item(
            dict(
                event_topic="loid_events",
                event_type="ss.create_loid",
                payload={
                    'loid_new': True,
                    'loid': loid.loid,
                    'loid_created': to_epoch_milliseconds(loid.created),
                    'loid_version': 0,

                    'user_id': context.user._id,
                    'user_name': context.user.name,

                    'request_url': request.fullpath,
                    'domain': request.host,
                    'geoip_country': context.location,
                    'oauth2_client_id': context.oauth2_client._id,
                    'oauth2_client_app_type': context.oauth2_client.app_type,
                    'oauth2_client_name': context.oauth2_client.name,
                    'referrer_domain': self.domain_mock(),
                    'referrer_url': request.headers.get(),
                    'user_agent': request.user_agent,
                    'user_agent_parsed': request.parsed_agent.to_dict(),
                    'obfuscated_data': {
                        'client_ip': request.ip,
                    }
                },
            )
        )

    def test_ftue_nocreate(self):
        request = MagicMock()
        context = MagicMock()
        request.cookies = {}
        loid = LoId.load(request, context, create=False)
        self.assertFalse(loid.new)
        self.assertFalse(loid.serializable)
        loid.save()
        self.assertFalse(bool(context.cookies.add.called))

    def test_returning(self):
        request = MagicMock()
        context = MagicMock()
        request.cookies = {LOID_COOKIE: "foo", LOID_CREATED_COOKIE: "bar"}
        loid = LoId.load(request, context, create=False)
        self.assertEqual(loid.loid, "foo")
        self.assertNotEqual(loid.created, "bar")
        self.assertFalse(loid.new)
        self.assertTrue(loid.serializable)
        loid.save()
        self.assertFalse(bool(context.cookies.add.called))
<EOF>
<BOF>
#!/usr/bin/env python
# coding=utf-8
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import datetime
import unittest

from r2.lib.configparse import ConfigValue


class TestConfigValue(unittest.TestCase):

    def test_str(self):
        self.assertEquals('x', ConfigValue.str('x'))

    def test_int(self):
        self.assertEquals(3, ConfigValue.int('3'))
        self.assertEquals(-3, ConfigValue.int('-3'))
        with self.assertRaises(ValueError):
            ConfigValue.int('asdf')

    def test_float(self):
        self.assertEquals(3.0, ConfigValue.float('3'))
        self.assertEquals(-3.0, ConfigValue.float('-3'))
        with self.assertRaises(ValueError):
            ConfigValue.float('asdf')

    def test_bool(self):
        self.assertEquals(True, ConfigValue.bool('TrUe'))
        self.assertEquals(False, ConfigValue.bool('fAlSe'))
        with self.assertRaises(ValueError):
            ConfigValue.bool('asdf')

    def test_tuple(self):
        self.assertEquals((), ConfigValue.tuple(''))
        self.assertEquals(('a', 'b'), ConfigValue.tuple('a, b'))

    def test_set(self):
        self.assertEquals(set([]), ConfigValue.set(''))
        self.assertEquals(set(['a', 'b']), ConfigValue.set('a, b'))

    def test_set_of(self):
        self.assertEquals(set([]), ConfigValue.set_of(str)(''))
        self.assertEquals(set(['a', 'b']), ConfigValue.set_of(str)('a, b, b'))
        self.assertEquals(set(['a', 'b']),
                          ConfigValue.set_of(str, delim=':')('b : a : b'))

    def test_tuple_of(self):
        self.assertEquals((), ConfigValue.tuple_of(str)(''))
        self.assertEquals(('a', 'b'), ConfigValue.tuple_of(str)('a, b'))
        self.assertEquals(('a', 'b'),
                          ConfigValue.tuple_of(str, delim=':')('a : b'))

    def test_dict(self):
        self.assertEquals({}, ConfigValue.dict(str, str)(''))
        self.assertEquals({'a': ''}, ConfigValue.dict(str, str)('a'))
        self.assertEquals({'a': 3}, ConfigValue.dict(str, int)('a: 3'))
        self.assertEquals({'a': 3, 'b': 4},
                          ConfigValue.dict(str, int)('a: 3, b: 4'))
        self.assertEquals({'a': (3, 5), 'b': (4, 6)},
                          ConfigValue.dict(
                              str, ConfigValue.tuple_of(int), delim=';')
                          ('a: 3, 5;  b: 4, 6'))

    def test_choice(self):
        self.assertEquals(1, ConfigValue.choice(alpha=1)('alpha'))
        self.assertEquals(2, ConfigValue.choice(alpha=1, beta=2)('beta'))
        with self.assertRaises(ValueError):
            ConfigValue.choice(alpha=1)('asdf')

    def test_timeinterval(self):
        self.assertEquals(datetime.timedelta(0, 60),
                          ConfigValue.timeinterval('1 minute'))
        with self.assertRaises(KeyError):
            ConfigValue.timeinterval('asdf')

# TODO: test ConfigValue.messages
# TODO: test ConfigValue.baseplate
# TODO: test ConfigValue.json_dict
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import uuid

import datetime as dt

from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons import request

from r2.lib.cookies import Cookies, Cookie, upgrade_cookie_security, NEVER
from r2.models import Account, bcrypt_password, COOKIE_TIMESTAMP_FORMAT
from r2.tests import RedditTestCase


class TestCookieUpgrade(RedditTestCase):

    def setUp(self):
        name = "unit_tester_%s" % uuid.uuid4().hex
        self._password = uuid.uuid4().hex
        self._account = Account(
            name=name,
            password=bcrypt_password(self._password)
        )
        self._account._id = 1337

        c.cookies = Cookies()
        c.secure = True
        c.user_is_loggedin = True
        c.user = self._account
        c.oauth_user = None
        request.method = "POST"

    def tearDown(self):
        c.cookies.clear()
        c.user_is_loggedin = False
        c.user = None

    def _setSessionCookie(self, days_old=0):
        date = dt.datetime.now() - dt.timedelta(days=days_old)
        date_str = date.strftime(COOKIE_TIMESTAMP_FORMAT)
        session_cookie = self._account.make_cookie(date_str)
        c.cookies[g.login_cookie] = Cookie(
            value=session_cookie,
            dirty=False,
        )

    def test_no_upgrade_loggedout(self):
        # We might have a now-invalid session cookie, don't bother upgrading
        # it if it's not acceptable.
        c.user_is_loggedin = False
        c.user = None
        self._setSessionCookie(days_old=60)
        upgrade_cookie_security()
        self.assertFalse(c.cookies[g.login_cookie].dirty)

    def test_no_upgrade_http(self):
        c.secure = False
        self._setSessionCookie(days_old=60)
        upgrade_cookie_security()
        self.assertFalse(c.cookies[g.login_cookie].dirty)

    def test_no_upgrade_no_cookie(self):
        # Don't send back a cookie if we didn't even use cookie auth
        upgrade_cookie_security()
        self.assertFalse(g.login_cookie in c.cookies)

    def test_no_upgrade_oauth(self):
        # When g.domain == g.oauth_domain we might send a cookie even though
        # we're not using it for auth. Don't echo it back in responses.
        c.oauth_user = self._account
        self._setSessionCookie(days_old=60)
        upgrade_cookie_security()
        self.assertFalse(c.cookies[g.login_cookie].dirty)

    def test_no_upgrade_gets(self):
        request.method = "GET"
        self._setSessionCookie(days_old=60)
        upgrade_cookie_security()
        self.assertFalse(c.cookies[g.login_cookie].dirty)

    def test_no_upgrade_secure_session(self):
        self._setSessionCookie(days_old=60)
        c.cookies["secure_session"] = Cookie(value="1")
        upgrade_cookie_security()
        self.assertFalse(c.cookies[g.login_cookie].dirty)

    def test_upgrade_posts(self):
        self._setSessionCookie(days_old=60)
        upgrade_cookie_security()
        self.assertTrue(c.cookies[g.login_cookie].dirty)
        self.assertTrue(c.cookies[g.login_cookie].secure)

    def test_cookie_unchanged(self):
        self._setSessionCookie(days_old=60)
        old_session = c.cookies[g.login_cookie].value
        upgrade_cookie_security()
        self.assertTrue(c.cookies[g.login_cookie].dirty)
        self.assertEqual(old_session, c.cookies[g.login_cookie].value)

    def test_remember_old_session(self):
        self._setSessionCookie(days_old=60)
        upgrade_cookie_security()
        self.assertTrue(c.cookies[g.login_cookie].dirty)
        self.assertEqual(c.cookies[g.login_cookie].expires, NEVER)

    def test_dont_remember_recent_session(self):
        self._setSessionCookie(days_old=5)
        upgrade_cookie_security()
        self.assertTrue(c.cookies[g.login_cookie].dirty)
        self.assertNotEqual(c.cookies[g.login_cookie].expires, NEVER)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.cssfilter import validate_css


class TestCSSFilter(unittest.TestCase):
    def assertInvalid(self, css):
        serialized, errors = validate_css(css, {})
        self.assertNotEqual(errors, [])

    def test_offsite_url(self):
        testcase = u"*{background-image:url('http://foobar/')}"
        self.assertInvalid(testcase)

    def test_nested_url(self):
        testcase = u"*{background-image:calc(url('http://foobar/'))}"
        self.assertInvalid(testcase)

    def test_url_prelude(self):
        testcase = u"*[foo=url('http://foobar/')]{color:red;}"
        self.assertInvalid(testcase)

    def test_invalid_property(self):
        testcase = u"*{foo: red;}"
        self.assertInvalid(testcase)

    def test_import(self):
        testcase = u"@import 'foobar'; *{}"
        self.assertInvalid(testcase)

    def test_import_rule(self):
        testcase = u"*{ @import 'foobar'; }"
        self.assertInvalid(testcase)

    # IE<8 XSS
    def test_invalid_function(self):
        testcase = u"*{color:expression(alert(1));}"
        self.assertInvalid(testcase)

    def test_invalid_function_prelude(self):
        testcase = u"*[foo=expression(alert(1))]{color:red;}"
        self.assertInvalid(testcase)

    # Safari 5.x parser resynchronization issues
    def test_semicolon_function(self):
        testcase = u"*{color: calc(;color:red;);}"
        self.assertInvalid(testcase)

    def test_semicolon_block(self):
        testcase = u"*{color: [;color:red;];}"
        self.assertInvalid(testcase)

    # Safari 5.x prelude escape
    def test_escape_prelude(self):
        testcase = u"*[foo=bar{}*{color:blue}]{color:red;}"
        self.assertInvalid(testcase)

    # Multi-browser url() escape via spaces inside quotes
    def test_escape_url(self):
        testcase = u"*{background-image: url('foo bar');}"
        self.assertInvalid(testcase)

    # Control chars break out of quotes in multiple browsers
    def test_control_chars(self):
        testcase = u"*{font-family:'foobar\x03;color:red;';}"
        self.assertInvalid(testcase)

    def test_embedded_nulls(self):
        testcase = u"*{font-family:'foo\x00bar'}"
        self.assertInvalid(testcase)

    # Firefox allows backslashes in function names
    def test_escaped_url(self):
        testcase = u"*{background-image:\\u\\r\\l('http://foobar/')}"
        self.assertInvalid(testcase)

    # IE<8 allows backslash escapes in place of pretty much any char
    def test_escape_function_obfuscation(self):
        testcase = u"*{color: expression\\28 alert\\28 1 \\29 \\29 }"
        self.assertInvalid(testcase)

    # This is purely speculative, and may never affect actual browsers
    # https://developer.mozilla.org/en-US/docs/Web/CSS/attr
    def test_attr_url(self):
        testcase = u"*{background-image:attr(foobar url);}"
        self.assertInvalid(testcase)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.permissions import PermissionSet, ModeratorPermissionSet

class TestPermissionSet(PermissionSet):
    info = dict(x={}, y={})

class PermissionSetTest(unittest.TestCase):
    def test_dumps(self):
        self.assertEquals(
            '+all', PermissionSet(all=True).dumps())
        self.assertEquals(
            '+all', PermissionSet(all=True, other=True).dumps())
        self.assertEquals(
            '+a,-b', PermissionSet(a=True, b=False).dumps())

    def test_loads(self):
        self.assertEquals("", TestPermissionSet.loads(None).dumps())
        self.assertEquals("", TestPermissionSet.loads("").dumps())
        self.assertEquals("+x,+y", TestPermissionSet.loads("+x,+y").dumps())
        self.assertEquals("+x,-y", TestPermissionSet.loads("+x,-y").dumps())
        self.assertEquals("+all", TestPermissionSet.loads("+x,-y,+all").dumps())
        self.assertEquals("+x,-y,+z",
                          TestPermissionSet.loads("+x,-y,+z").dumps())
        self.assertRaises(ValueError,
                          TestPermissionSet.loads, "+x,-y,+z", validate=True)
        self.assertEquals(
            "+x,-y",
            TestPermissionSet.loads("-all,+x,-y", validate=True).dumps())

    def test_is_superuser(self):
        perm_set = PermissionSet()
        self.assertFalse(perm_set.is_superuser())
        perm_set[perm_set.ALL] = True
        self.assertTrue(perm_set.is_superuser())
        perm_set[perm_set.ALL] = False
        self.assertFalse(perm_set.is_superuser())

    def test_is_valid(self):
        perm_set = PermissionSet()
        self.assertFalse(perm_set.is_valid())

        perm_set = TestPermissionSet()
        self.assertTrue(perm_set.is_valid())
        perm_set['x'] = True
        self.assertTrue(perm_set.is_valid())
        perm_set[perm_set.ALL] = True
        self.assertTrue(perm_set.is_valid())
        perm_set['z'] = True
        self.assertFalse(perm_set.is_valid())

    def test_getitem(self):
        perm_set = PermissionSet()
        perm_set[perm_set.ALL] = True
        self.assertFalse(perm_set['x'])

        perm_set = TestPermissionSet()
        perm_set['x'] = True
        self.assertTrue(perm_set['x'])
        self.assertFalse(perm_set['y'])
        perm_set['x'] = False
        self.assertFalse(perm_set['x'])
        perm_set[perm_set.ALL] = True
        self.assertTrue(perm_set['x'])
        self.assertTrue(perm_set['y'])
        self.assertFalse(perm_set['z'])
        self.assertTrue(perm_set.get('x', False))
        self.assertFalse(perm_set.get('z', False))
        self.assertTrue(perm_set.get('z', True))


class ModeratorPermissionSetTest(unittest.TestCase):
    def test_loads(self):
        self.assertTrue(ModeratorPermissionSet.loads(None).is_superuser())
        self.assertFalse(ModeratorPermissionSet.loads('').is_superuser())

<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest
from r2.tests import RedditTestCase

from datetime import datetime as dt
from mock import MagicMock, patch
from pylons import tmpl_context as c
from pylons import app_globals as g
from webob.exc import HTTPForbidden

from r2.lib.errors import errors, ErrorSet
from r2.lib.validator import (
    VByName,
    VSubmitParent,
    VSubredditName,
    ValidEmail,
)
from r2.models import Account, Comment, Link, Message, Subreddit


class ValidatorTests(RedditTestCase):
    def _test_failure(self, input, error):
        """Helper for testing bad inputs."""
        self.validator.run(input)
        self.assertTrue(self.validator.has_errors)
        self.assertTrue(c.errors.get((error, None)))

    def _test_success(self, input, assertEqual=True):
        result = self.validator.run(input)
        self.assertFalse(self.validator.has_errors)
        self.assertEqual(len(c.errors), 0)
        if assertEqual:
            self.assertEqual(result, input)

        return result


class TestVSubmitParent(ValidatorTests):
    def setUp(self):
        super(TestVSubmitParent, self).setUp()
        # Reset the validator state and errors before every test.
        self.validator = VSubmitParent(None)
        c.errors = ErrorSet()

        c.user_is_loggedin = True
        c.user_is_admin = False
        c.user = Account(id=100)

        self.autopatch(Account, "enemy_ids", return_value=[])
        self.autopatch(Subreddit, "_byID", return_value=None)

    def _mock_message(self, id=1, author_id=1, **kwargs):
        kwargs['id'] = id
        kwargs['author_id'] = author_id

        message = Message(**kwargs)
        self.autopatch(VByName, "run", return_value=message)

        return message

    def _mock_link(self, id=1, author_id=1, sr_id=1, can_comment=True,
                   can_view_promo=True, **kwargs):
        kwargs['id'] = id
        kwargs['author_id'] = author_id
        kwargs['sr_id'] = sr_id

        link = Link(**kwargs)
        self.autopatch(VByName, "run", return_value=link)

        sr = Subreddit(id=sr_id)
        self.autopatch(Subreddit, "_byID", return_value=sr)
        self.autopatch(Subreddit, "can_comment", return_value=can_comment)
        self.autopatch(Link, "can_view_promo", return_value=can_view_promo)

        return link

    def _mock_comment(self,
                      id=1, author_id=1, link_id=1, sr_id=1, can_comment=True,
                      can_view_promo=True, is_moderator=False, **kwargs):
        kwargs['id'] = id
        kwargs['author_id'] = author_id
        kwargs['link_id'] = link_id
        kwargs['sr_id'] = sr_id

        comment = Comment(**kwargs)
        self.autopatch(VByName, "run", return_value=comment)

        link = Link(id=link_id, sr_id=sr_id)
        self.autopatch(Link, "_byID", return_value=link)

        sr = Subreddit(id=sr_id)
        self.autopatch(Subreddit, "_byID", return_value=sr)
        self.autopatch(Subreddit, "can_comment", return_value=can_comment)
        self.autopatch(Link, "can_view_promo", return_value=can_view_promo)
        self.autopatch(Subreddit, "is_moderator", return_value=is_moderator)

        return comment

    def test_no_fullname(self):
        with self.assertRaises(HTTPForbidden):
            self.validator.run('', None)

        self.assertFalse(self.validator.has_errors)

    def test_not_found(self):
        with self.assertRaises(HTTPForbidden):
            with patch.object(VByName, "run", return_value=None):
                self.validator.run('fullname', None)

        self.assertFalse(self.validator.has_errors)

    def test_invalid_thing(self):
        with self.assertRaises(HTTPForbidden):
            sr = Subreddit(id=1)
            with patch.object(VByName, "run", return_value=sr):
                self.validator.run('fullname', None)

        self.assertFalse(self.validator.has_errors)

    def test_not_loggedin(self):
        with self.assertRaises(HTTPForbidden):
            c.user_is_loggedin = False

            self._mock_comment()
            self.validator.run('fullname', None)

        self.assertFalse(self.validator.has_errors)

    def test_blocked_user(self):
        message = self._mock_message()
        with patch.object(
            Account, "enemy_ids", return_value=[message.author_id]
        ):
            result = self.validator.run('fullname', None)

            self.assertEqual(result, message)
            self.assertTrue(self.validator.has_errors)
            self.assertIn((errors.USER_BLOCKED, None), c.errors)

    def test_valid_message(self):
        message = self._mock_message()
        result = self.validator.run('fullname', None)

        self.assertEqual(result, message)
        self.assertFalse(self.validator.has_errors)

    def test_valid_link(self):
        link = self._mock_link()
        result = self.validator.run('fullname', None)

        self.assertEqual(result, link)
        self.assertFalse(self.validator.has_errors)

    def test_removed_link(self):
        link = self._mock_link(_spam=True)
        result = self.validator.run('fullname', None)

        self.assertEqual(result, link)
        self.assertFalse(self.validator.has_errors)

    def test_archived_link(self):
        link = self._mock_link(date=dt.now(g.tz).replace(year=2000))
        result = self.validator.run('fullname', None)

        self.assertEqual(result, link)
        self.assertTrue(self.validator.has_errors)
        self.assertIn((errors.TOO_OLD, None), c.errors)

    def test_locked_link(self):
        link = self._mock_link(locked=True)
        with patch.object(Subreddit, "can_distinguish", return_value=False):
            result = self.validator.run('fullname', None)

            self.assertEqual(result, link)
            self.assertTrue(self.validator.has_errors)
            self.assertIn((errors.THREAD_LOCKED, None), c.errors)

    def test_locked_link_mod_reply(self):
        link = self._mock_link(locked=True)
        with patch.object(Subreddit, "can_distinguish", return_value=True):
            result = self.validator.run('fullname', None)

            self.assertEqual(result, link)
            self.assertFalse(self.validator.has_errors)

    def test_invalid_link(self):
        with self.assertRaises(HTTPForbidden):
            self._mock_link(can_comment=False)
            self.validator.run('fullname', None)

        self.assertFalse(self.validator.has_errors)

    def test_invalid_promo(self):
        with self.assertRaises(HTTPForbidden):
            self._mock_link(can_view_promo=False)
            self.validator.run('fullname', None)

        self.assertFalse(self.validator.has_errors)

    def test_valid_comment(self):
        comment = self._mock_comment()
        result = self.validator.run('fullname', None)

        self.assertEqual(result, comment)
        self.assertFalse(self.validator.has_errors)

    def test_deleted_comment(self):
        comment = self._mock_comment(_deleted=True)
        result = self.validator.run('fullname', None)

        self.assertEqual(result, comment)
        self.assertTrue(self.validator.has_errors)
        self.assertIn((errors.DELETED_COMMENT, None), c.errors)

    def test_removed_comment(self):
        comment = self._mock_comment(_spam=True)
        result = self.validator.run('fullname', None)

        self.assertEqual(result, comment)
        self.assertTrue(self.validator.has_errors)
        self.assertIn((errors.DELETED_COMMENT, None), c.errors)

    def test_removed_comment_self_reply(self):
        comment = self._mock_comment(author_id=c.user._id, _spam=True)
        result = self.validator.run('fullname', None)

        self.assertEqual(result, comment)
        self.assertFalse(self.validator.has_errors)

    def test_removed_comment_mod_reply(self):
        comment = self._mock_comment(_spam=True, is_moderator=True)
        result = self.validator.run('fullname', None)

        self.assertEqual(result, comment)
        self.assertFalse(self.validator.has_errors)

    def test_invalid_comment(self):
        with self.assertRaises(HTTPForbidden):
            comment = self._mock_comment(can_comment=False)
            self.validator.run('fullname', None)

        self.assertFalse(self.validator.has_errors)


class TestVSubredditName(ValidatorTests):
    def setUp(self):
        # Reset the validator state and errors before every test.
        self.validator = VSubredditName(None)
        c.errors = ErrorSet()

    def _test_failure(self, input, error=errors.BAD_SR_NAME):
        super(TestVSubredditName, self)._test_failure(input, error)

    # Most of this validator's logic is already covered in `IsValidNameTest`.

    def test_slash_r_slash(self):
        result = self._test_success('/r/foo', assertEqual=False)
        self.assertEqual(result, 'foo')

    def test_r_slash(self):
        result = self._test_success('r/foo', assertEqual=False)
        self.assertEqual(result, 'foo')

    def test_two_prefixes(self):
        self._test_failure('/r/r/foo')

    def test_slash_not_prefix(self):
        self._test_failure('foo/r/')


class TestValidEmail(ValidatorTests):
    """Lightly test email address ("addr-spec") validation against RFC 2822.

    http://www.faqs.org/rfcs/rfc2822.html
    """
    def setUp(self):
        # Reset the validator state and errors before every test.
        self.validator = ValidEmail()
        c.errors = ErrorSet()

    def test_valid_emails(self):
        self._test_success('test@example.com')
        self._test_success('test@example.co.uk')
        self._test_success('test+foo@example.com')

    def _test_failure(self, email, error=errors.BAD_EMAIL):
        super(TestValidEmail, self)._test_failure(email, error)

    def test_blank_email(self):
        self._test_failure('', errors.NO_EMAIL)
        self.setUp()
        self._test_failure(' ', errors.NO_EMAIL)

    def test_no_whitespace(self):
        self._test_failure('test @example.com')
        self.setUp()
        self._test_failure('test@ example.com')
        self.setUp()
        self._test_failure('test@example. com')
        self.setUp()
        self._test_failure("test@\texample.com")

    def test_no_hostname(self):
        self._test_failure('example')
        self.setUp()
        self._test_failure('example@')

    def test_no_username(self):
        self._test_failure('example.com')
        self.setUp()
        self._test_failure('@example.com')

    def test_two_hostnames(self):
        self._test_failure('test@example.com@example.com')
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import uuid
import unittest

from pylons import tmpl_context as c
from webob.exc import HTTPException

# Needs to be done before other r2 imports, since some code run on module import
# expects a sane pylons env
from r2.tests import RedditTestCase

from r2.lib.db.thing import NotFound
from r2.lib.errors import errors, ErrorSet, UserRequiredException
from r2.lib.validator import VVerifyPassword
from r2.models import Account, AccountExists, bcrypt_password


class TestVVerifyPassword(unittest.TestCase):
    """Test that only the current user's password satisfies VVerifyPassword"""
    @classmethod
    def setUpClass(cls):
        # Create a dummy account for testing with; won't touch the database
        # as long as we don't `._commit()`
        name = "unit_tester_%s" % uuid.uuid4().hex
        cls._password = uuid.uuid4().hex
        cls._account = Account(
            name=name,
            password=bcrypt_password(cls._password)
        )

    def setUp(self):
        c.user_is_loggedin = True
        c.user = self._account

    def _checkFails(self, password, fatal=False, error=errors.WRONG_PASSWORD):
        # So we don't have any stale errors laying around
        c.errors = ErrorSet()
        validator = VVerifyPassword('dummy', fatal=fatal)

        if fatal:
            try:
                validator.run(password)
            except HTTPException:
                return True
            return False
        else:
            validator.run(password)

            return validator.has_errors or c.errors.get((error, None))

    def test_loggedout(self):
        c.user = ""
        c.user_is_loggedin = False
        self.assertRaises(UserRequiredException, self._checkFails, "dummy")

    def test_right_password(self):
        self.assertFalse(self._checkFails(self._password, fatal=False))
        self.assertFalse(self._checkFails(self._password, fatal=True))

    def test_wrong_password(self):
        bad_pass = "~" + self._password[1:]
        self.assertTrue(self._checkFails(bad_pass, fatal=False))
        self.assertTrue(self._checkFails(bad_pass, fatal=True))

    def test_no_password(self):
        self.assertTrue(self._checkFails(None, fatal=False))
        self.assertTrue(self._checkFails(None, fatal=True))

        self.assertTrue(self._checkFails("", fatal=False))
        self.assertTrue(self._checkFails("", fatal=True))
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from mock import MagicMock, Mock, patch
from unittest import TestCase

from r2.lib.authorize.api import (TRANSACTION_NOT_FOUND,
                                  TRANSACTION_ERROR,
                                  TRANSACTION_DUPLICATE,
                                  AuthorizationHoldNotFound,
                                  AuthorizeNetException,
                                  DuplicateTransactionError,
                                  TransactionError,
                                  create_customer_profile,
                                  get_customer_profile,
                                  create_payment_profile,
                                  update_payment_profile,
                                  delete_payment_profile,
                                  create_authorization_hold,
                                  capture_authorization_hold,
                                  void_authorization_hold,
                                  refund_transaction)
from r2.tests import RedditTestCase


class AuthorizeNetExceptionTest(RedditTestCase):

    def test_exception_message(self):
        from r2.lib.authorize.api import AuthorizeNetException
        card_number = "<cardNumber>1111222233334444</cardNumber>"
        expected = "<cardNumber>...4444</cardNumber>"
        full_msg = "Wrong Card %s was given"

        exp = AuthorizeNetException(full_msg % (card_number))

        self.assertNotEqual(str(exp), (full_msg % card_number))
        self.assertEqual(str(exp), (full_msg % expected))

class SimpleXMLObjectTest(RedditTestCase):

    def setUp(self):
        from r2.lib.authorize.api import SimpleXMLObject
        self.basic_object = SimpleXMLObject(name="Test",
                                           test="123",
                                           )

    def test_to_xml(self):
        self.assertEqual(self.basic_object.toXML(),
                         "<test>123</test><name>Test</name>",
                         "Unexpected XML produced")

    def test_simple_tag(self):
        from r2.lib.authorize.api import SimpleXMLObject
        xml_output = SimpleXMLObject.simple_tag("cat", "Jini", breed="calico",
                                                               demenor="evil",
                                                               )
        self.assertEqual(xml_output,
                         '<cat breed="calico" demenor="evil">Jini</cat>')

    def test_from_xml(self):
        from r2.lib.authorize.api import SimpleXMLObject
        from BeautifulSoup import BeautifulStoneSoup
        class TestXML(SimpleXMLObject):
            _keys = ["color", "breed"]

        parsed = BeautifulStoneSoup("<dog>" +
                                    "<color>black</color>" +
                                    "<breed>mixed</breed>" +
                                    "<something>else</something>" +
                                    "</dog>")
        constructed = TestXML.fromXML(parsed)
        expected = SimpleXMLObject(color="black",
                                   breed="mixed",
                                   )
        self.assertEqual(constructed.toXML(), expected.toXML(),
                         "Constructed does not match expected")

    def test_address(self):
        from r2.lib.authorize import Address
        address = Address(firstName="Bob",
                          lastName="Smith",
                          company="Reddit Inc.",
                          address="123 Main St.",
                          city="San Francisco",
                          state="California",
                          zip="12345",
                          country="USA",
                          phoneNumber="415-555-1234",
                          faxNumber="415-555-4321",
                          customerPaymentProfileId="1234567890",
                          customerAddressId="2233",
                          )
        expected = ("<firstName>Bob</firstName>" +
                   "<lastName>Smith</lastName>" +
                   "<company>Reddit Inc.</company>" +
                   "<address>123 Main St.</address>" +
                   "<city>San Francisco</city>" +
                   "<state>California</state>" +
                   "<zip>12345</zip>" +
                   "<country>USA</country>" +
                   "<phoneNumber>415-555-1234</phoneNumber>" +
                   "<faxNumber>415-555-4321</faxNumber>" +
                   "<customerPaymentProfileId>1234567890</customerPaymentProfileId>" +
                   "<customerAddressId>2233</customerAddressId>")

        self.assertEqual(address.toXML(), expected)

    def test_credit_card(self):
        from r2.lib.authorize import CreditCard
        card = CreditCard(cardNumber="1111222233334444",
                          expirationDate="11/22/33",
                          cardCode="123"
                          )
        expected = ("<cardNumber>1111222233334444</cardNumber>" +
                    "<expirationDate>11/22/33</expirationDate>" +
                    "<cardCode>123</cardCode>")
        self.assertEqual(card.toXML(), expected)

    def test_payment_profile(self):
        from r2.lib.authorize.api import PaymentProfile
        profile = PaymentProfile(billTo="Joe",
                                 customerPaymentProfileId="222",
                                 card="1111222233334444",
                                 validationMode="42",
                                 )
        expected = ("<billTo>Joe</billTo>" +
                    "<payment>" +
                        "<creditCard>1111222233334444</creditCard>" +
                    "</payment>" +
                    "<customerPaymentProfileId>222</customerPaymentProfileId>" +
                    "<validationMode>42</validationMode>")
        self.assertEqual(profile.toXML(), expected)

    def test_transaction(self):
        from r2.lib.authorize.api import Transaction
        transaction = Transaction(amount="42.42",
                                  customerProfileId="112233",
                                  customerPaymentProfileId="1111",
                                  transId="2222",
                                  order="42",
                                  )

        expected = ("<transaction>" +
                        "<amount>42.42</amount>" +
                        "<customerProfileId>112233</customerProfileId>" +
                        "<customerPaymentProfileId>1111</customerPaymentProfileId>" +
                        "<transId>2222</transId>" +
                        "<order>42</order>" +
                    "</transaction>")
        self.assertEqual(transaction.toXML(), expected)


class ApiFunctionTest(TestCase):

    def setUp(self):
        # Set up a few commonly used variables
        self.customer_id = 1
        self.payment_profile_id = 1000
        self.amount = 100
        self.transaction_id = 99

    @patch('r2.lib.authorize.api.CreateCustomerProfileRequest')
    @patch('r2.lib.authorize.api.Profile')
    def test_create_customer_profile(self, Profile, CreateRequest):
        merchant_customer_id = 99
        description = 'some description'

        # Set up profile mock
        profile = Mock()
        Profile.return_value = profile
        # Set up request mock
        _request = MagicMock()
        _request.make_request.return_value = self.customer_id
        CreateRequest.return_value = _request

        # Scenario: successful call
        return_value = create_customer_profile(merchant_customer_id,
                                               description)
        Profile.assert_called_once_with(description=description,
                                        merchantCustomerId=merchant_customer_id,
                                        paymentProfiles=None,
                                        customerProfileId=None)
        CreateRequest.assert_called_once_with(profile=profile)
        self.assertTrue(_request.make_request.called)
        self.assertEqual(return_value, self.customer_id)

        # Scenario: call raises AuthorizeNetException
        _request.make_request.side_effect = AuthorizeNetException('')
        return_value = create_customer_profile(merchant_customer_id,
                                               description)
        self.assertEqual(return_value, None)

    @patch('r2.lib.authorize.api.GetCustomerProfileRequest')
    def test_get_customer_profile(self, GetRequest):
        profile_mock = Mock()
        _request = Mock()
        _request.make_request.return_value = profile_mock
        GetRequest.return_value = _request

        # Scenario: call is successful
        return_value = get_customer_profile(self.customer_id)
        GetRequest.assert_called_once_with(customerProfileId=self.customer_id)
        self.assertTrue(_request.make_request.called)
        self.assertEqual(return_value, profile_mock)

        # Scenario: call raises AuthorizeNetException
        _request.make_request.side_effect = AuthorizeNetException('')
        return_value = get_customer_profile(self.customer_id)
        self.assertEqual(return_value, None)

    @patch('r2.lib.authorize.api.CreateCustomerPaymentProfileRequest')
    @patch('r2.lib.authorize.api.PaymentProfile')
    def test_create_payment_profile(self, PaymentProfile, CreateRequest):
        payment_profile = Mock()
        PaymentProfile.return_value = payment_profile
        _request = Mock()
        _request.make_request.return_value = self.payment_profile_id
        CreateRequest.return_value = _request

        # Scenario: call is successful, no validationMode is passed
        return_value = create_payment_profile(self.customer_id, 'address',
                                              'credit_card')
        PaymentProfile.assert_called_once_with(billTo='address',
                                               card='credit_card')
        CreateRequest.assert_called_once_with(customerProfileId=self.customer_id,
                                              paymentProfile=payment_profile,
                                              validationMode=None)
        self.assertTrue(_request.make_request.called)
        self.assertEqual(return_value, self.payment_profile_id)

        # Scenario: call is successful, validationMode is passed
        create_payment_profile(self.customer_id, 'address', 'credit_card',
                               'liveMode')
        CreateRequest.assert_called_with(customerProfileId=self.customer_id,
                                         paymentProfile=payment_profile,
                                         validationMode='liveMode')

        # Scenario: call raises AuthorizeNetException
        _request.make_request.side_effect = AuthorizeNetException('')
        self.assertRaises(AuthorizeNetException, create_payment_profile,
                          self.customer_id, 'address', 'credit_card')

    @patch('r2.lib.authorize.api.UpdateCustomerPaymentProfileRequest')
    @patch('r2.lib.authorize.api.PaymentProfile')
    def test_update_payment_profile(self, PaymentProfile, UpdateRequest):
        _request = Mock()
        _request.make_request.return_value = self.payment_profile_id
        UpdateRequest.return_value = _request

        # Scenario: call is successful
        return_value = update_payment_profile(self.customer_id,
                                              self.payment_profile_id,
                                              'address', 1234)
        self.assertTrue(UpdateRequest.called)
        self.assertTrue(_request.make_request.called)
        self.assertEqual(return_value, self.payment_profile_id)

        # Scenario: call raises AuthorizeNetException
        _request.make_request.side_effect = AuthorizeNetException('')
        self.assertRaises(AuthorizeNetException, update_payment_profile,
                          self.customer_id, self.payment_profile_id, 'address',
                          1234)

    @patch('r2.lib.authorize.api.DeleteCustomerPaymentProfileRequest')
    def test_delete_payment_profile(self, DeleteRequest):
        _request = Mock()
        DeleteRequest.return_value = _request

        # Scenario: call is successful
        return_value = delete_payment_profile(self.customer_id,
                                              self.payment_profile_id)
        DeleteRequest.assert_called_once_with(customerProfileId=self.customer_id,
                                              customerPaymentProfileId=self.payment_profile_id)
        self.assertTrue(return_value)

        # Scenario: call raises AuthorizeNetException
        _request.make_request.side_effect = AuthorizeNetException('')
        return_value = delete_payment_profile(self.customer_id,
                                              self.payment_profile_id)
        self.assertFalse(return_value)


    @patch('r2.lib.authorize.api.CreateCustomerProfileTransactionRequest')
    def test_create_authorization_hold(self, CreateRequest):
        _response = Mock()
        _response.trans_id = self.transaction_id
        _request = Mock()
        _request.make_request.return_value = (True, _response)
        CreateRequest.return_value = _request

        # Scenario: call is successful; pass customer_ip
        return_value = create_authorization_hold(self.customer_id,
                                                 self.payment_profile_id,
                                                 self.amount, 12345,
                                                 '127.0.0.1')
        self.assertTrue(CreateRequest.called)
        args, kwargs = CreateRequest.call_args
        self.assertEqual(kwargs['extraOptions'], {'x_customer_ip': '127.0.0.1'})
        self.assertEqual(return_value, self.transaction_id)

        # Scenario: call raises transaction_error
        _request.make_request.return_value = (False, _response)
        self.assertRaises(TransactionError, create_authorization_hold,
                          self.customer_id, self.payment_profile_id,
                          self.amount, 12345, '127.0.0.1')

        # Scenario: call returns duplicate errors
        _response.response_code = TRANSACTION_ERROR
        _response.response_reason_code = TRANSACTION_DUPLICATE
        _request.make_request.return_value = (True, _response)
        self.assertRaises(DuplicateTransactionError, create_authorization_hold,
                          self.customer_id, self.payment_profile_id,
                          self.amount, 12345)

    @patch('r2.lib.authorize.api.CreateCustomerProfileTransactionRequest')
    def test_capture_authorization_hold(self, CreateRequest):
        _response = Mock()
        _request = Mock()
        _request.make_request.return_value = (True, _response)
        CreateRequest.return_value = _request

        # Scenario: call is successful
        return_value = capture_authorization_hold(self.customer_id,
                                                  self.payment_profile_id,
                                                  self.amount,
                                                  self.transaction_id)
        self.assertTrue(CreateRequest.called)
        self.assertTrue(_request.make_request.called)
        self.assertEqual(return_value, None)

        # Scenario: call raises TransactionError
        _request.make_request.return_value = (False, _response)
        self.assertRaises(TransactionError, capture_authorization_hold,
                          self.customer_id, self.payment_profile_id,
                          self.amount, self.transaction_id)

        # Scenario: _request call returns not found error
        _response.get.return_value = TRANSACTION_NOT_FOUND
        self.assertRaises(AuthorizationHoldNotFound, capture_authorization_hold,
                          self.customer_id, self.payment_profile_id,
                          self.amount, self.transaction_id)

    @patch('r2.lib.authorize.api.CreateCustomerProfileTransactionRequest')
    def test_void_authorization_hold(self, CreateRequest):
        _response = Mock()
        _response.trans_id = self.transaction_id
        _request = Mock()
        _request.make_request.return_value = (True, _response)
        CreateRequest.return_value = _request

        # Scenario: call is successful
        return_value = void_authorization_hold(self.customer_id,
                                               self.payment_profile_id,
                                               self.transaction_id)
        self.assertTrue(CreateRequest.called)
        self.assertTrue(_request.make_request.called)
        self.assertEqual(return_value, self.transaction_id)

        # Scenario: call raises TransactionError
        _request.make_request.return_value = (False, _response)
        self.assertRaises(TransactionError, void_authorization_hold,
                          self.customer_id, self.payment_profile_id,
                          self.transaction_id)

    @patch('r2.lib.authorize.api.CreateCustomerProfileTransactionRequest')
    def test_refund_transaction(self, CreateRequest):
        _request = Mock()
        _request.make_request.return_value = (True, None)
        CreateRequest.return_value = _request

        # Scenario: call is successful
        refund_transaction(self.customer_id, self.payment_profile_id,
                           self.amount, self.transaction_id)
        self.assertTrue(_request.make_request.called)

        # Scenario: call raises TransactionError
        _request.make_request.return_value = (False, Mock())
        self.assertRaises(TransactionError, refund_transaction,
                          self.customer_id, self.payment_profile_id,
                          self.amount, self.transaction_id)

<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from mock import MagicMock, Mock, patch
from unittest import TestCase

from r2.lib.authorize.api import (AuthorizationHoldNotFound,
                                  DuplicateTransactionError,
                                  TRANSACTION_NOT_FOUND,
                                  TransactionError,)
from r2.lib.authorize.interaction import (get_or_create_customer_profile,
                                          add_payment_method,
                                          update_payment_method,
                                          delete_payment_method,
                                          add_or_update_payment_method,
                                          auth_freebie_transaction,
                                          auth_transaction,
                                          charge_transaction,
                                          void_transaction,
                                          refund_transaction,)
from r2.lib.db.thing import NotFound
from r2.models import Account, Link
from r2.tests import RedditTestCase


class InteractionTest(RedditTestCase):

    def setUp(self):
        self.user = Mock(spec=Account)
        self.user._id = 1
        self.user.name = 'name'
        self.user._fullname = 'fullname'

    @patch('r2.lib.authorize.interaction.api.get_customer_profile')
    @patch('r2.lib.authorize.interaction.api.create_customer_profile')
    def test_get_or_create_customer_profile(self, create_customer_profile,
                                            get_customer_profile):
        """Test get_or_create_customer_profile"""
        create_customer_profile.return_value = 123

        profile = MagicMock()
        profile.merchantCustomerId = self.user._fullname
        get_customer_profile.return_value = profile

        get_or_create_customer_profile(self.user)

        # Assert that on the first pass, a customer is created and retrieved
        self.assertEqual(create_customer_profile.call_count, 1)
        self.assertEqual(get_customer_profile.call_count, 1)

        with patch('r2.lib.authorize.interaction.CustomerID.get_id') as get_id:
            get_id.return_value = create_customer_profile.return_value

            get_or_create_customer_profile(self.user)

            # Assert that on the second pass, a customer is only retrieved
            self.assertEqual(create_customer_profile.call_count, 1)
            self.assertEqual(get_customer_profile.call_count, 2)

    @patch('r2.lib.authorize.interaction.PayID.add')
    @patch('r2.lib.authorize.interaction.api.create_payment_profile')
    @patch('r2.lib.authorize.interaction.CustomerID.get_id')
    def test_add_payment_method(self, get_id, create_payment_profile, add):
        """Test add_payment_method"""
        payment_method_id = 999
        create_payment_profile.return_value = payment_method_id

        return_value = add_payment_method(self.user, Mock(), Mock())

        # Assert that get_id is called once
        get_id.assert_called_once_with(self.user._id)
        # Assert that create_payment_profile is called
        self.assertTrue(create_payment_profile.called)
        # Assert that add is called
        self.assertTrue(add.called)
        # Assert that function returns payment_method_id value
        self.assertEqual(return_value, payment_method_id)

    @patch('r2.lib.authorize.interaction.api.update_payment_profile')
    def test_update_payment_method(self, update_payment_profile):
        """Test update_payment_method"""
        update_payment_method(self.user, Mock(), Mock(), Mock())

        # Assert that update_payment_profile was called once
        self.assertEqual(update_payment_profile.call_count, 1)

    @patch('r2.lib.authorize.interaction.PayID.delete')
    @patch('r2.lib.authorize.interaction.api.delete_payment_profile')
    def test_delete_payment_method(self, delete_payment_profile, delete):
        """Test delete_payment_method"""
        delete_payment_method(self.user, Mock())

        # Assert that delete_payment_profile was called once
        self.assertEqual(delete_payment_profile.call_count, 1)
        # Assert that delete was called once
        self.assertEqual(delete.call_count, 1)

        # Reset delete mock and run test again
        delete.reset_mock()
        delete_payment_method(self.user, Mock())

        # Assert that delete_payment_profile was called twice
        self.assertEqual(delete_payment_profile.call_count, 2)
        # Assert that delete was not called again
        self.assertEqual(delete.call_count, 1)

    @patch('r2.lib.authorize.interaction.add_payment_method')
    @patch('r2.lib.authorize.interaction.update_payment_method')
    def test_add_or_update_payment_method(self, update_payment_method,
                                          add_payment_method):
        """Test add_or_update_payment_method"""
        # If pay_id is None, assert that payment method is only added
        add_or_update_payment_method(self.user, Mock(), Mock())
        self.assertTrue(add_payment_method.called)
        self.assertFalse(update_payment_method.called)

        # Reset mocks
        add_payment_method.reset_mock()
        update_payment_method.reset_mock()

        # If pay_id is set, assert that payment method is only updated
        add_or_update_payment_method(self.user, Mock(), Mock(), 999)
        self.assertFalse(add_payment_method.called)
        self.assertTrue(update_payment_method.called)

    @patch('r2.lib.authorize.interaction.Bid._new')
    def test_auth_freebie_transaction(self, _new):
        """Test auth_freebie_transaction"""
        link = Mock(spec=Link)
        link._id = 99
        amount = 100
        campaign_id = 99

        # Can't test that NotFound is thrown since the exception is handled,
        # so assert that _new is called
        return_value = auth_freebie_transaction(amount, self.user, link,
                                                campaign_id)
        self.assertTrue(_new.called)
        # Assert that return value of auth_freebie_transaction is correct
        self.assertEqual(return_value, (-link._id, ''))

        # When a Bid is found, assert that auth is called
        with patch('r2.lib.authorize.interaction.Bid.one') as one:
            one_mock = MagicMock()
            one.return_value = one_mock
            auth_freebie_transaction(amount, self.user, link, campaign_id)
            self.assertTrue(one_mock.auth.called)

    @patch('r2.lib.authorize.interaction.request')
    @patch('r2.lib.authorize.interaction.api.create_authorization_hold')
    @patch('r2.lib.authorize.interaction.CustomerID.get_id')
    @patch('r2.lib.authorize.interaction.PayID.get_ids')
    def test_auth_transaction(self, get_ids, get_id, create_authorization_hold,
                              request):
        """Test auth_transaction"""
        link = Mock(spec=Link)
        link._id = 99
        amount = 100
        payment_method_id = 50
        campaign_id = 99
        request.ip = '127.0.0.1'
        transaction_id = 123

        # If get_ids is empty, assert that the proper value is returned
        get_ids.return_value = []
        return_value = auth_transaction(amount, self.user, payment_method_id,
                                        link, campaign_id)
        self.assertEqual(return_value, (None, 'invalid payment method'))

        # Make get_ids return a valid payment_method_id
        get_ids.return_value.append(payment_method_id)
        # Assign arbitrary CustomerID, which comes from Authorize
        get_id.return_value = 1000
        create_authorization_hold.return_value = transaction_id

        # Scenario: create_authorization_hold raises DuplicateTransactionError
        duplicate_transaction_error = DuplicateTransactionError(transaction_id=transaction_id)
        create_authorization_hold.side_effect = duplicate_transaction_error
        # Why does patch.multiple return an AttributeError?
        with patch('r2.lib.authorize.interaction.Bid.one') as one:
            one.side_effect = NotFound()
            return_value = auth_transaction(amount, self.user,
                                            payment_method_id, link,
                                            campaign_id)
            # If create_authorization_hold raises NotFound, assert return value
            self.assertEqual(return_value, (transaction_id, None))

        # Scenario: create_authorization_hold successfully returns
        with patch('r2.lib.authorize.interaction.Bid._new') as _new:
            return_value = auth_transaction(amount, self.user,
                                            payment_method_id, link,
                                            campaign_id)
            self.assertTrue(_new.called)
            # If create_authorization_hold works, assert return value
            self.assertEqual(return_value, (transaction_id, None))

        # Scenario: creat_authorization_hold raises TransactionError
        create_authorization_hold.side_effect = TransactionError('')
        return_value = auth_transaction(amount, self.user, payment_method_id,
                                        link, campaign_id)
        # If create_authorization_hold raises TransactionError, assert return
        self.assertEqual(return_value[0], None)

    @patch('r2.lib.authorize.interaction.api.capture_authorization_hold')
    @patch('r2.lib.authorize.interaction.Bid.one')
    def test_charge_transaction(self, one, capture_authorization_hold):
        transaction_id = 123
        campaign_id = 99
        bid = Mock()

        one.return_value = bid

        # Scenario: bid.is_charged() return True
        bid.is_charged.return_value = True
        return_value = charge_transaction(self.user, transaction_id,
                                          campaign_id)
        self.assertEqual(return_value, (True, None))

        # Scenario: transaction_id < 0
        bid.is_charged.return_value = False
        return_value = charge_transaction(self.user, -transaction_id,
                                          campaign_id)
        self.assertTrue(bid.charged.called)
        self.assertEqual(return_value, (True, None))

        # Scenario: capture_authorization_hold is successful
        return_value = charge_transaction(self.user, transaction_id,
                                          campaign_id)
        self.assertTrue(bid.charged.called)
        self.assertEqual(return_value, (True, None))

        # Scenario: capture_authorization_hold raises AuthorizationHoldNotFound
        capture_authorization_hold.side_effect = AuthorizationHoldNotFound('')
        return_value = charge_transaction(self.user, transaction_id,
                                          campaign_id)
        self.assertTrue(bid.void.called)
        self.assertEqual(return_value, (False, TRANSACTION_NOT_FOUND))

        # Scenario: capture_authorization_hold raises TransactionError
        capture_authorization_hold.side_effect = TransactionError('')
        return_value = charge_transaction(self.user, transaction_id,
                                          campaign_id)
        self.assertEqual(return_value[0], False)

    @patch('r2.lib.authorize.interaction.Bid.one')
    def test_void_transaction(self, one):
        bid = Mock()
        bid.pay_id = 111
        transaction_id = 123
        campaign_id = 99

        one.return_value = bid

        # Scenario: transaction_id < 0
        return_value = void_transaction(self.user, -transaction_id, campaign_id)
        self.assertTrue(bid.void.called)
        self.assertEqual(return_value, (True, None))

        with patch('r2.lib.authorize.interaction.api.void_authorization_hold') as void:
            # Scenario: void_authorization_hold is successful
            return_value = void_transaction(self.user, transaction_id,
                                            campaign_id)
            self.assertTrue(bid.void.called)
            self.assertEqual(return_value, (True, None))

            # Scenario: void_authorization_hold raises TransactionError
            void.side_effect = TransactionError('')
            return_value = void_transaction(self.user, transaction_id,
                                            campaign_id)
            self.assertEqual(return_value[0], False)

    @patch('r2.lib.authorize.interaction.Bid.one')
    def test_refund_transaction(self, one):
        bid = Mock()
        bid.pay_id = 111
        transaction_id = 123
        campaign_id = 99
        amount = 100

        one.return_value = bid

        # Scenario: transaction_id < 0
        return_value = refund_transaction(self.user, -transaction_id,
                                          campaign_id, amount)
        bid.refund.assert_called_once_with(amount)
        self.assertEqual(return_value, (True, None))

        with patch('r2.lib.authorize.interaction.api.refund_transaction') as refund:
            # Scenario: refund_transaction is successful
            bid.reset_mock()
            return_value = refund_transaction(self.user, transaction_id,
                                              campaign_id, amount)
            bid.refund.assert_called_once_with(amount)
            self.assertEqual(return_value, (True, None))

            # Scenario: refund_transaction raises TransactionError
            bid.reset_mock()
            refund.side_effect = TransactionError('')
            return_value = refund_transaction(self.user, transaction_id,
                                              campaign_id, amount)
            self.assertEqual(return_value[0], False)
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.providers.image_resizing.unsplashit import UnsplashitImageResizingProvider


class TestLocalResizer(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.provider = UnsplashitImageResizingProvider()

    def test_no_resize(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=200,
                      height=800)
        url = self.provider.resize_image(image)
        self.assertEqual(url, 'https://unsplash.it/200/400')

    def test_resize(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)

        for width in (108, 216, 320, 640, 960, 1080):
            url = self.provider.resize_image(image, width)
            self.assertEqual(url, 'https://unsplash.it/%d/%d' % (width,
                width*2))
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.tests import RedditTestCase

from r2.lib.providers.image_resizing import NotLargeEnough
from r2.lib.providers.image_resizing.imgix import ImgixImageResizingProvider
from r2.lib.utils import UrlParser


URLENCODED_COMMA = '%2C'


class TestImgixResizer(RedditTestCase):
    def setUp(self):
        self.provider = ImgixImageResizingProvider()
        self.patch_g(
            imgix_domain='example.com',
            imgix_signing=False,
        )

    def test_no_resize(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)
        url = self.provider.resize_image(image)
        self.assertEqual(url, 'https://example.com/a.jpg')

    def test_too_small(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=12,
                      height=8)
        with self.assertRaises(NotLargeEnough):
            self.provider.resize_image(image, 108)

    def test_resize(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)
        for width in (108, 216, 320, 640, 960, 1080):
            url = self.provider.resize_image(image, width)
            self.assertEqual(url, 'https://example.com/a.jpg?w=%d' % width)

    def test_cropping(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)
        max_ratio = 0.5
        url = self.provider.resize_image(image, max_ratio=max_ratio)
        crop = URLENCODED_COMMA.join(('faces', 'entropy'))
        self.assertEqual(url,
                ('https://example.com/a.jpg?fit=crop&crop=%s&arh=%s'
                    % (crop, max_ratio)))

        width = 108
        url = self.provider.resize_image(image, width, max_ratio=max_ratio)
        self.assertEqual(url,
                ('https://example.com/a.jpg?fit=crop&crop=%s&arh=%s&w=%s'
                    % (crop, max_ratio, width)))

    def test_sign_url(self):
        u = UrlParser('http://examples.imgix.net/frog.jpg?w=100')
        signed_url = self.provider._sign_url(u, 'abcdef')
        self.assertEqual(signed_url.unparse(),
                'http://examples.imgix.net/frog.jpg?w=100&s=cd3bdf071108af73b15c21bdcee5e49c')

        u = UrlParser('http://examples.imgix.net/frog.jpg')
        u.update_query(w=100)
        signed_url = self.provider._sign_url(u, 'abcdef')
        self.assertEqual(signed_url.unparse(),
                'http://examples.imgix.net/frog.jpg?w=100&s=cd3bdf071108af73b15c21bdcee5e49c')

    def test_censor(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)
        url = self.provider.resize_image(image, censor_nsfw=True)
        self.assertEqual(url, 'https://example.com/a.jpg?blur=600&px=32')
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.providers.image_resizing.no_op import NoOpImageResizingProvider


class TestLocalResizer(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.provider = NoOpImageResizingProvider()

    def test_no_resize(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)
        url = self.provider.resize_image(image)
        self.assertEqual(url, 'http://s3.amazonaws.com/a.jpg')

    def test_resize(self):
        image = dict(url='http://s3.amazonaws.com/a.jpg', width=1200,
                      height=800)

        for width in (108, 216, 320, 640, 960, 1080):
            url = self.provider.resize_image(image, width)
            self.assertEqual(url, 'http://s3.amazonaws.com/a.jpg')
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import re
import string

from pylons import tmpl_context as c
from pylons import app_globals as g

from reddit_base import RedditController
from r2.lib import utils
from r2.lib.pages import *
from r2.lib.pages.things import hot_links_by_url_listing
from r2.lib.template_helpers import add_sr
from r2.lib.validator import *
from r2.models import *
from r2.models.admintools import is_shamed_domain

# strips /r/foo/, /s/, or both
strip_sr          = re.compile('\A/r/[a-zA-Z0-9_-]+')
strip_s_path      = re.compile('\A/s/')
leading_slash     = re.compile('\A/+')
has_protocol      = re.compile('\A[a-zA-Z_-]+:')
allowed_protocol  = re.compile('\Ahttps?:')
need_insert_slash = re.compile('\Ahttps?:/[^/]')
def demangle_url(path):
    # there's often some URL mangling done by the stack above us, so
    # let's clean up the URL before looking it up
    path = strip_sr.sub('', path)
    path = strip_s_path.sub('', path)
    path = leading_slash.sub("", path)

    if has_protocol.match(path):
        if not allowed_protocol.match(path):
            return None
    else:
        path = '%s://%s' % (g.default_scheme, path)

    if need_insert_slash.match(path):
        path = string.replace(path, '/', '//', 1)

    try:
        path = utils.sanitize_url(path)
    except TypeError:
        return None

    return path

def force_html():
    """Because we can take URIs like /s/http://.../foo.png, and we can
       guarantee that the toolbar will never be used with a non-HTML
       render style, we don't want to interpret the extension from the
       target URL. So here we rewrite Middleware's interpretation of
       the extension to force it to be HTML
    """

    c.render_style = 'html'
    c.extension = None
    c.content_type = 'text/html; charset=UTF-8'


class ToolbarController(RedditController):

    allow_stylesheets = True

    @validate(link1 = VByName('id'),
              link2 = VLink('id', redirect = False))
    def GET_goto(self, link1, link2):
        """Support old /goto?id= urls. deprecated"""
        link = link2 if link2 else link1
        if link:
            return self.redirect(add_sr("/tb/" + link._id36))
        return self.abort404()

    @validate(urloid=nop('urloid'))
    def GET_s(self, urloid):
        """/s/http://..., show a given URL with the toolbar. if it's
           submitted, redirect to /tb/$id36"""
        force_html()
        path = demangle_url(request.fullpath)

        if not path:
            # it was malformed
            self.abort404()

        # if the domain is shame-banned, bail out.
        if is_shamed_domain(path)[0]:
            self.abort404()

        listing = hot_links_by_url_listing(path, sr=c.site, num=1)
        link = listing.things[0] if listing.things else None

        if link:
            # we were able to find it, let's send them to the
            # toolbar (if enabled) or comments (if not)
            return self.redirect(add_sr("/tb/" + link._id36))
        else:
            # It hasn't been submitted yet. Give them a chance to
            qs = utils.query_string({"url": path})
            return self.redirect(add_sr("/submit" + qs))

    def GET_redirect(self):
        return self.redirect('/', code=301)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime

from pylons import response
from pylons import app_globals as g
from pylons import tmpl_context as c
from pylons.i18n import _

from r2.controllers.reddit_base import MinimalController
from r2.lib import embeds
from r2.lib.base import abort
from r2.lib.errors import errors, ForbiddenError
from r2.lib.filters import scriptsafe_dumps, websafe, _force_unicode
from r2.lib.utils import url_to_thing, UrlParser
from r2.lib.template_helpers import format_html, make_url_https
from r2.lib.validator import can_view_link_comments, validate, VBoolean, VUrl
from r2.models import Comment, Link, Subreddit

_OEMBED_BASE = {
    "version": "1.0",
    "provider_name": "reddit",
    "provider_url": make_url_https('/'),
}

EMBEDLY_SCRIPT = 'https://embed.redditmedia.com/widgets/platform.js'
SCRIPT_TEMPLATE = '<script async src="%(embedly_script)s" charset="UTF-8"></script>'
POST_EMBED_TEMPLATE = """
    <blockquote class="reddit-card" %(live_data_attr)s>
      <a href="%(link_url)s">%(title)s</a> from
      <a href="%(subreddit_url)s">%(subreddit_name)s</a>
    </blockquote>
    %(script)s
"""

def _oembed_for(thing, **embed_options):
    """Given a Thing, return a dict of oEmbed data for that thing.

    Raises NotImplementedError if this Thing type does not yet support oEmbeds.
    """

    if isinstance(thing, Comment):
        return _oembed_comment(thing, **embed_options)
    elif isinstance(thing, Link):
        return _oembed_post(thing, **embed_options)

    raise NotImplementedError("Unable to render oembed for thing '%r'", thing)


def _oembed_post(thing, **embed_options):
    subreddit = thing.subreddit_slow
    if (not can_view_link_comments(thing) or
            subreddit.type in Subreddit.private_types):
        raise ForbiddenError(errors.POST_NOT_ACCESSIBLE)

    live = ''
    if embed_options.get('live'):
        time = datetime.now(g.tz).isoformat()
        live = 'data-card-created="{}"'.format(time)

    script = ''
    if not embed_options.get('omitscript', False):
        script = format_html(SCRIPT_TEMPLATE,
                             embedly_script=EMBEDLY_SCRIPT,
                             )

    link_url = UrlParser(thing.make_permalink_slow(force_domain=True))
    link_url.update_query(ref='share', ref_source='embed')

    author_name = ""
    if not thing._deleted:
        author = thing.author_slow
        if author._deleted:
            author_name = _("[account deleted]")
        else:
            author_name = author.name

    html = format_html(POST_EMBED_TEMPLATE,
                       live_data_attr=live,
                       link_url=link_url.unparse(),
                       title=websafe(thing.title),
                       subreddit_url=make_url_https(subreddit.path),
                       subreddit_name=subreddit.name,
                       script=script,
                       )

    oembed_response = dict(_OEMBED_BASE,
                           type="rich",
                           title=thing.title,
                           author_name=author_name,
                           html=html,
                           )

    return oembed_response


def _oembed_comment(thing, **embed_options):
    link = thing.link_slow
    subreddit = link.subreddit_slow
    if (not can_view_link_comments(link) or
            subreddit.type in Subreddit.private_types):
        raise ForbiddenError(errors.COMMENT_NOT_ACCESSIBLE)

    if not thing._deleted:
        author = thing.author_slow
        if author._deleted:
            author_name = _("[account deleted]")
        else:
            author_name = author.name

        title = _('%(author)s\'s comment from discussion "%(title)s"') % {
            "author": author_name,
            "title": _force_unicode(link.title),
        }
    else:
        author_name = ""
        title = ""

    html = format_html(embeds.get_inject_template(embed_options.get('omitscript')),
                       media=g.media_domain,
                       parent="true" if embed_options.get('parent') else "false",
                       live="true" if embed_options.get('live') else "false",
                       created=datetime.now(g.tz).isoformat(),
                       comment=thing.make_permalink_slow(force_domain=True),
                       link=link.make_permalink_slow(force_domain=True),
                       title=websafe(title),
                       )

    oembed_response = dict(_OEMBED_BASE,
                           type="rich",
                           title=title,
                           author_name=author_name,
                           html=html,
                           )

    if author_name:
        oembed_response['author_url'] = make_url_https('/user/' + author_name)

    return oembed_response


class OEmbedController(MinimalController):
    def pre(self):
        c.user = g.auth_provider.get_authenticated_account()
        if c.user and c.user._deleted:
            c.user = None
        c.user_is_loggedin = bool(c.user)

    @validate(
        url=VUrl('url'),
        parent=VBoolean("parent", default=False),
        live=VBoolean("live", default=False),
        omitscript=VBoolean("omitscript", default=False)
    )
    def GET_oembed(self, url, parent, live, omitscript):
        """Get the oEmbed response for a URL, if any exists.

        Spec: http://www.oembed.com/

        Optional parameters (parent, live) are passed through as embed options
        to oEmbed renderers.
        """
        response.content_type = "application/json"

        thing = url_to_thing(url)
        if not thing:
            abort(404)

        embed_options = {
            "parent": parent,
            "live": live,
            "omitscript": omitscript
        }

        try:
            return scriptsafe_dumps(_oembed_for(thing, **embed_options))
        except ForbiddenError:
            abort(403)
        except NotImplementedError:
            abort(404)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import tmpl_context as c
from pylons.controllers.util import abort

from r2.lib.base import BaseController
from r2.lib.validator import chkuser
from r2.models import Subreddit


class RedirectController(BaseController):
    def pre(self, *k, **kw):
        BaseController.pre(self, *k, **kw)
        c.extension = request.environ.get('extension')

    def GET_redirect(self, dest):
        return self.redirect(str(dest))

    def GET_user_redirect(self, username, rest=None):
        user = chkuser(username)
        if not user:
            abort(400)
        url = "/user/" + user
        if rest:
            url += "/" + rest
        if request.query_string:
            url += "?" + request.query_string
        return self.redirect(str(url), code=301)

    def GET_timereddit_redirect(self, timereddit, rest=None):
        sr_name = "t:" + timereddit
        if not Subreddit.is_valid_name(sr_name, allow_time_srs=True):
            abort(400)
        if rest:
            rest = str(rest)
        else:
            rest = ''
        return self.redirect("/r/%s/%s" % (sr_name, rest), code=301)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.controllers.reddit_base import RedditController
from r2.lib.base import proxyurl
from r2.lib.csrf import csrf_exempt
from r2.lib.template_helpers import get_domain
from r2.lib.pages import Embed, BoringPage, HelpPage
from r2.lib.filters import websafe, SC_OFF, SC_ON
from r2.lib.memoize import memoize

from pylons.i18n import _
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from BeautifulSoup import BeautifulSoup, Tag

from urllib2 import HTTPError

@memoize("renderurl_cached", time=60)
def renderurl_cached(path):
    # Needed so http://reddit.com/help/ works
    fp = path.rstrip("/")
    u = "https://code.reddit.com/wiki" + fp + '?stripped=1'

    g.log.debug("Pulling %s for help" % u)

    try:
        return fp, proxyurl(u)
    except HTTPError, e:
        if e.code != 404:
            print "error %s" % e.code
            print e.fp.read()
        return (None, None)

class EmbedController(RedditController):
    allow_stylesheets = True

    def rendercontent(self, input, fp):
        soup = BeautifulSoup(input)

        output = soup.find("div", { 'class':'wiki', 'id':'content'} )

        # Replace all links to "/wiki/help/..." with "/help/..."
        for link in output.findAll('a'):
            if link.has_key('href') and link['href'].startswith("/wiki/help"):
                link['href'] = link['href'][5:]

        output = SC_OFF + unicode(output) + SC_ON

        return HelpPage(_("help"),
                        content = Embed(content=output),
                        show_sidebar = None).render()

    @csrf_exempt
    def renderurl(self, override=None):
        if override:
            path = override
        else:
            path = request.path

        fp, content = renderurl_cached(path)
        if content is None:
            self.abort404()
        return self.rendercontent(content, fp)

    GET_help = POST_help = renderurl

    def GET_blog(self):
        return self.redirect("https://blog.%s/" %
                             get_domain(subreddit=False, no_www=True))

    def GET_faq(self):
        if c.default_sr:
            return self.redirect('/help/faq')
        else:
            return self.renderurl('/help/faqs/' + c.site.name)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from reddit_base import RedditController
import StringIO
import r2.lib.captcha as captcha
from pylons import response

from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.oauth2 import allow_oauth2_access

class CaptchaController(RedditController):
    @allow_oauth2_access
    @api_doc(api_section.captcha, uri='/captcha/{iden}')
    def GET_captchaimg(self, iden):
        """
        Request a CAPTCHA image given an `iden`.

        An iden is given as the `captcha` field with a `BAD_CAPTCHA`
        error, you should use this endpoint if you get a
        `BAD_CAPTCHA` error response.

        Responds with a 120x50 `image/png` which should be displayed
        to the user.

        The user's response to the CAPTCHA should be sent as `captcha`
        along with your request.

        To request a new CAPTCHA,
        use [/api/new_captcha](#POST_api_new_captcha).
        """
        image = captcha.get_image(iden)
        f = StringIO.StringIO()
        image.save(f, "PNG")
        response.content_type = "image/png;"
        return f.getvalue()
    
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from collections import defaultdict
from datetime import datetime, timedelta

from babel.dates import format_date
from babel.numbers import format_number
import hashlib
import hmac
import json
import urllib
import mimetypes
import os

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, N_

from r2.config import feature
from r2.controllers.api import ApiController
from r2.controllers.listingcontroller import ListingController
from r2.controllers.reddit_base import RedditController
from r2.lib.authorize import (
    get_or_create_customer_profile,
    add_or_update_payment_method,
    PROFILE_LIMIT,
)
from r2.lib.authorize.api import AuthorizeNetException
from r2.lib import (
    hooks,
    inventory,
    media,
    promote,
    s3_helpers,
)
from r2.lib.base import abort
from r2.lib.db import queries
from r2.lib.errors import errors
from r2.lib.filters import (
    jssafe,
    scriptsafe_dumps,
    websafe,
)
from r2.lib.template_helpers import (
    add_sr,
    format_html,
)
from r2.lib.memoize import memoize
from r2.lib.menus import NamedButton, NavButton, NavMenu, QueryButton
from r2.lib.pages import (
    LinkInfoPage,
    PaymentForm,
    PromoteInventory,
    PromotePage,
    PromoteLinkEdit,
    PromoteLinkNew,
    PromoteReport,
    Reddit,
    RefundPage,
    RenderableCampaign,
    SponsorLookupUser,
)
from r2.lib.pages.things import default_thing_wrapper, wrap_links
from r2.lib.system_messages import user_added_messages
from r2.lib.utils import (
    constant_time_compare,
    is_subdomain,
    to_date,
    to36,
    UrlParser,
)
from r2.lib.validator import (
    json_validate,
    nop,
    noresponse,
    VAccountByName,
    ValidAddress,
    validate,
    validatedMultipartForm,
    validatedForm,
    ValidCard,
    ValidEmail,
    VBoolean,
    VByName,
    VCollection,
    VDate,
    VExistingUname,
    VFloat,
    VFrequencyCap,
    VImageType,
    VInt,
    VLength,
    VLink,
    VList,
    VLocation,
    VModhash,
    VOneOf,
    VOSVersion,
    VPrintable,
    VPriority,
    VPromoCampaign,
    VPromoTarget,
    VRatelimit,
    VMarkdownLength,
    VShamedDomain,
    VSponsor,
    VSponsorAdmin,
    VSponsorAdminOrAdminSecret,
    VVerifiedSponsor,
    VSubmitSR,
    VTitle,
    VUploadLength,
    VUrl,
)
from r2.models import (
    Account,
    AccountsByCanonicalEmail,
    calc_impressions,
    Collection,
    Frontpage,
    Link,
    Message,
    NotFound,
    PromoCampaign,
    PromotionLog,
    PromotionPrices,
    PromotionWeights,
    PROMOTE_STATUS,
    Subreddit,
    Target,
)
from r2.models.promo import PROMOTE_COST_BASIS, PROMOTE_PRIORITIES

IOS_DEVICES = ('iPhone', 'iPad', 'iPod',)
ANDROID_DEVICES = ('phone', 'tablet',)

ADZERK_URL_MAX_LENGTH = 499

EXPIRES_DATE_FORMAT = "%Y-%m-%dT%H:%M:%S"
ALLOWED_IMAGE_TYPES = set(["image/jpg", "image/jpeg", "image/png"])

def _format_expires(expires):
    return expires.strftime(EXPIRES_DATE_FORMAT)


def _get_callback_hmac(username, key, expires):
    secret = g.secrets["s3_direct_post_callback"]
    expires_str = _format_expires(expires)
    data = "|".join([username, key, expires_str])

    return hmac.new(secret, data, hashlib.sha256).hexdigest()


def _force_images(link, thumbnail, mobile):
    changed = False

    if thumbnail:
        media.force_thumbnail(link, thumbnail["data"], thumbnail["ext"])
        changed = True

    if feature.is_enabled("mobile_targeting") and mobile:
        media.force_mobile_ad_image(link, mobile["data"], mobile["ext"])
        changed = True

    return changed


def campaign_has_oversold_error(form, campaign):
    if campaign.priority.inventory_override:
        return

    return has_oversold_error(
        form,
        campaign,
        start=campaign.start_date,
        end=campaign.end_date,
        total_budget_pennies=campaign.total_budget_pennies,
        cpm=campaign.bid_pennies,
        target=campaign.target,
        location=campaign.location,
    )


def has_oversold_error(form, campaign, start, end, total_budget_pennies, cpm,
        target, location):
    ndays = (to_date(end) - to_date(start)).days
    total_request = calc_impressions(total_budget_pennies, cpm)
    daily_request = int(total_request / ndays)
    oversold = inventory.get_oversold(
        target, start, end, daily_request, ignore=campaign, location=location)

    if oversold:
        min_daily = min(oversold.values())
        available = min_daily * ndays
        msg_params = {
            'available': format_number(available, locale=c.locale),
            'target': target.pretty_name,
            'start': start.strftime('%m/%d/%Y'),
            'end': end.strftime('%m/%d/%Y'),
        }
        c.errors.add(errors.OVERSOLD_DETAIL, field='total_budget_dollars',
                     msg_params=msg_params)
        form.has_errors('total_budget_dollars', errors.OVERSOLD_DETAIL)
        return True


def _key_to_dict(key, data=False):
    timer = g.stats.get_timer("providers.s3.get_ads_key_meta.with_%s" %
        ("data" if data else "no_data"))
    timer.start()

    url = key.generate_url(expires_in=0, query_auth=False)
    # Generating an S3 url without authentication fails for IAM roles.
    # This removes the bad query params.
    # see: https://github.com/boto/boto/issues/2043
    url = promote.update_query(url, {"x-amz-security-token": None}, unset=True)

    result = {
        "url": url,
        "data": key.get_contents_as_string() if data else None,
        "ext": key.get_metadata("ext"),
    }

    timer.stop()

    return result


def _get_ads_keyspace(thing):
    return "ads/%s/" % thing._fullname


def _get_ads_images(thing, data=False, **kwargs):
    images = {}

    timer = g.stats.get_timer("providers.s3.get_ads_image_keys")
    timer.start()

    keys = s3_helpers.get_keys(g.s3_client_uploads_bucket, prefix=_get_ads_keyspace(thing), **kwargs)

    timer.stop()

    for key in keys:
        filename = os.path.basename(key.key)
        name, ext = os.path.splitext(filename)

        if name not in ("mobile", "thumbnail"):
            continue

        images[name] = _key_to_dict(key, data=data)

    return images


def _clear_ads_images(thing):
    timer = g.stats.get_timer("providers.s3.delete_ads_image_keys")
    timer.start()

    s3_helpers.delete_keys(g.s3_client_uploads_bucket, prefix=_get_ads_keyspace(thing))

    timer.stop()


class PromoteController(RedditController):
    @validate(VSponsor())
    def GET_new_promo(self):
        ads_images = _get_ads_images(c.user)
        images = {k: v.get("url") for k, v in ads_images.iteritems()}

        return PromotePage(title=_("create sponsored link"),
                           content=PromoteLinkNew(images),
                           extra_js_config={
                            "ads_virtual_page": "new-promo",
                           }).render()

    @validate(VSponsor('link'),
              link=VLink('link'))
    def GET_edit_promo(self, link):
        if not link or link.promoted is None:
            return self.abort404()
        rendered = wrap_links(link, skip=False)
        form = PromoteLinkEdit(link, rendered)
        page = PromotePage(title=_("edit sponsored link"), content=form,
                      show_sidebar=False, extension_handling=False)
        return page.render()

    @validate(VSponsorAdmin(),
              link=VLink("link"),
              campaign=VPromoCampaign("campaign"))
    def GET_refund(self, link, campaign):
        if link._id != campaign.link_id:
            return self.abort404()

        content = RefundPage(link, campaign)
        return Reddit("refund", content=content, show_sidebar=False).render()

    @validate(VVerifiedSponsor("link"),
              link=VLink("link"),
              campaign=VPromoCampaign("campaign"))
    def GET_pay(self, link, campaign):
        if link._id != campaign.link_id:
            return self.abort404()

        # no need for admins to play in the credit card area
        if c.user_is_loggedin and c.user._id != link.author_id:
            return self.abort404()

        if g.authorizenetapi:
            data = get_or_create_customer_profile(c.user)
            content = PaymentForm(link, campaign,
                                  customer_id=data.customerProfileId,
                                  profiles=data.paymentProfiles,
                                  max_profiles=PROFILE_LIMIT)
        else:
            content = None
        res = LinkInfoPage(link=link,
                            content=content,
                            show_sidebar=False,
                            extra_js_config={
                              "ads_virtual_page": "checkout",
                            })
        return res.render()


class SponsorController(PromoteController):
    @validate(VSponsorAdminOrAdminSecret('secret'),
              start=VDate('startdate'),
              end=VDate('enddate'),
              link_text=nop('link_text'),
              owner=VAccountByName('owner'),
              grouping=VOneOf("grouping", ("total", "day"), default="total"))
    def GET_report(self, start, end, grouping, link_text=None, owner=None):
        now = datetime.now(g.tz).replace(hour=0, minute=0, second=0,
                                         microsecond=0)
        if not start or not end:
            start = promote.promo_datetime_now(offset=1).date()
            end = promote.promo_datetime_now(offset=8).date()
            c.errors.remove((errors.BAD_DATE, 'startdate'))
            c.errors.remove((errors.BAD_DATE, 'enddate'))
        end = end or now - timedelta(days=1)
        start = start or end - timedelta(days=7)

        links = []
        bad_links = []
        owner_name = owner.name if owner else ''

        if owner:
            campaign_ids = PromotionWeights.get_campaign_ids(
                start, end, author_id=owner._id)
            campaigns = PromoCampaign._byID(campaign_ids, data=True)
            link_ids = {camp.link_id for camp in campaigns.itervalues()}
            links.extend(Link._byID(link_ids, data=True, return_dict=False))

        if link_text is not None:
            id36s = link_text.replace(',', ' ').split()
            try:
                links_from_text = Link._byID36(id36s, data=True)
            except NotFound:
                links_from_text = {}

            bad_links = [id36 for id36 in id36s if id36 not in links_from_text]
            links.extend(links_from_text.values())

        content = PromoteReport(links, link_text, owner_name, bad_links, start,
                                end, group_by_date=grouping == "day")
        if c.render_style == 'csv':
            return content.as_csv()
        else:
            return PromotePage(title=_("sponsored link report"),
                               content=content).render()

    @validate(
        VSponsorAdmin(),
        start=VDate('startdate'),
        end=VDate('enddate'),
        sr_name=nop('sr_name'),
        collection_name=nop('collection_name'),
    )
    def GET_promote_inventory(self, start, end, sr_name, collection_name):
        if not start or not end:
            start = promote.promo_datetime_now(offset=1).date()
            end = promote.promo_datetime_now(offset=8).date()
            c.errors.remove((errors.BAD_DATE, 'startdate'))
            c.errors.remove((errors.BAD_DATE, 'enddate'))

        target = Target(Frontpage.name)
        if sr_name:
            try:
                sr = Subreddit._by_name(sr_name)
                target = Target(sr.name)
            except NotFound:
                c.errors.add(errors.SUBREDDIT_NOEXIST, field='sr_name')
        elif collection_name:
            collection = Collection.by_name(collection_name)
            if not collection:
                c.errors.add(errors.COLLECTION_NOEXIST, field='collection_name')
            else:
                target = Target(collection)

        content = PromoteInventory(start, end, target)

        if c.render_style == 'csv':
            return content.as_csv()
        else:
            return PromotePage(title=_("sponsored link inventory"),
                               content=content).render()

    @validate(
        VSponsorAdmin(),
        id_user=VByName('name', thing_cls=Account),
        email=ValidEmail("email"),
    )
    def GET_lookup_user(self, id_user, email):
        email_users = AccountsByCanonicalEmail.get_accounts(email)
        content = SponsorLookupUser(
            id_user=id_user, email=email, email_users=email_users)
        return PromotePage(title="look up user", content=content).render()


class PromoteListingController(ListingController):
    where = 'promoted'
    render_cls = PromotePage
    titles = {
        'future_promos': N_('unapproved promoted links'),
        'pending_promos': N_('accepted promoted links'),
        'unpaid_promos': N_('unpaid promoted links'),
        'rejected_promos': N_('rejected promoted links'),
        'live_promos': N_('live promoted links'),
        'edited_live_promos': N_('edited live promoted links'),
        'all': N_('all promoted links'),
    }
    base_path = '/promoted'

    default_filters = [
        NamedButton('all_promos', dest='',
                    use_params=False,
                    aliases=['/sponsor']),
        NamedButton('future_promos',
                    use_params=False),
        NamedButton('unpaid_promos',
                    use_params=False),
        NamedButton('rejected_promos',
                    use_params=False),
        NamedButton('pending_promos',
                    use_params=False),
        NamedButton('live_promos',
                    use_params=False),
        NamedButton('edited_live_promos',
                    use_params=False),
    ]

    def title(self):
        return _(self.titles[self.sort])

    @property
    def title_text(self):
        return _('promoted by you')

    @property
    def menus(self):
        filters = [
            NamedButton('all_promos', dest='',
                        use_params=False,
                        aliases=['/sponsor']),
            NamedButton('future_promos',
                        use_params=False),
            NamedButton('unpaid_promos',
                        use_params=False),
            NamedButton('rejected_promos',
                        use_params=False),
            NamedButton('pending_promos',
                        use_params=False),
            NamedButton('live_promos',
                        use_params=False),
        ]
        menus = [NavMenu(filters, base_path=self.base_path, title='show',
                         type='lightdrop')]
        return menus

    def builder_wrapper(self, thing):
        builder_wrapper = default_thing_wrapper()
        w = builder_wrapper(thing)
        w.hide_after_seen = self.sort == "future_promos"

        return w

    def keep_fn(self):
        def keep(item):
            if self.sort == "future_promos":
                # this sort is used to review links that need to be approved
                # skip links that don't have any paid campaigns
                campaigns = list(PromoCampaign._by_link(item._id))
                if not any(promote.authed_or_not_needed(camp)
                           for camp in campaigns):
                    return False

            if item.promoted and not item._deleted:
                return True
            else:
                return False
        return keep

    def query(self):
        if self.sort == "future_promos":
            return queries.get_unapproved_links(c.user._id)
        elif self.sort == "pending_promos":
            return queries.get_accepted_links(c.user._id)
        elif self.sort == "unpaid_promos":
            return queries.get_unpaid_links(c.user._id)
        elif self.sort == "rejected_promos":
            return queries.get_rejected_links(c.user._id)
        elif self.sort == "live_promos":
            return queries.get_live_links(c.user._id)
        elif self.sort == "edited_live_promos":
            return queries.get_edited_live_links(c.user._id)
        elif self.sort == "all":
            return queries.get_promoted_links(c.user._id)

    @validate(VSponsor())
    def GET_listing(self, sort="all", **env):
        self.sort = sort
        return ListingController.GET_listing(self, **env)


class SponsorListingController(PromoteListingController):
    titles = dict(PromoteListingController.titles.items() + {
        'underdelivered': N_('underdelivered promoted links'),
        'reported': N_('reported promoted links'),
        'house': N_('house promoted links'),
        'fraud': N_('fraud suspected promoted links'),
    }.items())
    base_path = '/sponsor/promoted'

    @property
    def title_text(self):
        return _('promos on reddit')

    @property
    def menus(self):
        managed_menu = NavMenu([
            QueryButton("exclude managed", dest=None,
                        query_param='include_managed'),
            QueryButton("include managed", dest="yes",
                        query_param='include_managed'),
        ], base_path=request.path, type='lightdrop')

        if self.sort in {'underdelivered', 'reported', 'house', 'fraud'}:
            menus = []

            if self.sort == 'fraud':
                fraud_menu = NavMenu([
                    QueryButton("exclude unpaid", dest=None,
                                query_param='exclude_unpaid'),
                    QueryButton("include unpaid", dest="no",
                                query_param='exclude_unpaid'),
                ], base_path=request.path, type='lightdrop')
                menus.append(fraud_menu)
            if self.sort in ('house', 'fraud'):
                menus.append(managed_menu)
        else:
            menus = super(SponsorListingController, self).menus
            menus.append(managed_menu)

        if self.sort == 'live_promos':
            srnames = promote.all_live_promo_srnames()
            buttons = [NavButton('all', '', use_params=True)]
            try:
                srnames.remove(Frontpage.name)
                frontbutton = NavButton('FRONTPAGE', Frontpage.name,
                                        use_params=True,
                                        aliases=['/promoted/live_promos/%s' %
                                                 urllib.quote(Frontpage.name)])
                buttons.append(frontbutton)
            except KeyError:
                pass

            srnames = sorted(srnames, key=lambda name: name.lower())
            buttons.extend(
                NavButton(name, name, use_params=True) for name in srnames)
            base_path = self.base_path + '/live_promos'
            menus.append(NavMenu(buttons, base_path=base_path,
                                 title='subreddit', type='lightdrop'))
        return menus

    @classmethod
    @memoize('live_by_subreddit', time=300)
    def _live_by_subreddit(cls, sr_names):
        promotuples = promote.get_live_promotions(sr_names)
        return [pt.link for pt in promotuples]

    def live_by_subreddit(cls, sr):
        return cls._live_by_subreddit([sr.name])

    @classmethod
    @memoize('house_link_names', time=60)
    def get_house_link_names(cls):
        now = promote.promo_datetime_now()
        campaign_ids = PromotionWeights.get_campaign_ids(now)
        q = PromoCampaign._query(PromoCampaign.c._id.in_(campaign_ids),
                                 PromoCampaign.c.priority_name == 'house',
                                 data=True)
        link_names = {Link._fullname_from_id36(to36(camp.link_id))
                      for camp in q}
        return sorted(link_names, reverse=True)

    def keep_fn(self):
        base_keep_fn = PromoteListingController.keep_fn(self)

        if self.exclude_unpaid:
            exclude = set(queries.get_all_unpaid_links())
        else:
            exclude = set()

        def keep(item):
            if not self.include_managed and item.managed_promo:
                return False

            if self.exclude_unpaid and item._fullname in exclude:
                return False

            return base_keep_fn(item)
        return keep

    def query(self):
        if self.sort == "future_promos":
            return queries.get_all_unapproved_links()
        elif self.sort == "pending_promos":
            return queries.get_all_accepted_links()
        elif self.sort == "unpaid_promos":
            return queries.get_all_unpaid_links()
        elif self.sort == "rejected_promos":
            return queries.get_all_rejected_links()
        elif self.sort == "live_promos" and self.sr:
            return self.live_by_subreddit(self.sr)
        elif self.sort == 'live_promos':
            return queries.get_all_live_links()
        elif self.sort == 'edited_live_promos':
            return queries.get_all_edited_live_links()
        elif self.sort == 'underdelivered':
            q = queries.get_underdelivered_campaigns()
            campaigns = PromoCampaign._by_fullname(list(q), data=True,
                                                   return_dict=False)
            link_ids = [camp.link_id for camp in campaigns]
            return [Link._fullname_from_id36(to36(id)) for id in link_ids]
        elif self.sort == 'reported':
            return queries.get_reported_links(Subreddit.get_promote_srid())
        elif self.sort == 'fraud':
            return queries.get_payment_flagged_links()
        elif self.sort == 'house':
            return self.get_house_link_names()
        elif self.sort == 'all':
            return queries.get_all_promoted_links()

    def listing(self):
        """For sponsors, update wrapped links to include their campaigns."""
        pane = super(self.__class__, self).listing()

        if c.user_is_sponsor:
            link_ids = {item._id for item in pane.things}
            campaigns = PromoCampaign._by_link(link_ids)
            campaigns_by_link = defaultdict(list)
            for camp in campaigns:
                campaigns_by_link[camp.link_id].append(camp)

            for item in pane.things:
                campaigns = campaigns_by_link[item._id]
                item.campaigns = RenderableCampaign.from_campaigns(
                    item, campaigns, full_details=False)
                item.cachable = False
                item.show_campaign_summary = True
        return pane

    @validate(
        VSponsorAdmin(),
        srname=nop('sr'),
        include_managed=VBoolean("include_managed"),
        exclude_unpaid=VBoolean("exclude_unpaid"),
    )
    def GET_listing(self, srname=None, include_managed=False,
                    exclude_unpaid=None, sort="all", **kw):
        self.sort = sort
        self.sr = None
        self.include_managed = include_managed

        if "exclude_unpaid" not in request.GET:
            self.exclude_unpaid = self.sort == "fraud"
        else:
            self.exclude_unpaid = exclude_unpaid

        if srname:
            try:
                self.sr = Subreddit._by_name(srname)
            except NotFound:
                pass
        return ListingController.GET_listing(self, **kw)


def allowed_location_and_target(location, target):
    if c.user_is_sponsor or feature.is_enabled('ads_auction'):
        return True

    # regular users can only use locations when targeting frontpage
    is_location = location and location.country
    is_frontpage = (not target.is_collection and
                    target.subreddit_name == Frontpage.name)
    return not is_location or is_frontpage


class PromoteApiController(ApiController):
    @json_validate(sr=VSubmitSR('sr', promotion=True),
                   collection=VCollection('collection'),
                   location=VLocation(),
                   start=VDate('startdate'),
                   end=VDate('enddate'),
                   platform=VOneOf('platform', ('mobile', 'desktop', 'all'),
                                   default='all'))
    def GET_check_inventory(self, responder, sr, collection, location, start,
                            end, platform):
        if collection:
            target = Target(collection)
            sr = None
        else:
            sr = sr or Frontpage
            target = Target(sr.name)

        if not allowed_location_and_target(location, target):
            return abort(403, 'forbidden')

        available = inventory.get_available_pageviews(
                        target, start, end, location=location, platform=platform,
                        datestr=True)

        return {'inventory': available}

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VLink("link_id36"),
                   campaign=VPromoCampaign("campaign_id36"))
    def POST_freebie(self, form, jquery, link, campaign):
        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        if campaign_has_oversold_error(form, campaign):
            form.set_text(".freebie", _("target oversold, can't freebie"))
            return

        if promote.is_promo(link) and campaign:
            promote.free_campaign(link, campaign, c.user)
            form.redirect(promote.promo_edit_url(link))

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VByName("link"),
                   note=nop("note"))
    def POST_promote_note(self, form, jquery, link, note):
        if promote.is_promo(link):
            text = PromotionLog.add(link, note)
            form.find(".notes").children(":last").after(
                format_html("<p>%s</p>", text))

    @validatedForm(
        VSponsorAdmin(),
        VModhash(),
        thing = VByName("thing_id"),
        is_fraud=VBoolean("fraud"),
    )
    def POST_review_fraud(self, form, jquery, thing, is_fraud):
        if not thing or not getattr(thing, "promoted", False):
            return

        promote.review_fraud(thing, is_fraud)

        button = jquery(".id-%s .fraud-button" % thing._fullname)
        button.text(_("fraud" if is_fraud else "not fraud"))
        form.parents('.link').fadeOut()

    @noresponse(VSponsorAdmin(),
                VModhash(),
                thing=VByName('id'))
    def POST_promote(self, thing):
        if promote.is_promo(thing):
            promote.accept_promotion(thing)

    @noresponse(VSponsorAdmin(),
                VModhash(),
                thing=VByName('id'),
                reason=nop("reason"))
    def POST_unpromote(self, thing, reason):
        if promote.is_promo(thing):
            promote.reject_promotion(thing, reason=reason)

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VLink('link'),
                   campaign=VPromoCampaign('campaign'))
    def POST_refund_campaign(self, form, jquery, link, campaign):
        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        # If created before switch to auction, use old billing method
        if hasattr(campaign, 'cpm'):
            billable_impressions = promote.get_billable_impressions(campaign)
            billable_amount = promote.get_billable_amount(campaign,
                billable_impressions)
            refund_amount = promote.get_refund_amount(campaign, billable_amount)
        # Otherwise, use adserver_spent_pennies
        else:
            billable_amount = campaign.total_budget_pennies / 100.
            refund_amount = (billable_amount -
                (campaign.adserver_spent_pennies / 100.))
            billable_impressions = None

        if refund_amount <= 0:
            form.set_text('.status', _('refund not needed'))
            return

        if promote.refund_campaign(link, campaign, refund_amount,
                billable_amount, billable_impressions):
            form.set_text('.status', _('refund succeeded'))
        else:
            form.set_text('.status', _('refund failed'))

    @validatedForm(
        VSponsor('link_id36'),
        VModhash(),
        VRatelimit(rate_user=True,
                   rate_ip=True,
                   prefix='create_promo_'),
        VShamedDomain('url'),
        username=VLength('username', 100, empty_error=None),
        title=VTitle('title'),
        url=VUrl('url', allow_self=False),
        selftext=VMarkdownLength('text', max_length=40000),
        kind=VOneOf('kind', ['link', 'self']),
        disable_comments=VBoolean("disable_comments"),
        sendreplies=VBoolean("sendreplies"),
        media_url=VUrl("media_url", allow_self=False,
                       valid_schemes=('http', 'https')),
        gifts_embed_url=VUrl("gifts_embed_url", allow_self=False,
                             valid_schemes=('http', 'https')),
        media_url_type=VOneOf("media_url_type", ("redditgifts", "scrape")),
        media_autoplay=VBoolean("media_autoplay"),
        media_override=VBoolean("media-override"),
        domain_override=VLength("domain", 100),
        third_party_tracking=VUrl("third_party_tracking"),
        third_party_tracking_2=VUrl("third_party_tracking_2"),
        is_managed=VBoolean("is_managed"),
    )
    def POST_create_promo(self, form, jquery, username, title, url,
                          selftext, kind, disable_comments, sendreplies,
                          media_url, media_autoplay, media_override,
                          iframe_embed_url, media_url_type, domain_override,
                          third_party_tracking, third_party_tracking_2,
                          is_managed):

        images = _get_ads_images(c.user, data=True, meta=True)

        return self._edit_promo(
            form, jquery, username, title, url,
            selftext, kind, disable_comments, sendreplies,
            media_url, media_autoplay, media_override,
            iframe_embed_url, media_url_type, domain_override,
            third_party_tracking, third_party_tracking_2, is_managed,
            thumbnail=images.get("thumbnail", None),
            mobile=images.get("mobile", None),
        )

    @validatedForm(
        VSponsor('link_id36'),
        VModhash(),
        VRatelimit(rate_user=True,
                   rate_ip=True,
                   prefix='create_promo_'),
        VShamedDomain('url'),
        username=VLength('username', 100, empty_error=None),
        title=VTitle('title'),
        url=VUrl('url', allow_self=False),
        selftext=VMarkdownLength('text', max_length=40000),
        kind=VOneOf('kind', ['link', 'self']),
        disable_comments=VBoolean("disable_comments"),
        sendreplies=VBoolean("sendreplies"),
        media_url=VUrl("media_url", allow_self=False,
                       valid_schemes=('http', 'https')),
        gifts_embed_url=VUrl("gifts_embed_url", allow_self=False,
                             valid_schemes=('http', 'https')),
        media_url_type=VOneOf("media_url_type", ("redditgifts", "scrape")),
        media_autoplay=VBoolean("media_autoplay"),
        media_override=VBoolean("media-override"),
        domain_override=VLength("domain", 100),
        third_party_tracking=VUrl("third_party_tracking"),
        third_party_tracking_2=VUrl("third_party_tracking_2"),
        is_managed=VBoolean("is_managed"),
        l=VLink('link_id36'),
    )
    def POST_edit_promo(self, form, jquery, username, title, url,
                        selftext, kind, disable_comments, sendreplies,
                        media_url, media_autoplay, media_override,
                        iframe_embed_url, media_url_type, domain_override,
                        third_party_tracking, third_party_tracking_2,
                        is_managed, l):

        images = _get_ads_images(l, data=True, meta=True)

        return self._edit_promo(
            form, jquery, username, title, url,
            selftext, kind, disable_comments, sendreplies,
            media_url, media_autoplay, media_override,
            iframe_embed_url, media_url_type, domain_override,
            third_party_tracking, third_party_tracking_2, is_managed,
            l=l,
            thumbnail=images.get("thumbnail", None),
            mobile=images.get("mobile", None),
        )

    def _edit_promo(self, form, jquery, username, title, url,
                    selftext, kind, disable_comments, sendreplies,
                    media_url, media_autoplay, media_override,
                    iframe_embed_url, media_url_type, domain_override,
                    third_party_tracking, third_party_tracking_2,
                    is_managed, l=None, thumbnail=None, mobile=None):
        should_ratelimit = False
        is_self = (kind == "self")
        is_link = not is_self
        is_new_promoted = not l
        if not c.user_is_sponsor:
            should_ratelimit = True

        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        # check for user override
        if is_new_promoted and c.user_is_sponsor and username:
            try:
                user = Account._by_name(username)
            except NotFound:
                c.errors.add(errors.USER_DOESNT_EXIST, field="username")
                form.set_error(errors.USER_DOESNT_EXIST, "username")
                return

            if not user.email:
                c.errors.add(errors.NO_EMAIL_FOR_USER, field="username")
                form.set_error(errors.NO_EMAIL_FOR_USER, "username")
                return

            if not user.email_verified:
                c.errors.add(errors.NO_VERIFIED_EMAIL, field="username")
                form.set_error(errors.NO_VERIFIED_EMAIL, "username")
                return
        else:
            user = c.user

        # check for shame banned domains
        if form.has_errors("url", errors.DOMAIN_BANNED):
            g.stats.simple_event('spam.shame.link')
            return

        # demangle URL in canonical way
        if url:
            if isinstance(url, (unicode, str)):
                form.set_inputs(url=url)
            elif isinstance(url, tuple) or isinstance(url[0], Link):
                # there's already one or more links with this URL, but
                # we're allowing mutliple submissions, so we really just
                # want the URL
                url = url[0].url

            # Adzerk limits URLs length for creatives
            if len(url) > ADZERK_URL_MAX_LENGTH:
                c.errors.add(errors.TOO_LONG, field='url',
                    msg_params={'max_length': PROMO_URL_MAX_LENGTH})

        if is_link:
            if form.has_errors('url', errors.NO_URL, errors.BAD_URL,
                    errors.TOO_LONG):
                return

        # users can change the disable_comments on promoted links
        if ((is_new_promoted or not promote.is_promoted(l)) and
            (form.has_errors('title', errors.NO_TEXT, errors.TOO_LONG) or
             jquery.has_errors('ratelimit', errors.RATELIMIT))):
            return

        if is_self and form.has_errors('text', errors.TOO_LONG):
            return

        if is_new_promoted:
            # creating a new promoted link
            l = promote.new_promotion(
                is_self=is_self,
                title=title,
                content=(selftext if is_self else url),
                author=user,
                ip=request.ip,
            )

            if c.user_is_sponsor:
                l.managed_promo = is_managed
                l.domain_override = domain_override or None
                l.third_party_tracking = third_party_tracking or None
                l.third_party_tracking_2 = third_party_tracking_2 or None
            l._commit()

            _force_images(l, thumbnail=thumbnail, mobile=mobile)

            form.redirect(promote.promo_edit_url(l))

        elif not promote.is_promo(l):
            return

        changed = False
        if title and title != l.title:
            l.title = title
            changed = True

        if _force_images(l, thumbnail=thumbnail, mobile=mobile):
            changed = True

        # type changing
        if is_self != l.is_self:
            l.set_content(is_self, selftext if is_self else url)
            changed = True

        if is_link and url and url != l.url:
            l.url = url
            changed = True

        # only trips if changed by a non-sponsor
        if changed and not c.user_is_sponsor and promote.is_promoted(l):
            promote.edited_live_promotion(l)

        # selftext can be changed at any time
        if is_self:
            l.selftext = selftext

        # comment disabling and sendreplies is free to be changed any time.
        l.disable_comments = disable_comments
        l.sendreplies = sendreplies

        if c.user_is_sponsor:
            if (form.has_errors("media_url", errors.BAD_URL) or
                    form.has_errors("gifts_embed_url", errors.BAD_URL)):
                return

        scraper_embed = media_url_type == "scrape"
        media_url = media_url or None
        gifts_embed_url = gifts_embed_url or None

        if c.user_is_sponsor and scraper_embed and media_url != l.media_url:
            if media_url:
                scraped = media._scrape_media(
                    media_url, autoplay=media_autoplay,
                    save_thumbnail=False, use_cache=True)

                if scraped:
                    l.set_media_object(scraped.media_object)
                    l.set_secure_media_object(scraped.secure_media_object)
                    l.media_url = media_url
                    l.gifts_embed_url = None
                    l.media_autoplay = media_autoplay
                else:
                    c.errors.add(errors.SCRAPER_ERROR, field="media_url")
                    form.set_error(errors.SCRAPER_ERROR, "media_url")
                    return
            else:
                l.set_media_object(None)
                l.set_secure_media_object(None)
                l.media_url = None
                l.gifts_embed_url = None
                l.media_autoplay = False

        if (c.user_is_sponsor and not scraper_embed and
                gifts_embed_url != l.gifts_embed_url):
            if gifts_embed_url:
                parsed = UrlParser(gifts_embed_url)
                if not is_subdomain(parsed.hostname, "redditgifts.com"):
                    c.errors.add(errors.BAD_URL, field="gifts_embed_url")
                    form.set_error(errors.BAD_URL, "gifts_embed_url")
                    return

                sandbox = (
                    'allow-popups',
                    'allow-forms',
                    'allow-same-origin',
                    'allow-scripts',
                )
                iframe_attributes = {
                    'embed_url': websafe(iframe_embed_url),
                    'sandbox': ' '.join(sandbox),
                }
                iframe = """
                    <iframe class="redditgifts-embed"
                            src="%(embed_url)s"
                            width="710" height="500" scrolling="no"
                            frameborder="0" allowfullscreen
                            sandbox="%(sandbox)s">
                    </iframe>
                """ % iframe_attributes
                media_object = {
                    'oembed': {
                        'description': 'redditgifts embed',
                        'height': 500,
                        'html': iframe,
                        'provider_name': 'redditgifts',
                        'provider_url': 'http://www.redditgifts.com/',
                        'title': 'redditgifts secret santa 2014',
                        'type': 'rich',
                        'width': 710},
                        'type': 'redditgifts'
                }
                l.set_media_object(media_object)
                l.set_secure_media_object(media_object)
                l.media_url = None
                l.gifts_embed_url = gifts_embed_url
                l.media_autoplay = False
            else:
                l.set_media_object(None)
                l.set_secure_media_object(None)
                l.media_url = None
                l.gifts_embed_url = None
                l.media_autoplay = False

        if c.user_is_sponsor:
            l.media_override = media_override
            l.domain_override = domain_override or None
            l.third_party_tracking = third_party_tracking or None
            l.third_party_tracking_2 = third_party_tracking_2 or None
            l.managed_promo = is_managed

        l._commit()

        # ensure plugins are notified of the final edits to the link.
        # other methods also call this hook earlier in the process.
        # see: `promote.unapprove_promotion`
        if not is_new_promoted:
            hooks.get_hook('promote.edit_promotion').call(link=l)

        # clean up so the same images don't reappear if they create
        # another link
        _clear_ads_images(thing=c.user if is_new_promoted else l)

        form.redirect(promote.promo_edit_url(l))

    def _lowest_max_cpm_bid_dollars(self, total_budget_dollars, bid_dollars,
                                    start, end):
        """
        Calculate the lower between g.max_bid_pennies
        and maximum bid per day by budget
        """
        ndays = (to_date(end) - to_date(start)).days
        max_daily_bid = total_budget_dollars / ndays
        max_bid_dollars = g.max_bid_pennies / 100.

        return min(max_daily_bid, max_bid_dollars)

    @validatedForm(
        VSponsor('link_id36'),
        VModhash(),
        is_auction=VBoolean('is_auction'),
        start=VDate('startdate', required=False),
        end=VDate('enddate'),
        link=VLink('link_id36'),
        target=VPromoTarget(),
        campaign_id36=nop("campaign_id36"),
        frequency_cap=VFrequencyCap(("frequency_capped",
                                     "frequency_cap"),),
        priority=VPriority("priority"),
        location=VLocation(),
        platform=VOneOf("platform", ("mobile", "desktop", "all"), default="desktop"),
        mobile_os=VList("mobile_os", choices=["iOS", "Android"]),
        os_versions=VOneOf('os_versions', ('all', 'filter'), default='all'),
        ios_devices=VList('ios_device', choices=IOS_DEVICES),
        android_devices=VList('android_device', choices=ANDROID_DEVICES),
        ios_versions=VOSVersion('ios_version_range', 'ios'),
        android_versions=VOSVersion('android_version_range', 'android'),
        total_budget_dollars=VFloat('total_budget_dollars', coerce=False),
        cost_basis=VOneOf('cost_basis', ('cpc', 'cpm',), default=None),
        bid_dollars=VFloat('bid_dollars', coerce=True),
    )
    def POST_edit_campaign(self, form, jquery, is_auction, link, campaign_id36,
                           start, end, target, frequency_cap,
                           priority, location, platform, mobile_os,
                           os_versions, ios_devices, ios_versions,
                           android_devices, android_versions,
                           total_budget_dollars, cost_basis, bid_dollars):
        if not link:
            return

        if (form.has_errors('frequency_cap', errors.INVALID_FREQUENCY_CAP) or
                form.has_errors('frequency_cap', errors.FREQUENCY_CAP_TOO_LOW)):
            return

        if not target:
            # run form.has_errors to populate the errors in the response
            form.has_errors('sr', errors.SUBREDDIT_NOEXIST,
                            errors.SUBREDDIT_NOTALLOWED,
                            errors.SUBREDDIT_REQUIRED)
            form.has_errors('collection', errors.COLLECTION_NOEXIST)
            form.has_errors('targeting', errors.INVALID_TARGET)
            return

        if form.has_errors('location', errors.INVALID_LOCATION):
            return

        if not allowed_location_and_target(location, target):
            return abort(403, 'forbidden')

        if (form.has_errors('startdate', errors.BAD_DATE) or
                form.has_errors('enddate', errors.BAD_DATE)):
            return

        if not campaign_id36 and not start:
            c.errors.add(errors.BAD_DATE, field='startdate')
            form.set_error('startdate', errors.BAD_DATE)

        if (not feature.is_enabled('mobile_targeting') and
                platform != 'desktop'):
            return abort(403, 'forbidden')

        if link.over_18 and not target.over_18:
            c.errors.add(errors.INVALID_NSFW_TARGET, field='targeting')
            form.has_errors('targeting', errors.INVALID_NSFW_TARGET)
            return

        if not feature.is_enabled('cpc_pricing'):
            cost_basis = 'cpm'

        # Setup campaign details for existing campaigns
        campaign = None
        if campaign_id36:
            try:
                campaign = PromoCampaign._byID36(campaign_id36, data=True)
            except NotFound:
                pass

            if (not campaign
                    or (campaign._deleted or link._id != campaign.link_id)):
                return abort(404, 'not found')

            requires_reapproval = False
            is_live = promote.is_live_promo(link, campaign)
            is_complete = promote.is_complete_promo(link, campaign)

            if not c.user_is_sponsor:
                # If campaign is live, start_date and total_budget_dollars
                # must not be changed
                if is_live:
                    start = campaign.start_date
                    total_budget_dollars = campaign.total_budget_dollars

        # Configure priority, cost_basis, and bid_pennies
        if feature.is_enabled('ads_auction'):
            if c.user_is_sponsor:
                if is_auction:
                    priority = PROMOTE_PRIORITIES['auction']
                    cost_basis = PROMOTE_COST_BASIS[cost_basis]
                else:
                    cost_basis = PROMOTE_COST_BASIS.fixed_cpm
            else:
                # if non-sponsor, is_auction is not part of the POST request,
                # so must be set independently
                is_auction = True
                priority = PROMOTE_PRIORITIES['auction']
                cost_basis = PROMOTE_COST_BASIS[cost_basis]

                # Error if bid is outside acceptable range
                min_bid_dollars = g.min_bid_pennies / 100.
                max_bid_dollars = self._lowest_max_bid_dollars(
                    total_budget_dollars=total_budget_dollars,
                    bid_dollars=bid_dollars,
                    start=start,
                    end=end)

                if bid_dollars < min_bid_dollars or bid_dollars > max_bid_dollars:
                    c.errors.add(errors.BAD_BID, field='bid',
                        msg_params={'min': '%.2f' % round(min_bid_dollars, 2),
                                    'max': '%.2f' % round(max_bid_dollars, 2)}
                    )
                    form.has_errors('bid', errors.BAD_BID)
                    return

        else:
            cost_basis = PROMOTE_COST_BASIS.fixed_cpm

        if priority == PROMOTE_PRIORITIES['auction']:
            bid_pennies = bid_dollars * 100
        else:
            link_owner = Account._byID(link.author_id)
            bid_pennies = PromotionPrices.get_price(link_owner, target,
                location)

        if platform == 'desktop':
            mobile_os = None
        else:
            # check if platform includes mobile, but no mobile OS is selected
            if not mobile_os:
                c.errors.add(errors.BAD_PROMO_MOBILE_OS, field='mobile_os')
                form.set_error(errors.BAD_PROMO_MOBILE_OS, 'mobile_os')
                return
            elif os_versions == 'filter':
                # check if OS is selected, but OS devices are not
                if (('iOS' in mobile_os and not ios_devices) or
                        ('Android' in mobile_os and not android_devices)):
                    c.errors.add(errors.BAD_PROMO_MOBILE_DEVICE, field='os_versions')
                    form.set_error(errors.BAD_PROMO_MOBILE_DEVICE, 'os_versions')
                    return
                # check if OS versions are invalid
                if form.has_errors('os_version', errors.INVALID_OS_VERSION):
                    c.errors.add(errors.INVALID_OS_VERSION, field='os_version')
                    form.set_error(errors.INVALID_OS_VERSION, 'os_version')
                    return

        min_start, max_start, max_end = promote.get_date_limits(
            link, c.user_is_sponsor)

        if campaign:
            if feature.is_enabled('ads_auction'):
                # non-sponsors cannot update fixed CPM campaigns,
                # even if they haven't launched (due to auction)
                if not c.user_is_sponsor and not campaign.is_auction:
                    c.errors.add(errors.COST_BASIS_CANNOT_CHANGE,
                        field='cost_basis')
                    form.set_error(errors.COST_BASIS_CANNOT_CHANGE, 'cost_basis')
                    return

            if not c.user_is_sponsor:
                # If target is changed, require reapproval
                if campaign.target != target:
                    requires_reapproval = True

            if campaign.start_date.date() != start.date():
                # Can't edit the start date of campaigns that have served
                if campaign.has_served:
                    c.errors.add(errors.START_DATE_CANNOT_CHANGE, field='startdate')
                    form.has_errors('startdate', errors.START_DATE_CANNOT_CHANGE)
                    return

                if is_live or is_complete:
                    c.errors.add(errors.START_DATE_CANNOT_CHANGE, field='startdate')
                    form.has_errors('startdate', errors.START_DATE_CANNOT_CHANGE)
                    return

        elif start.date() < min_start:
            c.errors.add(errors.DATE_TOO_EARLY,
                         msg_params={'day': min_start.strftime("%m/%d/%Y")},
                         field='startdate')
            form.has_errors('startdate', errors.DATE_TOO_EARLY)
            return

        if start.date() > max_start:
            c.errors.add(errors.DATE_TOO_LATE,
                         msg_params={'day': max_start.strftime("%m/%d/%Y")},
                         field='startdate')
            form.has_errors('startdate', errors.DATE_TOO_LATE)
            return

        if end.date() > max_end:
            c.errors.add(errors.DATE_TOO_LATE,
                         msg_params={'day': max_end.strftime("%m/%d/%Y")},
                         field='enddate')
            form.has_errors('enddate', errors.DATE_TOO_LATE)
            return

        if end < start:
            c.errors.add(errors.BAD_DATE_RANGE, field='enddate')
            form.has_errors('enddate', errors.BAD_DATE_RANGE)
            return

        # Limit the number of PromoCampaigns a Link can have
        # Note that the front end should prevent the user from getting
        # this far
        existing_campaigns = list(PromoCampaign._by_link(link._id))
        if len(existing_campaigns) > g.MAX_CAMPAIGNS_PER_LINK:
            c.errors.add(errors.TOO_MANY_CAMPAIGNS,
                         msg_params={'count': g.MAX_CAMPAIGNS_PER_LINK},
                         field='title')
            form.has_errors('title', errors.TOO_MANY_CAMPAIGNS)
            return

        if not priority == PROMOTE_PRIORITIES['house']:
            # total_budget_dollars is submitted as a float;
            # convert it to pennies
            total_budget_pennies = int(total_budget_dollars * 100)
            if c.user_is_sponsor:
                min_total_budget_pennies = 0
                max_total_budget_pennies = 0
            else:
                min_total_budget_pennies = g.min_total_budget_pennies
                max_total_budget_pennies = g.max_total_budget_pennies

            if (total_budget_pennies is None or
                    total_budget_pennies < min_total_budget_pennies or
                    (max_total_budget_pennies and
                    total_budget_pennies > max_total_budget_pennies)):
                c.errors.add(errors.BAD_BUDGET, field='total_budget_dollars',
                             msg_params={'min': min_total_budget_pennies,
                                         'max': max_total_budget_pennies or
                                         g.max_total_budget_pennies})
                form.has_errors('total_budget_dollars', errors.BAD_BUDGET)
                return

            # you cannot edit the bid of a live ad unless it's a freebie
            if (campaign and
                    total_budget_pennies != campaign.total_budget_pennies and
                    promote.is_live_promo(link, campaign) and
                    not campaign.is_freebie()):
                c.errors.add(errors.BUDGET_LIVE, field='total_budget_dollars')
                form.has_errors('total_budget_dollars', errors.BUDGET_LIVE)
                return
        else:
            total_budget_pennies = 0

        # Check inventory
        campaign = campaign if campaign_id36 else None
        if not priority.inventory_override:
            oversold = has_oversold_error(form, campaign, start, end,
                                          total_budget_pennies, bid_pennies,
                                          target, location)
            if oversold:
                return

        # Always set frequency_cap_default for auction campaign if frequency_cap
        # is not set
        if not frequency_cap and is_auction:
            frequency_cap = g.frequency_cap_default

        dates = (start, end)

        campaign_dict = {
            'dates': dates,
            'target': target,
            'frequency_cap': frequency_cap,
            'priority': priority,
            'location': location,
            'total_budget_pennies': total_budget_pennies,
            'cost_basis': cost_basis,
            'bid_pennies': bid_pennies,
            'platform': platform,
            'mobile_os': mobile_os,
            'ios_devices': ios_devices,
            'ios_version_range': ios_versions,
            'android_devices': android_devices,
            'android_version_range': android_versions,
        }

        if campaign:
            if requires_reapproval and promote.is_accepted(link):
                campaign_dict['is_approved'] = False

            promote.edit_campaign(
                link,
                campaign,
                **campaign_dict
            )
        else:
            campaign = promote.new_campaign(
                link,
                **campaign_dict
            )
        rc = RenderableCampaign.from_campaigns(link, campaign)
        jquery.update_campaign(campaign._fullname, rc.render_html())

    @validatedForm(VSponsor('link_id36'),
                   VModhash(),
                   l=VLink('link_id36'),
                   campaign=VPromoCampaign("campaign_id36"))
    def POST_delete_campaign(self, form, jquery, l, campaign):
        if not campaign or not l or l._id != campaign.link_id:
            return abort(404, 'not found')

        promote.delete_campaign(l, campaign)

    @validatedForm(
        VSponsor('link_id36'),
        VModhash(),
        link=VLink('link_id36'),
        campaign=VPromoCampaign('campaign_id36'),
        should_pause=VBoolean('should_pause'),)
    def POST_toggle_pause_campaign(self, form, jquery, link, campaign,
            should_pause=False):
        if (not link or not campaign or link._id != campaign.link_id
                or not feature.is_enabled('pause_ads')):
            return abort(404, 'not found')

        if campaign.paused == should_pause:
            return

        promote.toggle_pause_campaign(link, campaign, should_pause)
        rc = RenderableCampaign.from_campaigns(link, campaign)
        jquery.update_campaign(campaign._fullname, rc.render_html())

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VLink('link_id36'),
                   campaign=VPromoCampaign("campaign_id36"))
    def POST_terminate_campaign(self, form, jquery, link, campaign):
        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        promote.terminate_campaign(link, campaign)
        rc = RenderableCampaign.from_campaigns(link, campaign)
        jquery.update_campaign(campaign._fullname, rc.render_html())

    @validatedForm(
        VVerifiedSponsor('link'),
        VModhash(),
        link=VByName("link"),
        campaign=VPromoCampaign("campaign"),
        customer_id=VInt("customer_id", min=0),
        pay_id=VInt("account", min=0),
        edit=VBoolean("edit"),
        address=ValidAddress(
            ["firstName", "lastName", "company", "address", "city", "state",
             "zip", "country", "phoneNumber"]
        ),
        creditcard=ValidCard(["cardNumber", "expirationDate", "cardCode"]),
    )
    def POST_update_pay(self, form, jquery, link, campaign, customer_id, pay_id,
                        edit, address, creditcard):

        def _handle_failed_payment(reason=None):
            promote.failed_payment_method(c.user, link)
            msg = reason or _("failed to authenticate card. sorry.")
            form.set_text(".status", msg)

        if not g.authorizenetapi:
            return

        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        # Check inventory
        if not campaign.is_auction:
            if campaign_has_oversold_error(form, campaign):
                return

        # check the campaign dates are still valid (user may have created
        # the campaign a few days ago)
        min_start, max_start, max_end = promote.get_date_limits(
            link, c.user_is_sponsor)

        if campaign.start_date.date() > max_start:
            msg = _("please change campaign start date to %(date)s or earlier")
            date = format_date(max_start, format="short", locale=c.locale)
            msg %= {'date': date}
            form.set_text(".status", msg)
            return

        if campaign.start_date.date() < min_start:
            msg = _("please change campaign start date to %(date)s or later")
            date = format_date(min_start, format="short", locale=c.locale)
            msg %= {'date': date}
            form.set_text(".status", msg)
            return

        new_payment = not pay_id

        address_modified = new_payment or edit
        if address_modified:
            address_fields = ["firstName", "lastName", "company", "address",
                              "city", "state", "zip", "country", "phoneNumber"]
            card_fields = ["cardNumber", "expirationDate", "cardCode"]

            if (form.has_errors(address_fields, errors.BAD_ADDRESS) or
                    form.has_errors(card_fields, errors.BAD_CARD)):
                return

            try:
                pay_id = add_or_update_payment_method(
                    c.user, address, creditcard, pay_id)

                if pay_id:
                    promote.new_payment_method(user=c.user,
                                               ip=request.ip,
                                               address=address,
                                               link=link)

            except AuthorizeNetException:
                _handle_failed_payment()
                return

        if pay_id:
            success, reason = promote.auth_campaign(link, campaign, c.user,
                                                    pay_id)

            if success:
                hooks.get_hook("promote.campaign_paid").call(link=link, campaign=campaign)
                if not address and g.authorizenetapi:
                    profiles = get_or_create_customer_profile(c.user).paymentProfiles
                    profile = {p.customerPaymentProfileId: p for p in profiles}[pay_id]

                    address = profile.billTo

                promote.successful_payment(link, campaign, request.ip, address)

                jquery.payment_redirect(promote.promo_edit_url(link),
                        new_payment, campaign.total_budget_pennies)
                return
            else:
                _handle_failed_payment(reason)

        else:
            _handle_failed_payment()

    @json_validate(
        VSponsor("link"),
        VModhash(),
        link=VLink("link"),
        kind=VOneOf("kind", ["thumbnail", "mobile"]),
        filepath=nop("filepath"),
        ajax=VBoolean("ajax", default=True)
    )
    def POST_ad_s3_params(self, responder, link, kind, filepath, ajax):
        filename, ext = os.path.splitext(filepath)
        mime_type, encoding = mimetypes.guess_type(filepath)

        if not mime_type or mime_type not in ALLOWED_IMAGE_TYPES:
            request.environ["extra_error_data"] = {
                "message": _("image must be a jpg or png"),
            }
            abort(403)

        keyspace = _get_ads_keyspace(link if link else c.user)
        key = os.path.join(keyspace, kind)
        redirect = None

        if not ajax:
            now = datetime.now().replace(tzinfo=g.tz)
            signature = _get_callback_hmac(
                username=c.user.name,
                key=key,
                expires=now,
            )
            path = ("/api/ad_s3_callback?hmac=%s&ts=%s" %
                (signature, _format_expires(now)))
            redirect = add_sr(path, sr_path=False)

        return s3_helpers.get_post_args(
            bucket=g.s3_client_uploads_bucket,
            key=key,
            success_action_redirect=redirect,
            success_action_status="201",
            content_type=mime_type,
            meta={
                "x-amz-meta-ext": ext,
            },
        )

    @validate(
        VSponsor(),
        expires=VDate("ts", format=EXPIRES_DATE_FORMAT),
        signature=VPrintable("hmac", 255),
        callback=nop("callback"),
        key=nop("key"),
    )
    def GET_ad_s3_callback(self, expires, signature, callback, key):
        now = datetime.now(tz=g.tz)
        if (expires + timedelta(minutes=10) < now):
            self.abort404()

        expected_mac = _get_callback_hmac(
            username=c.user.name,
            key=key,
            expires=expires,
        )

        if not constant_time_compare(signature, expected_mac):
            self.abort404()

        template = "<script>parent.__s3_callbacks__[%(callback)s](%(data)s);</script>"
        image = _key_to_dict(
            s3_helpers.get_key(g.s3_client_uploads_bucket, key))
        response = {
            "callback": scriptsafe_dumps(callback),
            "data": scriptsafe_dumps(image),
        }

        return format_html(template, response)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib.validator import VRatelimit
from r2.lib import amqp
from r2.lib import emailer
from r2.lib import hooks
from r2.lib import newsletter
from r2.lib.base import abort
from r2.lib.errors import errors, reddit_http_error

from r2.models.account import register, AccountExists


def handle_login(
    controller, form, responder, user, rem=None, signature=None, **kwargs
):
    def _event(error):
        g.events.login_event(
            'login_attempt',
            error_msg=error,
            user_name=request.urlvars.get('url_user'),
            remember_me=rem,
            signature=signature,
            request=request,
            context=c)

    if signature and not signature.is_valid():
        _event(error="SIGNATURE")
        abort(403)

    hook_error = hooks.get_hook("account.login").call_until_return(
        responder=responder,
        request=request,
        context=c,
    )
    # if any of the hooks returned an error, abort the login.  The
    # set_error in this case also needs to exist in the hook.
    if hook_error:
        _event(error=hook_error)
        return

    exempt_ua = (request.user_agent and
                 any(ua in request.user_agent for ua
                     in g.config.get('exempt_login_user_agents', ())))
    if (errors.LOGGED_IN, None) in c.errors:
        if user == c.user or exempt_ua:
            # Allow funky clients to re-login as the current user.
            c.errors.remove((errors.LOGGED_IN, None))
        else:
            _event(error='LOGGED_IN')
            abort(reddit_http_error(409, errors.LOGGED_IN))

    if responder.has_errors("ratelimit", errors.RATELIMIT):
        _event(error='RATELIMIT')

    elif responder.has_errors("passwd", errors.WRONG_PASSWORD):
        _event(error='WRONG_PASSWORD')

    else:
        controller._login(responder, user, rem)
        _event(error=None)


def handle_register(
    controller, form, responder, name, email,
    password, rem=None, newsletter_subscribe=False,
    sponsor=False, signature=None, **kwargs
):

    def _event(error):
        g.events.login_event(
            'register_attempt',
            error_msg=error,
            user_name=request.urlvars.get('url_user'),
            email=request.POST.get('email'),
            remember_me=rem,
            newsletter=newsletter_subscribe,
            signature=signature,
            request=request,
            context=c)

    if signature and not signature.is_valid():
        _event(error="SIGNATURE")
        abort(403)

    if responder.has_errors('user', errors.USERNAME_TOO_SHORT):
        _event(error='USERNAME_TOO_SHORT')

    elif responder.has_errors('user', errors.USERNAME_INVALID_CHARACTERS):
        _event(error='USERNAME_INVALID_CHARACTERS')

    elif responder.has_errors('user', errors.USERNAME_TAKEN_DEL):
        _event(error='USERNAME_TAKEN_DEL')

    elif responder.has_errors('user', errors.USERNAME_TAKEN):
        _event(error='USERNAME_TAKEN')

    elif responder.has_errors('email', errors.BAD_EMAIL):
        _event(error='BAD_EMAIL')

    elif responder.has_errors('passwd', errors.SHORT_PASSWORD):
        _event(error='SHORT_PASSWORD')

    elif responder.has_errors('passwd', errors.BAD_PASSWORD):
        # BAD_PASSWORD is set when SHORT_PASSWORD is set
        _event(error='BAD_PASSWORD')

    elif responder.has_errors('passwd2', errors.BAD_PASSWORD_MATCH):
        _event(error='BAD_PASSWORD_MATCH')

    elif responder.has_errors('ratelimit', errors.RATELIMIT):
        _event(error='RATELIMIT')

    elif (not g.disable_captcha and
            responder.has_errors('captcha', errors.BAD_CAPTCHA)):
        _event(error='BAD_CAPTCHA')

    elif newsletter_subscribe and not email:
        c.errors.add(errors.NEWSLETTER_NO_EMAIL, field="email")
        form.has_errors("email", errors.NEWSLETTER_NO_EMAIL)
        _event(error='NEWSLETTER_NO_EMAIL')

    elif sponsor and not email:
        c.errors.add(errors.SPONSOR_NO_EMAIL, field="email")
        form.has_errors("email", errors.SPONSOR_NO_EMAIL)
        _event(error='SPONSOR_NO_EMAIL')

    else:
        try:
            user = register(name, password, request.ip)
        except AccountExists:
            c.errors.add(errors.USERNAME_TAKEN, field="user")
            form.has_errors("user", errors.USERNAME_TAKEN)
            _event(error='USERNAME_TAKEN')
            return

        VRatelimit.ratelimit(rate_ip=True, prefix="rate_register_")

        # anything else we know (email, languages)?
        if email:
            user.set_email(email)
            emailer.verify_email(user)

        user.pref_lang = c.lang
        user._commit()

        amqp.add_item('new_account', user._fullname)

        hooks.get_hook("account.registered").call(user=user)

        reject = hooks.get_hook("account.spotcheck").call(account=user)
        if any(reject):
            _event(error='ACCOUNT_SPOTCHECK')
            return

        if newsletter_subscribe and email:
            try:
                newsletter.add_subscriber(email, source="register")
            except newsletter.NewsletterError as e:
                g.log.warning("Failed to subscribe: %r" % e)

        controller._login(responder, user, rem)
        _event(error=None)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import re
from collections import defaultdict
from itertools import chain
import inspect
from os.path import abspath, relpath

from pylons import app_globals as g
from pylons.i18n import _
from reddit_base import RedditController
from r2.lib.utils import Storage
from r2.lib.pages import BoringPage, ApiHelp
from r2.lib.validator import validate, VOneOf

# API sections displayed in the documentation page.
# Each section can have a title and a markdown-formatted description.
section_info = {
    'account': {
        'title': 'account',
    },
    'flair': {
        'title': 'flair',
    },
    'gold': {
        'title': 'reddit gold',
    },
    'links_and_comments': {
        'title': 'links & comments',
    },
    'messages': {
        'title': 'private messages',
    },
    'moderation': {
        'title': 'moderation',
    },
    'misc': {
        'title': 'misc',
    },
    'listings': {
        'title': 'listings',
    },
    'search': {
        'title': 'search',
    },
    'subreddits': {
        'title': 'subreddits',
    },
    'multis': {
        'title': 'multis',
    },
    'users': {
        'title': 'users',
    },
    'wiki': {
        'title': 'wiki',
    },
    'captcha': {
        'title': 'captcha',
    }
}

api_section = Storage((k, k) for k in section_info)

def api_doc(section, uses_site=False, **kwargs):
    """
    Add documentation annotations to the decorated function.

    See ApidocsController.docs_from_controller for a list of annotation fields.
    """
    def add_metadata(api_function):
        doc = api_function._api_doc = getattr(api_function, '_api_doc', {})
        if 'extends' in kwargs:
            kwargs['extends'] = kwargs['extends']._api_doc
        doc.update(kwargs)
        doc['uses_site'] = uses_site
        doc['section'] = section

        return api_function
    return add_metadata

class ApidocsController(RedditController):
    @staticmethod
    def docs_from_controller(controller, url_prefix='/api', oauth_only=False):
        """
        Examines a controller for documentation.  A dictionary index of
        sections containing dictionaries of URLs is returned.  For each URL, a
        dictionary of HTTP methods (GET, POST, etc.) is contained.  For each
        URL/method pair, a dictionary containing the following items is
        available:

        - `doc`: Markdown-formatted docstring.
        - `uri`: Manually-specified URI to list the API method as
        - `uri_variants`: Alternate URIs to access the API method from
        - `supports_rss`: Indicates the URI also supports rss consumption
        - `parameters`: Dictionary of possible parameter names and descriptions.
        - `extends`: API method from which to inherit documentation
        - `json_model`: The JSON model used instead of normal POST parameters
        """

        api_docs = defaultdict(lambda: defaultdict(dict))
        for name, func in controller.__dict__.iteritems():
            method, sep, action = name.partition('_')
            if not action:
                continue

            valid_methods = ('GET', 'POST', 'PUT', 'DELETE', 'PATCH')
            api_doc = getattr(func, '_api_doc', None)
            if api_doc and 'section' in api_doc and method in valid_methods:
                docs = {}
                docs['doc'] = inspect.getdoc(func)

                if 'extends' in api_doc:
                    docs.update(api_doc['extends'])
                    # parameters are handled separately.
                    docs['parameters'] = {}
                docs.update(api_doc)

                # hide parameters that don't need to be public
                if 'parameters' in api_doc:
                    docs['parameters'].pop('timeout', None)

                # append a message to the docstring if supplied
                notes = docs.get("notes")
                if notes:
                    notes = "\n".join(notes)
                    if docs["doc"]:
                        docs["doc"] += "\n\n" + notes
                    else:
                        docs["doc"] = notes

                uri = docs.get('uri') or '/'.join((url_prefix, action))
                docs['uri'] = uri

                if 'supports_rss' not in docs:
                    docs['supports_rss'] = False

                if api_doc['uses_site']:
                    docs["in-subreddit"] = True

                oauth_perms = getattr(func, 'oauth2_perms', {})
                oauth_allowed = oauth_perms.get('oauth2_allowed', False)
                if not oauth_allowed:
                    # Endpoint is not available over OAuth
                    docs['oauth_scopes'] = []
                else:
                    # [None] signifies to the template to state
                    # that the endpoint is accessible to any oauth client
                    docs['oauth_scopes'] = (oauth_perms['required_scopes'] or
                                            [None])

                # add every variant to the index -- the templates will filter
                # out variants in the long-form documentation
                if oauth_only:
                    if not oauth_allowed:
                        continue
                    for scope in docs['oauth_scopes']:
                        for variant in chain([uri],
                                             docs.get('uri_variants', [])):
                            api_docs[scope][variant][method] = docs
                else:
                    for variant in chain([uri], docs.get('uri_variants', [])):
                        api_docs[docs['section']][variant][method] = docs

        return api_docs

    @validate(
        mode=VOneOf('mode', options=('methods', 'oauth'), default='methods'))
    def GET_docs(self, mode):
        # controllers to gather docs from.
        from r2.controllers.api import ApiController, ApiminimalController
        from r2.controllers.apiv1.user import APIv1UserController
        from r2.controllers.apiv1.gold import APIv1GoldController
        from r2.controllers.apiv1.scopes import APIv1ScopesController
        from r2.controllers.captcha import CaptchaController
        from r2.controllers.front import FrontController
        from r2.controllers.wiki import WikiApiController, WikiController
        from r2.controllers.multi import MultiApiController
        from r2.controllers import listingcontroller

        api_controllers = [
            (APIv1UserController, '/api/v1'),
            (APIv1GoldController, '/api/v1'),
            (APIv1ScopesController, '/api/v1'),
            (ApiController, '/api'),
            (ApiminimalController, '/api'),
            (WikiApiController, '/api/wiki'),
            (WikiController, '/wiki'),
            (MultiApiController, '/api/multi'),
            (CaptchaController, ''),
            (FrontController, ''),
        ]
        for name, value in vars(listingcontroller).iteritems():
            if name.endswith('Controller'):
                api_controllers.append((value, ''))

        # bring in documented plugin controllers
        api_controllers.extend(g.plugins.get_documented_controllers())

        # merge documentation info together.
        api_docs = defaultdict(dict)
        oauth_index = defaultdict(set)
        for controller, url_prefix in api_controllers:
            controller_docs = self.docs_from_controller(controller, url_prefix,
                                                        mode == 'oauth')
            for section, contents in controller_docs.iteritems():
                api_docs[section].update(contents)
                for variant, method_dict in contents.iteritems():
                    for method, docs in method_dict.iteritems():
                        for scope in docs['oauth_scopes']:
                            oauth_index[scope].add((section, variant, method))

        return BoringPage(
            _('api documentation'),
            content=ApiHelp(
                api_docs=api_docs,
                oauth_index=oauth_index,
                mode=mode,
            ),
            css_class="api-help",
            show_sidebar=False,
            show_infobar=False
        ).render()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from reddit_base import RedditController
from r2.lib.pages import AdminPage, AdminCreddits, AdminGold
from r2.lib.validator import nop, validate, VAdmin

class AdminToolController(RedditController):
    @validate(
        VAdmin(),
        recipient=nop('recipient'),
    )
    def GET_creddits(self, recipient):
        return AdminPage(content=AdminCreddits(recipient)).render()

    @validate(
        VAdmin(),
        recipient=nop('recipient'),
    )
    def GET_gold(self, recipient):
        return AdminPage(content=AdminGold(recipient)).render()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import hmac
import json
import re

from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.controllers.reddit_base import RedditController, abort_with_error
from r2.lib.base import abort
from r2.lib.cache_poisoning import make_poisoning_report_mac
from r2.lib.csrf import csrf_exempt
from r2.lib.utils import constant_time_compare, UrlParser, is_subdomain
from r2.lib.validator import (
    nop,
    validate,
    VFloat,
    VInt,
    VOneOf,
    VPrintable,
    VRatelimit,
    VValidatedJSON,
)


class WebLogController(RedditController):
    on_validation_error = staticmethod(abort_with_error)

    @csrf_exempt
    @validate(
        VRatelimit(rate_user=False, rate_ip=True, prefix='rate_weblog_'),
        level=VOneOf('level', ('error',)),
        logs=VValidatedJSON('logs',
            VValidatedJSON.ArrayOf(VValidatedJSON.PartialObject({
                'msg': VPrintable('msg', max_length=256),
                'url': VPrintable('url', max_length=256),
                'tag': VPrintable('tag', max_length=32),
            }))
        ),
    )
    def POST_message(self, level, logs):
        # Whitelist tags to keep the frontend from creating too many keys in statsd
        valid_frontend_log_tags = {
            'unknown',
            'reddit-config-migrate-error',
        }

        # prevent simple CSRF by requiring a custom header
        if not request.headers.get('X-Loggit'):
            abort(403)

        uid = c.user._id if c.user_is_loggedin else '-'

        # only accept a maximum of 3 entries per request
        for log in logs[:3]:
            if 'msg' not in log or 'url' not in log:
                continue

            tag = 'unknown'

            if log.get('tag') in valid_frontend_log_tags:
                tag = log['tag']

            g.stats.simple_event('frontend.error.' + tag)

            g.log.warning('[web frontend] %s: %s | U: %s FP: %s UA: %s',
                          level, log['msg'], uid, log['url'],
                          request.user_agent)

        VRatelimit.ratelimit(rate_user=False, rate_ip=True,
                             prefix="rate_weblog_", seconds=10)

    def OPTIONS_report_cache_poisoning(self):
        """Send CORS headers for cache poisoning reports."""
        if "Origin" not in request.headers:
            return
        origin = request.headers["Origin"]
        parsed_origin = UrlParser(origin)
        if not is_subdomain(parsed_origin.hostname, g.domain):
            return
        response.headers["Access-Control-Allow-Origin"] = origin
        response.headers["Access-Control-Allow-Methods"] = "POST"
        response.headers["Access-Control-Allow-Headers"] = \
            "Authorization, X-Loggit, "
        response.headers["Access-Control-Allow-Credentials"] = "false"
        response.headers['Access-Control-Expose-Headers'] = \
            self.COMMON_REDDIT_HEADERS

    @csrf_exempt
    @validate(
        VRatelimit(rate_user=False, rate_ip=True, prefix='rate_poison_'),
        report_mac=VPrintable('report_mac', 255),
        poisoner_name=VPrintable('poisoner_name', 255),
        poisoner_id=VInt('poisoner_id'),
        poisoner_canary=VPrintable('poisoner_canary', 2, min_length=2),
        victim_canary=VPrintable('victim_canary', 2, min_length=2),
        render_time=VInt('render_time'),
        route_name=VPrintable('route_name', 255),
        url=VPrintable('url', 2048),
        # To differentiate between web and mweb in the future
        source=VOneOf('source', ('web', 'mweb')),
        cache_policy=VOneOf('cache_policy',
            ('loggedin_www', 'loggedin_www_new', 'loggedin_mweb')
        ),
        # JSON-encoded response headers from when our script re-requested
        # the poisoned page
        resp_headers=nop('resp_headers'),
    )
    def POST_report_cache_poisoning(
            self,
            report_mac,
            poisoner_name,
            poisoner_id,
            poisoner_canary,
            victim_canary,
            render_time,
            route_name,
            url,
            source,
            cache_policy,
            resp_headers,
    ):
        """Report an instance of cache poisoning and its details"""

        self.OPTIONS_report_cache_poisoning()

        if c.errors:
            abort(400)

        # prevent simple CSRF by requiring a custom header
        if not request.headers.get('X-Loggit'):
            abort(403)

        # Eh? Why are you reporting this if the canaries are the same?
        if poisoner_canary == victim_canary:
            abort(400)

        expected_mac = make_poisoning_report_mac(
            poisoner_canary=poisoner_canary,
            poisoner_name=poisoner_name,
            poisoner_id=poisoner_id,
            cache_policy=cache_policy,
            source=source,
            route_name=route_name,
        )
        if not constant_time_compare(report_mac, expected_mac):
            abort(403)

        if resp_headers:
            try:
                resp_headers = json.loads(resp_headers)
                # Verify this is a JSON map of `header_name => [value, ...]`
                if not isinstance(resp_headers, dict):
                    abort(400)
                for hdr_name, hdr_vals in resp_headers.iteritems():
                    if not isinstance(hdr_name, basestring):
                        abort(400)
                    if not all(isinstance(h, basestring) for h in hdr_vals):
                        abort(400)
            except ValueError:
                abort(400)

        if not resp_headers:
            resp_headers = {}

        poison_info = dict(
            poisoner_name=poisoner_name,
            poisoner_id=str(poisoner_id),
            # Convert the JS timestamp to a standard one
            render_time=render_time * 1000,
            route_name=route_name,
            url=url,
            source=source,
            cache_policy=cache_policy,
            resp_headers=resp_headers,
        )

        # For immediate feedback when tracking the effects of caching changes
        g.stats.simple_event("cache.poisoning.%s.%s" % (source, cache_policy))
        # For longer-term diagnosing of caching issues
        g.events.cache_poisoning_event(poison_info, request=request, context=c)

        VRatelimit.ratelimit(rate_ip=True, prefix="rate_poison_", seconds=10)

        return self.api_wrapper({})
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import os
import random

import pylons

from webob.exc import HTTPFound, HTTPMovedPermanently
from pylons.i18n import _
from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g


try:
    # place all r2 specific imports in here.  If there is a code error, it'll
    # get caught and the stack trace won't be presented to the user in
    # production
    from r2.config import extensions
    from r2.controllers.reddit_base import RedditController, UnloggedUser
    from r2.lib.cookies import Cookies
    from r2.lib.errors import ErrorSet
    from r2.lib.filters import (
        safemarkdown,
        scriptsafe_dumps,
        websafe,
        websafe_json,
    )
    from r2.lib import log, pages
    from r2.lib.strings import get_funny_translated_string
    from r2.lib.template_helpers import static
    from r2.lib.base import abort
    from r2.models.link import Link
    from r2.models.subreddit import DefaultSR, Subreddit
except Exception, e:
    if g.debug:
        # if debug mode, let the error filter up to pylons to be handled
        raise e
    else:
        # production environment: protect the code integrity!
        print "HuffmanEncodingError: make sure your python compiles before deploying, stupid!"
        # kill this app
        os._exit(1)


redditbroke =  \
'''<html>
  <head>
    <title>reddit broke!</title>
  </head>
  <body>
    <div style="margin: auto; text-align: center">
      <p>
        <a href="/">
          <img border="0" src="%s" alt="you broke reddit" />
        </a>
      </p>
      <p>
        %s
      </p>
  </body>
</html>
'''


FAILIEN_COUNT = 3
def make_failien_url():
    failien_number = random.randint(1, FAILIEN_COUNT)
    failien_name = "youbrokeit%d.png" % failien_number
    return static(failien_name)


class ErrorController(RedditController):
    """Generates error documents as and when they are required.

    The ErrorDocuments middleware forwards to ErrorController when error
    related status codes are returned from the application.

    This behaviour can be altered by changing the parameters to the
    ErrorDocuments middleware in your config/middleware.py file.
    """
    # Handle POST endpoints redirecting to the error controller
    handles_csrf = True

    def check_for_bearer_token(self):
        pass

    allowed_render_styles = ('html', 'xml', 'js', 'embed', '', "compact", 'api')
    # List of admins to blame (skip the first admin, "reddit")
    # If list is empty, just blame "an admin"
    admins = g.admins[1:] or ["an admin"]
    def __before__(self):
        try:
            c.error_page = True
            RedditController.__before__(self)
        except (HTTPMovedPermanently, HTTPFound):
            # ignore an attempt to redirect from an error page
            pass
        except Exception as e:
            handle_awful_failure("ErrorController.__before__: %r" % e)

        # c.error_page is special-cased in a couple places to bypass
        # c.site checks. We shouldn't allow the user to get here other
        # than through `middleware.py:error_mapper`.
        if not request.environ.get('pylons.error_call'):
            abort(403, "direct access to error controller disallowed")

    def __after__(self): 
        try:
            RedditController.__after__(self)
        except Exception as e:
            handle_awful_failure("ErrorController.__after__: %r" % e)

    def __call__(self, environ, start_response):
        try:
            return RedditController.__call__(self, environ, start_response)
        except Exception as e:
            return handle_awful_failure("ErrorController.__call__: %r" % e)


    def send400(self):
        if 'usable_error_content' in request.environ:
            return request.environ['usable_error_content']
        else:
            res = pages.RedditError(
                title=_("bad request (%(domain)s)") % dict(domain=g.domain),
                message=_("you sent an invalid request"),
                explanation=request.GET.get('explanation'))
            return res.render()

    def send403(self):
        c.site = DefaultSR()
        if 'usable_error_content' in request.environ:
            return request.environ['usable_error_content']
        else:
            res = pages.RedditError(
                title=_("forbidden (%(domain)s)") % dict(domain=g.domain),
                message=_("you are not allowed to do that"),
                explanation=request.GET.get('explanation'))
            return res.render()

    def send404(self):
        if 'usable_error_content' in request.environ:
            return request.environ['usable_error_content']
        return pages.RedditError(_("page not found"),
                                 _("the page you requested does not exist")).render()

    def send429(self):
        retry_after = request.environ.get("retry_after")
        if retry_after:
            response.headers["Retry-After"] = str(retry_after)
            template_name = '/ratelimit_toofast.html'
        else:
            template_name = '/ratelimit_throttled.html'

        template = g.mako_lookup.get_template(template_name)
        return template.render(
            logo_url=static(g.default_header_url),
            retry_after=retry_after,
        )

    def send503(self):
        retry_after = request.environ.get("retry_after")
        if retry_after:
            response.headers["Retry-After"] = str(retry_after)
        return request.environ['usable_error_content']

    def GET_document(self):
        try:
            c.errors = c.errors or ErrorSet()
            # clear cookies the old fashioned way 
            c.cookies = Cookies()

            code =  request.GET.get('code', '')
            try:
                code = int(code)
            except ValueError:
                code = 404
            srname = request.GET.get('srname', '')
            takedown = request.GET.get('takedown', '')
            error_name = request.GET.get('error_name', '')

            if isinstance(c.user, basestring):
                # somehow requests are getting here with c.user unset
                c.user_is_loggedin = False
                c.user = UnloggedUser(browser_langs=None)

            if srname:
                c.site = Subreddit._by_name(srname)

            if request.GET.has_key('allow_framing'):
                c.allow_framing = bool(request.GET['allow_framing'] == '1')

            if (error_name == 'IN_TIMEOUT' and
                    not 'usable_error_content' in request.environ):
                timeout_days_remaining = c.user.days_remaining_in_timeout

                errpage = pages.InterstitialPage(
                    _("suspended"),
                    content=pages.InTimeoutInterstitial(
                        timeout_days_remaining=timeout_days_remaining,
                    ),
                )
                request.environ['usable_error_content'] = errpage.render()

            if code in (204, 304):
                # NEVER return a content body on 204/304 or downstream
                # caches may become very confused.
                return ""
            elif c.render_style not in self.allowed_render_styles:
                return str(code)
            elif c.render_style in extensions.API_TYPES:
                data = request.environ.get('extra_error_data', {'error': code})
                message = request.GET.get('message', '')
                if message:
                    data['message'] = message
                if request.environ.get("WANT_RAW_JSON"):
                    return scriptsafe_dumps(data)
                return websafe_json(json.dumps(data))
            elif takedown and code == 404:
                link = Link._by_fullname(takedown)
                return pages.TakedownPage(link).render()
            elif code == 400:
                return self.send400()
            elif code == 403:
                return self.send403()
            elif code == 429:
                return self.send429()
            elif code == 500:
                failien_url = make_failien_url()
                sad_message = get_funny_translated_string("500_page")
                sad_message %= {'admin': random.choice(self.admins)}
                sad_message = safemarkdown(sad_message)
                return redditbroke % (failien_url, sad_message)
            elif code == 503:
                return self.send503()
            elif c.site:
                return self.send404()
            else:
                return "page not found"
        except Exception as e:
            return handle_awful_failure("ErrorController.GET_document: %r" % e)

    POST_document = GET_document
    PUT_document = GET_document
    PATCH_document = GET_document
    DELETE_document = GET_document


def handle_awful_failure(fail_text):
    """
    Makes sure that no errors generated in the error handler percolate
    up to the user unless debug is enabled.
    """
    if g.debug:
        import sys
        s = sys.exc_info()
        # reraise the original error with the original stack trace
        raise s[1], None, s[2]
    try:
        # log the traceback, and flag the "path" as the error location
        import traceback
        log.write_error_summary(fail_text)
        for line in traceback.format_exc().splitlines():
            g.log.error(line)
        return redditbroke % (make_failien_url(), websafe(fail_text))
    except:
        # we are doomed.  Admit defeat
        return "This is an error that should never occur.  You win."
<EOF>
<BOF>
# -*- coding: utf-8 -*-
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import urllib

from oauth2 import require_oauth2_scope
from reddit_base import RedditController, base_listing, paginated_listing

from r2.models import *
from r2.models.query_cache import CachedQuery, MergedCachedQuery
from r2.config.extensions import is_api
from r2.lib.filters import _force_unicode
from r2.lib.jsontemplates import get_usertrophies
from r2.lib.pages import *
from r2.lib.pages.things import wrap_links
from r2.lib.menus import TimeMenu, CommentsTimeMenu, SortMenu, RecSortMenu, ProfileSortMenu
from r2.lib.menus import ControversyTimeMenu, ProfileOverviewTimeMenu, menu, QueryButton
from r2.lib.rising import get_rising, normalized_rising
from r2.lib.wrapped import Wrapped
from r2.lib.normalized_hot import normalized_hot
from r2.lib.db.thing import Query, Merge, Relations
from r2.lib.db import queries
from r2.lib.strings import Score
from r2.lib.template_helpers import add_sr
from r2.lib.csrf import csrf_exempt
from r2.lib.utils import (
    extract_user_mentions,
    iters,
    query_string,
    timeago,
    to36,
    trunc_string,
    precise_format_timedelta,
)
from r2.lib import hooks, organic, trending
from r2.lib.memoize import memoize
from r2.lib.validator import *
import socket

from api_docs import api_doc, api_section

from pylons import app_globals as g
from pylons.i18n import _

from datetime import timedelta
import random
from functools import partial

class ListingController(RedditController):
    """Generalized controller for pages with lists of links."""


    # toggle skipping of links based on the users' save/hide/vote preferences
    skip = True

    # allow stylesheets on listings
    allow_stylesheets = True

    # toggle sending origin-only referrer headers on cross-domain navigation
    private_referrer = True

    # toggles showing numbers
    show_nums = True

    # any text that should be shown on the top of the page
    infotext = None
    infotext_class = None

    # builder class to use to generate the listing. if none, we'll try
    # to figure it out based on the query type
    builder_cls = None

    # page title
    title_text = ''

    # login box, subreddit box, submit box, etc, visible
    show_sidebar = True
    show_chooser = False
    suppress_reply_buttons = False

    # class (probably a subclass of Reddit) to use to render the page.
    render_cls = Reddit

    # class for suggestions next to "next/prev" buttons
    next_suggestions_cls = None

    #extra parameters to send to the render_cls constructor
    render_params = {}
    extra_page_classes = ['listing-page']

    @property
    def menus(self):
        """list of menus underneat the header (e.g., sort, time, kind,
        etc) to be displayed on this listing page"""
        return []

    def can_send_referrer(self):
        """Return whether links within this listing may have full referrers"""
        if not self.private_referrer:
            return c.site.allows_referrers
        return False

    def build_listing(self, num, after, reverse, count, sr_detail=None, **kwargs):
        """uses the query() method to define the contents of the
        listing and renders the page self.render_cls(..).render() with
        the listing as contents"""
        self.num = num
        self.count = count
        self.after = after
        self.reverse = reverse
        self.sr_detail = sr_detail

        if c.site.login_required and not c.user_is_loggedin:
            raise UserRequiredException

        self.query_obj = self.query()
        self.builder_obj = self.builder()

        # Don't leak info about things the user can't view
        if after and not self.builder_obj.valid_after(after):
            listing_name = self.__class__.__name__
            g.stats.event_count("listing.invalid_after", listing_name)
            self.abort403()

        self.listing_obj = self.listing()

        content = self.content()
        return self.render_cls(content=content,
                               page_classes=self.extra_page_classes,
                               show_sidebar=self.show_sidebar,
                               show_chooser=self.show_chooser,
                               show_newsletterbar=True,
                               nav_menus=self.menus,
                               title=self.title(),
                               infotext=self.infotext,
                               infotext_class=self.infotext_class,
                               robots=getattr(self, "robots", None),
                               **self.render_params).render()

    def content(self):
        """Renderable object which will end up as content of the render_cls"""
        return self.listing_obj

    def query(self):
        """Query to execute to generate the listing"""
        raise NotImplementedError

    def builder(self):
        #store the query itself so it can be used elsewhere
        if self.builder_cls:
            builder_cls = self.builder_cls
        elif isinstance(self.query_obj, Query):
            builder_cls = QueryBuilder
        elif isinstance(self.query_obj, g.search.SearchQuery):
            builder_cls = SearchBuilder
        elif isinstance(self.query_obj, iters):
            builder_cls = IDBuilder
        elif isinstance(self.query_obj, (queries.CachedResults, queries.MergedCachedResults)):
            builder_cls = IDBuilder
        elif isinstance(self.query_obj, (CachedQuery, MergedCachedQuery)):
            builder_cls = IDBuilder

        builder = builder_cls(
            self.query_obj,
            num=self.num,
            skip=self.skip,
            after=self.after,
            count=self.count,
            reverse=self.reverse,
            keep_fn=self.keep_fn(),
            sr_detail=self.sr_detail,
            wrap=self.builder_wrapper,
            prewrap_fn=self.prewrap_fn(),
        )
        return builder

    def keep_fn(self):
        def keep(item):
            wouldkeep = item.keep_item(item)

            if isinstance(c.site, AllSR):
                if not item.subreddit.discoverable:
                    return False
            elif isinstance(c.site, FriendsSR):
                if item.author._deleted or item.author._spam:
                    return False

            if getattr(item, "promoted", None) is not None:
                return False

            if item._deleted and not c.user_is_admin:
                return False

            return wouldkeep

        return keep

    def prewrap_fn(self):
        return

    def listing(self):
        """Listing to generate from the builder"""
        if (getattr(c.site, "_id", -1) == Subreddit.get_promote_srid() and
            not c.user_is_sponsor):
            abort(403, 'forbidden')

        model = LinkListing(self.builder_obj, show_nums=self.show_nums)

        suggestions = None
        if c.render_style == "html" and self.next_suggestions_cls:
            suggestions = self.next_suggestions_cls()

        pane = model.listing(next_suggestions=suggestions)

        # Indicate that the comment tree wasn't built for comments
        for i in pane:
            if hasattr(i, 'full_comment_path'):
                i.child = None
            i.suppress_reply_buttons = self.suppress_reply_buttons

        return pane

    def title(self):
        """Page <title>"""
        return _(self.title_text) + " : " + c.site.name

    def rightbox(self):
        """Contents of the right box when rendering"""
        pass

    builder_wrapper = staticmethod(default_thing_wrapper())

    @require_oauth2_scope("read")
    @base_listing
    def GET_listing(self, **env):
        if isinstance(c.site, ModSR):
            VNotInTimeout().run(action_name="pageview",
                details_text="mod_subreddit")
        if self.can_send_referrer():
            c.referrer_policy = "always"
        return self.build_listing(**env)

listing_api_doc = partial(
    api_doc,
    section=api_section.listings,
    extends=ListingController.GET_listing,
    notes=[paginated_listing.doc_note],
    supports_rss=True,
)


class SubredditListingController(ListingController):
    private_referrer = False

    def _build_og_title(self, max_length=256):
        sr_fragment = "/r/" + c.site.name
        title = c.site.title.strip()
        if not title:
            return trunc_string(sr_fragment, max_length)

        if sr_fragment in title:
            return _force_unicode(trunc_string(title, max_length))

        # We'd like to always show the whole subreddit name, so let's
        # truncate the title while still ensuring the entire thing is under
        # the limit.
        # This doesn't handle `max_length`s shorter than `sr_fragment`.
        # Unknown what the behavior should be, but realistically it shouldn't
        # happen, since this is scoped pretty small.
        max_title_length = max_length - len(u" • %s" % sr_fragment)
        title = trunc_string(title, max_title_length)

        return u"%s • %s" % (_force_unicode(title), sr_fragment)

    def canonical_link(self):
        """Return the canonical link of the subreddit.

        Ordinarily canonical links are created using request.url.
        In the case of subreddits, we perform a bit of magic to strip the
        subreddit path from the url. This means that a path like:

        https:///www.reddit.com/r/hiphopheads/

        will instead show:

        https://www.reddit.com/

        See SubredditMiddleware for more information.

        This method constructs our url from scratch given other information.
        """
        return add_sr('/', force_https=True)

    def _build_og_description(self):
        description = c.site.public_description.strip()
        if not description:
            description = _(g.short_description)
        return _force_unicode(trunc_string(description, MAX_DESCRIPTION_LENGTH))

    @property
    def render_params(self):
        render_params = {}

        if isinstance(c.site, DefaultSR):
            render_params.update({'show_locationbar': True})
        else:
            if not c.user_is_loggedin:
                # This data is only for scrapers, which shouldn't be logged in.
                twitter_card = {
                    "site": "reddit",
                    "card": "summary",
                    "title": self._build_og_title(max_length=70),
                    # Twitter will fall back to any defined OpenGraph
                    # attributes, so we don't need to define
                    # 'twitter:image' or 'twitter:description'.
                }
                hook = hooks.get_hook('subreddit_listing.twitter_card')
                hook.call(tags=twitter_card, sr_name=c.site.name)

                render_params.update({
                    "og_data": {
                        "site_name": "reddit",
                        "title": self._build_og_title(),
                        "image": static('icon.png', absolute=True),
                        "description": self._build_og_description(),
                    },
                    "twitter_card": twitter_card,
                })

        # event target for screenviews
        event_target = {
            'target_type': 'listing',
        }
        if not isinstance(c.site, FakeSubreddit):
            event_target['target_fullname'] = c.site._fullname
            event_target['target_id'] = c.site._id
        if hasattr(self, 'sort'):
            event_target['target_sort'] = self.sort
        elif hasattr(self, 'where'):
            event_target['target_sort'] = self.where
        if hasattr(self, 'time'):
            event_target['target_filter_time'] = self.time
        if self.after:
            event_target['target_count'] = self.count
            if self.reverse:
                event_target['target_before'] = self.after._fullname
            else:
                event_target['target_after'] = self.after._fullname
        render_params['extra_js_config'] = {'event_target': event_target}

        render_params['canonical_link'] = self.canonical_link()

        return render_params


class ListingWithPromos(SubredditListingController):
    show_organic = False

    def make_requested_ad(self, requested_ad):
        try:
            link = Link._by_fullname(requested_ad, data=True)
        except NotFound:
            self.abort404()

        is_link_creator = c.user_is_loggedin and (c.user._id == link.author_id)
        if (not (is_link_creator or c.user_is_sponsor) and
                not promote.is_live_on_sr(link, c.site)):
            self.abort403()

        res = wrap_links([link._fullname], wrapper=self.builder_wrapper,
                         skip=False)
        res.parent_name = "promoted"
        if res.things:
            return res

    def make_single_ad(self):
        keywords = promote.keywords_from_context(c.user, c.site)
        if keywords:
            return SpotlightListing(show_promo=c.site.allow_ads, keywords=keywords,
                                    navigable=False).listing()

    def make_spotlight(self):
        """Build the Spotlight.

        The frontpage gets a Spotlight box that contains promoted and organic
        links from the user's subscribed subreddits and promoted links targeted
        to the frontpage. If the user has disabled ads promoted links will not
        be shown. Promoted links are requested from the adserver client-side.

        """

        organic_fullnames = organic.organic_links(c.user)
        promoted_links = []

        show_promo = False
        keywords = []
        can_show_promo = not c.user.pref_hide_ads or not c.user.gold and c.site.allow_ads
        try_show_promo = ((c.user_is_loggedin and random.random() > 0.5) or
                          not c.user_is_loggedin)

        if can_show_promo and try_show_promo:
            keywords = promote.keywords_from_context(c.user, c.site)
            if keywords:
                show_promo = True

        def organic_keep_fn(item):
            base_keep_fn = super(ListingWithPromos, self).keep_fn()
            would_keep = base_keep_fn(item)
            return would_keep and item.fresh

        random.shuffle(organic_fullnames)
        organic_fullnames = organic_fullnames[:10]
        b = IDBuilder(organic_fullnames,
                      wrap=self.builder_wrapper,
                      keep_fn=organic_keep_fn,
                      skip=True)
        organic_links = b.get_items()[0]

        has_subscribed = c.user.has_subscribed
        interestbar_prob = g.live_config['spotlight_interest_sub_p'
                                         if has_subscribed else
                                         'spotlight_interest_nosub_p']
        interestbar = InterestBar(has_subscribed)

        s = SpotlightListing(organic_links=organic_links,
                             interestbar=interestbar,
                             interestbar_prob=interestbar_prob,
                             show_promo=show_promo,
                             keywords=keywords,
                             max_num = self.listing_obj.max_num,
                             max_score = self.listing_obj.max_score).listing()
        return s

    def content(self):
        # only send a spotlight listing for HTML rendering
        if c.render_style == "html":
            spotlight = None
            show_sponsors = not c.user.pref_hide_ads or not c.user.gold
            show_organic = self.show_organic and c.user.pref_organic
            on_frontpage = isinstance(c.site, DefaultSR)
            requested_ad = request.GET.get('ad')

            if on_frontpage:
                self.extra_page_classes = \
                    self.extra_page_classes + ['front-page']

            if requested_ad:
                spotlight = self.make_requested_ad(requested_ad)
            elif on_frontpage and show_organic:
                spotlight = self.make_spotlight()
            elif show_sponsors:
                spotlight = self.make_single_ad()

            self.spotlight = spotlight

            if spotlight:
                return PaneStack([spotlight, self.listing_obj],
                                 css_class='spacer')
        return self.listing_obj


class HotController(ListingWithPromos):
    where = 'hot'
    extra_page_classes = ListingController.extra_page_classes + ['hot-page']
    show_chooser = True
    next_suggestions_cls = ListingSuggestions
    show_organic = True

    def query(self):

        if isinstance(c.site, DefaultSR):
            sr_ids = Subreddit.user_subreddits(c.user)
            return normalized_hot(sr_ids)
        elif isinstance(c.site, MultiReddit):
            return normalized_hot(c.site.kept_sr_ids, obey_age_limit=False,
                                  ageweight=c.site.ageweight)
        else:
            sticky_fullnames = c.site.sticky_fullnames
            if sticky_fullnames:
                # make a copy of the list so we're not inadvertently modifying
                # the subreddit's list of sticky fullnames
                link_list = sticky_fullnames[:]
                
                wrapped = wrap_links(link_list,
                                     wrapper=self.builder_wrapper,
                                     keep_fn=self.keep_fn(),
                                     skip=True)
                # add all other items and decrement count if sticky is visible
                if wrapped.things:
                    link_list += [l for l in c.site.get_links('hot', 'all')
                                    if l not in sticky_fullnames]
                    if not self.after:
                        self.count -= len(sticky_fullnames)
                        self.num += len(sticky_fullnames)
                    return link_list
            
            # no sticky or sticky hidden
            return c.site.get_links('hot', 'all')

    @classmethod
    def trending_info(cls):
        if not c.user.pref_show_trending:
            return None

        trending_data = trending.get_trending_subreddits()

        if not trending_data:
            return None

        link = Link._byID(trending_data['link_id'], data=True, stale=True)
        return {
            'subreddit_names': trending_data['subreddit_names'],
            'comment_url': trending_data['permalink'],
            'comment_count': link.num_comments,
        }

    def content(self):
        content = super(HotController, self).content()

        if c.render_style == "html":
            stack = None

            hot_hook = hooks.get_hook("hot.get_content")
            hot_pane = hot_hook.call_until_return(controller=self)

            if hot_pane:
                stack = [
                    self.spotlight,
                    hot_pane,
                    self.listing_obj
                ]
            elif isinstance(c.site, DefaultSR) and not self.listing_obj.prev:
                trending_info = self.trending_info()
                if trending_info:
                    stack = [
                        self.spotlight,
                        TrendingSubredditsBar(**trending_info),
                        self.listing_obj,
                    ]

            if stack:
                return PaneStack(filter(None, stack), css_class='spacer')

        return content

    def title(self):
        return c.site.title

    @require_oauth2_scope("read")
    @listing_api_doc(uri='/hot', uses_site=True)
    def GET_listing(self, **env):
        self.infotext = request.GET.get('deactivated') and strings.user_deactivated
        return ListingController.GET_listing(self, **env)

class NewController(ListingWithPromos):
    where = 'new'
    title_text = _('newest submissions')
    extra_page_classes = ListingController.extra_page_classes + ['new-page']
    show_chooser = True
    next_suggestions_cls = ListingSuggestions

    def keep_fn(self):
        def keep(item):
            if item.promoted is not None:
                return False
            else:
                return item.keep_item(item)
        return keep

    def query(self):
        return c.site.get_links('new', 'all')

    @csrf_exempt
    def POST_listing(self, **env):
        # Redirect to GET mode in case of any legacy requests
        return self.redirect(request.fullpath)

    @require_oauth2_scope("read")
    @listing_api_doc(uri='/new', uses_site=True)
    def GET_listing(self, **env):
        return ListingController.GET_listing(self, **env)

class RisingController(NewController):
    where = 'rising'
    title_text = _('rising submissions')
    extra_page_classes = ListingController.extra_page_classes + ['rising-page']

    def query(self):
        if isinstance(c.site, DefaultSR):
            sr_ids = Subreddit.user_subreddits(c.user)
            return normalized_rising(sr_ids)
        elif isinstance(c.site, MultiReddit):
            return normalized_rising(c.site.kept_sr_ids)

        return get_rising(c.site)

class BrowseController(ListingWithPromos):
    where = 'browse'
    show_chooser = True
    next_suggestions_cls = ListingSuggestions

    def keep_fn(self):
        """For merged time-listings, don't show items that are too old
           (this can happen when mr_top hasn't run in a while)"""
        if self.time != 'all' and c.default_sr:
            oldest = timeago('1 %s' % (str(self.time),))
            def keep(item):
                if isinstance(c.site, AllSR):
                    if not item.subreddit.discoverable:
                        return False
                return item._date > oldest and item.keep_item(item)
            return keep
        else:
            return ListingController.keep_fn(self)

    @property
    def menus(self):
        return [ControversyTimeMenu(default = self.time)]

    def query(self):
        return c.site.get_links(self.sort, self.time)

    @csrf_exempt
    @validate(t = VMenu('sort', ControversyTimeMenu))
    def POST_listing(self, sort, t, **env):
        # VMenu validator will save the value of time before we reach this
        # point. Now just redirect to GET mode.
        return self.redirect(
            request.fullpath + query_string(dict(sort=sort, t=t)))

    @require_oauth2_scope("read")
    @validate(t = VMenu('sort', ControversyTimeMenu))
    @listing_api_doc(uri='/{sort}', uri_variants=['/top', '/controversial'],
                     uses_site=True)
    def GET_listing(self, sort, t, **env):
        self.sort = sort
        if sort == 'top':
            self.title_text = _('top scoring links')
            self.extra_page_classes = \
                self.extra_page_classes + ['top-page']
        elif sort == 'controversial':
            self.title_text = _('most controversial links')
            self.extra_page_classes = \
                self.extra_page_classes + ['controversial-page']
        else:
            # 'sort' is forced to top/controversial by routing.py,
            # but in case something has gone wrong...
            abort(404)
        self.time = t
        return ListingController.GET_listing(self, **env)


class AdsController(SubredditListingController):
    where = 'ads'
    builder_cls = CampaignBuilder
    title_text = _('promoted links')

    @property
    def infotext(self):
        infotext = _("want to advertise? [click here!](%(link)s)")
        if c.user.pref_show_promote or c.user_is_sponsor:
            return infotext % {'link': '/promoted'}
        else:
            return infotext % {'link': '/advertising'}

    def keep_fn(self):
        def keep(item):
            if item._fullname in self.promos:
                return False
            if item.promoted and not item._deleted:
                self.promos.add(item._fullname)
                return True
            return False
        return keep

    def query(self):
        try:
            return c.site.get_live_promos()
        except NotImplementedError:
            self.abort404()

    def listing(self):
        listing = ListingController.listing(self)
        return listing

    def GET_listing(self, *a, **kw):
        self.promos = set()
        if not c.site.allow_ads:
            self.abort404()
        return SubredditListingController.GET_listing(self, *a, **kw)


class RandomrisingController(ListingWithPromos):
    where = 'randomrising'
    title_text = _('you\'re really bored now, eh?')
    next_suggestions_cls = ListingSuggestions

    def query(self):
        links = get_rising(c.site)

        if not links:
            # just pull from the new page if the rising page isn't
            # populated for some reason
            links = c.site.get_links('new', 'all')
            if isinstance(links, Query):
                links._limit = 200
                links = [x._fullname for x in links]

        links = list(links)
        random.shuffle(links)

        return links

class ByIDController(ListingController):
    title_text = _('API')
    skip = False

    def query(self):
        return self.names

    @require_oauth2_scope("read")
    @validate(links=VByName("names", thing_cls=Link,
                            ignore_missing=True, multiple=True))
    @api_doc(api_section.listings, uri='/by_id/{names}')
    def GET_listing(self, links, **env):
        """Get a listing of links by fullname.

        `names` is a list of fullnames for links separated by commas or spaces.

        """
        if not links:
            return self.abort404()
        self.names = [l._fullname for l in links]
        return ListingController.GET_listing(self, **env)


class UserController(ListingController):
    render_cls = ProfilePage
    show_nums = False
    skip = True

    @property
    def menus(self):
        res = []
        if (self.where in ('overview', 'submitted', 'comments')):
            res.append(ProfileSortMenu(default = self.sort))
            if self.sort not in ("hot", "new"):
                if self.where == "comments":
                    res.append(CommentsTimeMenu(default = self.time))
                elif self.where == "overview":
                    res.append(ProfileOverviewTimeMenu(default = self.time))
                else:
                    res.append(TimeMenu(default = self.time))
        if self.where == 'saved' and c.user.gold:
            srnames = LinkSavesBySubreddit.get_saved_subreddits(self.vuser)
            srnames += CommentSavesBySubreddit.get_saved_subreddits(self.vuser)
            srs = Subreddit._by_name(set(srnames), stale=True)
            srnames = [name for name, sr in srs.iteritems()
                            if sr.can_view(c.user)]
            srnames = sorted(set(srnames), key=lambda name: name.lower())
            if len(srnames) > 1:
                sr_buttons = [QueryButton(_('all'), None, query_param='sr',
                                        css_class='primary')]
                for srname in srnames:
                    sr_buttons.append(QueryButton(srname, srname, query_param='sr'))
                base_path = '/user/%s/saved' % self.vuser.name
                if self.savedcategory:
                    base_path += '/%s' % urllib.quote(self.savedcategory)
                sr_menu = NavMenu(sr_buttons, base_path=base_path,
                                  title=_('filter by subreddit'),
                                  type='lightdrop')
                res.append(sr_menu)
            categories = LinkSavesByCategory.get_saved_categories(self.vuser)
            categories += CommentSavesByCategory.get_saved_categories(self.vuser)
            categories = sorted(set(categories))
            if len(categories) >= 1:
                cat_buttons = [NavButton(_('all'), '/', css_class='primary')]
                for cat in categories:
                    cat_buttons.append(NavButton(cat,
                                                 urllib.quote(cat),
                                                 use_params=True))
                base_path = '/user/%s/saved/' % self.vuser.name
                cat_menu = NavMenu(cat_buttons, base_path=base_path,
                                   title=_('filter by category'),
                                   type='lightdrop')
                res.append(cat_menu)
        elif (self.where == 'gilded' and
                (c.user == self.vuser or c.user_is_admin)):
            path = '/user/%s/gilded/' % self.vuser.name
            buttons = [NavButton(_("gildings received"), dest='/'),
                       NavButton(_("gildings given"), dest='/given')]
            res.append(NavMenu(buttons, base_path=path, type='flatlist'))

        return res

    def title(self):
        titles = {'overview': _("overview for %(user)s"),
                  'comments': _("comments by %(user)s"),
                  'submitted': _("submitted by %(user)s"),
                  'gilded': _("gilded by %(user)s"),
                  'upvoted': _("upvoted by %(user)s"),
                  'downvoted': _("downvoted by %(user)s"),
                  'saved': _("saved by %(user)s"),
                  'hidden': _("hidden by %(user)s"),
                  'promoted': _("promoted by %(user)s")}
        if self.where == 'gilded' and self.show == 'given':
            return _("gildings given by %(user)s") % {'user': self.vuser.name}

        title = titles.get(self.where, _('profile for %(user)s')) \
            % dict(user = self.vuser.name, site = c.site.name)
        return title

    def keep_fn(self):
        def keep(item):
            if self.where == 'promoted':
                return bool(getattr(item, "promoted", None))

            if item._deleted and not c.user_is_admin:
                return False

            if c.user == self.vuser:
                if not item.likes and self.where == 'upvoted':
                    g.stats.simple_event("vote.missing_votes_by_account")
                    return False
                if item.likes is not False and self.where == 'downvoted':
                    g.stats.simple_event("vote.missing_votes_by_account")
                    return False
                if self.where == 'saved' and not item.saved:
                    return False

            if (self.time != 'all' and
                item._date <= utils.timeago('1 %s' % str(self.time))):
                return False

            if self.where == 'gilded' and item.gildings <= 0:
                return False

            if self.where == 'deleted' and not item._deleted:
                return False

            is_promoted = getattr(item, "promoted", None) is not None
            if self.where != 'saved' and is_promoted:
                return False

            return True
        return keep

    def query(self):
        q = None
        if self.where == 'overview':
            q = queries.get_overview(self.vuser, self.sort, self.time)

        elif self.where == 'comments':
            q = queries.get_comments(self.vuser, self.sort, self.time)

        elif self.where == 'submitted':
            q = queries.get_submitted(self.vuser, self.sort, self.time)

        elif self.where == 'gilded':
            if self.show == 'given':
                q = queries.get_user_gildings(self.vuser)
            else:
                q = queries.get_gilded_user(self.vuser)

        elif self.where in ('upvoted', 'downvoted'):
            if self.where == 'upvoted':
                q = queries.get_liked(self.vuser)
            else:
                q = queries.get_disliked(self.vuser)

        elif self.where == 'hidden':
            q = queries.get_hidden(self.vuser)

        elif self.where == 'saved':
            if not self.savedcategory and c.user.gold:
                self.builder_cls = SavedBuilder
            sr_id = self.savedsr._id if self.savedsr else None
            q = queries.get_saved(self.vuser, sr_id,
                                  category=self.savedcategory)
        elif self.where == 'actions':
            if not votes_visible(self.vuser):
                q = queries.get_overview(self.vuser, self.sort, self.time)
            else:
                q = queries.get_user_actions(self.vuser, 'new', 'all')
                self.builder_cls = ActionBuilder

        elif c.user_is_sponsor and self.where == 'promoted':
            q = queries.get_promoted_links(self.vuser._id)

        if q is None:
            return self.abort404()

        return q

    @require_oauth2_scope("history")
    @validate(vuser = VExistingUname('username', allow_deleted=True),
              sort = VMenu('sort', ProfileSortMenu, remember = False),
              time = VMenu('t', TimeMenu, remember = False),
              show=VOneOf('show', ('given',)))
    @listing_api_doc(section=api_section.users, uri='/user/{username}/{where}',
                     uri_variants=['/user/{username}/' + where for where in [
                                       'overview', 'submitted', 'comments',
                                       'upvoted', 'downvoted', 'hidden',
                                       'saved', 'gilded']])
    def GET_listing(self, where, vuser, sort, time, show, **env):
        # the validator will ensure that vuser is a valid account
        if not vuser:
            return self.abort404()

        if (vuser.in_timeout and
                vuser != c.user and
                not c.user_is_admin and
                not vuser.timeout_expiration):
            errpage = InterstitialPage(
                _("suspended"),
                content=BannedUserInterstitial(),
            )
            request.environ['usable_error_content'] = errpage.render()
            return self.abort403()

        if (c.user_is_loggedin and
                not c.user_is_admin and
                vuser._id in c.user.enemies):
            errpage = InterstitialPage(
                _("blocked"),
                content=UserBlockedInterstitial(),
            )
            request.environ['usable_error_content'] = errpage.render()
            return self.abort403()

        # continue supporting /liked and /disliked paths for API clients
        # but 301 redirect non-API users to the new location
        changed_wheres = {"liked": "upvoted", "disliked": "downvoted"}
        new_where = changed_wheres.get(where)
        if new_where:
            where = new_where
            if not is_api():
                path = "/".join(("/user", vuser.name, where))
                query_string = request.environ.get('QUERY_STRING')
                if query_string:
                    path += "?" + query_string
                return self.redirect(path, code=301)
        
        self.where = where
        self.sort = sort
        self.time = time
        self.show = show

        # only allow admins to view deleted users
        if vuser._deleted and not c.user_is_admin:
            errpage = InterstitialPage(
                _("deleted"),
                content=DeletedUserInterstitial(),
            )
            request.environ['usable_error_content'] = errpage.render()
            return self.abort404()

        if c.user_is_admin:
            c.referrer_policy = "always"

        if self.sort in ('hot', 'new'):
            self.time = 'all'

        # hide spammers profile pages
        if vuser._spam and not vuser.banned_profile_visible:
            if (not c.user_is_loggedin or
                    not (c.user._id == vuser._id or
                         c.user_is_admin or
                         c.user_is_sponsor and where == "promoted")):
                return self.abort404()

        if where in ('upvoted', 'downvoted') and not votes_visible(vuser):
            return self.abort403()

        if ((where in ('saved', 'hidden') or
                (where == 'gilded' and show == 'given')) and
                not (c.user_is_loggedin and c.user._id == vuser._id) and
                not c.user_is_admin):
            return self.abort403()

        if where == 'saved':
            self.show_chooser = True
            category = VSavedCategory('category').run(env.get('category'))
            srname = request.GET.get('sr')
            if srname and c.user.gold:
                try:
                    sr = Subreddit._by_name(srname)
                except NotFound:
                    sr = None
            else:
                sr = None
            if category and not c.user.gold:
                category = None
            self.savedsr = sr
            self.savedcategory = category

        self.vuser = vuser

        c.profilepage = True
        self.suppress_reply_buttons = True

        if vuser.pref_hide_from_robots:
            self.robots = 'noindex,nofollow'

        return ListingController.GET_listing(self, **env)

    @property
    def render_params(self):
        render_params = {'user': self.vuser}

        # event target for screenviews
        event_target = {
            'target_type': 'account',
            'target_fullname': self.vuser._fullname,
            'target_id': self.vuser._id,
            'target_name': self.vuser.name,
            'target_sort': self.sort,
            'target_filter_time': self.time,
        }
        if self.after:
            event_target['target_count'] = self.count
            if self.reverse:
                event_target['target_before'] = self.after._fullname
            else:
                event_target['target_after'] = self.after._fullname
        render_params['extra_js_config'] = {'event_target': event_target}

        return render_params

    @require_oauth2_scope("read")
    @validate(vuser = VExistingUname('username'))
    @api_doc(section=api_section.users, uri='/user/{username}/about')
    def GET_about(self, vuser):
        """Return information about the user, including karma and gold status."""
        if (not is_api() or
                not vuser or
                (vuser._spam and vuser != c.user)):
            return self.abort404()
        return Reddit(content = Wrapped(vuser)).render()

    def GET_saved_redirect(self):
        if not c.user_is_loggedin:
            abort(404)

        dest = "/".join(("/user", c.user.name, "saved"))
        extension = request.environ.get('extension')
        if extension:
            dest = ".".join((dest, extension))
        query_string = request.environ.get('QUERY_STRING')
        if query_string:
            dest += "?" + query_string
        return self.redirect(dest)

    @validate(VUser())
    def GET_rel_user_redirect(self, rest=""):
        url = "/user/%s/%s" % (c.user.name, rest)
        if request.query_string:
            url += "?" + request.query_string
        return self.redirect(url, code=302)

    @validate(
        user=VAccountByName('username'),
    )
    def GET_trophies(self, user):
        """Return a list of trophies for the a given user."""
        if not is_api():
            return self.abort404()
        return self.api_wrapper(get_usertrophies(user))


class MessageController(ListingController):
    show_nums = False
    render_cls = MessagePage
    allow_stylesheets = False
    # note: this intentionally replaces the listing-page class which doesn't
    # conceptually fit for styling these pages.
    extra_page_classes = ['messages-page']

    @property
    def show_sidebar(self):
        if c.default_sr and not isinstance(c.site, (ModSR, MultiReddit)):
            return False

        return self.where in ("moderator", "multi")

    @property
    def menus(self):
        if c.default_sr and self.where in ('inbox', 'messages', 'comments',
                          'selfreply', 'unread', 'mentions'):
            buttons = [NavButton(_("all"), "inbox"),
                       NavButton(_("unread"), "unread"),
                       NavButton(plurals.messages, "messages"),
                       NavButton(_("comment replies"), 'comments'),
                       NavButton(_("post replies"), 'selfreply'),
                       NavButton(_("username mentions"), "mentions"),
            ]

            return [NavMenu(buttons, base_path = '/message/',
                            default = 'inbox', type = "flatlist")]
        elif not c.default_sr or self.where in ('moderator', 'multi'):
            buttons = (NavButton(_("all"), "inbox"),
                       NavButton(_("unread"), "unread"))
            return [NavMenu(buttons, base_path = '/message/moderator/',
                            default = 'inbox', type = "flatlist")]
        return []


    def title(self):
        return _('messages') + ': ' + _(self.where)

    def keep_fn(self):
        def keep(item):
            wouldkeep = True
            # TODO: Consider a flag to disable this (and see above plus builder.py)
            if item._deleted and not c.user_is_admin:
                return False
            if (item._spam and
                    item.author_id != c.user._id and
                    not c.user_is_admin):
                return False

            if self.where == 'unread' or self.subwhere == 'unread':
                # don't show user their own unread stuff
                if item.author_id == c.user._id:
                    wouldkeep = False
                else:
                    wouldkeep = item.new
            elif item.is_mention:
                wouldkeep = (
                    c.user.name.lower() in extract_user_mentions(item.body)
                )

            if c.user_is_admin:
                return wouldkeep

            if (hasattr(item, "subreddit") and
                    item.subreddit.is_moderator(c.user)):
                return wouldkeep

            if item.author_id in c.user.enemies:
                return False

            # do not show messages which were deleted on recipient
            if (isinstance(item, Message) and
                    item.to_id == c.user._id and item.del_on_recipient):
                return False

            if item.author_id == c.user._id:
                return wouldkeep

            return wouldkeep and item.keep_item(item)
        return keep

    @staticmethod
    def builder_wrapper(thing):
        if isinstance(thing, Comment):
            f = thing._fullname
            w = Wrapped(thing)
            w.render_class = Message
            w.to_id = c.user._id
            w.was_comment = True
            w._fullname = f
        else:
            w = ListingController.builder_wrapper(thing)

        return w

    def builder(self):
        if (self.where == 'messages' or
            (self.where in ("moderator", "multi") and self.subwhere != "unread")):
            root = c.user
            message_cls = UserMessageBuilder

            if self.where == "multi":
                root = c.site
                message_cls = MultiredditMessageBuilder
            elif not c.default_sr:
                root = c.site
                message_cls = SrMessageBuilder
            elif self.where == 'moderator' and self.subwhere != 'unread':
                message_cls = ModeratorMessageBuilder
            elif self.message and self.message.sr_id:
                sr = self.message.subreddit_slow
                if sr.is_moderator_with_perms(c.user, 'mail'):
                    # this is a moderator message and the user is a moderator.
                    # use the ModeratorMessageBuilder because not all messages
                    # will be in the user's mailbox
                    message_cls = ModeratorMessageBuilder

            parent = None
            skip = False
            if self.message:
                if self.message.first_message:
                    parent = Message._byID(self.message.first_message,
                                           data=True)
                else:
                    parent = self.message
            elif c.user.pref_threaded_messages:
                skip = (c.render_style == "html")

            if (message_cls is UserMessageBuilder and parent and parent.sr_id
                and not parent.from_sr):
                # Make sure we use the subreddit message builder for modmail,
                # because the per-user cache will be wrong if more than two
                # parties are involved in the thread.
                root = Subreddit._byID(parent.sr_id)
                message_cls = SrMessageBuilder

            enable_threaded = (
                (self.where == "moderator" or
                    parent and parent.sr_id) and
                c.user.pref_threaded_modmail and
                c.render_style == "html"
            )

            return message_cls(
                root,
                wrap=self.builder_wrapper,
                parent=parent,
                skip=skip,
                num=self.num,
                after=self.after,
                keep_fn=self.keep_fn(),
                reverse=self.reverse,
                threaded=enable_threaded,
            )
        return ListingController.builder(self)

    def _verify_inbox_count(self, kept_msgs):
        """If a user has experienced drift in their inbox counts, correct it.

        A small percentage (~0.2%) of users are seeing drift in their inbox
        counts (presumably because _incr is experiencing rare failures). If the
        user has no unread messages in their inbox currently, this will repair
        that drift and log it. Yes, this is a hack.
        """
        if g.disallow_db_writes:
            return

        if not len(kept_msgs) and c.user.inbox_count != 0:
            g.log.info(
                "Fixing inbox drift for %r. Kept msgs: %d. Inbox_count: %d.",
                c.user,
                len(kept_msgs),
                c.user.inbox_count,
            )
            g.stats.simple_event("inbox_counts.drift_fix")
            c.user._incr('inbox_count', -c.user.inbox_count)

    def listing(self):
        if not c.default_sr:
            target = c.site if not isinstance(c.site, FakeSubreddit) else None
            VNotInTimeout().run(action_name="pageview",
                details_text="modmail", target=target)
        if (self.where == 'messages' and
            (c.user.pref_threaded_messages or self.message)):
            return Listing(self.builder_obj).listing()
        pane = ListingController.listing(self)

        # Indicate that the comment tree wasn't built for comments
        for i in pane.things:
            if i.was_comment:
                i.child = None

        if self.where == 'unread':
            self._verify_inbox_count(pane.things)

        return pane

    def query(self):
        if self.where == 'messages':
            q = queries.get_inbox_messages(c.user)
        elif self.where == 'comments':
            q = queries.get_inbox_comments(c.user)
        elif self.where == 'selfreply':
            q = queries.get_inbox_selfreply(c.user)
        elif self.where == 'mentions':
            q = queries.get_inbox_comment_mentions(c.user)
        elif self.where == 'inbox':
            q = queries.get_inbox(c.user)
        elif self.where == 'unread':
            q = queries.get_unread_inbox(c.user)
        elif self.where == 'sent':
            q = queries.get_sent(c.user)
        elif self.where == 'multi' and self.subwhere == 'unread':
            q = queries.get_unread_subreddit_messages_multi(c.site.kept_sr_ids)
        elif self.where == 'moderator' and self.subwhere == 'unread':
            if c.default_sr:
                srids = Subreddit.reverse_moderator_ids(c.user)
                srs = [sr for sr in Subreddit._byID(srids, data=False,
                                                    return_dict=False)
                       if sr.is_moderator_with_perms(c.user, 'mail')]
                q = queries.get_unread_subreddit_messages_multi(srs)
            else:
                q = queries.get_unread_subreddit_messages(c.site)
        elif self.where in ('moderator', 'multi'):
            if c.have_mod_messages and self.mark != 'false':
                c.have_mod_messages = False
                c.user.modmsgtime = False
                c.user._commit()
            # the query is handled by the builder on the moderator page
            return
        else:
            return self.abort404()
        if self.where != 'sent':
            #reset the inbox
            if c.have_messages and c.user.pref_mark_messages_read and self.mark != 'false':
                c.have_messages = False

        return q

    @property
    def render_params(self):
        render_params = {'source': self.source}

        # event target for screenviews
        event_target = {}
        if self.message:
            event_target['target_type'] = 'message'
            event_target['target_fullname'] = self.message._fullname
            event_target['target_id'] = self.message._id
        if self.after:
            event_target['target_count'] = self.count
            if self.reverse:
                event_target['target_before'] = self.after._fullname
            else:
                event_target['target_after'] = self.after._fullname
        render_params['extra_js_config'] = {'event_target': event_target}

        return render_params

    @require_oauth2_scope("privatemessages")
    @validate(VUser(),
              message = VMessageID('mid'),
              mark = VOneOf('mark',('true','false')))
    @listing_api_doc(section=api_section.messages,
                     uri='/message/{where}',
                     uri_variants=['/message/inbox', '/message/unread', '/message/sent'])
    def GET_listing(self, where, mark, message, subwhere = None, **env):
        if not (c.default_sr
                or c.site.is_moderator_with_perms(c.user, 'mail')
                or c.user_is_admin):
            abort(403, "forbidden")
        if isinstance(c.site, MultiReddit):
            if not (c.user_is_admin or c.site.is_moderator(c.user)):
                self.abort403()
            self.where = "multi"
        elif isinstance(c.site, ModSR) or not c.default_sr:
            self.where = "moderator"
        else:
            self.where = where

        # don't allow access to modmail when user is in timeout
        if self.where == "moderator":
            VNotInTimeout().run(action_name="pageview", details_text="modmail",
                target=message)

        self.subwhere = subwhere
        self.message = message
        if mark is not None:
            self.mark = mark
        elif self.message:
            self.mark = "false"
        elif is_api():
            self.mark = 'false'
        elif c.render_style and c.render_style == "xml":
            self.mark = 'false'
        else:
            self.mark = 'true'
        if c.user_is_admin:
            c.referrer_policy = "always"
        if self.where == 'unread':
            self.next_suggestions_cls = UnreadMessagesSuggestions

        if self.message:
            self.source = "permalink"
        elif self.where in {"moderator", "multi"}:
            self.source = "modmail"
        else:
            self.source = "usermail"

        return ListingController.GET_listing(self, **env)

    @validate(
        VUser(),
        to=nop('to'),
        subject=nop('subject'),
        message=nop('message'),
    )
    def GET_compose(self, to, subject, message):
        mod_srs = []
        subreddit_message = False
        only_as_subreddit = False
        self.where = "compose"

        if isinstance(c.site, MultiReddit):
            mod_srs = c.site.srs_with_perms(c.user, "mail")
            if not mod_srs:
                abort(403)
            subreddit_message = True
        elif not isinstance(c.site, FakeSubreddit):
            if not c.site.is_moderator_with_perms(c.user, "mail"):
                abort(403)
            mod_srs = [c.site]
            subreddit_message = True
            only_as_subreddit = True
        elif c.user.is_moderator_somewhere:
            mod_srs = Mod.srs_with_perms(c.user, "mail")
            subreddit_message = bool(mod_srs)

        captcha = Captcha() if c.user.needs_captcha() else None

        if subreddit_message:
            content = ModeratorMessageCompose(
                mod_srs,
                only_as_subreddit=only_as_subreddit,
                to=to,
                subject=subject,
                captcha=captcha,
                message=message,
                restrict_recipient=c.user.in_timeout)
        else:
            content = MessageCompose(
                to=to,
                subject=subject,
                captcha=captcha,
                message=message,
                restrict_recipient=c.user.in_timeout)

        return MessagePage(
            content=content,
            title=self.title(),
            page_classes=self.extra_page_classes + ['compose-page'],
        ).render()


class RedditsController(ListingController):
    render_cls = SubredditsPage
    extra_page_classes = ListingController.extra_page_classes + ['subreddits-page']

    def title(self):
        return _('subreddits')

    def keep_fn(self):
        base_keep_fn = ListingController.keep_fn(self)
        def keep(item):
            if self.where == 'featured':
                if item.type not in ('public', 'restricted'):
                    return False
                if not item.discoverable:
                    return False
            return base_keep_fn(item) and (c.over18 or not item.over_18)
        return keep

    def query(self):
        if self.where == 'banned' and c.user_is_admin:
            reddits = Subreddit._query(Subreddit.c._spam == True,
                                       sort = desc('_date'),
                                       write_cache = True,
                                       read_cache = True,
                                       cache_time = 5 * 60,
                                       stale = True)
        else:
            reddits = None
            if self.where == 'new':
                reddits = Subreddit._query( write_cache = True,
                                            read_cache = True,
                                            cache_time = 5 * 60,
                                            stale = True)
                reddits._sort = desc('_date')
            elif self.where == 'employee':
                if c.user_is_loggedin and c.user.employee:
                    reddits = Subreddit._query(
                        Subreddit.c.type=='employees_only',
                        write_cache=True,
                        read_cache=True,
                        cache_time=5 * 60,
                        stale=True,
                    )
                    reddits._sort = desc('_downs')
                else:
                    abort(404)
            elif self.where == 'quarantine':
                if c.user_is_admin:
                    reddits = Subreddit._query(
                        Subreddit.c.quarantine==True,
                        write_cache=True,
                        read_cache=True,
                        cache_time=5 * 60,
                        stale=True,
                    )
                    reddits._sort = desc('_downs')
                else:
                    abort(404)
            elif self.where == 'gold':
                reddits = Subreddit._query(
                    Subreddit.c.type=='gold_only',
                    write_cache=True,
                    read_cache=True,
                    cache_time=5 * 60,
                    stale=True,
                )
                reddits._sort = desc('_downs')
            elif self.where == 'default':
                return [
                    sr._fullname
                    for sr in Subreddit.default_subreddits(ids=False)
                ]
            elif self.where == 'featured':
                return [
                    sr._fullname
                    for sr in Subreddit.featured_subreddits()
                ]
            else:
                reddits = Subreddit._query( write_cache = True,
                                            read_cache = True,
                                            cache_time = 60 * 60,
                                            stale = True)
                reddits._sort = desc('_downs')

            if g.domain != 'reddit.com':
                # don't try to render /r/promos on opensource installations
                promo_sr_id = Subreddit.get_promote_srid()
                if promo_sr_id:
                    reddits._filter(Subreddit.c._id != promo_sr_id)

        return reddits

    @property
    def render_params(self):
        render_params = {}

        if self.where == 'popular':
            render_params['show_interestbar'] = True

        # event target for screenviews (/subreddits)
        event_target = {
            'target_sort': self.where,
        }
        if self.after:
            event_target['target_count'] = self.count
            if self.reverse:
                event_target['target_before'] = self.after._fullname
            else:
                event_target['target_after'] = self.after._fullname
        render_params['extra_js_config'] = {'event_target': event_target}

        return render_params

    @require_oauth2_scope("read")
    @listing_api_doc(section=api_section.subreddits,
                     uri='/subreddits/{where}',
                     uri_variants=[
                         '/subreddits/popular',
                         '/subreddits/new',
                         '/subreddits/gold',
                         '/subreddits/default',
                     ])
    def GET_listing(self, where, **env):
        """Get all subreddits.

        The `where` parameter chooses the order in which the subreddits are
        displayed.  `popular` sorts on the activity of the subreddit and the
        position of the subreddits can shift around. `new` sorts the subreddits
        based on their creation date, newest first.

        """
        self.where = where
        return ListingController.GET_listing(self, **env)

class MyredditsController(ListingController):
    render_cls = MySubredditsPage
    extra_page_classes = ListingController.extra_page_classes + ['subreddits-page']

    @property
    def menus(self):
        buttons = (NavButton(plurals.subscriber,  'subscriber'),
                    NavButton(getattr(plurals, "approved submitter"), 'contributor'),
                    NavButton(plurals.moderator,   'moderator'))

        return [NavMenu(buttons, base_path = '/subreddits/mine/',
                        default = 'subscriber', type = "flatlist")]

    def title(self):
        return _('subreddits: ') + self.where

    def builder_wrapper(self, thing):
        w = ListingController.builder_wrapper(thing)
        if self.where == 'moderator':
            is_moderator = thing.is_moderator(c.user)
            if is_moderator:
                w.mod_permissions = is_moderator.get_permissions()
        return w

    def query(self):
        if self.where == 'moderator' and not c.user.is_moderator_somewhere:
            return []

        if self.where == "subscriber":
            sr_ids = Subreddit.subscribed_ids_by_user(c.user)
        else:
            q = SRMember._simple_query(
                ["_thing1_id"],
                SRMember.c._name == self.where,
                SRMember.c._thing2_id == c.user._id,
                #hack to prevent the query from
                #adding it's own date
                sort=(desc('_t1_ups'), desc('_t1_date')),
            )
            sr_ids = [row._thing1_id for row in q]

        sr_fullnames = [
            Subreddit._fullname_from_id36(to36(sr_id)) for sr_id in sr_ids]
        return sr_fullnames

    def content(self):
        user = c.user if c.user_is_loggedin else None
        num_subscriptions = len(Subreddit.subscribed_ids_by_user(user))
        if self.where == 'subscriber' and num_subscriptions == 0:
            message = strings.sr_messages['empty']
        else:
            message = strings.sr_messages.get(self.where)

        stack = PaneStack()

        if message:
            stack.append(InfoBar(message=message))

        stack.append(self.listing_obj)

        return stack

    def build_listing(self, after=None, **kwargs):
        if after and not isinstance(after, Subreddit):
            abort(400, 'gimme a subreddit')

        return ListingController.build_listing(self, after=after, **kwargs)

    @property
    def render_params(self):
        render_params = {}

        # event target for screenviews (/subreddits/mine)
        event_target = {
            'target_sort': self.where,
        }
        if self.after:
            event_target['target_count'] = self.count
            if self.reverse:
                event_target['target_before'] = self.after._fullname
            else:
                event_target['target_after'] = self.after._fullname
        render_params['extra_js_config'] = {'event_target': event_target}

        return render_params

    @require_oauth2_scope("mysubreddits")
    @validate(VUser())
    @listing_api_doc(section=api_section.subreddits,
                     uri='/subreddits/mine/{where}',
                     uri_variants=['/subreddits/mine/subscriber', '/subreddits/mine/contributor', '/subreddits/mine/moderator'])
    def GET_listing(self, where='subscriber', **env):
        """Get subreddits the user has a relationship with.

        The `where` parameter chooses which subreddits are returned as follows:

        * `subscriber` - subreddits the user is subscribed to
        * `contributor` - subreddits the user is an approved submitter in
        * `moderator` - subreddits the user is a moderator of

        See also: [/api/subscribe](#POST_api_subscribe),
        [/api/friend](#POST_api_friend), and
        [/api/accept_moderator_invite](#POST_api_accept_moderator_invite).

        """
        self.where = where
        return ListingController.GET_listing(self, **env)


class CommentsController(SubredditListingController):
    title_text = _('comments')

    def keep_fn(self):
        def keep(item):
            if c.user_is_admin:
                return True

            if item._deleted:
                return False

            if isinstance(c.site, FriendsSR):
                if item.author._deleted or item.author._spam:
                    return False

            if c.user_is_loggedin:
                if item.subreddit.is_moderator(c.user):
                    return True

                if item.author_id == c.user._id:
                    return True

            if item._spam:
                return False
            return item.keep_item(item)
        return keep

    def query(self):
        return c.site.get_all_comments()

    @require_oauth2_scope("read")
    def GET_listing(self, **env):
        c.profilepage = True
        self.suppress_reply_buttons = True
        return ListingController.GET_listing(self, **env)


class UserListListingController(ListingController):
    builder_cls = UserListBuilder
    allow_stylesheets = False
    skip = False
    friends_compat = True

    @property
    def infotext(self):
        if self.where == 'friends':
            return strings.friends % Friends.path
        elif self.where == 'blocked':
            return _("To block a user click 'block user'  below a message"
                     " from a user you wish to block from messaging you.")

    @property
    def render_params(self):
        params = {}
        is_wiki_action = self.where in ["wikibanned", "wikicontributors"]
        params["show_wiki_actions"] = is_wiki_action
        return params

    @property
    def render_cls(self):
        if self.where in ["friends", "blocked"]:
            return PrefsPage
        return Reddit

    def moderator_wrap(self, rel, invited=False):
        rel._permission_class = ModeratorPermissionSet
        cls = ModTableItem if not invited else InvitedModTableItem
        return cls(rel, editable=self.editable)

    @property
    def builder_wrapper(self):
        if self.where == 'banned':
            cls = BannedTableItem
        elif self.where == 'muted':
            cls = MutedTableItem
        elif self.where == 'moderators':
            return self.moderator_wrap
        elif self.where == 'wikibanned':
            cls = WikiBannedTableItem
        elif self.where == 'contributors':
            cls = ContributorTableItem
        elif self.where == 'wikicontributors':
            cls = WikiMayContributeTableItem
        elif self.where == 'friends':
            cls = FriendTableItem
        elif self.where == 'blocked':
            cls = EnemyTableItem
        return lambda rel : cls(rel, editable=self.editable)

    def title(self):
        section_title = menu[self.where]

        # We'll probably want to slowly start opting more and more things into
        # having this suffix, to make similar tabs on different subreddits
        # distinct.
        if self.where == 'moderators':
            return '%(section)s - /r/%(subreddit)s' % {
                'section': section_title,
                'subreddit': c.site.name,
            }

        return section_title

    def rel(self):
        if self.where in ['friends', 'blocked']:
            return Friend
        return SRMember

    def name(self):
        return self._names.get(self.where)

    _names = {
              'friends': 'friend',
              'blocked': 'enemy',
              'moderators': 'moderator',
              'contributors': 'contributor',
              'banned': 'banned',
              'muted': 'muted',
              'wikibanned': 'wikibanned',
              'wikicontributors': 'wikicontributor',
             }

    def query(self):
        rel = self.rel()
        if self.where in ["friends", "blocked"]:
            thing1_id = c.user._id
        else:
            thing1_id = c.site._id
        reversed_types = ["friends", "moderators", "blocked"]
        sort = desc if self.where not in reversed_types else asc
        q = rel._query(rel.c._thing1_id == thing1_id,
                       rel.c._name == self.name(),
                       sort=sort('_date'),
                       data=True)
        if self.jump_to_val:
            thing2_id = self.user._id if self.user else None
            q._filter(rel.c._thing2_id == thing2_id)
        return q

    def listing(self):
        listing = self.listing_cls(self.builder_obj,
                                   addable=self.editable,
                                   show_jump_to=self.show_jump_to,
                                   jump_to_value=self.jump_to_val,
                                   show_not_found=self.show_not_found,
                                   nextprev=self.paginated,
                                   has_add_form=self.editable)
        return listing.listing()

    def invited_mod_listing(self):
        query = SRMember._query(SRMember.c._name == 'moderator_invite',
                                SRMember.c._thing1_id == c.site._id,
                                sort=asc('_date'), data=True)
        wrapper = lambda rel: self.moderator_wrap(rel, invited=True)
        b = self.builder_cls(query,
                             keep_fn=self.keep_fn(),
                             wrap=wrapper,
                             skip=False,
                             num=0)
        return InvitedModListing(b, nextprev=False).listing()

    def content(self):
        is_api = c.render_style in extensions.API_TYPES
        if self.where == 'moderators' and self.editable and not is_api:
            # Do not stack the invited mod list in api mode
            # to allow for api compatibility with older api users.
            content = PaneStack()
            content.append(self.listing_obj)
            content.append(self.invited_mod_listing())
        elif self.where == 'friends' and is_api and self.friends_compat:
            content = PaneStack()
            content.append(self.listing_obj)
            empty_builder = IDBuilder([])
            # Append an empty UserList on the api for backwards
            # compatibility with the old blocked list.
            content.append(UserListing(empty_builder, nextprev=False).listing())
        else:
            content = self.listing_obj
        return content

    @require_oauth2_scope("read")
    @validate(VUser())
    @base_listing
    @listing_api_doc(section=api_section.account,
                     uri='/prefs/{where}',
                     uri_variants=['/prefs/friends', '/prefs/blocked',
                         '/api/v1/me/friends', '/api/v1/me/blocked'])
    def GET_user_prefs(self, where, **kw):
        self.where = where

        self.listing_cls = None
        self.editable = True
        self.paginated = False
        self.jump_to_val = None
        self.show_not_found = False
        self.show_jump_to = False
        # The /prefs/friends version of this endpoint used to contain
        # two lists of users: friends AND blocked users. For backwards
        # compatibility with the old JSON structure, an empty list
        # of "blocked" users is sent.
        # The /api/v1/me/friends version of the friends list does not
        # have this requirement, so it will send just the "friends"
        # data structure.
        self.friends_compat = not request.path.startswith('/api/v1/me/')

        if where == 'friends':
            self.listing_cls = FriendListing
        elif where == 'blocked':
            self.listing_cls = EnemyListing
            self.show_not_found = True
        else:
            abort(404)

        kw['num'] = 0
        return self.build_listing(**kw)


    @require_oauth2_scope("read")
    @validate(user=VAccountByName('user'))
    @base_listing
    @listing_api_doc(section=api_section.subreddits,
                     uses_site=True,
                     uri='/about/{where}',
                     uri_variants=['/about/' + where for where in [
                        'banned', 'muted', 'wikibanned', 'contributors',
                        'wikicontributors', 'moderators']])
    def GET_listing(self, where, user=None, **kw):
        if isinstance(c.site, FakeSubreddit):
            return self.abort404()

        self.where = where

        has_mod_access = ((c.user_is_loggedin and
                           c.site.is_moderator_with_perms(c.user, 'access'))
                          or c.user_is_admin)

        if not c.user_is_loggedin and where not in ['contributors', 'moderators']:
            abort(403)

        self.listing_cls = None
        self.editable = not (c.user_is_loggedin and c.user.in_timeout)
        self.paginated = True
        self.jump_to_val = request.GET.get('user')
        self.show_not_found = bool(self.jump_to_val)

        if where == 'contributors':
            # On public reddits, only moderators may see the whitelist.
            if c.site.type == 'public' and not has_mod_access:
                abort(403)
            # Used for subreddits like /r/lounge
            if c.site.hide_subscribers:
                abort(403)
            # used for subreddits that don't allow access to approved submitters
            if c.site.hide_contributors:
                abort(403)
            self.listing_cls = ContributorListing
            self.editable = self.editable and has_mod_access 

        elif where == 'banned':
            if not has_mod_access:
                abort(403)
            VNotInTimeout().run(action_name="pageview",
                details_text="banned", target=c.site)
            self.listing_cls = BannedListing

        elif where == 'muted':
            if not (c.user_is_admin or (has_mod_access and
                    c.site.is_moderator_with_perms(c.user, 'mail'))):
                abort(403)
            VNotInTimeout().run(action_name="pageview",
                details_text="muted", target=c.site)
            self.listing_cls = MutedListing

        elif where == 'wikibanned':
            if not (c.site.is_moderator_with_perms(c.user, 'wiki') or
                    c.user_is_admin):
                abort(403)
            VNotInTimeout().run(action_name="pageview",
                details_text="wikibanned", target=c.site)
            self.listing_cls = WikiBannedListing

        elif where == 'wikicontributors':
            if not (c.site.is_moderator_with_perms(c.user, 'wiki') or
                    c.user_is_admin):
                abort(403)
            VNotInTimeout().run(action_name="pageview",
                details_text="wikicontributors", target=c.site)
            self.listing_cls = WikiMayContributeListing

        elif where == 'moderators':
            self.editable = ((self.editable and
                              c.user_is_loggedin and
                              c.site.is_unlimited_moderator(c.user)) or
                             c.user_is_admin)
            self.listing_cls = ModListing
            self.paginated = False

        if not self.listing_cls:
            abort(404)

        self.user = user
        self.show_jump_to = self.paginated

        if not self.paginated:
            kw['num'] = 0

        return self.build_listing(**kw)


class GildedController(SubredditListingController):
    where = 'gilded'
    title_text = _("gilded")

    @property
    def infotext(self):
        if isinstance(c.site, FakeSubreddit):
            return ''

        seconds = c.site.gilding_server_seconds
        if not seconds:
            return ''

        delta = timedelta(seconds=seconds)
        server_time = precise_format_timedelta(
            delta, threshold=5, locale=c.locale)
        message = _("gildings in this subreddit have paid for %(time)s of "
                    "server time")
        return message % {'time': server_time}

    @property
    def infotext_class(self):
        return "rounded gold-accent"

    def keep_fn(self):
        def keep(item):
            return item.gildings > 0 and not item._deleted and not item._spam
        return keep

    def query(self):
        try:
            return c.site.get_gilded()
        except NotImplementedError:
            abort(404)

    @require_oauth2_scope("read")
    def GET_listing(self, **env):
        c.profilepage = True
        self.suppress_reply_buttons = True
        if not c.site.allow_gilding:
            self.abort404()
        return ListingController.GET_listing(self, **env)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

_reddit_controllers = {}
_plugin_controllers = {}

def get_controller(name):
    name = name.lower() + 'controller'
    if name in _reddit_controllers:
        return _reddit_controllers[name]
    elif name in _plugin_controllers:
        return _plugin_controllers[name]
    else:
        raise KeyError(name)

def add_controller(controller):
    name = controller.__name__.lower()
    assert name not in _plugin_controllers
    _plugin_controllers[name] = controller
    return controller

def load_controllers():
    from listingcontroller import ListingController
    from listingcontroller import HotController
    from listingcontroller import NewController
    from listingcontroller import RisingController
    from listingcontroller import BrowseController
    from listingcontroller import AdsController
    from listingcontroller import UserListListingController
    from listingcontroller import MessageController
    from listingcontroller import RedditsController
    from listingcontroller import ByIDController
    from listingcontroller import RandomrisingController
    from listingcontroller import UserController
    from listingcontroller import CommentsController
    from listingcontroller import GildedController

    from listingcontroller import MyredditsController

    from admin import AdminToolController
    from front import FormsController
    from front import FrontController
    from health import HealthController
    from buttons import ButtonsController
    from captcha import CaptchaController
    from embed import EmbedController
    from error import ErrorController
    from post import PostController
    from toolbar import ToolbarController
    from awards import AwardsController
    from newsletter import NewsletterController
    from googletagmanager import GoogleTagManagerController
    from promotecontroller import PromoteController
    from promotecontroller import SponsorController
    from promotecontroller import PromoteApiController
    from promotecontroller import PromoteListingController
    from promotecontroller import SponsorListingController
    from mediaembed import MediaembedController
    from mediaembed import AdController
    from oembed import OEmbedController
    from policies import PoliciesController
    from web import WebLogController

    from wiki import WikiController
    from wiki import WikiApiController

    from api import ApiController
    from api import ApiminimalController
    from api_docs import ApidocsController
    from apiv1.user import APIv1UserController
    from apiv1.login import APIv1LoginController
    from apiv1.gold import APIv1GoldController
    from apiv1.scopes import APIv1ScopesController
    from multi import MultiApiController
    from oauth2 import OAuth2FrontendController
    from oauth2 import OAuth2AccessController
    from redirect import RedirectController
    from robots import RobotsController
    from ipn import IpnController
    from ipn import StripeController
    from ipn import CoinbaseController
    from ipn import RedditGiftsController
    from mailgun import MailgunWebhookController

    _reddit_controllers.update((name.lower(), obj) for name, obj in locals().iteritems())
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import csv
from collections import defaultdict
import hashlib
import re
import urllib
import urllib2

from r2.controllers.reddit_base import (
    abort_with_error,
    cross_domain,
    generate_modhash,
    is_trusted_origin,
    MinimalController,
    paginated_listing,
    RedditController,
    set_user_cookie,
)

from pylons.i18n import _
from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib.validator import *

from r2.models import *

from r2.lib import amqp
from r2.lib import recommender
from r2.lib import hooks
from r2.lib.ratelimit import SimpleRateLimit

from r2.lib.utils import (
    blockquote_text,
    extract_user_mentions,
    get_title,
    query_string,
    randstr,
    sanitize_url,
    timefromnow,
    timeuntil,
    tup,
)

from r2.lib.pages import (
    BoringPage,
    ClickGadget,
    CssError,
    FormPage,
    Reddit,
    responsive,
    UploadedImage,
    UrlParser,
    WrappedUser,
)
from r2.lib.pages import FlairList, FlairCsv, FlairTemplateEditor, \
    FlairSelector
from r2.lib.pages import PrefApps
from r2.lib.pages import (
    BannedTableItem,
    ContributorTableItem,
    FriendTableItem,
    InvitedModTableItem,
    ModTableItem,
    MutedTableItem,
    ReportForm,
    SubredditReportForm,
    SubredditStylesheet,
    WikiBannedTableItem,
    WikiMayContributeTableItem,
)

from r2.lib.pages.things import (
    default_thing_wrapper,
    hot_links_by_url_listing,
    wrap_links,
)

from r2.lib.menus import CommentSortMenu
from r2.lib.captcha import get_iden
from r2.lib.strings import strings
from r2.lib.template_helpers import format_html, header_url
from r2.lib.filters import _force_unicode, _force_utf8, websafe_json, websafe, spaceCompress
from r2.lib.db import queries
from r2.lib import media
from r2.lib.db import tdb_cassandra
from r2.lib import promote
from r2.lib import tracking, emailer, newsletter
from r2.lib.subreddit_search import search_reddits
from r2.lib.filters import safemarkdown
from r2.lib.media import str_to_image
from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.oauth2 import require_oauth2_scope, allow_oauth2_access
from r2.lib.template_helpers import (
    add_sr,
    get_domain,
    make_url_protocol_relative,
)
from r2.lib.system_messages import (
    notify_user_added,
    send_ban_message,
    send_mod_removal_message,
)
from r2.controllers.ipn import generate_blob, update_blob
from r2.controllers.login import handle_login, handle_register
from r2.lib.lock import TimeoutExpired
from r2.lib.csrf import csrf_exempt
from r2.lib.voting import cast_vote

from r2.models import wiki
from r2.models.ip import set_account_ip
from r2.models.recommend import AccountSRFeedback, FEEDBACK_ACTIONS
from r2.models.rules import SubredditRules
from r2.models.vote import Vote
from r2.lib.merge import ConflictException

from datetime import datetime, timedelta
from urlparse import urlparse


class ApiminimalController(MinimalController):
    """
    Put API calls in here which don't rely on the user being logged in
    """

    # Since this is only a MinimalController, the
    # @allow_oauth2_access decorator has little effect other than
    # (1) to add the endpoint to /dev/api/oauth, and
    # (2) to future-proof in case the function moves elsewhere
    @allow_oauth2_access
    @csrf_exempt
    @validatedForm()
    @api_doc(api_section.captcha)
    def POST_new_captcha(self, form, jquery, *a, **kw):
        """
        Responds with an `iden` of a new CAPTCHA.

        Use this endpoint if a user cannot read a given CAPTCHA,
        and wishes to receive a new CAPTCHA.

        To request the CAPTCHA image for an iden, use
        [/captcha/`iden`](#GET_captcha_{iden}).
        """

        iden = get_iden()
        jquery("body").captcha(iden)
        form._send_data(iden = iden) 


class ApiController(RedditController):
    """
    Controller which deals with almost all AJAX site interaction.  
    """
    @validatedForm()
    def ajax_login_redirect(self, form, jquery, dest):
        form.redirect("/login" + query_string(dict(dest=dest)))

    @require_oauth2_scope("read")
    @validate(
        things=VByName('id', multiple=True, ignore_missing=True, limit=100),
        url=VUrl('url'),
    )
    @api_doc(api_section.links_and_comments, uses_site=True)
    def GET_info(self, things, url):
        """
        Return a listing of things specified by their fullnames.

        Only Links, Comments, and Subreddits are allowed.

        """

        if url:
            return self.GET_url_info()

        thing_classes = (Link, Comment, Subreddit)
        things = things or []
        things = filter(lambda thing: isinstance(thing, thing_classes), things)

        c.update_last_visit = False
        listing = wrap_links(things)
        return BoringPage(_("API"), content=listing).render()

    @require_oauth2_scope("read")
    @validate(
        url=VUrl('url'),
        count=VLimit('limit'),
        things=VByName('id', multiple=True, limit=100),
    )
    def GET_url_info(self, url, count, things):
        """
        Return a list of links with the given URL.

        If a subreddit is provided, only links in that subreddit will be
        returned.

        """

        if things and not url:
            return self.GET_info()

        c.update_last_visit = False

        if url:
            listing = hot_links_by_url_listing(url, sr=c.site, num=count)
        else:
            listing = None
        return BoringPage(_("API"), content=listing).render()

    @json_validate()
    def GET_me(self, responder):
        """Get info about the currently authenticated user.

        Response includes a modhash, karma, and new mail status.

        """
        if c.user_is_loggedin:
            user_data = Wrapped(c.user).render()
            user_data['data'].update({'features': feature.all_enabled(c.user)})
            return user_data
        else:
            return {'data': {'features': feature.all_enabled(None)}}

    @json_validate(user=VUname(("user",)))
    @api_doc(api_section.users)
    def GET_username_available(self, responder, user):
        """
        Check whether a username is available for registration.
        """
        if not (responder.has_errors("user", errors.BAD_USERNAME)):
            return bool(user)

    @csrf_exempt
    @json_validate(user=VUname(("user",)))
    def POST_check_username(self, responder, user):
        """
        Check whether a username is valid.
        """

        if not (responder.has_errors("user",
                    errors.USERNAME_TOO_SHORT,
                    errors.USERNAME_INVALID_CHARACTERS,
                    errors.USERNAME_TAKEN_DEL,
                    errors.USERNAME_TAKEN)):
            # Pylons does not handle 204s correctly.
            return {}

    @csrf_exempt
    @json_validate(password=VPassword(("passwd")))
    def POST_check_password(self, responder, password):
        """
        Check whether a password is valid.
        """
    
        if not (responder.has_errors("passwd", errors.SHORT_PASSWORD) or
                responder.has_errors("passwd", errors.BAD_PASSWORD)):
            # Pylons does not handle 204s correctly.
            return {}

    @csrf_exempt
    @json_validate(email=ValidEmail("email"),
                   newsletter_subscribe=VBoolean("newsletter_subscribe", default=False),
                   sponsor=VBoolean("sponsor", default=False))
    def POST_check_email(self, responder, email, newsletter_subscribe, sponsor):
        """
        Check whether an email is valid. Allows blank emails.

        Additionally checks if a newsletter is requested, and will be strict
        on blank emails if so.
        """
        if newsletter_subscribe and not email:
            c.errors.add(errors.NEWSLETTER_NO_EMAIL, field="email")
            responder.has_errors("email", errors.NEWSLETTER_NO_EMAIL)
            return

        if sponsor and not email:
            c.errors.add(errors.SPONSOR_NO_EMAIL, field="email")
            responder.has_errors("email", errors.SPONSOR_NO_EMAIL)
            return

        if not (responder.has_errors("email", errors.BAD_EMAIL)):
            # Pylons does not handle 204s correctly.
            return {}

    @cross_domain(allow_credentials=True)
    @json_validate(
        VModhashIfLoggedIn(),
        VRatelimit(rate_ip=True, prefix="rate_newsletter_"),
        email=ValidEmail("email"),
        source=VOneOf('source', ['newsletterbar', 'standalone'])
    )
    def POST_newsletter(self, responder, email, source):
        """Add an email to our newsletter."""

        VRatelimit.ratelimit(rate_ip=True,
                             prefix="rate_newsletter_")

        try:
            newsletter.add_subscriber(email, source=source)
        except newsletter.EmailUnacceptableError as e:
            c.errors.add(errors.NEWSLETTER_EMAIL_UNACCEPTABLE, field="email")
            responder.has_errors("email", errors.NEWSLETTER_EMAIL_UNACCEPTABLE)
            return
        except newsletter.NewsletterError as e:
            g.log.warning("Failed to subscribe: %r" % e)
            abort(500)

    @allow_oauth2_access
    @json_validate()
    @api_doc(api_section.captcha)
    def GET_needs_captcha(self, responder):
        """
        Check whether CAPTCHAs are needed for API methods that define the
        "captcha" and "iden" parameters.
        """
        return bool(c.user.needs_captcha())

    @require_oauth2_scope("privatemessages")
    @validatedForm(
        VCaptcha(),
        VUser(),
        VModhash(),
        from_sr=VSRByName('from_sr', required=False),
        to=VMessageRecipient('to'),
        subject=VLength('subject', 100, empty_error=errors.NO_SUBJECT),
        body=VMarkdownLength(['text', 'message'], max_length=10000),
    )
    @api_doc(api_section.messages)
    def POST_compose(self, form, jquery, from_sr, to, subject, body):
        """
        Handles message composition under /message/compose.
        """
        if (form.has_errors("to",
                    errors.USER_DOESNT_EXIST, errors.NO_USER,
                    errors.SUBREDDIT_NOEXIST, errors.USER_BLOCKED,
                ) or
                form.has_errors("subject", errors.NO_SUBJECT) or
                form.has_errors("subject", errors.TOO_LONG) or
                form.has_errors("text", errors.NO_TEXT, errors.TOO_LONG) or
                form.has_errors("message", errors.TOO_LONG) or
                form.has_errors("captcha", errors.BAD_CAPTCHA) or
                form.has_errors("from_sr", errors.SUBREDDIT_NOEXIST)):
            return

        if form.has_errors("to", errors.USER_MUTED):
            g.events.muted_forbidden_event("muted", target=to,
                request=request, context=c)
            form.set_inputs(to="", subject="", text="", captcha="")
            return

        if from_sr and isinstance(to, Subreddit):
            c.errors.add(errors.NO_SR_TO_SR_MESSAGE, field="from")
            form.has_errors("from", errors.NO_SR_TO_SR_MESSAGE)
            return

        if from_sr and BlockedSubredditsByAccount.is_blocked(to, from_sr):
            c.errors.add(errors.USER_BLOCKED_MESSAGE, field="to")
            form.has_errors("to", errors.USER_BLOCKED_MESSAGE)
            return

        if from_sr and from_sr._spam:
            return

        if from_sr:
            if not from_sr.is_moderator_with_perms(c.user, "mail"):
                abort(403)
            elif from_sr.is_muted(to) and not c.user_is_admin:
                c.errors.add(errors.MUTED_FROM_SUBREDDIT, field="to")
                form.has_errors("to", errors.MUTED_FROM_SUBREDDIT)
                g.events.muted_forbidden_event("muted mod", subreddit=from_sr,
                    target=to, request=request, context=c)
                form.set_inputs(to="", subject="", text="", captcha="")
                return

            # Don't allow mods in timeout to send a message
            VNotInTimeout().run(target=to, subreddit=from_sr)
            m, inbox_rel = Message._new(c.user, to, subject, body, request.ip,
                                        sr=from_sr, from_sr=True)
        else:
            # Only let users in timeout message the admins
            if (to and not (isinstance(to, Subreddit) and
                    '/r/%s' % to.name == g.admin_message_acct)):
                VNotInTimeout().run(target=to)
            m, inbox_rel = Message._new(c.user, to, subject, body, request.ip)

        form.set_text(".status", _("your message has been delivered"))
        form.set_inputs(to = "", subject = "", text = "", captcha="")
        queries.new_message(m, inbox_rel)

    @require_oauth2_scope("submit")
    @json_validate()
    @api_doc(api_section.subreddits, uses_site=True)
    def GET_submit_text(self, responder):
        """Get the submission text for the subreddit.

        This text is set by the subreddit moderators and intended to be
        displayed on the submission form.

        See also: [/api/site_admin](#POST_api_site_admin).

        """
        if c.site.over_18 and not c.over18:
            submit_text = None
            submit_text_html = None
        else:
            submit_text = c.site.submit_text
            submit_text_html = safemarkdown(c.site.submit_text)
        return {'submit_text': submit_text,
                'submit_text_html': submit_text_html}

    @require_oauth2_scope("submit")
    @validatedForm(
        VUser(),
        VModhash(),
        VCaptcha(),
        VRatelimit(rate_user=True, rate_ip=True, prefix="rate_submit_"),
        VShamedDomain('url'),
        sr=VSubmitSR('sr', 'kind'),
        url=VUrl('url'),
        title=VTitle('title'),
        sendreplies=VBoolean('sendreplies'),
        selftext=VMarkdown('text'),
        kind=VOneOf('kind', ['link', 'self']),
        extension=VLength("extension", 20,
                          docs={"extension": "extension used for redirects"}),
        resubmit=VBoolean('resubmit'),
    )
    @api_doc(api_section.links_and_comments)
    def POST_submit(self, form, jquery, url, selftext, kind, title,
                    sr, extension, sendreplies, resubmit):
        """Submit a link to a subreddit.

        Submit will create a link or self-post in the subreddit `sr` with the
        title `title`. If `kind` is `"link"`, then `url` is expected to be a
        valid URL to link to. Otherwise, `text`, if present, will be the
        body of the self-post.

        If a link with the same URL has already been submitted to the specified
        subreddit an error will be returned unless `resubmit` is true.
        `extension` is used for determining which view-type (e.g. `json`,
        `compact` etc.) to use for the redirect that is generated if the
        `resubmit` error occurs.

        """

        from r2.models.admintools import is_banned_domain

        if url:
            if url.lower() == 'self':
                url = kind = 'self'

            # VUrl may have replaced 'url' by adding 'http://'
            form.set_inputs(url=url)

        is_self = (kind == "self")

        if not kind or form.has_errors('sr', errors.INVALID_OPTION):
            return

        if form.has_errors('captcha', errors.BAD_CAPTCHA):
            return

        if form.has_errors('sr',
                errors.SUBREDDIT_NOEXIST,
                errors.SUBREDDIT_NOTALLOWED,
                errors.SUBREDDIT_REQUIRED,
                errors.INVALID_OPTION,
                errors.NO_SELFS,
                errors.NO_LINKS,
                errors.IN_TIMEOUT,
        ):
            return

        if not sr.can_submit_text(c.user) and is_self:
            # this could happen if they actually typed "self" into the
            # URL box and we helpfully translated it for them
            c.errors.add(errors.NO_SELFS, field='sr')
            form.has_errors('sr', errors.NO_SELFS)
            return

        if form.has_errors("title", errors.NO_TEXT, errors.TOO_LONG):
            return

        if not sr.should_ratelimit(c.user, 'link'):
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))
        else:
            if form.has_errors('ratelimit', errors.RATELIMIT):
                return

        if not is_self:
            if form.has_errors("url", errors.NO_URL, errors.BAD_URL):
                return

            if form.has_errors("url", errors.DOMAIN_BANNED):
                g.stats.simple_event('spam.shame.link')
                return

            if not resubmit:
                listing = hot_links_by_url_listing(url, sr=sr, num=1)
                links = listing.things
                if links:
                    c.errors.add(errors.ALREADY_SUB, field='url')
                    form.has_errors('url', errors.ALREADY_SUB)
                    u = links[0].already_submitted_link(url, title)
                    if extension:
                        u = UrlParser(u)
                        u.set_extension(extension)
                        u = u.unparse()
                    form.redirect(u)
                    return

        if not c.user_is_admin and is_self:
            if len(selftext) > Link.SELFTEXT_MAX_LENGTH:
                c.errors.add(errors.TOO_LONG, field='text',
                    msg_params={'max_length': Link.SELFTEXT_MAX_LENGTH})
                form.set_error(errors.TOO_LONG, 'text')
                return

        VNotInTimeout().run(action_name="submit", details_text=kind, target=sr)

        if not request.POST.get('sendreplies'):
            sendreplies = is_self

        # get rid of extraneous whitespace in the title
        cleaned_title = re.sub(r'\s+', ' ', title, flags=re.UNICODE)
        cleaned_title = cleaned_title.strip()

        l = Link._submit(
            is_self=is_self,
            title=cleaned_title,
            content=selftext if is_self else url,
            author=c.user,
            sr=sr,
            ip=request.ip,
            sendreplies=sendreplies,
        )

        if not is_self:
            ban = is_banned_domain(url)
            if ban:
                g.stats.simple_event('spam.domainban.link_url')
                admintools.spam(l, banner = "domain (%s)" % ban.banmsg)
                hooks.get_hook('banned_domain.submit').call(item=l, url=url,
                                                            ban=ban)

        if sr.should_ratelimit(c.user, 'link'):
            VRatelimit.ratelimit(rate_user=True, rate_ip = True,
                                 prefix = "rate_submit_")

        queries.new_link(l)
        l.update_search_index()
        g.events.submit_event(l, request=request, context=c)

        path = add_sr(l.make_permalink_slow())
        if extension:
            path += ".%s" % extension

        form.redirect(path)
        form._send_data(url=path)
        form._send_data(id=l._id36)
        form._send_data(name=l._fullname)

    @csrf_exempt
    @validatedForm(VRatelimit(rate_ip = True,
                              rate_user = True,
                              prefix = 'fetchtitle_'),
                   VUser(),
                   url = VSanitizedUrl('url'))
    def POST_fetch_title(self, form, jquery, url):
        if form.has_errors('ratelimit', errors.RATELIMIT):
            form.set_text(".title-status", "")
            return

        VRatelimit.ratelimit(rate_ip = True, rate_user = True,
                             prefix = 'fetchtitle_', seconds=1)
        if url:
            title = get_title(url)
            if title:
                form.set_inputs(title = title)
                form.set_text(".title-status", "")
            else:
                form.set_text(".title-status", _("no title found"))
            form._send_data(title=title)
        
    def _login(self, responder, user, rem = None):
        """
        AJAX login handler, used by both login and register to set the
        user cookie and send back a redirect.
        """
        c.user = user
        c.user_is_loggedin = True
        self.login(user, rem = rem)

        if request.params.get("hoist") != "cookie":
            responder._send_data(modhash=generate_modhash())
            responder._send_data(cookie  = user.make_cookie())
        responder._send_data(need_https=feature.is_enabled("force_https"))

    @csrf_exempt
    @cross_domain(allow_credentials=True)
    @validatedForm(
        VLoggedOut(),
        user=VThrottledLogin(['user', 'passwd']),
        rem=VBoolean('rem'),
    )
    def POST_login(self, form, responder, user, rem=None, **kwargs):
        """Log into an account.

        `rem` specifies whether or not the session cookie returned should last
        beyond the current browser session (that is, if `rem` is `True` the
        cookie will have an explicit expiration far in the future indicating
        that it is not a session cookie).

        """
        kwargs.update(dict(
            controller=self,
            form=form,
            responder=responder,
            user=user,
            rem=rem,
        ))
        return handle_login(**kwargs)

    @csrf_exempt
    @cross_domain(allow_credentials=True)
    @validatedForm(
        VRatelimit(rate_ip=True, prefix="rate_register_"),
        name=VUname(['user']),
        email=ValidEmail("email"),
        password=VPasswordChange(['passwd', 'passwd2']),
        rem=VBoolean('rem'),
        newsletter_subscribe=VBoolean('newsletter_subscribe', default=False),
        sponsor=VBoolean('sponsor', default=False),
    )
    def POST_register(self, form, responder, name, email, password, **kwargs):
        """Create a new account.

        `rem` specifies whether or not the session cookie returned should last
        beyond the current browser session (that is, if `rem` is `True` the
        cookie will have an explicit expiration far in the future indicating
        that it is not a session cookie).

        """
        kwargs.update(dict(
            controller=self,
            form=form,
            responder=responder,
            name=name,
            email=email,
            password=password,
        ))
        return handle_register(**kwargs)

    @require_oauth2_scope("modself")
    @noresponse(VUser(),
                VModhash(),
                container = VByName('id'))
    @api_doc(api_section.moderation)
    def POST_leavemoderator(self, container):
        """Abdicate moderator status in a subreddit.

        See also: [/api/friend](#POST_api_friend).

        """
        if container and container.is_moderator(c.user):
            container.remove_moderator(c.user)
            ModAction.create(container, c.user, 'removemoderator', target=c.user, 
                             details='remove_self')

    @require_oauth2_scope("modself")
    @noresponse(VUser(),
                VModhash(),
                container = VByName('id'))
    @api_doc(api_section.moderation)
    def POST_leavecontributor(self, container):
        """Abdicate approved submitter status in a subreddit.

        See also: [/api/friend](#POST_api_friend).

        """
        if container and container.is_contributor(c.user):
            container.remove_contributor(c.user)


    _sr_friend_types = (
        'moderator',
        'moderator_invite',
        'contributor',
        'banned',
        'muted',
        'wikibanned',
        'wikicontributor',
    )

    _sr_friend_types_with_permissions = (
        'moderator',
        'moderator_invite',
    )

    # Changes to this dict should also update docstrings for
    # POST_friend and POST_unfriend
    api_friend_scope_map = {
        'moderator': {"modothers"},
        'moderator_invite': {"modothers"},
        'contributor': {"modcontributors"},
        'banned': {"modcontributors"},
        'muted': {"modcontributors"},
        'wikibanned': {"modcontributors", "modwiki"},
        'wikicontributor': {"modcontributors", "modwiki"},
        'friend': None,  # Handled with API v1 endpoint
        'enemy': {"privatemessages"},  # Only valid for POST_unfriend
    }

    def check_api_friend_oauth_scope(self, type_):
        if c.oauth_user:
            needed_scopes = self.api_friend_scope_map[type_]
            if needed_scopes is None:
                # OAuth2 access not allowed for this friend rel type
                # via /api/friend
                self._auth_error(400, "invalid_request")
            if not c.oauth_scope.has_access(c.site.name, needed_scopes):
                # Token does not have the necessary scope to complete
                # this request.
                self._auth_error(403, "insufficient_scope")

    @allow_oauth2_access
    @noresponse(VUser(),
                VModhash(),
                nuser = VExistingUname('name'),
                iuser = VByName('id'),
                container = nop('container'),
                type = VOneOf('type', ('friend', 'enemy') +
                                      _sr_friend_types))
    @api_doc(api_section.users, uses_site=True)
    def POST_unfriend(self, nuser, iuser, container, type):
        """Remove a relationship between a user and another user or subreddit

        The user can either be passed in by name (nuser)
        or by [fullname](#fullnames) (iuser).  If type is friend or enemy,
        'container' MUST be the current user's fullname;
        for other types, the subreddit must be set
        via URL (e.g., /r/funny/api/unfriend)

        OAuth2 use requires appropriate scope based
        on the 'type' of the relationship:

        * moderator: `modothers`
        * moderator_invite: `modothers`
        * contributor: `modcontributors`
        * banned: `modcontributors`
        * muted: `modcontributors`
        * wikibanned: `modcontributors` and `modwiki`
        * wikicontributor: `modcontributors` and `modwiki`
        * friend: Use [/api/v1/me/friends/{username}](#DELETE_api_v1_me_friends_{username})
        * enemy: `privatemessages`

        Complement to [POST_friend](#POST_api_friend)

        """
        self.check_api_friend_oauth_scope(type)

        victim = iuser or nuser

        if not victim:
            abort(400, 'No user specified')
        
        if type in self._sr_friend_types:
            mod_action_by_type = dict(
                banned='unbanuser',
                moderator='removemoderator',
                moderator_invite='uninvitemoderator',
                wikicontributor='removewikicontributor',
                wikibanned='wikiunbanned',
                contributor='removecontributor',
                muted='unmuteuser',
            )
            action = mod_action_by_type.get(type, type)

            if isinstance(c.site, FakeSubreddit):
                abort(403, 'forbidden')
            container = c.site
            if not (c.user == victim and type == 'moderator'):
                # The requesting user is marked as spam or banned, and is
                # trying to do a mod action. The only action they should be
                # allowed to do and have it stick is demodding themself.
                if c.user._spam:
                    return
                VNotInTimeout().run(action_name=action, target=victim)
        else:
            container = VByName('container').run(container)
            if not container:
                return

        # The user who made the request must be an admin or a moderator
        # for the privilege change to succeed.
        # (Exception: a user can remove privilege from oneself)
        required_perms = []
        if c.user != victim:
            if type.startswith('wiki'):
                required_perms.append('wiki')
            else:
                required_perms.append('access')
                # ability to unmute requires access and mail permissions
                if type == 'muted':
                    required_perms.append('mail')

        if (
            not c.user_is_admin and
            type in self._sr_friend_types and
            not container.is_moderator_with_perms(c.user, *required_perms)
        ):
            abort(403, 'forbidden')
        if (
            type == "moderator" and
            not c.user_is_admin and
            not container.can_demod(c.user, victim)
        ):
            abort(403, 'forbidden')

        # if we are (strictly) unfriending, the container had better
        # be the current user.
        if type in ("friend", "enemy") and container != c.user:
            abort(403, 'forbidden')

        fn = getattr(container, 'remove_' + type)
        new = fn(victim)

        # for mod removals, let the now ex-mod know (NOTE: doing this earlier
        # will make the message show up in their mod inbox, which they will
        # immediately lose access to.)
        if new and type == 'moderator' and victim != c.user:
            send_mod_removal_message(container, c.user, victim)

        # Log this action
        if new and type in self._sr_friend_types:
            ModAction.create(container, c.user, action, target=victim)

        if type == "friend" and c.user.gold:
            c.user.friend_rels_cache(_update=True)

        if type in ('banned', 'wikibanned'):
            container.unschedule_unban(victim, type)

        if type == 'muted':
            MutedAccountsBySubreddit.unmute(container, victim)

    @require_oauth2_scope("modothers")
    @validatedForm(VSrModerator(), VModhash(),
                   target=VExistingUname('name'),
                   type_and_permissions=VPermissions('type', 'permissions'))
    @api_doc(api_section.users, uses_site=True)
    def POST_setpermissions(self, form, jquery, target, type_and_permissions):
        if form.has_errors('name', errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        if form.has_errors('type', errors.INVALID_PERMISSION_TYPE):
            return
        if form.has_errors('permissions', errors.INVALID_PERMISSIONS):
            return

        if c.user._spam:
            return

        type, permissions = type_and_permissions
        update = None

        if type in ("moderator", "moderator_invite"):
            if not c.user_is_admin:
                if type == "moderator" and (
                    c.user == target or not c.site.can_demod(c.user, target)):
                    abort(403, 'forbidden')
                if (type == "moderator_invite"
                    and not c.site.is_unlimited_moderator(c.user)):
                    abort(403, 'forbidden')
                # Don't allow mods in timeout to set permissions
                VNotInTimeout().run(action_name="editsettings",
                    details_text="set_permissions", target=target)
            if type == "moderator":
                rel = c.site.get_moderator(target)
            if type == "moderator_invite":
                rel = c.site.get_moderator_invite(target)
            rel.set_permissions(permissions)
            rel._commit()
            update = rel.encoded_permissions
            ModAction.create(c.site, c.user, action='setpermissions',
                             target=target, details='permission_' + type,
                             description=update)

        if update:
            row = form.closest('tr')
            editor = row.find('.permissions').data('PermissionEditor')
            editor.onCommit(update)

    @allow_oauth2_access
    @validatedForm(
        VUser(),
        VModhash(),
        friend=VExistingUname('name'),
        container=nop('container'),
        type=VOneOf('type', ('friend',) + _sr_friend_types),
        type_and_permissions=VPermissions('type', 'permissions'),
        note=VLength('note', 300),
        ban_reason=VLength('ban_reason', 100),
        duration=VInt('duration', min=1, max=999),
        ban_message=VMarkdownLength('ban_message', max_length=1000,
            empty_error=None),
    )
    @api_doc(api_section.users, uses_site=True)
    def POST_friend(self, form, jquery, friend,
            container, type, type_and_permissions, note, ban_reason,
            duration, ban_message):
        """Create a relationship between a user and another user or subreddit

        OAuth2 use requires appropriate scope based
        on the 'type' of the relationship:

        * moderator: Use "moderator_invite"
        * moderator_invite: `modothers`
        * contributor: `modcontributors`
        * banned: `modcontributors`
        * muted: `modcontributors`
        * wikibanned: `modcontributors` and `modwiki`
        * wikicontributor: `modcontributors` and `modwiki`
        * friend: Use [/api/v1/me/friends/{username}](#PUT_api_v1_me_friends_{username})
        * enemy: Use [/api/block](#POST_api_block)

        Complement to [POST_unfriend](#POST_api_unfriend)

        """
        self.check_api_friend_oauth_scope(type)

        if type in self._sr_friend_types:
            if isinstance(c.site, FakeSubreddit):
                abort(403, 'forbidden')
            container = c.site
        else:
            container = VByName('container').run(container)
            if not container:
                return

        if type == "moderator" and not c.user_is_admin:
            # attempts to add moderators now create moderator invites.
            type = "moderator_invite"

        fn = getattr(container, 'add_' + type)

        # Make sure the user making the request has the correct permissions
        # to be able to make this status change
        if type in self._sr_friend_types:
            mod_action_by_type = {
                "banned": "banuser",
                "muted": "muteuser",
                "contributor": "addcontributor",
                "moderator": "addmoderator",
                "moderator_invite": "invitemoderator",
            }
            action = mod_action_by_type.get(type, type)

            if c.user_is_admin:
                has_perms = True
            elif type.startswith('wiki'):
                has_perms = container.is_moderator_with_perms(c.user, 'wiki')
            elif type == 'moderator_invite':
                has_perms = container.is_unlimited_moderator(c.user)
            else:
                has_perms = container.is_moderator_with_perms(c.user, 'access')

            if not has_perms:
                abort(403, 'forbidden')

            # Don't let banned users make subreddit access changes
            if c.user._spam:
                return
            VNotInTimeout().run(action_name=action, target=friend)

        if type == 'moderator_invite':
            invites = sum(1 for i in container.each_moderator_invite())
            if invites >= g.sr_invite_limit:
                c.errors.add(errors.SUBREDDIT_RATELIMIT, field="name")
                form.set_error(errors.SUBREDDIT_RATELIMIT, "name")
                return

        if (type in self._sr_friend_types and
                not c.user_is_admin and
                container.use_quotas):
            sr_ratelimit = SimpleRateLimit(
                name="sr_%s_%s" % (str(type), container._id36),
                seconds=g.sr_quota_time,
                limit=getattr(g, "sr_%s_quota" % type),
            )
            if not sr_ratelimit.record_and_check():
                form.set_text(".status", errors.SUBREDDIT_RATELIMIT)
                c.errors.add(errors.SUBREDDIT_RATELIMIT)
                form.set_error(errors.SUBREDDIT_RATELIMIT, None)
                return

        # if we are (strictly) friending, the container
        # had better be the current user.
        if type == "friend" and container != c.user:
            abort(403,'forbidden')

        elif form.has_errors("name", errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        elif form.has_errors(("note", "ban_reason"), errors.TOO_LONG):
            return

        if type == "banned":
            if form.has_errors("ban_message", errors.TOO_LONG):
                return
            if ban_reason and note:
                note = "%s: %s" % (ban_reason, note)
            elif ban_reason:
                note = ban_reason

        if type in self._sr_friend_types_with_permissions:
            if form.has_errors('type', errors.INVALID_PERMISSION_TYPE):
                return
            if form.has_errors('permissions', errors.INVALID_PERMISSIONS):
                return
        else:
            permissions = None

        if type == "moderator_invite" and container.is_moderator(friend):
            c.errors.add(errors.ALREADY_MODERATOR, field="name")
            form.set_error(errors.ALREADY_MODERATOR, "name")
            return
        elif type in ("banned", "muted") and container.is_moderator(friend):
            c.errors.add(errors.CANT_RESTRICT_MODERATOR, field="name")
            form.set_error(errors.CANT_RESTRICT_MODERATOR, "name")
            return

        if type == "muted" and not container.can_mute(c.user, friend):
            abort(403)

        # don't allow increasing privileges of banned or muted users
        unbanned_types = ("moderator", "moderator_invite",
                          "contributor", "wikicontributor")
        if type in unbanned_types:
            if container.is_banned(friend):
                c.errors.add(errors.BANNED_FROM_SUBREDDIT, field="name")
                form.set_error(errors.BANNED_FROM_SUBREDDIT, "name")
                return
            elif container.is_muted(friend):
                c.errors.add(errors.MUTED_FROM_SUBREDDIT, field="name")
                form.set_error(errors.MUTED_FROM_SUBREDDIT, "name")
                return

        if type == "moderator":
            container.remove_moderator_invite(friend)

        new = fn(friend, permissions=type_and_permissions[1])

        if type == "friend" and c.user.gold:
            # Yes, the order of the next two lines is correct.
            # First you recalculate the rel_ids, then you find
            # the right one and update its data.
            c.user.friend_rels_cache(_update=True)
            c.user.add_friend_note(friend, note or '')

        # additional logging/info needed for bans
        tempinfo = None
        log_details = None
        log_description = None

        if type in ('banned', 'wikibanned', 'muted'):
            container.add_rel_note(type, friend, note)
            log_description = note

        if type in ('banned', 'wikibanned'):
            existing_ban = None
            if not new:
                existing_ban = container.get_tempbans(type, friend.name)
            if duration:
                ban_buffer = timedelta(hours=6)
                # Temp ban that doesn't have a preceding temp ban that ends
                # ends within ban_buffer of this new duration. this is just a
                # small buffer to prevent repetitive ban messages sent to users
                # when the mod wants to update a note but not the duration
                if existing_ban:
                    now = datetime.now(g.tz)
                    ban_remaining = existing_ban[friend.name] - now
                    timediff = abs(timedelta(days=duration) - ban_remaining)
                if not existing_ban or timediff >= ban_buffer:
                    container.unschedule_unban(friend, type)
                    tempinfo = container.schedule_unban(
                        type,
                        friend,
                        c.user,
                        duration,
                    )
                    log_details = "changed to " if not new else ""
                    log_details += "%d days" % duration
            elif not new and existing_ban:
                # Preexisting temp ban and no duration specified means turn
                # the temporary ban into a permanent one.
                container.unschedule_unban(friend, type)
                log_details = "changed to permanent"
            elif new:
                # New ban without a duration is permanent
                log_details = "permanent"
        elif new and type == 'muted':
            MutedAccountsBySubreddit.mute(container, friend, c.user)

        # Log this action
        if (new or log_details) and type in self._sr_friend_types:
            mod_action_by_type = {
                "banned": "banuser",
                "muted": "muteuser",
                "contributor": "addcontributor",
                "moderator": "addmoderator",
                "moderator_invite": "invitemoderator",
            }
            action = mod_action_by_type.get(type, type)

            ModAction.create(
                container,
                c.user,
                action,
                target=friend,
                details=log_details,
                description=log_description,
            )

        row_cls = dict(friend=FriendTableItem,
                       moderator=ModTableItem,
                       moderator_invite=InvitedModTableItem,
                       contributor=ContributorTableItem,
                       wikicontributor=WikiMayContributeTableItem,
                       banned=BannedTableItem,
                       muted=MutedTableItem,
                       wikibanned=WikiBannedTableItem).get(type)

        form.set_inputs(name = "")
        if note:
            form.set_inputs(note = "")
        form.removeClass("edited")

        if new and row_cls:
            new._thing2 = friend
            user_row = row_cls(new)
            if tempinfo:
                BannedListing.populate_from_tempbans(user_row, tempinfo)
            form.set_text(".status:first", user_row.executed_message)
            rev_types = ["moderator", "moderator_invite", "friend"]
            index = 0 if user_row.type not in rev_types else -1
            table = jquery("." + type + "-table").show().find("table")
            table.insert_table_rows(user_row, index=index)
            table.find(".notfound").hide()

        if type == "banned":
            # If the ban is new or has had the duration changed,
            # send a ban message
            if (friend.has_interacted_with(container) and 
                    (new or log_details)):
                send_ban_message(container, c.user, friend,
                    ban_message, duration, new)
        elif new:
            notify_user_added(type, c.user, friend, container)

    @validatedForm(VGold(),
                   VModhash(),
                   friend = VExistingUname('name'),
                   note = VLength('note', 300))
    def POST_friendnote(self, form, jquery, friend, note):
        if form.has_errors("note", errors.TOO_LONG):
            return
        c.user.add_friend_note(friend, note)
        form.set_text('.status', _("saved"))

    @validatedForm(
        VModhash(),
        type=VOneOf('type', ('bannednote', 'wikibannednote', 'mutednote')),
        user=VExistingUname('name'),
        note=VLength('note', 300),
    )
    def POST_relnote(self, form, jquery, type, user, note):
        perm = 'wiki' if type.startswith('wiki') else 'access'
        if (not c.user_is_admin
            and (not c.site.is_moderator_with_perms(c.user, perm))):
            if c.user._spam:
                return
            else:
                abort(403, 'forbidden')

        # Don't allow users in timeout to add relnote
        VNotInTimeout().run(action_name="editrelnote", details_text=type,
            target=user)

        if form.has_errors("note", errors.TOO_LONG):
            # NOTE: there's no error displayed in the form
            return
        c.site.add_rel_note(type[:-4], user, note)

    @require_oauth2_scope("modself")
    @validatedForm(VUser(),
                   VModhash())
    @api_doc(api_section.moderation, uses_site=True)
    def POST_accept_moderator_invite(self, form, jquery):
        """Accept an invite to moderate the specified subreddit.

        The authenticated user must have been invited to moderate the subreddit
        by one of its current moderators.

        See also: [/api/friend](#POST_api_friend) and
        [/subreddits/mine](#GET_subreddits_mine_{where}).

        """

        rel = c.site.get_moderator_invite(c.user)
        if not c.site.remove_moderator_invite(c.user):
            c.errors.add(errors.NO_INVITE_FOUND)
            form.set_error(errors.NO_INVITE_FOUND, None)
            return

        permissions = rel.get_permissions()
        ModAction.create(c.site, c.user, "acceptmoderatorinvite")
        c.site.add_moderator(c.user, permissions=rel.get_permissions())
        notify_user_added("accept_moderator_invite", c.user, c.user, c.site)
        jquery.refresh()

    @validatedForm(
        VUser(),
        VModhash(),
        password=VVerifyPassword("curpass", fatal=False),
        dest=VDestination(),
    )
    def POST_clear_sessions(self, form, jquery, password, dest):
        """Clear all session cookies and replace the current one.

        A valid password (`curpass`) must be supplied.

        """
        # password is required to proceed
        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return

        form.set_text('.status',
                      _('all other sessions have been logged out'))
        form.set_inputs(curpass = "")

        # deauthorize all access tokens
        OAuth2AccessToken.revoke_all_by_user(c.user)
        OAuth2RefreshToken.revoke_all_by_user(c.user)

    def revoke_sessions_and_login(self, user, password):
        self.revoke_sessions(user)

        # run the change password command to get a new salt
        change_password(c.user, password)
        # the password salt has changed, so the user's cookie has been
        # invalidated.  drop a new cookie.
        self.login(c.user)

    def revoke_sessions(self, user):
        # deauthorize all access tokens
        OAuth2AccessToken.revoke_all_by_user(user)
        OAuth2RefreshToken.revoke_all_by_user(user)

    @validatedForm(
        VUser(),
        VModhash(),
        VVerifyPassword("curpass", fatal=False),
        email=ValidEmails("email", num=1),
        verify=VBoolean("verify"),
        dest=VDestination(),
    )
    def POST_update_email(self, form, jquery, email, verify, dest):
        """Update account email address.

        Called by /prefs/update on the site.

        """

        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return

        if not form.has_errors("email", errors.BAD_EMAILS) and email:
            if (not hasattr(c.user, 'email') or c.user.email != email):
                if c.user.email:
                    emailer.email_change_email(c.user)

                c.user.set_email(email)
                c.user.email_verified = None
                c.user._commit()
                Award.take_away("verified_email", c.user)

            if verify:
                if dest == '/':
                    dest = None

                emailer.verify_email(c.user, dest=dest)
                form.set_inputs(curpass="")
                form.set_text('.status',
                     _("you should be getting a verification email shortly."))
            else:
                form.set_text('.status', _('your email has been updated'))

        # user is removing their email
        if (not email and c.user.email and 
            (errors.NO_EMAILS, 'email') in c.errors):
            c.errors.remove((errors.NO_EMAILS, 'email'))
            if c.user.email:
                emailer.email_change_email(c.user)
            c.user.set_email('')
            c.user.email_verified = None
            c.user.pref_email_messages = False
            c.user._commit()
            Award.take_away("verified_email", c.user)
            form.set_text('.status', _('your email has been updated'))

    @validatedForm(
        VUser(),
        VModhash(),
        VVerifyPassword("curpass", fatal=False),
        password=VPasswordChange(['newpass', 'verpass']),
        invalidate_oauth=VBoolean("invalidate_oauth"),
    )
    def POST_update_password(self, form, jquery, password, invalidate_oauth):
        """Update account password.

        Called by /prefs/update on the site. For frontend form verification
        purposes, `newpass` and `verpass` must be equal for a password change
        to succeed.

        """

        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return

        if (password and
            not (form.has_errors("newpass", errors.BAD_PASSWORD) or
                 form.has_errors("verpass", errors.BAD_PASSWORD_MATCH))):
            if invalidate_oauth:
                self.revoke_sessions(c.user)

            change_password(c.user, password)

            if c.user.email:
                emailer.password_change_email(c.user)

            form.set_text('.status', _('your password has been updated'))
            form.set_inputs(curpass="", newpass="", verpass="")

            # the password has changed, so the user's cookie has been
            # invalidated.  drop a new cookie.
            self.login(c.user)

    @validatedForm(VUser(),
                   VModhash(),
                   deactivate_message = VLength("deactivate_message", max_length=500),
                   username = VRequired("user", errors.NOT_USER),
                   user = VThrottledLogin(["user", "passwd"]),
                   confirm = VBoolean("confirm"))
    def POST_deactivate_user(self, form, jquery, deactivate_message, username, user, confirm):
        """Deactivate the currently logged in account.

        A valid username/password and confirmation must be supplied. An
        optional `deactivate_message` may be supplied to explain the reason the
        account is to be deleted.

        Called by /prefs/deactivate on the site.

        """
        if username and username.lower() != c.user.name.lower():
            c.errors.add(errors.NOT_USER, field="user")

        if not confirm:
            c.errors.add(errors.CONFIRM, field="confirm")

        if not (form.has_errors('ratelimit', errors.RATELIMIT) or
                form.has_errors("user", errors.NOT_USER) or
                form.has_errors("passwd", errors.WRONG_PASSWORD) or
                form.has_errors("deactivate_message", errors.TOO_LONG) or
                form.has_errors("confirm", errors.CONFIRM)):
            redirect_url = "/?deactivated=true"
            c.user.delete(deactivate_message)
            form.redirect(redirect_url)

    @require_oauth2_scope("edit")
    @noresponse(VUser(),
                VModhash(),
                thing = VByNameIfAuthor('id'))
    @api_doc(api_section.links_and_comments)
    def POST_del(self, thing):
        """Delete a Link or Comment."""
        if not thing: return
        was_deleted = thing._deleted
        thing._deleted = True
        if (getattr(thing, "promoted", None) is not None and
            not promote.is_promoted(thing)):
            promote.reject_promotion(thing)
        thing._commit()

        thing.update_search_index()

        if isinstance(thing, Link):
            amqp.add_item("deleted_link", thing._fullname)
            queries.delete(thing)
            thing.subreddit_slow.remove_sticky(thing)
            if thing.preview_object:
                thing.set_preview_object(None)
                thing._commit()
        elif isinstance(thing, Comment):
            link = thing.link_slow

            if not was_deleted:
                # get lock before writing to avoid multiple decrements when
                # there are simultaneous duplicate requests
                lock_key = "lock:del_{link}_{comment}".format(
                    link=link._id36,
                    comment=thing._id36,
                )
                if g.lock_cache.add(lock_key, "", time=60):
                    link._incr('num_comments', -1)

            link.remove_sticky_comment(comment=thing, set_by=c.user)

            queries.new_comment(thing, None)  # possible inbox_rels are
                                              # handled by unnotify
            queries.unnotify(thing)
            amqp.add_item("deleted_comment", thing._fullname)
            queries.delete(thing)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(),
                VModhash(),
                VSrCanBan('id'),
                thing=VByName('id', thing_cls=Link))
    @api_doc(api_section.links_and_comments)
    def POST_lock(self, thing):
        """Lock a link.

        Prevents a post from receiving new comments.

        See also: [/api/unlock](#POST_api_unlock).

        """
        if thing.archived_slow:
            return abort(400, "Bad Request")
        VNotInTimeout().run(action_name="lock", target=thing)
        thing.locked = True
        thing._commit()

        ModAction.create(thing.subreddit_slow, c.user, target=thing,
                         action='lock')

    @require_oauth2_scope("modposts")
    @noresponse(VUser(),
                VModhash(),
                VSrCanBan('id'),
                thing=VByName('id', thing_cls=Link))
    @api_doc(api_section.links_and_comments)
    def POST_unlock(self, thing):
        """Unlock a link.

        Allow a post to receive new comments.

        See also: [/api/lock](#POST_api_lock).

        """
        if thing.archived_slow:
            return abort(400, "Bad Request")
        VNotInTimeout().run(action_name="unlock", target=thing)
        thing.locked = False
        thing._commit()

        ModAction.create(thing.subreddit_slow, c.user, target=thing,
                         action='unlock')

    @require_oauth2_scope("modposts")
    @noresponse(VUser(),
                VModhash(),
                VSrCanAlter('id'),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_marknsfw(self, thing):
        """Mark a link NSFW.

        See also: [/api/unmarknsfw](#POST_api_unmarknsfw).

        """
        thing.over_18 = True
        thing._commit()

        if c.user._id != thing.author_id:
            ModAction.create(thing.subreddit_slow, c.user, target=thing,
                             action='marknsfw')

        thing.update_search_index()

    @require_oauth2_scope("modposts")
    @noresponse(VUser(),
                VModhash(),
                VSrCanAlter('id'),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_unmarknsfw(self, thing):
        """Remove the NSFW marking from a link.

        See also: [/api/marknsfw](#POST_api_marknsfw).

        """

        if promote.is_promo(thing):
            if c.user_is_sponsor:
                # set the override attribute so this link won't be automatically
                # reset as nsfw by promote.make_daily_promotions
                thing.over_18_override = True
            else:
                abort(403,'forbidden')

        thing.over_18 = False
        thing._commit()

        if c.user._id != thing.author_id:
            ModAction.create(thing.subreddit_slow, c.user, target=thing,
                             action='marknsfw', details='remove')

        thing.update_search_index()

    @require_oauth2_scope("edit")
    @noresponse(VUser(),
                VModhash(),
                thing=VByNameIfAuthor('id'),
                state=VBoolean('state'))
    @api_doc(api_section.links_and_comments)
    def POST_sendreplies(self, thing, state):
        """Enable or disable inbox replies for a link or comment.

        `state` is a boolean that indicates whether you are enabling or
        disabling inbox replies - true to enable, false to disable.

        """
        if not isinstance(thing, (Link, Comment)):
            return

        thing.sendreplies = state
        thing._commit()

    @noresponse(VUser(),
                VModhash(),
                VSrCanAlter('id'),
                thing=VByName('id'))
    def POST_rescrape(self, thing):
        """Re-queues the link in the media scraper."""
        if not isinstance(thing, Link):
            return

        # KLUDGE: changing the cache entry to a placeholder for this URL will
        # cause the media scraper to force a rescrape.  This will be fixed
        # when parameters can be passed to the scraper queue.
        media_cache.MediaByURL.add_placeholder(thing.url, autoplay=False)

        amqp.add_item("scraper_q", thing._fullname)

    @require_oauth2_scope("modposts")
    @validatedForm(
        VUser(),
        VModhash(),
        VSrCanBan("id"),
        thing=VByName("id", thing_cls=Link),
        sort=VOneOf("sort", CommentSortMenu.suggested_sort_options),
        timeout=VNotInTimeout("id"),
    )
    @api_doc(api_section.links_and_comments)
    def POST_set_suggested_sort(self, form, jquery, thing, sort, timeout):
        """Set a suggested sort for a link.

        Suggested sorts are useful to display comments in a certain preferred way
        for posts. For example, casual conversation may be better sorted by new
        by default, or AMAs may be sorted by Q&A. A sort of an empty string
        clears the default sort.
        """
        if c.user._id != thing.author_id:
            ModAction.create(thing.subreddit_slow, c.user, target=thing,
                             action='setsuggestedsort')

        thing.suggested_sort = sort
        thing._commit()
        jquery.refresh()

    @require_oauth2_scope("modposts")
    @validatedForm(
        VUser(),
        VModhash(),
        VSrCanBan("id"),
        thing=VByName("id"),
        state=VBoolean("state"),
        timeout=VNotInTimeout("id"),
    )
    @api_doc(api_section.links_and_comments)
    def POST_set_contest_mode(self, form, jquery, thing, state, timeout):
        """Set or unset "contest mode" for a link's comments.
        
        `state` is a boolean that indicates whether you are enabling or
        disabling contest mode - true to enable, false to disable.

        """
        thing.contest_mode = state
        thing._commit()
        if state:
            action = 'setcontestmode'
        else:
            action = 'unsetcontestmode'
        ModAction.create(thing.subreddit_slow, c.user, action, target=thing)
        jquery.refresh()

    @require_oauth2_scope("modposts")
    @validatedForm(
        VUser(),
        VModhash(),
        VSrCanBan('id'),
        thing=VByName('id'),
        state=VBoolean('state'),
        num=VInt("num", min=1, max=Subreddit.MAX_STICKIES, coerce=True),
        timeout=VNotInTimeout("id"),
    )
    @api_doc(api_section.links_and_comments)
    def POST_set_subreddit_sticky(self, form, jquery, thing, state, num,
            timeout):
        """Set or unset a Link as the sticky in its subreddit.
        
        `state` is a boolean that indicates whether to sticky or unsticky
        this post - true to sticky, false to unsticky.

        The `num` argument is optional, and only used when stickying a post.
        It allows specifying a particular "slot" to sticky the post into, and
        if there is already a post stickied in that slot it will be replaced.
        If there is no post in the specified slot to replace, or `num` is None,
        the bottom-most slot will be used.
        
        """
        if not isinstance(thing, Link):
            return

        sr = thing.subreddit_slow
        stickied = thing.is_stickied(sr)

        if not stickied and (thing._deleted or thing._spam):
            abort(400, "Can't sticky a removed or deleted post")

        if state:
            if not thing.is_stickyable():
                abort(400, "Post not stickyable")

            if stickied:
                abort(409, "Already stickied")
            sr.set_sticky(thing, c.user, num=num)
        else:
            sr.remove_sticky(thing, c.user)

        jquery.refresh()

    @require_oauth2_scope("report")
    @validatedForm(
        VUser(),
        VModhash(),
        thing=VByName('thing_id'),
        reason=VLength('reason', max_length=100, empty_error=None),
        site_reason=VLength('site_reason', max_length=100, empty_error=None),
        other_reason=VLength('other_reason', max_length=100, empty_error=None),
    )
    @api_doc(api_section.links_and_comments)
    def POST_report(self, form, jquery, thing, reason, site_reason, other_reason):
        """Report a link, comment or message.

        Reporting a thing brings it to the attention of the subreddit's
        moderators. Reporting a message sends it to a system for admin review.

        For links and comments, the thing is implicitly hidden as well (see
        [/api/hide](#POST_api_hide) for details).

        """
        if not thing:
            # preserve old behavior: we used to send the thing's fullname as the
            # "id" parameter, but we can't use that because that name is used to
            # send the form's id
            thing_id = request.POST.get('id')
            if thing_id:
                thing = VByName('id').run(thing_id)

        if not thing or thing._deleted:
            return

        if (form.has_errors("reason", errors.TOO_LONG) or
            form.has_errors("site_reason", errors.TOO_LONG) or
            form.has_errors("other_reason", errors.TOO_LONG)):
            return

        sr = getattr(thing, 'subreddit_slow', None)

        if reason == "site_reason_selected":
            reason = site_reason
        elif reason == "other":
            reason = other_reason

        # if it is a message that is being reported, ban it.
        # every user is admin over their own personal inbox
        if isinstance(thing, Message):
            # Ensure the message is either to them directly or indirectly
            # (through modmail), to prevent unauthorized banning through
            # spoofing.
            if (c.user._id != thing.to_id and
                    not (sr and c.user._id in sr.moderator_ids())):
                abort(403)
            admintools.spam(thing, False, True, c.user.name)
        # auto-hide links that are reported
        elif isinstance(thing, Link):
            # don't hide items from admins/moderators when reporting
            if not (c.user_is_admin or sr.is_moderator(c.user)):
                thing._hide(c.user)
        # TODO: be nice to be able to remove comments that are reported
        # from a user's inbox so they don't have to look at them.
        elif isinstance(thing, Comment):
            pass

        # Don't allow a user in timeout to report things, but continue
        # to hide the links of the reported items
        VNotInTimeout().run(action_name="report", target=thing)

        hooks.get_hook("thing.report").call(thing=thing)

        if not (c.user._spam or
                c.user.ignorereports or
                (sr and sr.is_banned(c.user))):
            Report.new(c.user, thing, reason)
            admintools.report(thing)

        g.events.report_event(
            reason=reason,
            details_text=reason,
            subreddit=sr,
            target=thing,
            request=request,
            context=c,
        )

        if isinstance(thing, (Link, Message)):
            button = jquery(".id-%s .report-button" % thing._fullname)
        elif isinstance(thing, Comment):
            button = jquery(".id-%s .entry:first .report-button" % thing._fullname)
        else:
            return

        button.text(_("reported"))
        parent_div = jquery(".report-%s.reportform" % thing._fullname)
        parent_div.removeClass("active")
        parent_div.html("")

    @require_oauth2_scope("privatemessages")
    @noresponse(
        VUser(),
        VModhash(),
        thing=VByName('id'),
    )
    @api_doc(api_section.messages)
    def POST_del_msg(self, thing):
        """Delete messages from the recipient's view of their inbox."""
        if not thing:
            return

        if not isinstance(thing, Message):
            return

        if thing.to_id != c.user._id:
            return

        thing.del_on_recipient = True
        thing._commit()

        # report the message deletion to data pipeline
        g.events.message_event(thing, event_type="ss.delete_message",
                               request=request, context=c)

    @require_oauth2_scope("privatemessages")
    @noresponse(
        VUser(),
        VModhash(),
        thing=VByName('id'),
    )
    @api_doc(api_section.messages)
    def POST_block(self, thing):
        '''For blocking via inbox.'''
        if not thing:
            return

        # don't allow blocking yourself
        if thing.author_id == c.user._id:
            return

        try:
            sr = Subreddit._byID(thing.sr_id) if thing.sr_id else None
        except NotFound:
            sr = None

        if getattr(thing, "from_sr", False) and sr:
            # Users may only block a subreddit they don't mod
            if not (sr.is_moderator(c.user) or c.user_is_admin):
                BlockedSubredditsByAccount.block(c.user, sr)
            return

        # Users may only block someone who has actively harassed them
        # directly (i.e. comment/link reply or PM). Make sure that 'thing'
        # is in the user's inbox somewhere, unless it's modmail to a
        # subreddit that the user moderates (since then it's not
        # necessarily in their personal inbox)
        is_modmail = (isinstance(thing, Message)
            and sr
            and sr.is_moderator_with_perms(c.user, 'mail'))

        if not is_modmail:
            inbox_cls = Inbox.rel(Account, thing.__class__)
            rels = inbox_cls._fast_query(c.user, thing,
                                        ("inbox", "selfreply", "mention"))
            if not any(rels.values()):
                return

        block_acct = Account._byID(thing.author_id)
        display_author = getattr(thing, "display_author", None)
        if block_acct.name in g.admins or display_author:
            return
        c.user.add_enemy(block_acct)

        # report the user blocking to data pipeline
        g.events.report_event(
            subreddit=sr,
            target=thing,
            request=request,
            context=c,
            event_type="ss.block_user"
        )


    @require_oauth2_scope("privatemessages")
    @noresponse(
        VUser(),
        VModhash(),
        thing=VByName('id'),
    )
    @api_doc(api_section.messages)
    def POST_unblock_subreddit(self, thing):
        if not thing:
            return

        try:
            sr = Subreddit._byID(thing.sr_id) if thing.sr_id else None
        except NotFound:
            sr = None

        if getattr(thing, "from_sr", False) and sr:
            BlockedSubredditsByAccount.unblock(c.user, sr)
            return

    @require_oauth2_scope("modcontributors")
    @noresponse(
        VUser(),
        VModhash(),
        message=VByName('id'),
    )
    @api_doc(api_section.moderation)
    def POST_mute_message_author(self, message):
        '''For muting user via modmail.'''
        if not message:
            return
        subreddit = message.subreddit_slow

        if not subreddit:
            abort(403, 'Not modmail')

        user = message.author_slow
        if not subreddit.can_mute(c.user, user):
            abort(403)

        if not c.user_is_admin:
            if not subreddit.is_moderator_with_perms(c.user, 'access', 'mail'):
                abort(403, 'Invalid mod permissions')

            if subreddit.use_quotas:
                sr_ratelimit = SimpleRateLimit(
                    name="sr_muted_%s" % subreddit._id36,
                    seconds=g.sr_quota_time,
                    limit=g.sr_muted_quota,
                )
                if not sr_ratelimit.record_and_check():
                    abort(403, errors.SUBREDDIT_RATELIMIT)

        # Don't allow a user in timeout to mute users
        VNotInTimeout().run(action_name="muteuser", details_text="modmail",
            target=user, subreddit=subreddit)

        added = subreddit.add_muted(user)
        # Don't mute the user and create another modaction if already muted
        if added:
            MutedAccountsBySubreddit.mute(subreddit, user, c.user, message)
            permalink = message.make_permalink(force_domain=True)
            ModAction.create(subreddit, c.user, 'muteuser',
                target=user, description=permalink)
            subreddit.add_rel_note('muted', user, permalink)

    @require_oauth2_scope("modcontributors")
    @noresponse(
        VUser(),
        VModhash(),
        message=VByName('id'),
    )
    @api_doc(api_section.moderation)
    def POST_unmute_message_author(self, message):
        '''For unmuting user via modmail.'''
        if not message:
            return
        subreddit = message.subreddit_slow

        if not subreddit:
            abort(403, 'Not modmail')

        user = message.author_slow
        if not c.user_is_admin:
            if not subreddit.is_moderator_with_perms(c.user, 'access', 'mail'):
                abort(403, 'Invalid mod permissions')

        # Don't allow a user in timeout to unmute users
        VNotInTimeout().run(action_name="unmuteuser", details_text="modmail",
            target=user, subreddit=subreddit)

        removed = subreddit.remove_muted(user)
        if removed:
            MutedAccountsBySubreddit.unmute(subreddit, user)
            ModAction.create(subreddit, c.user, 'unmuteuser', target=user)

    @require_oauth2_scope("edit")
    @validatedForm(
        VUser(),
        VModhash(),
        item=VByNameIfAuthor('thing_id'),
        text=VMarkdown('text'),
    )
    @api_doc(api_section.links_and_comments)
    def POST_editusertext(self, form, jquery, item, text):
        """Edit the body text of a comment or self-post."""
        if (form.has_errors('text', errors.NO_TEXT) or
                form.has_errors("thing_id", errors.NOT_AUTHOR)):
            return

        if isinstance(item, Link) and not item.is_self:
            return abort(403, "forbidden")
            
        if getattr(item, 'admin_takedown', False):
            # this item has been takendown by the admins,
            # and not not be edited
            # would love to use a 451 (legal) here, but pylons throws an error
            return abort(403, "this content is locked and can not be edited")

        if isinstance(item, Comment):
            max_length = 10000
            admin_override = False
        else:
            max_length = Link.SELFTEXT_MAX_LENGTH
            admin_override = c.user_is_admin

        if not admin_override and len(text) > max_length:
            c.errors.add(errors.TOO_LONG, field='text',
                         msg_params={'max_length': max_length})
            form.set_error(errors.TOO_LONG, 'text')
            return

        removed_mentions = None
        original_text = item.body
        if isinstance(item, Comment):
            kind = 'comment'
            prev_mentions = extract_user_mentions(original_text)
            new_mentions = extract_user_mentions(text)
            removed_mentions = prev_mentions - new_mentions
            item.body = text
        elif isinstance(item, Link):
            kind = 'link'
            if not getattr(item, "is_self", False):
                return abort(403, "forbidden")
            item.selftext = text
        else:
            g.log.warning("%s tried to edit usertext on %r", c.user, item)
            return

        if item._deleted:
            return abort(403, "forbidden")

        if item._age > timedelta(minutes=3) or item.num_votes > 2:
            item.editted = c.start_time

        item.ignore_reports = False

        item._commit()

        # only add to the edited page if this is marked as edited
        if hasattr(item, "editted"):
            queries.edit(item)

        item.update_search_index()

        amqp.add_item('%s_text_edited' % kind, item._fullname)

        hooks.get_hook("thing.edit").call(
            thing=item, original_text=original_text)

        # new mentions are subject to more constraints, handled in butler_q
        if removed_mentions:
            queries.unnotify(item, list(Account._names_to_ids(
                removed_mentions,
                ignore_missing=True,
            )))

        wrapper = default_thing_wrapper(expand_children = True)
        jquery("body>div.content").replace_things(item, True, True, wrap = wrapper)
        jquery("body>div.content .link .rank").hide()

    @allow_oauth2_access
    @validatedForm(
        VUser(),
        VModhash(),
        VRatelimit(rate_user=True, rate_ip=True, prefix="rate_comment_"),
        parent=VSubmitParent(['thing_id', 'parent']),
        comment=VMarkdownLength(['text', 'comment'], max_length=10000),
    )
    @api_doc(api_section.links_and_comments)
    def POST_comment(self, commentform, jquery, parent, comment):
        """Submit a new comment or reply to a message.

        `parent` is the fullname of the thing being replied to. Its value
        changes the kind of object created by this request:

        * the fullname of a Link: a top-level comment in that Link's thread. (requires `submit` scope)
        * the fullname of a Comment: a comment reply to that comment. (requires `submit` scope)
        * the fullname of a Message: a message reply to that message. (requires `privatemessages` scope)

        `text` should be the raw markdown body of the comment or message.

        To start a new message thread, use [/api/compose](#POST_api_compose).

        """
        should_ratelimit = True
        #check the parent type here cause we need that for the
        #ratelimit checks
        if isinstance(parent, Message):
            if (c.oauth_user and not
                    c.oauth_scope.has_any_scope({'privatemessages', 'submit'})):
                abort(403, 'forbidden')
            if not getattr(parent, "repliable", True):
                abort(403, 'forbidden')
            if not parent.can_view_slow():
                abort(403, 'forbidden')

            if parent.sr_id and not c.user_is_admin:
                sr = parent.subreddit_slow

                if sr.is_moderator(c.user) and not c.user_is_admin:
                    # don't let a moderator message a muted user
                    muted_user = parent.get_muted_user_in_conversation()
                    if muted_user:
                        c.errors.add(
                            errors.MUTED_FROM_SUBREDDIT, field="parent")
                        g.events.muted_forbidden_event("muted mod",
                            sr, parent_message=parent, target=muted_user,
                            request=request, context=c,
                        )
                elif sr.is_muted(c.user):
                    # don't let a muted user message the subreddit
                    c.errors.add(errors.USER_MUTED, field="parent")
                    g.events.muted_forbidden_event("muted",
                        parent_message=parent, target=sr,
                        request=request, context=c,
                    )

            is_message = True
            should_ratelimit = False
        else:
            if (c.oauth_user and not
                    c.oauth_scope.has_access(c.site.name, {'submit'})):
                abort(403, 'forbidden')

            is_message = False
            if isinstance(parent, Link):
                link = parent
                parent_comment = None
            else:
                link = Link._byID(parent.link_id)
                parent_comment = parent

            sr = Subreddit._byID(parent.sr_id, stale=True)
            is_author = link.author_id == c.user._id
            if (is_author and (link.is_self or promote.is_promo(link)) or
                    not sr.should_ratelimit(c.user, 'comment')):
                should_ratelimit = False

            hooks.get_hook("comment.validate").call(sr=sr, link=link,
                           parent_comment=parent_comment)

        #remove the ratelimit error if the user's karma is high
        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        if (commentform.has_errors("text", errors.NO_TEXT, errors.TOO_LONG) or
                commentform.has_errors("comment", errors.TOO_LONG) or
                commentform.has_errors("ratelimit", errors.RATELIMIT) or
                commentform.has_errors("parent", errors.DELETED_COMMENT,
                    errors.TOO_OLD, errors.USER_BLOCKED,
                    errors.USER_MUTED, errors.MUTED_FROM_SUBREDDIT,
                    errors.THREAD_LOCKED)
        ):
            return

        if is_message:
            if parent.from_sr:
                to = Subreddit._byID(parent.sr_id)
            else:
                to = Account._byID(parent.author_id)

            # Restrict messaging for users in timeout
            if to:
                sr_name = None
                if isinstance(to, Subreddit):
                    sr_name = to.name
                # Replies in modmail have an Account as their target, but act
                # like they're sent to everyone involved in the conversation.
                elif isinstance(to, Account) and parent and parent.sr_id:
                    sr = Subreddit._byID(parent.sr_id, data=True)
                    if sr:
                        sr_name = sr.name
                is_messaging_admins = ('/r/%s' % sr_name) == g.admin_message_acct

                # Users in timeout can only message the admins.
                if not (sr_name and is_messaging_admins):
                    VNotInTimeout().run(action_name='messagereply', target=parent)

            subject = parent.subject
            re = "re: "
            if not subject.startswith(re):
                subject = re + subject

            item, inbox_rel = Message._new(c.user, to, subject, comment,
                                           request.ip, parent=parent)
        else:
            # Don't let users in timeout comment
            VNotInTimeout().run(action_name='comment', target=parent)

            item, inbox_rel = Comment._new(c.user, link, parent_comment,
                                           comment, request.ip)

        if is_message:
            queries.new_message(item, inbox_rel)
        else:
            queries.new_comment(item, inbox_rel)

        if should_ratelimit:
            VRatelimit.ratelimit(rate_user=True, rate_ip = True,
                                 prefix = "rate_comment_")

        # clean up the submission form and remove it from the DOM (if reply)
        t = commentform.find("textarea")
        t.attr('rows', 3).html("").val("")
        if isinstance(parent, (Comment, Message)):
            commentform.remove()
            jquery.things(parent._fullname).set_text(".reply-button:first",
                                                     _("replied"))

        # insert the new comment
        jquery.insert_things(item)

        # remove any null listings that may be present
        jquery("#noresults").hide()

    @validatedForm(
        VUser(),
        VModhash(),
        VShareRatelimit(),
        share_to=ValidEmailsOrExistingUnames("share_to"),
        message=VLength("message", max_length=1000),
        link=VByName('parent', thing_cls=Link),
    )
    def POST_share(self, shareform, jquery, share_to, message, link):
        if not link:
            abort(404, 'not found')

        # remove the ratelimit error if the user's karma is high
        sr = link.subreddit_slow
        should_ratelimit = sr.should_ratelimit(c.user, 'link')
        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        if shareform.has_errors("message", errors.TOO_LONG):
            return
        elif shareform.has_errors("share_to", errors.BAD_EMAILS,
                                  errors.NO_EMAILS,
                                  errors.TOO_MANY_EMAILS):
            return
        elif shareform.has_errors("ratelimit", errors.RATELIMIT):
            return

        subreddit = link.subreddit_slow

        if subreddit.quarantine or not subreddit.can_view(c.user):
            return abort(403, 'forbidden')

        VNotInTimeout().run(target=link, subreddit=subreddit)

        emails, users = share_to

        # disallow email share for accounts without a verified email address
        if emails and (not c.user.email or not c.user.email_verified):
            return abort(403, 'forbidden')

        link_title = _force_unicode(link.title)

        if getattr(link, "promoted", None) and link.disable_comments:
            message = blockquote_text(message) + "\n\n" if message else ""
            message += '\n%s\n\n%s\n\n' % (link_title, link.url)
            email_message = pm_message = message
        else:
            message = blockquote_text(message) + "\n\n" if message else ""
            message += '\n%s\n' % link_title

            message_body = '\n'

            # Deliberately not translating this, as it'd be in the
            # sender's language
            if link.num_comments:
                count = ("There are currently %(num_comments)s comments " +
                         "on this link.  You can view them here:")
                if link.num_comments == 1:
                    count = ("There is currently %(num_comments)s " +
                             "comment on this link.  You can view it here:")
                numcom = count % {'num_comments': link.num_comments}
                message_body = message_body + "%s\n\n" % numcom
            else:
                message_body = message_body + "You can leave a comment here:\n\n"

            url = add_sr(link.make_permalink_slow(), force_hostname=True)
            url_parser = UrlParser(url)
            url_parser.update_query(ref="share", ref_source="email")
            email_comments_url = url_parser.unparse()
            url_parser.update_query(ref_source="pm")
            pm_comments_url = url_parser.unparse()

            message_body += '%(comments_url)s'
            email_message = message + message_body % {
                    "comments_url": email_comments_url,
                }
            pm_message = message + message_body % {
                    "comments_url": pm_comments_url,
                }
        
        # E-mail everyone
        emailer.share(link, emails, body=email_message or "")

        # Send the PMs
        subject = "%s has shared a link with you!" % c.user.name
        # Prepend this subject to the message - we're repeating ourselves
        # because it looks very abrupt without it.
        pm_message = "%s\n\n%s" % (subject, pm_message)
        
        for target in users:
            m, inbox_rel = Message._new(c.user, target, subject,
                                        pm_message, request.ip)
            # Queue up this PM
            amqp.add_item('new_message', m._fullname)

            queries.new_message(m, inbox_rel)

        g.stats.simple_event('share.email_sent', len(emails))
        g.stats.simple_event('share.pm_sent', len(users))

        # Set the ratelimiter.
        VShareRatelimit.ratelimit()

    @require_oauth2_scope("vote")
    @noresponse(VUser(),
                VModhash(),
                direction=VInt("dir", min=-1, max=1,
                    docs={"dir": "vote direction. one of (1, 0, -1)"}
                ),
                thing=VByName('id'),
                rank=VInt("rank", min=1))
    @api_doc(api_section.links_and_comments)
    def POST_vote(self, direction, thing, rank):
        """Cast a vote on a thing.

        `id` should be the fullname of the Link or Comment to vote on.

        `dir` indicates the direction of the vote. Voting `1` is an upvote,
        `-1` is a downvote, and `0` is equivalent to "un-voting" by clicking
        again on a highlighted arrow.

        **Note: votes must be cast by humans.** That is, API clients proxying a
        human's action one-for-one are OK, but bots deciding how to vote on
        content or amplifying a human's vote are not. See [the reddit
        rules](/rules) for more details on what constitutes vote cheating.

        """

        # a persistent A/A to provide a consistent event stream and confidence
        # in bucketing to the data team
        feature.is_enabled('persistent_vote_a_a')

        if not thing or thing._deleted:
            return self.abort404()

        if not thing.is_votable:
            abort(400, "That type of thing can't be voted on.")

        hooks.get_hook("vote.validate").call(thing=thing)

        if isinstance(thing, Link) and promote.is_promo(thing):
            if not promote.is_promoted(thing):
                return abort(400, "Bad Request")

        if thing.archived_slow:
            return abort(400,
                "This thing is archived and may no longer be voted on")

        # Don't allow users in timeout to vote
        VNotInTimeout().run(target=thing)

        # convert vote direction to enum value
        if direction == 1:
            direction = Vote.DIRECTIONS.up
        elif direction == -1:
            direction = Vote.DIRECTIONS.down
        elif direction == 0:
            direction = Vote.DIRECTIONS.unvote

        cast_vote(c.user, thing, direction, rank=rank)

    @require_oauth2_scope("modconfig")
    @validatedForm(VSrModerator(perms='config'),
                   VModhash(),
                   # nop is safe: handled after auth checks below
                   stylesheet_contents=nop('stylesheet_contents',
                       docs={"stylesheet_contents":
                             "the new stylesheet content"}),
                   reason=VPrintable('reason', 256, empty_error=None),
                   op = VOneOf('op',['save','preview']))
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_subreddit_stylesheet(self, form, jquery,
                                  stylesheet_contents = '', prevstyle='',
                                  op='save', reason=None):
        """Update a subreddit's stylesheet.

        `op` should be `save` to update the contents of the stylesheet.

        """

        if g.css_killswitch:
            return abort(403, 'forbidden')

        css_errors, parsed = c.site.parse_css(stylesheet_contents)

        # The hook passes errors back by setting them on the form.
        hooks.get_hook('subreddit.css.validate').call(
            request=request, form=form, op=op,
            stylesheet_contents=stylesheet_contents,
            parsed_stylesheet=parsed,
            css_errors=css_errors,
            subreddit=c.site,
            user=c.user
        )

        if css_errors:
            error_items = [CssError(x).render(style='html') for x in css_errors]
            form.set_text(".status", _('validation errors'))
            form.set_html(".errors ul", ''.join(error_items))
            form.find('.errors').show()
            c.errors.add(errors.BAD_CSS, field="stylesheet_contents")
            form.has_errors("stylesheet_contents", errors.BAD_CSS)
            return
        else:
            form.find('.errors').hide()
            form.set_html(".errors ul", '')

        # Don't allow users in timeout to modify the stylesheet
        VNotInTimeout().run(action_name="editsettings",
            details_text="%s_stylesheet" % op, target=c.site)

        if op == 'save' and not form.has_error():
            wr = c.site.change_css(stylesheet_contents, parsed, reason=reason)
            form.find('.errors').hide()
            form.set_text(".status", _('saved'))
            form.set_html(".errors ul", "")

        jquery.apply_stylesheet(parsed)

        if op == 'preview':
            # try to find a link to use, otherwise give up and
            # return
            links = SubredditStylesheet.find_preview_links(c.site)
            if links:

                jquery('#preview-table').show()
    
                # do a regular link
                jquery('#preview_link_normal').html(
                    SubredditStylesheet.rendered_link(
                        links, media='off', compress=False))
                # now do one with media
                jquery('#preview_link_media').html(
                    SubredditStylesheet.rendered_link(
                        links, media='on', compress=False))
                # do a compressed link
                jquery('#preview_link_compressed').html(
                    SubredditStylesheet.rendered_link(
                        links, media='off', compress=True))
                # do a stickied link
                jquery('#preview_link_stickied').html(
                    SubredditStylesheet.rendered_link(
                        links, media='off', compress=False, stickied=True))
    
            # and do a comment
            comments = SubredditStylesheet.find_preview_comments(c.site)
            if comments:
                jquery('#preview_comment').html(
                    SubredditStylesheet.rendered_comment(comments))

                jquery('#preview_comment_gilded').html(
                    SubredditStylesheet.rendered_comment(
                        comments, gilded=True))

    @require_oauth2_scope("modconfig")
    @validatedForm(VSrModerator(perms='config'),
                   VModhash(),
                   name = VCssName('img_name'))
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_delete_sr_img(self, form, jquery, name):
        """Remove an image from the subreddit's custom image set.

        The image will no longer count against the subreddit's image limit.
        However, the actual image data may still be accessible for an
        unspecified amount of time. If the image is currently referenced by the
        subreddit's stylesheet, that stylesheet will no longer validate and
        won't be editable until the image reference is removed.

        See also: [/api/upload_sr_img](#POST_api_upload_sr_img).

        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return abort(403, 'forbidden')

        if form.has_errors("img_name", errors.BAD_CSS_NAME):
            return

        # Don't allow users in timeout to modify the stylesheet
        VNotInTimeout().run(action_name="editsettings",
            details_text="del_image", target=c.site)

        wiki.ImagesByWikiPage.delete_image(c.site, "config/stylesheet", name)
        ModAction.create(c.site, c.user, action='editsettings', 
                         details='del_image', description=name)

    @require_oauth2_scope("modconfig")
    @validatedForm(
        VSrModerator(perms='config'),
        VModhash(),
        VNotInTimeout(),
    )
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_delete_sr_header(self, form, jquery):
        """Remove the subreddit's custom header image.

        The sitewide-default header image will be shown again after this call.

        See also: [/api/upload_sr_img](#POST_api_upload_sr_img).

        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return abort(403, 'forbidden')

        if c.site.header:
            c.site.header = None
            c.site.header_size = None
            c.site._commit()
            ModAction.create(c.site, c.user, action='editsettings', 
                             details='del_header')

        # hide the button which started this
        form.find('.delete-img').hide()
        # hide the preview box
        form.find('.img-preview-container').hide()
        # reset the status boxes
        form.set_text('.img-status', _("deleted"))
        
    @require_oauth2_scope("modconfig")
    @validatedForm(
        VSrModerator(perms='config'),
        VModhash(),
        VNotInTimeout(),
    )
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_delete_sr_icon(self, form, jquery):
        """Remove the subreddit's custom mobile icon.

        See also: [/api/upload_sr_img](#POST_api_upload_sr_img).

        """
        if c.site.icon_img:
            c.site.icon_img = None
            c.site.icon_size = None
            c.site._commit()
            ModAction.create(c.site, c.user, action='editsettings',
                             details='del_icon')

        # hide the button which started this
        form.find('.delete-img').hide()
        # hide the preview box
        form.find('.img-preview-container').hide()
        # reset the status boxes
        form.set_text('.img-status', _("deleted"))

    @require_oauth2_scope("modconfig")
    @validatedForm(
        VSrModerator(perms='config'),
        VModhash(),
        VNotInTimeout(),
    )
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_delete_sr_banner(self, form, jquery):
        """Remove the subreddit's custom mobile banner.

        See also: [/api/upload_sr_img](#POST_api_upload_sr_img).

        """
        if c.site.banner_img:
            c.site.banner_img = None
            c.site.banner_size = None
            c.site._commit()
            ModAction.create(c.site, c.user, action='editsettings',
                             details='del_banner')

        # hide the button which started this
        form.find('.delete-img').hide()
        # hide the preview box
        form.find('.img-preview-container').hide()
        # reset the status boxes
        form.set_text('.img-status', _("deleted"))

    def GET_upload_sr_img(self, *a, **kw):
        """
        Completely unnecessary method which exists because safari can
        be dumb too.  On page reload after an image has been posted in
        safari, the iframe to which the request posted preserves the
        URL of the POST, and safari attempts to execute a GET against
        it.  The iframe is hidden, so what it returns is completely
        irrelevant.
        """
        return "nothing to see here."

    @require_oauth2_scope("modconfig")
    @validate(VSrModerator(perms='config'),
              VModhash(),
              file = VUploadLength('file', max_length=1024*500),
              name = VCssName("name"),
              img_type = VImageType('img_type'),
              form_id = VLength('formid', max_length = 100,
                                docs={"formid": "(optional) can be ignored"}),
              upload_type = VOneOf('upload_type',
                                   ('img', 'header', 'icon', 'banner')),
              header = VInt('header', max=1, min=0))
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_upload_sr_img(self, file, header, name, form_id, img_type,
                           upload_type=None):
        """Add or replace a subreddit image, custom header logo, custom mobile
        icon, or custom mobile banner.

        * If the `upload_type` value is `img`, an image for use in the
        subreddit stylesheet is uploaded with the name specified in `name`.
        * If the `upload_type` value is `header` then the image uploaded will
        be the subreddit's new logo and `name` will be ignored.
        * If the `upload_type` value is `icon` then the image uploaded will be
        the subreddit's new mobile icon and `name` will be ignored.
        * If the `upload_type` value is `banner` then the image uploaded will
        be the subreddit's new mobile banner and `name` will be ignored.

        For backwards compatibility, if `upload_type` is not specified, the
        `header` field will be used instead:

        * If the `header` field has value `0`, then `upload_type` is `img`.
        * If the `header` field has value `1`, then `upload_type` is `header`.

        The `img_type` field specifies whether to store the uploaded image as a
        PNG or JPEG.

        Subreddits have a limited number of images that can be in use at any
        given time. If no image with the specified name already exists, one of
        the slots will be consumed.

        If an image with the specified name already exists, it will be
        replaced.  This does not affect the stylesheet immediately, but will
        take effect the next time the stylesheet is saved.

        See also: [/api/delete_sr_img](#POST_api_delete_sr_img),
        [/api/delete_sr_header](#POST_api_delete_sr_header),
        [/api/delete_sr_icon](#POST_api_delete_sr_icon), and
        [/api/delete_sr_banner](#POST_api_delete_sr_banner).

        """

        if c.site.quarantine:
            abort(403)

        # default error list (default values will reset the errors in
        # the response if no error is raised)
        errors = dict(BAD_CSS_NAME = "", IMAGE_ERROR = "")

        # for backwards compatibility, map header to upload_type
        if upload_type is None:
            upload_type = 'header' if header else 'img'
        
        if upload_type == 'img' and not name:
            # error if the name wasn't specified and the image was not for a sponsored link or header
            # this may also fail if a sponsored image was added and the user is not an admin
            errors['BAD_CSS_NAME'] = _("bad image name")
        
        if upload_type == 'img' and not c.user_is_admin:
            image_count = wiki.ImagesByWikiPage.get_image_count(
                c.site, "config/stylesheet")
            if image_count >= g.max_sr_images:
                errors['IMAGE_ERROR'] = _("too many images (you only get %d)") % g.max_sr_images

        try:
            size = str_to_image(file).size
        except (IOError, TypeError):
            errors['IMAGE_ERROR'] = _('Invalid image or general image error')
        else:
            if upload_type == 'icon':
                if size != Subreddit.ICON_EXACT_SIZE:
                    errors['IMAGE_ERROR'] = (
                        _('must be %dx%d pixels') % Subreddit.ICON_EXACT_SIZE)
            elif upload_type == 'banner':
                aspect_ratio = float(size[0]) / size[1]
                if abs(Subreddit.BANNER_ASPECT_RATIO - aspect_ratio) > 0.01:
                    errors['IMAGE_ERROR'] = _('10:3 aspect ratio required')
                elif size > Subreddit.BANNER_MAX_SIZE:
                    errors['IMAGE_ERROR'] = (
                        _('max %dx%d pixels') % Subreddit.BANNER_MAX_SIZE)
                elif size < Subreddit.BANNER_MIN_SIZE:
                    errors['IMAGE_ERROR'] = (
                        _('min %dx%d pixels') % Subreddit.BANNER_MIN_SIZE)

        if any(errors.values()):
            return UploadedImage("", "", "", errors=errors, form_id=form_id).render()
        else:
            try:
                new_url = media.upload_media(file, file_type="." + img_type)
            except Exception as e:
                g.log.warning("error uploading subreddit image: %s", e)
                errors['IMAGE_ERROR'] = _("Invalid image or general image error")
                return UploadedImage("", "", "", errors=errors, form_id=form_id).render()

            details_text = "upload_image"
            if not upload_type == "img":
                details_text = "upload_image_%s" % upload_type
            VNotInTimeout().run(action_name="editsettings",
                details_text=details_text, target=c.site)

            if upload_type == 'img':
                wiki.ImagesByWikiPage.add_image(c.site, "config/stylesheet",
                                                name, new_url)
                kw = dict(details='upload_image', description=name)
            elif upload_type == 'header':
                c.site.header = new_url
                c.site.header_size = size
                c.site._commit()
                kw = dict(details='upload_image_header')
            elif upload_type == 'icon':
                c.site.icon_img = new_url
                c.site.icon_size = size
                c.site._commit()
                kw = dict(details='upload_image_icon')
            elif upload_type == 'banner':
                c.site.banner_img = new_url
                c.site.banner_size = size
                c.site._commit()
                kw = dict(details='upload_image_banner')

            ModAction.create(c.site, c.user, action='editsettings', **kw)

            return UploadedImage(_('saved'), new_url, name, 
                                 errors=errors, form_id=form_id).render()

    @require_oauth2_scope("modconfig")
    @validatedForm(VUser(),
                   VCaptcha(),
                   VModhash(),
                   VRatelimit(rate_user = True,
                              rate_ip = True,
                              prefix = 'create_reddit_'),
                   sr = VByName('sr'),
                   name = VAvailableSubredditName("name"),
                   title = VLength("title", max_length = 100),
                   header_title = VLength("header-title", max_length = 500),
                   domain = VCnameDomain("domain"),
                   submit_text = VMarkdownLength("submit_text", max_length=1024),
                   public_description = VMarkdownLength("public_description", max_length = 500),
                   description = VMarkdownLength("description", max_length = 5120),
                   lang = VLang("lang"),
                   over_18 = VBoolean('over_18'),
                   allow_top = VBoolean('allow_top'),
                   show_media = VBoolean('show_media'),
                   # show_media_preview = VBoolean('show_media_preview'),
                   public_traffic = VBoolean('public_traffic'),
                   collapse_deleted_comments = VBoolean('collapse_deleted_comments'),
                   exclude_banned_modqueue = VBoolean('exclude_banned_modqueue'),
                   spam_links = VOneOf('spam_links', ('low', 'high', 'all')),
                   spam_selfposts = VOneOf('spam_selfposts', ('low', 'high', 'all')),
                   spam_comments = VOneOf('spam_comments', ('low', 'high', 'all')),
                   type = VOneOf('type', Subreddit.valid_types),
                   link_type = VOneOf('link_type', ('any', 'link', 'self')),
                   submit_link_label=VLength('submit_link_label', max_length=60),
                   submit_text_label=VLength('submit_text_label', max_length=60),
                   comment_score_hide_mins=VInt('comment_score_hide_mins',
                       coerce=False, num_default=0, min=0, max=1440),
                   wikimode = VOneOf('wikimode', ('disabled', 'modonly', 'anyone')),
                   wiki_edit_karma = VInt("wiki_edit_karma", coerce=False, num_default=0, min=0),
                   wiki_edit_age = VInt("wiki_edit_age", coerce=False, num_default=0, min=0),
                   hide_ads = VBoolean("hide_ads"),
                   suggested_comment_sort=VOneOf('suggested_comment_sort',
                                                 CommentSortMenu._options,
                                                 default=None),
                   # related_subreddits = VSubredditList('related_subreddits', limit=20),
                   # key_color = VColor('key_color'),
                   )
    @api_doc(api_section.subreddits)
    def POST_site_admin(self, form, jquery, name, sr, **kw):
        """Create or configure a subreddit.

        If `sr` is specified, the request will attempt to modify the specified
        subreddit. If not, a subreddit with name `name` will be created.

        This endpoint expects *all* values to be supplied on every request.  If
        modifying a subset of options, it may be useful to get the current
        settings from [/about/edit.json](#GET_r_{subreddit}_about_edit.json)
        first.

        For backwards compatibility, `description` is the sidebar text and
        `public_description` is the publicly visible subreddit description.

        Most of the parameters for this endpoint are identical to options
        visible in the user interface and their meanings are best explained
        there.

        See also: [/about/edit.json](#GET_r_{subreddit}_about_edit.json).

        """
        def apply_wikid_field(sr, form, pagename, value, field):
            try:
                wikipage = wiki.WikiPage.get(sr, pagename)
            except tdb_cassandra.NotFound:
                wikipage = wiki.WikiPage.create(sr, pagename)
            wr = wikipage.revise(value, author=c.user._id36)
            setattr(sr, field, value)
            if wr:
                ModAction.create(sr, c.user, 'wikirevise',
                                 details=wiki.modactions.get(pagename))

        # This should be moved to @validatedForm above when we remove
        # the feature flag. Down here to avoid processing when flagged off
        # and to hide from API docs.
        if feature.is_enabled('mobile_settings'):
            validator = VColor('key_color')
            value = request.params.get('key_color')
            kw['key_color'] = validator.run(value)
        if feature.is_enabled('related_subreddits'):
            validator = VSubredditList('related_subreddits', limit=20)
            value = request.params.get('related_subreddits')
            kw['related_subreddits'] = validator.run(value)

        if feature.is_enabled('autoexpand_media_previews'):
            validator = VBoolean('show_media_preview')
            value = request.params.get('show_media_preview')
            kw["show_media_preview"] = validator.run(value)

        # the status button is outside the form -- have to reset by hand
        form.parent().set_html('.status', "")

        redir = False
        keyword_fields = [
            'allow_top',
            'collapse_deleted_comments',
            'comment_score_hide_mins',
            'description',
            'domain',
            'exclude_banned_modqueue',
            'header_title',
            'hide_ads',
            'lang',
            'link_type',
            'name',
            'over_18',
            'public_description',
            'public_traffic',
            'show_media',
            'show_media_preview',
            'spam_comments',
            'spam_links',
            'spam_selfposts',
            'submit_link_label',
            'submit_text',
            'submit_text_label',
            'suggested_comment_sort',
            'title',
            'type',
            'wiki_edit_age',
            'wiki_edit_karma',
            'wikimode',
        ]

        if feature.is_enabled('mobile_settings'):
            keyword_fields.append('key_color')
        if sr and feature.is_enabled('related_subreddits'):
            keyword_fields.append('related_subreddits')

        kw = {k: v for k, v in kw.iteritems() if k in keyword_fields}

        public_description = kw.pop('public_description')
        description = kw.pop('description')
        submit_text = kw.pop('submit_text')

        def update_wiki_text(sr):
            error = False
            apply_wikid_field(
                sr,
                form,
                'config/sidebar',
                description,
                'description',
            )

            apply_wikid_field(
                sr,
                form,
                'config/submit_text',
                submit_text,
                'submit_text',
            )

            apply_wikid_field(
                sr,
                form,
                'config/description',
                public_description,
                'public_description',
            )
        
        if not sr and not c.user.can_create_subreddit:
            form.set_error(errors.CANT_CREATE_SR, "")
            c.errors.add(errors.CANT_CREATE_SR, field="")

        # only care about captcha if this is creating a subreddit
        if not sr and form.has_errors("captcha", errors.BAD_CAPTCHA):
            return

        domain = kw['domain']
        cname_sr = domain and Subreddit._by_domain(domain)
        if cname_sr and (not sr or sr != cname_sr):
            c.errors.add(errors.USED_CNAME)

        can_set_archived = c.user_is_admin or (sr and sr.type == 'archived')
        if kw['type'] == 'archived' and not can_set_archived:
            c.errors.add(errors.INVALID_OPTION, field='type')

        can_set_gold_restricted = c.user_is_admin or (sr and sr.type == 'gold_restricted')
        if kw['type'] == 'gold_restricted' and not can_set_gold_restricted:
            c.errors.add(errors.INVALID_OPTION, field='type')

        # can't create a gold only subreddit without having gold
        can_set_gold_only = (c.user.gold or c.user.gold_charter or
                (sr and sr.type == 'gold_only'))
        if kw['type'] == 'gold_only' and not can_set_gold_only:
            form.set_error(errors.GOLD_REQUIRED, 'type')
            c.errors.add(errors.GOLD_REQUIRED, field='type')

        can_set_hide_ads = can_set_gold_only and kw['type'] == 'gold_only'
        if kw['hide_ads'] and not can_set_hide_ads:
            form.set_error(errors.GOLD_ONLY_SR_REQUIRED, 'hide_ads')
            c.errors.add(errors.GOLD_ONLY_SR_REQUIRED, field='hide_ads')
        elif not can_set_hide_ads and sr:
            kw['hide_ads'] = sr.hide_ads

        can_set_employees_only = c.user.employee
        if kw['type'] == 'employees_only' and not can_set_employees_only:
            c.errors.add(errors.INVALID_OPTION, field='type')

        if not sr and form.has_errors("ratelimit", errors.RATELIMIT):
            pass
        elif not sr and form.has_errors("", errors.CANT_CREATE_SR):
            pass
        # if existing subreddit is employees_only and trying to change type,
        # require that admin mode is on
        elif (sr and sr.type == 'employees_only' and kw['type'] != sr.type and
                not c.user_is_admin):
            form.set_error(errors.ADMIN_REQUIRED, 'type')
            c.errors.add(errors.ADMIN_REQUIRED, field='type')
        # if the user wants to convert an existing subreddit to gold_only,
        # let them know that they'll need to contact an admin to convert it.
        elif (sr and sr.type != 'gold_only' and kw['type'] == 'gold_only' and
                not c.user_is_admin):
            form.set_error(errors.CANT_CONVERT_TO_GOLD_ONLY, 'type')
            c.errors.add(errors.CANT_CONVERT_TO_GOLD_ONLY, field='type')
        elif form.has_errors('type', errors.GOLD_REQUIRED):
            pass
        elif not sr and form.has_errors("name", errors.SUBREDDIT_EXISTS,
                                        errors.BAD_SR_NAME):
            form.find('#example_name').hide()
        elif form.has_errors('title', errors.NO_TEXT, errors.TOO_LONG):
            form.find('#example_title').hide()
        elif form.has_errors('domain', errors.BAD_CNAME, errors.USED_CNAME):
            form.find('#example_domain').hide()
        elif (form.has_errors(('type', 'link_type', 'wikimode'),
                              errors.INVALID_OPTION) or
              form.has_errors(('public_description',
                               'submit_text',
                               'description'), errors.TOO_LONG)):
            pass
        elif (form.has_errors(('wiki_edit_karma', 'wiki_edit_age'), 
                              errors.BAD_NUMBER)):
            pass
        elif form.has_errors('comment_score_hide_mins', errors.BAD_NUMBER):
            pass
        elif form.has_errors('related_subreddits', errors.SUBREDDIT_NOEXIST,
                             errors.BAD_SR_NAME, errors.TOO_MANY_SUBREDDITS):
            pass
        elif form.has_errors('hide_ads', errors.GOLD_ONLY_SR_REQUIRED):
            pass
        #creating a new reddit
        elif not sr:
            # Don't allow user in timeout to create a new subreddit
            VNotInTimeout().run(action_name="createsubreddit", target=None)

            #sending kw is ok because it was sanitized above
            sr = Subreddit._new(name = name, author_id = c.user._id,
                                ip=request.ip, **kw)

            update_wiki_text(sr)
            sr._commit()

            hooks.get_hook("subreddit.new").call(subreddit=sr)

            Subreddit.subscribe_defaults(c.user)
            sr.add_subscriber(c.user)
            sr.add_moderator(c.user)

            if not sr.hide_contributors:
                sr.add_contributor(c.user)
            redir = sr.path + "about/edit/?created=true"
            if not c.user_is_admin:
                VRatelimit.ratelimit(rate_user=True,
                                     rate_ip = True,
                                     prefix = "create_reddit_")

            queries.new_subreddit(sr)
            sr.update_search_index()

        #editting an existing reddit
        elif sr.is_moderator_with_perms(c.user, 'config') or c.user_is_admin:
            # Don't allow user in timeout to edit subreddit settings
            VNotInTimeout().run(action_name="editsettings", target=sr)

            #assume sr existed, or was just built
            old_domain = sr.domain

            update_wiki_text(sr)

            if sr.quarantine:
                del kw['allow_top']
                del kw['show_media']
                del kw['show_media_preview']

            #notify ads if sr in a collection changes over_18 to true
            if kw.get('over_18', False) and not sr.over_18:
                collections = []
                for collection in Collection.get_all():
                    if (sr.name in collection.sr_names
                            and not collection.over_18):
                        collections.append(collection.name)

                if collections:
                    msg = "%s now NSFW, in collection(s) %s"
                    msg %= (sr.name, ', '.join(collections))
                    emailer.sales_email(msg)

            for k, v in kw.iteritems():
                if getattr(sr, k, None) != v:
                    ModAction.create(sr, c.user, action='editsettings',
                                     details=k)

                setattr(sr, k, v)
            sr._commit()

            #update the domain cache if the domain changed
            if sr.domain != old_domain:
                Subreddit._by_domain(old_domain, _update = True)
                Subreddit._by_domain(sr.domain, _update = True)

            sr.update_search_index()
            form.parent().set_text('.status', _("saved"))

        if form.has_error():
            return

        if redir:
            form.redirect(redir)
        else:
            jquery.refresh()

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing = VByName('id'),
                spam = VBoolean('spam', default=True))
    @api_doc(api_section.moderation)
    def POST_remove(self, thing, spam):
        """Remove a link, comment, or modmail message.

        If the thing is a link, it will be removed from all subreddit listings.
        If the thing is a comment, it will be redacted and removed from all
        subreddit comment listings.

        See also: [/api/approve](#POST_api_approve).

        """

        # Don't remove a promoted link
        if getattr(thing, "promoted", None):
            return
        action_name = "remove"
        if spam:
            action_name = "spam"
        VNotInTimeout().run(action_name=action_name, target=thing)

        if thing._deleted:
            return

        filtered = thing._spam
        kw = {'target': thing}

        if filtered and spam:
            kw['details'] = 'confirm_spam'
            train_spam = False
        elif filtered and not spam:
            kw['details'] = 'remove'
            admintools.unspam(thing, unbanner=c.user.name, insert=False)
            train_spam = False
        elif not filtered and spam:
            kw['details'] = 'spam'
            train_spam = True
        elif not filtered and not spam:
            kw['details'] = 'remove'
            train_spam = False

        admintools.spam(thing, auto=False,
                        moderator_banned=not c.user_is_admin,
                        banner=c.user.name,
                        train_spam=train_spam)

        if isinstance(thing, (Link, Comment)):
            sr = thing.subreddit_slow
            action = 'remove' + thing.__class__.__name__.lower()
            ModAction.create(sr, c.user, action, **kw)

        if isinstance(thing, Link):
            sr.remove_sticky(thing)
        elif isinstance(thing, Comment):
            thing.link_slow.remove_sticky_comment(comment=thing, set_by=c.user)
            queries.unnotify(thing)


    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing = VByName('id'))
    @api_doc(api_section.moderation)
    def POST_approve(self, thing):
        """Approve a link or comment.

        If the thing was removed, it will be re-inserted into appropriate
        listings. Any reports on the approved thing will be discarded.

        See also: [/api/remove](#POST_api_remove).

        """
        if not thing: return
        if thing._deleted: return
        if c.user._spam:
           self.abort403()

        # Don't allow user in timeout to approve link or comment
        VNotInTimeout().run(target=thing)

        kw = {'target': thing}
        if thing._spam:
            kw['details'] = 'unspam'
            train_spam = True
            insert = True
        else:
            kw['details'] = 'confirm_ham'
            train_spam = False
            insert = False

        admintools.unspam(thing, moderator_unbanned=not c.user_is_admin,
                          unbanner=c.user.name, train_spam=train_spam,
                          insert=insert)

        if isinstance(thing, (Link, Comment)):
            sr = thing.subreddit_slow
            action = 'approve' + thing.__class__.__name__.lower()
            ModAction.create(sr, c.user, action, **kw)

        if isinstance(thing, Comment) and insert:
            queries.renotify(thing)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing=VByName('id'))
    @api_doc(api_section.moderation)
    def POST_ignore_reports(self, thing):
        """Prevent future reports on a thing from causing notifications.

        Any reports made about a thing after this flag is set on it will not
        cause notifications or make the thing show up in the various moderation
        listings.

        See also: [/api/unignore_reports](#POST_api_unignore_reports).

        """
        if not thing: return
        if thing._deleted: return
        if thing.ignore_reports: return

        # Don't allow user in timeout to ignore reports
        VNotInTimeout().run(action_name="ignorereports", target=thing)

        thing.ignore_reports = True
        thing._commit()

        sr = thing.subreddit_slow
        ModAction.create(sr, c.user, 'ignorereports', target=thing)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing=VByName('id'))
    @api_doc(api_section.moderation)
    def POST_unignore_reports(self, thing):
        """Allow future reports on a thing to cause notifications.

        See also: [/api/ignore_reports](#POST_api_ignore_reports).

        """
        if not thing: return
        if thing._deleted: return
        if not thing.ignore_reports: return

        # Don't allow user in timeout to unignore reports
        VNotInTimeout().run(action_name="unignorereports", target=thing)

        thing.ignore_reports = False
        thing._commit()

        sr = thing.subreddit_slow
        ModAction.create(sr, c.user, 'unignorereports', target=thing)

    @require_oauth2_scope("modposts")
    @validatedForm(VUser(), VModhash(),
                   VCanDistinguish(('id', 'how')),
                   thing = VByName('id'),
                   how = VOneOf('how', ('yes','no','admin','special')),
                   # sticky=VBoolean('sticky', default=False),
                   )
    @api_doc(api_section.moderation)
    def POST_distinguish(self, form, jquery, thing, how):
        """Distinguish a thing's author with a sigil.

        This can be useful to draw attention to and confirm the identity of the
        user in the context of a link or comment of theirs. The options for
        distinguish are as follows:

        * `yes` - add a moderator distinguish (`[M]`). only if the user is a
                  moderator of the subreddit the thing is in.
        * `no` - remove any distinguishes.
        * `admin` - add an admin distinguish (`[A]`). admin accounts only.
        * `special` - add a user-specific distinguish. depends on user.

        The first time a top-level comment is moderator distinguished, the
        author of the link the comment is in reply to will get a notification
        in their inbox.

        """

        # To be added to API docs when fully enabled:
        #
        # `sticky` is a boolean flag for comments, which will stick the
        #  distingushed comment to the top of all comments threads. If a comment
        #  is marked sticky, it will override any other stickied comment for that
        #  link (as only one comment may be stickied at a time.) Only top-level
        #  comments may be stickied.

        if not thing:return

        # XXX: Temporary retrieval of sticky param down here to avoid it
        # showing up in API docs while in development. Move this to the
        # validatedForm above when live.
        sticky = False
        if feature.is_enabled('sticky_comments'):
            sticky_validator = VBoolean('sticky', default=False)
            sticky = sticky_validator.run(request.params.get('sticky'))

        if (feature.is_enabled('sticky_comments') and
                sticky and
                not isinstance(thing, Comment)):
            abort(400, "Only comments may be stickied from distinguish. To "
                       "sticky a link in a subreddit use set_subreddit_sticky."
                  )

        c.profilepage = request.params.get('profilepage') == 'True'
        log_modaction = True
        log_kw = {}
        send_message = False
        original = getattr(thing, 'distinguished', 'no')
        if how == original: # Distinguish unchanged
            log_modaction = False
        elif how in ('admin', 'special'): # Add admin/special
            log_modaction = False
            send_message = True
        elif (original in ('admin', 'special') and
                how == 'no'): # Remove admin/special
            log_modaction = False
        elif how == 'no': # From yes to no
            log_kw['details'] = 'remove'
        else: # From no to yes
            send_message = True

        if isinstance(thing, Comment):
            link = thing.link_slow

            # Send a message if this is a top-level comment on a submission or
            # comment that has disabled receiving inbox notifications of
            # replies, if it's the first distinguish for this comment, and if
            # the user isn't banned or blocked by the author (replying didn't
            # generate an inbox notification, send one now upon distinguishing
            # it)
            if not thing.parent_id:
                to = Account._byID(link.author_id, data=True)
                replies_enabled = link.sendreplies
            else:
                parent = Comment._byID(thing.parent_id, data=True)
                to = Account._byID(parent.author_id, data=True)
                replies_enabled = parent.sendreplies

            previously_distinguished = hasattr(thing, 'distinguished')
            user_can_notify = (not c.user._spam and
                               c.user._id not in to.enemies and
                               to.name != c.user.name)

            if (send_message and
                    not replies_enabled and
                    not previously_distinguished and
                    user_can_notify):
                inbox_rel = Inbox._add(to, thing, 'selfreply')
                queries.update_comment_notifications(thing, inbox_rel)

            # Sticky handling - done before commit so that if there is an error
            # setting sticky we don't distinguish. This ordering does leave the
            # potential for an erroneous sticky if there's a commit error on
            # distinguish, but a stickied comment that's not distinguished is
            # not the end of the world, and handling rollback would probably be
            # more error prone if we're hitting commit errors anyhow.
            if feature.is_enabled('sticky_comments'):
                try:
                    if not sticky or how == 'no':
                        # Un-distinguished a comment or sticky was False? Check
                        # to see if it was previously stickied and unsticky if
                        # so.
                        if link.sticky_comment_id == thing._id:
                            link.remove_sticky_comment(set_by=c.user)
                    elif sticky and how != 'no':
                        link.set_sticky_comment(thing, set_by=c.user)
                except RedditError as error:
                    abort_with_error(error, error.code or 400)

        thing.distinguished = how
        thing._commit()

        hooks.get_hook("thing.distinguish").call(thing=thing)

        wrapper = default_thing_wrapper(expand_children = True)
        w = wrap_links(thing, wrapper)
        jquery("body>div.content").replace_things(w, True, True)
        jquery("body>div.content .link .rank").hide()
        if log_modaction:
            sr = thing.subreddit_slow
            ModAction.create(sr, c.user, 'distinguish', target=thing, **log_kw)

    @require_oauth2_scope("save")
    @json_validate(VUser())
    @api_doc(api_section.links_and_comments)
    def GET_saved_categories(self, responder):
        """Get a list of categories in which things are currently saved.

        See also: [/api/save](#POST_api_save).

        """
        if not c.user.gold:
            abort(403)
        categories = LinkSavesByCategory.get_saved_categories(c.user)
        categories += CommentSavesByCategory.get_saved_categories(c.user)
        categories = sorted(set(categories), key=lambda name: name.lower())
        categories = [dict(category=category) for category in categories]
        return {'categories': categories}

    @require_oauth2_scope("save")
    @noresponse(
        VUser(),
        VModhash(),
        category=VSavedCategory('category'),
        thing=VByName('id'),
    )
    @api_doc(api_section.links_and_comments)
    def POST_save(self, thing, category):
        """Save a link or comment.

        Saved things are kept in the user's saved listing for later perusal.

        See also: [/api/unsave](#POST_api_unsave).

        """
        if not thing or not isinstance(thing, (Link, Comment)):
            abort(400)

        if category and not c.user.gold:
            category = None

        if ('BAD_SAVE_CATEGORY', 'category') in c.errors:
            abort(403)

        thing._save(c.user, category=category)

    @require_oauth2_scope("save")
    @noresponse(
        VUser(),
        VModhash(),
        thing=VByName('id'),
    )
    @api_doc(api_section.links_and_comments)
    def POST_unsave(self, thing):
        """Unsave a link or comment.

        This removes the thing from the user's saved listings as well.

        See also: [/api/save](#POST_api_save).

        """
        if not thing or not isinstance(thing, (Link, Comment)):
            abort(400)

        thing._unsave(c.user)

    def collapse_handler(self, things, collapse):
        if not things:
            return
        things = tup(things)
        srs = Subreddit._byID([t.sr_id for t in things if t.sr_id],
                              return_dict = True)
        for t in things:
            if hasattr(t, "to_id") and c.user._id == t.to_id:
                t.to_collapse = collapse
            elif hasattr(t, "author_id") and c.user._id == t.author_id:
                t.author_collapse = collapse
            elif isinstance(t, Message) and t.sr_id:
                if srs[t.sr_id].is_moderator(c.user):
                    t.to_collapse = collapse
            t._commit()

    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple = True))
    @api_doc(api_section.messages)
    def POST_collapse_message(self, things):
        """Collapse a message

        See also: [/api/uncollapse_message](#POST_uncollapse_message)

        """
        self.collapse_handler(things, True)

    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple = True))
    @api_doc(api_section.messages)
    def POST_uncollapse_message(self, things):
        """Uncollapse a message

        See also: [/api/collapse_message](#POST_collapse_message)

        """
        self.collapse_handler(things, False)

    @require_oauth2_scope("privatemessages")
    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple=True, limit=25))
    @api_doc(api_section.messages)
    def POST_unread_message(self, things):
        if not things:
            if (errors.TOO_MANY_THING_IDS, 'id') in c.errors:
                return abort(413)
            else:
                return abort(400)

        queries.unread_handler(things, c.user, unread=True)

    @require_oauth2_scope("privatemessages")
    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple=True, limit=25))
    @api_doc(api_section.messages)
    def POST_read_message(self, things):
        if not things:
            if (errors.TOO_MANY_THING_IDS, 'id') in c.errors:
                return abort(413)
            else:
                return abort(400)

        queries.unread_handler(things, c.user, unread=False)

    @require_oauth2_scope("privatemessages")
    @noresponse(VUser(),
                VModhash(),
                VRatelimit(rate_user=True, prefix="rate_read_all_", fatal=True))
    @api_doc(api_section.messages)
    def POST_read_all_messages(self):
        """Queue up marking all messages for a user as read.

        This may take some time, and returns 202 to acknowledge acceptance of
        the request.
        """
        amqp.add_item('mark_all_read', c.user._fullname)
        # Mark usage in the ratelimiter.
        VRatelimit.ratelimit(rate_user=True, prefix='rate_read_all_')

        return abort(202)

    @require_oauth2_scope("report")
    @noresponse(VUser(),
                VModhash(),
                links=VByName('id', thing_cls=Link, multiple=True, limit=50))
    @api_doc(api_section.links_and_comments)
    def POST_hide(self, links):
        """Hide a link.

        This removes it from the user's default view of subreddit listings.

        See also: [/api/unhide](#POST_api_unhide).

        """
        if not links:
            return abort(400)

        LinkHidesByAccount._hide(c.user, links)

    @require_oauth2_scope("report")
    @noresponse(VUser(),
                VModhash(),
                links=VByName('id', thing_cls=Link, multiple=True, limit=50))
    @api_doc(api_section.links_and_comments)
    def POST_unhide(self, links):
        """Unhide a link.

        See also: [/api/hide](#POST_api_hide).

        """
        if not links:
            return abort(400)

        LinkHidesByAccount._unhide(c.user, links)


    @csrf_exempt
    @validatedForm(VUser(),
                   parent = VByName('parent_id'))
    def POST_moremessages(self, form, jquery, parent):
        if not parent.can_view_slow():
            return abort(403, 'forbidden')

        if parent.sr_id:
            builder = SrMessageBuilder(parent.subreddit_slow,
                                       parent = parent, skip = False)
        else:
            builder = UserMessageBuilder(c.user, parent = parent, skip = False)

        listing = Listing(builder).listing()

        a = []
        for item in listing.things:
            a.append(item)
            if hasattr(item, "child"):
                for x in item.child.things:
                    a.append(x)

        for item in a:
            if hasattr(item, "child"):
                item.child = None

        jquery.things(parent._fullname).parent().replace_things(a, False, True)

    @require_oauth2_scope("read")
    @validatedForm(
        link=VByName('link_id', thing_cls=Link),
        sort=VMenu('morechildren', CommentSortMenu, remember=False),
        children=VCommentIDs('children'),
        mc_id=nop(
            "id",
            docs={"id": "(optional) id of the associated MoreChildren object"}),
    )
    @api_doc(api_section.links_and_comments)
    def GET_morechildren(self, form, jquery, link, sort, children, mc_id):
        """Retrieve additional comments omitted from a base comment tree.

        When a comment tree is rendered, the most relevant comments are
        selected for display first. Remaining comments are stubbed out with
        "MoreComments" links. This API call is used to retrieve the additional
        comments represented by those stubs, up to 20 at a time.

        The two core parameters required are `link` and `children`.  `link` is
        the fullname of the link whose comments are being fetched. `children`
        is a comma-delimited list of comment ID36s that need to be fetched.

        If `id` is passed, it should be the ID of the MoreComments object this
        call is replacing. This is needed only for the HTML UI's purposes and
        is optional otherwise.

        **NOTE:** you may only make one request at a time to this API endpoint.
        Higher concurrency will result in an error being returned.

        """

        CHILD_FETCH_COUNT = 20

        lock = None
        if c.user_is_loggedin:
            lock = g.make_lock("morechildren", "morechildren-" + c.user.name,
                               timeout=0)
            try:
                lock.acquire()
            except TimeoutExpired:
                abort(429)

        try:
            if not link or not link.subreddit_slow.can_view(c.user):
                return abort(403,'forbidden')

            if children:
                children = list(set(children))
                builder = CommentBuilder(link, CommentSortMenu.operator(sort),
                                         children=children,
                                         num=CHILD_FETCH_COUNT)
                listing = Listing(builder, nextprev = False)
                items = listing.get_items()
                def _children(cur_items):
                    items = []
                    for cm in cur_items:
                        items.append(cm)
                        if hasattr(cm, 'child'):
                            if hasattr(cm.child, 'things'):
                                items.extend(_children(cm.child.things))
                                cm.child = None
                            else:
                                items.append(cm.child)

                    return items
                # assumes there is at least one child
                # a = _children(items[0].child.things)
                a = []
                for item in items:
                    a.append(item)
                    if hasattr(item, 'child'):
                        a.extend(_children(item.child.things))
                        item.child = None

                # the result is not always sufficient to replace the
                # morechildren link
                jquery.things(str(mc_id)).remove()
                jquery.insert_things(a, append = True)
        finally:
            if lock:
                lock.release()

    @csrf_exempt
    @require_oauth2_scope("read")
    def POST_morechildren(self):
        """Wrapper around `GET_morechildren` for backwards-compatibility"""
        return self.GET_morechildren()

    @validatedForm(VUser(),
                   VModhash(),
                   code=VPrintable("code", 30))
    def POST_claimgold(self, form, jquery, code):
        status = ''
        if not code:
            c.errors.add(errors.NO_TEXT, field = "code")
            form.has_errors("code", errors.NO_TEXT)
            return

        rv = claim_gold(code, c.user._id)

        if rv is None:
            c.errors.add(errors.INVALID_CODE, field = "code")
        elif rv == "already claimed":
            c.errors.add(errors.CLAIMED_CODE, field = "code")
        else:
            days, subscr_id = rv
            if days <= 0:
                raise ValueError("days = %r?" % days)

            if subscr_id:
                c.user.gold_subscr_id = subscr_id

            if code.startswith("cr_"):
                c.user.gold_creddits += int(days / 31)
                c.user._commit()
                status = 'claimed-creddits'
            else:
                # send the user a message if they don't already have gold
                if not c.user.gold:
                    subject = "You claimed a reddit gold code!"
                    message = strings.gold_claimed_code
                    message += "\n\n" + strings.gold_benefits_msg

                    if g.lounge_reddit:
                        message += "\n\n" + strings.lounge_msg
                    message = append_random_bottlecap_phrase(message)

                    try:
                        send_system_message(c.user, subject, message,
                                            distinguished='gold-auto')
                    except MessageError:
                        g.log.error('claimgold: could not send system message')

                admintools.adjust_gold_expiration(c.user, days=days)

                status = 'claimed-gold'
                jquery(".lounge").show()

        # Activate any errors we just manually set
        if not form.has_errors("code", errors.INVALID_CODE, errors.CLAIMED_CODE,
                               errors.NO_TEXT):
            form.redirect("/gold/thanks?v=%s" % status)

    @csrf_exempt
    @validatedForm(
        VRatelimit(rate_ip=True, prefix="rate_password_"),
        user=VUserWithEmail('name'),
    )
    def POST_password(self, form, jquery, user):
        """Send password reset email."""
        def _event(error):
            email = user.email if user else None
            email_verified = bool(user.email_verified) if email else None
            g.events.login_event(
                'password_reset',
                error_msg=error,
                user_name=request.POST.get('name'),
                email=email,
                email_verified=email_verified,
                request=request,
                context=c)

        if form.has_errors('name', errors.USER_DOESNT_EXIST):
            _event(error='USER_DOESNT_EXIST')

        elif form.has_errors('name', errors.NO_EMAIL_FOR_USER):
            _event(error='NO_EMAIL_FOR_USER')

        elif form.has_errors('ratelimit', errors.RATELIMIT):
            _event(error='RATELIMIT')

        else:
            VRatelimit.ratelimit(rate_ip=True, prefix="rate_password_")
            if emailer.password_email(user):
                _event(error=None)
                form.set_text(".status",
                      _("an email will be sent to that account's address shortly"))
            else:
                _event(error='RESET_LIMIT')
                form.set_text(".status", _("try again tomorrow"))

    @csrf_exempt
    @validatedForm(token=VOneTimeToken(PasswordResetToken, "key"),
                   password=VPasswordChange(["passwd", "passwd2"]))
    def POST_resetpassword(self, form, jquery, token, password):
        # was the token invalid or has it expired?
        if not token:
            form.redirect("/password?expired=true")
            return

        # did they fill out the password form correctly?
        form.has_errors("passwd",  errors.BAD_PASSWORD)
        form.has_errors("passwd2", errors.BAD_PASSWORD_MATCH)
        if form.has_error():
            return

        # at this point, we should mark the token used since it's either
        # valid now or will never be valid again.
        token.consume()

        # load up the user and check that things haven't changed
        user = Account._by_fullname(token.user_id)
        if not token.valid_for_user(user):
            form.redirect('/password?expired=true')
            return

        # Prevent banned users from resetting, and thereby logging in
        if user._banned:
            return

        # successfully entered user name and valid new password
        change_password(user, password)
        if user.email:
            emailer.password_change_email(user)
        g.log.warning("%s did a password reset for %s via %s",
                      request.ip, user.name, token._id)

        # add this ip to the user's account so they can sign in even if
        # their account is being brute forced by a third party.
        set_account_ip(user._id, request.ip, c.start_time)

        # if the token is for the current user, their cookies will be
        # invalidated and they'll have to log in again.
        if not c.user_is_loggedin or c.user._fullname == token.user_id:
            jquery.redirect('/login')

        form.set_text(".status", _("password updated"))

    @require_oauth2_scope("subscribe")
    @noresponse(VUser(),
                VModhash(),
                action = VOneOf('action', ('sub', 'unsub')),
                sr = VSubscribeSR('sr', 'sr_name'))
    @api_doc(api_section.subreddits)
    def POST_subscribe(self, action, sr):
        """Subscribe to or unsubscribe from a subreddit.

        To subscribe, `action` should be `sub`. To unsubscribe, `action` should
        be `unsub`. The user must have access to the subreddit to be able to
        subscribe to it.

        See also: [/subreddits/mine/](#GET_subreddits_mine_{where}).

        """

        if not sr:
            return abort(404, 'not found')
        elif action == "sub" and not sr.can_view(c.user):
            return abort(403, 'permission denied')
        elif isinstance(sr, FakeSubreddit):
            return abort(403, 'permission denied')

        Subreddit.subscribe_defaults(c.user)

        if action == "sub":
            SubredditParticipationByAccount.mark_participated(c.user, sr)

            if not sr.is_subscriber(c.user):
                sr.add_subscriber(c.user)
        else:
            if sr.is_subscriber(c.user):
                sr.remove_subscriber(c.user)
            else:
                # tried to unsubscribe but user was not subscribed
                return abort(404, 'not found')
        sr.update_search_index(boost_only=True)

    @validatedForm(
        VAdmin(),
        VModhash(),
        subreddit=VByName('subreddit'),
        quarantine=VBoolean('quarantine'),
        subject=VLength('subject', 1000),
        body=VMarkdownLength('body', max_length=10000),
    )
    def POST_quarantine(self, form, jquery, subreddit, quarantine, subject, body):
        if subreddit.quarantine == quarantine:
            return

        subreddit.quarantine = quarantine
        subreddit._commit()
        system_user = Account.system_user()
        kw = dict(
            sr_id36=subreddit._id36,
            mod_id36=system_user._id36,
            action="editsettings",
            details="quarantine",
        )
        ma = ModAction(**kw)
        ma._commit()

        if config['r2.import_private']:
            from r2admin.lib.admin_utils import record_admin_event
            if quarantine:
                record_admin_event('quarantine', page="subreddit_page",
                    target_thing=subreddit)
            else:
                record_admin_event('unquarantine', page="subreddit_page",
                    target_thing=subreddit)

        if body.strip():
            send_system_message(subreddit, subject, body,
                distinguished='admin', repliable=False)

        # Refresh the CSS since images aren't allowed
        stylesheet_contents = subreddit.fetch_stylesheet_source()
        css_errors, parsed = subreddit.parse_css(stylesheet_contents)
        subreddit.change_css(stylesheet_contents, parsed, author=system_user)
        jquery.refresh()

    @require_oauth2_scope("subscribe")
    @noresponse(
        VUser(),
        VModhash(),
        sr=VSRByName('sr_name'),
    )
    def POST_quarantine_optout(self, sr):
        """Opt out from a quarantined subreddit"""
        if not sr:
            return abort(404, 'not found')
        else:
            g.events.quarantine_event('quarantine_opt_out', sr,
                request=request, context=c)
            QuarantinedSubredditOptInsByAccount.opt_out(c.user, sr)
        return self.redirect('/')

    @require_oauth2_scope("subscribe")
    @noresponse(
        VUser(),
        VModhash(),
        sr=VSRByName('sr_name'),
    )
    def POST_quarantine_optin(self, sr):
        """Opt in to a quarantined subreddit"""
        if not sr:
            return abort(404, 'not found')
        elif not c.user.email_verified:
            return abort(403, 'email not verified')
        else:
            g.events.quarantine_event('quarantine_opt_in', sr,
                request=request, context=c)
            QuarantinedSubredditOptInsByAccount.opt_in(c.user, sr)
        return self.redirect('/r/%s' % sr.name)

    @validatedForm(VAdmin(),
                   VModhash(),
                   hexkey=VLength("hexkey", max_length=32),
                   nickname=VLength("nickname", max_length = 1000),
                   status = VOneOf("status",
                      ("new", "severe", "interesting", "normal", "fixed")))
    def POST_edit_error(self, form, jquery, hexkey, nickname, status):
        if form.has_errors(("hexkey", "nickname", "status"),
                           errors.NO_TEXT, errors.INVALID_OPTION):
            pass

        if form.has_error():
            return

        key = "error_nickname-%s" % str(hexkey)
        g.hardcache.set(key, nickname, 86400 * 365)

        key = "error_status-%s" % str(hexkey)
        g.hardcache.set(key, status, 86400 * 365)

        form.set_text(".status", _('saved'))

    @validatedForm(VAdmin(),
                   VModhash(),
                   award=VByName("fullname"),
                   colliding_award=VAwardByCodename(("codename", "fullname")),
                   codename=VLength("codename", max_length = 100),
                   title=VLength("title", max_length = 100),
                   awardtype=VOneOf("awardtype",
                                    ("regular", "manual", "invisible")),
                   api_ok=VBoolean("api_ok"),
                   imgurl=VLength("imgurl", max_length = 1000))
    def POST_editaward(self, form, jquery, award, colliding_award, codename,
                       title, awardtype, api_ok, imgurl):
        if form.has_errors(("codename", "title", "awardtype", "imgurl"),
                           errors.NO_TEXT):
            pass

        if awardtype is None:
            form.set_text(".status", "bad awardtype")
            return

        if form.has_errors(("codename"), errors.INVALID_OPTION):
            form.set_text(".status", "some other award has that codename")
            pass

        url_ok = True

        if not imgurl.startswith("//"):
            url_ok = False
            form.set_text(".status", "the url must be protocol-relative")

        try:
            imgurl % 1
        except TypeError:
            url_ok = False
            form.set_text(".status", "the url must have a %d for size")

        if not url_ok:
            c.errors.add(errors.BAD_URL, field="imgurl")
            form.has_errors("imgurl", errors.BAD_URL)

        if form.has_error():
            return

        if award is None:
            Award._new(codename, title, awardtype, imgurl, api_ok)
            form.set_text(".status", "saved. reload to see it.")
            return

        award.codename = codename
        award.title = title
        award.awardtype = awardtype
        award.imgurl = imgurl
        award.api_ok = api_ok
        award._commit()
        form.set_text(".status", _('saved'))

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'),
                   VModhash(),
                   user = VFlairAccount("name"),
                   link = VFlairLink('link'),
                   text = VFlairText("text"),
                   css_class = VFlairCss("css_class"))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flair(self, form, jquery, user, link, text, css_class):
        if form.has_errors('css_class', errors.BAD_CSS_NAME):
            form.set_text(".status:first", _('invalid css class'))
            return
        if form.has_errors('css_class', errors.TOO_MUCH_FLAIR_CSS):
            form.set_text(".status:first", _('too many css classes'))
            return

        if link:
            if form.has_errors("link", errors.BAD_FLAIR_TARGET):
                return

            if not c.user_is_admin and not link.can_flair_slow(c.user):
                abort(403)

            # If the user is in timeout, don't let them set flair
            VNotInTimeout().run(action_name="editflair", details_text="set",
                target=link)

            link.set_flair(text, css_class, set_by=c.user)
        else:
            if form.has_errors("name", errors.BAD_FLAIR_TARGET):
                return

            # If the user is in timeout, don't let them set flair
            VNotInTimeout().run(action_name="editflair", details_text="set",
                target=user)

            user.set_flair(c.site, text, css_class, set_by=c.user)

            # XXX: this is still gross with all the UI code in here
            if not text and not css_class:
                jquery('#flairrow_%s' % user._id36).hide()
            elif not c.site.is_flair(user):
                jquery.redirect('?name=%s' % user.name)
                return

            wrapped_user = WrappedUser(
                user, force_show_flair=True, include_flair_selector=True)
            rendered = wrapped_user.render(style='html')
            jquery('.tagline .flairselectable.id-%s'
                % user._fullname).parent().html(rendered)
            jquery('input[name="text"]').data('saved', text)
            jquery('input[name="css_class"]').data('saved', css_class)
            form.set_text('.status', _('saved'))

    @require_oauth2_scope("modflair")
    @validatedForm(
        VSrModerator(perms='flair'),
        VModhash(),
        user=VFlairAccount("name"),
    )
    @api_doc(api_section.flair, uses_site=True)
    def POST_deleteflair(self, form, jquery, user):
        if form.has_errors('name', errors.USER_DOESNT_EXIST, errors.NO_USER):
            return

        VNotInTimeout().run(action_name="editflair", details_text="delete",
            target=user)
        user.set_flair(c.site, None, None, set_by=c.user)

        jquery('#flairrow_%s' % user._id36).remove()
        unflair = WrappedUser(
            user, include_flair_selector=True).render(style='html')
        jquery('.tagline .id-%s' % user._fullname).parent().html(unflair)

    @require_oauth2_scope("modflair")
    @validate(
        VSrModerator(perms='flair'),
        VModhash(),
        VNotInTimeout(),
        flair_csv=nop("flair_csv",
            docs={"flair_csv": "comma-seperated flair information"}),
    )
    @api_doc(api_section.flair, uses_site=True)
    def POST_flaircsv(self, flair_csv):
        """Change the flair of multiple users in the same subreddit with a
        single API call.

        Requires a string 'flair_csv' which has up to 100 lines of the form
        '`user`,`flairtext`,`cssclass`' (Lines beyond the 100th are ignored).

        If both `cssclass` and `flairtext` are the empty string for a given
        `user`, instead clears that user's flair.

        Returns an array of objects indicating if each flair setting was 
        applied, or a reason for the failure.

        """

        if not flair_csv:
            return
        
        limit = 100  # max of 100 flair settings per call
        results = FlairCsv()
        # encode to UTF-8, since csv module doesn't fully support unicode
        infile = csv.reader(flair_csv.strip().encode('utf-8').split('\n'))
        for i, row in enumerate(infile):
            line_result = results.add_line()
            line_no = i + 1
            if line_no > limit:
                line_result.error('row',
                                  'limit of %d rows per call reached' % limit)
                break

            try:
                name, text, css_class = row
            except ValueError:
                line_result.error('row', 'improperly formatted row, ignoring')
                continue

            user = VFlairAccount('name').run(name)
            if not user:
                line_result.error('user',
                                  "unable to resolve user `%s', ignoring"
                                  % name)
                continue

            orig_text = text
            text = VFlairText('text').run(orig_text)
            if text and orig_text and len(text) < len(orig_text):
                line_result.warn('text',
                                 'truncating flair text to %d chars'
                                 % len(text))

            if css_class and not VFlairCss('css_class').run(css_class):
                line_result.error('css',
                                  "invalid css class `%s', ignoring"
                                  % css_class)
                continue

            # all validation passed, enflair the user
            user.set_flair(
                c.site, text, css_class, set_by=c.user, log_details="csv")

            if text or css_class:
                mode = 'added'
            else:
                mode = 'removed'
            line_result.status = '%s flair for user %s' % (mode, user.name)
            line_result.ok = True

        return BoringPage(_("API"), content = results).render()

    @require_oauth2_scope("flair")
    @validatedForm(VUser(),
                   VModhash(),
                   flair_enabled = VBoolean("flair_enabled"))
    @api_doc(api_section.flair, uses_site=True)
    def POST_setflairenabled(self, form, jquery, flair_enabled):
        setattr(c.user, 'flair_%s_enabled' % c.site._id, flair_enabled)
        c.user._commit()
        jquery.refresh()

    @require_oauth2_scope("modflair")
    @validatedForm(
        VSrModerator(perms='flair'),
        VModhash(),
        flair_enabled=VBoolean("flair_enabled"),
        flair_position=VOneOf("flair_position", ("left", "right")),
        link_flair_position=VOneOf("link_flair_position",
            ("", "left", "right")),
        flair_self_assign_enabled=VBoolean("flair_self_assign_enabled"),
        link_flair_self_assign_enabled =
            VBoolean("link_flair_self_assign_enabled"),
        timeout=VNotInTimeout(),
    )
    @api_doc(api_section.flair, uses_site=True)
    def POST_flairconfig(self, form, jquery, flair_enabled, flair_position,
            link_flair_position, flair_self_assign_enabled,
            link_flair_self_assign_enabled, timeout):
        if c.site.flair_enabled != flair_enabled:
            c.site.flair_enabled = flair_enabled
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_enabled')
        if c.site.flair_position != flair_position:
            c.site.flair_position = flair_position
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_position')
        if c.site.link_flair_position != link_flair_position:
            c.site.link_flair_position = link_flair_position
            ModAction.create(c.site, c.user, action='editflair',
                             details='link_flair_position')
        if c.site.flair_self_assign_enabled != flair_self_assign_enabled:
            c.site.flair_self_assign_enabled = flair_self_assign_enabled
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_self_enabled')
        if (c.site.link_flair_self_assign_enabled
            != link_flair_self_assign_enabled):
            c.site.link_flair_self_assign_enabled = (
                link_flair_self_assign_enabled)
            ModAction.create(c.site, c.user, action='editflair',
                             details='link_flair_self_enabled')
        c.site._commit()
        jquery.refresh()

    @require_oauth2_scope("modflair")
    @paginated_listing(max_page_size=1000)
    @validate(
        VSrModerator(perms='flair'),
        user=VFlairAccount('name'),
    )
    @api_doc(api_section.flair, uses_site=True)
    def GET_flairlist(self, num, after, reverse, count, user):
        if user and user._deleted:
            return self.abort403()

        # Don't allow users in timeout to modify flairs
        VNotInTimeout().run(action_name="editflair", details_text="flair_list",
            target=user)

        flair = FlairList(num, after, reverse, '', user)
        return BoringPage(_("API"), content = flair).render()

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'),
                   VModhash(),
                   flair_template = VFlairTemplateByID('flair_template_id'),
                   text = VFlairText('text'),
                   css_class = VFlairCss('css_class'),
                   text_editable = VBoolean('text_editable'),
                   flair_type = VOneOf('flair_type', (USER_FLAIR, LINK_FLAIR),
                                       default=USER_FLAIR))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flairtemplate(self, form, jquery, flair_template, text,
                           css_class, text_editable, flair_type):
        if text is None:
            text = ''
        if css_class is None:
            css_class = ''

        # Check validation.
        if form.has_errors('css_class', errors.BAD_CSS_NAME):
            form.set_text(".status:first", _('invalid css class'))
            return
        if form.has_errors('css_class', errors.TOO_MUCH_FLAIR_CSS):
            form.set_text(".status:first", _('too many css classes'))
            return

        # Don't allow users in timeout to modify the flair templates
        VNotInTimeout().run(action_name="editflair",
            details_text="flair_template", target=c.site)

        # Load flair template thing.
        if flair_template:
            flair_template.text = text
            flair_template.css_class = css_class
            flair_template.text_editable = text_editable
            flair_template._commit()
            new = False
        else:
            try:
                flair_template = FlairTemplateBySubredditIndex.create_template(
                    c.site._id, text=text, css_class=css_class,
                    text_editable=text_editable,
                    flair_type=flair_type)
            except OverflowError:
                form.set_text(".status:first", _('max flair templates reached'))
                return

            new = True

        # Push changes back to client.
        if new:
            empty_ids = {
                USER_FLAIR: '#empty-user-flair-template',
                LINK_FLAIR: '#empty-link-flair-template',
            }
            empty_id = empty_ids[flair_type]
            jquery(empty_id).before(
                FlairTemplateEditor(flair_template, flair_type)
                .render(style='html'))
            empty_template = FlairTemplate()
            empty_template._committed = True  # to disable unnecessary warning
            jquery(empty_id).html(
                FlairTemplateEditor(empty_template, flair_type)
                .render(style='html'))
            form.set_text('.status', _('saved'))
        else:
            jquery('#%s' % flair_template._id).html(
                FlairTemplateEditor(flair_template, flair_type)
                .render(style='html'))
            form.set_text('.status', _('saved'))
            jquery('input[name="text"]').data('saved', text)
            jquery('input[name="css_class"]').data('saved', css_class)
        ModAction.create(c.site, c.user, action='editflair',
                             details='flair_template')

    @require_oauth2_scope("modflair")
    @validatedForm(
        VSrModerator(perms='flair'),
        VModhash(),
        flair_template=VFlairTemplateByID('flair_template_id'),
    )
    @api_doc(api_section.flair, uses_site=True)
    def POST_deleteflairtemplate(self, form, jquery, flair_template):
        if not flair_template:
            return self.abort404()

        VNotInTimeout().run(action_name="editflair",
            details_text="flair_delete_template", target=c.site)
        idx = FlairTemplateBySubredditIndex.by_sr(c.site._id)
        if idx.delete_by_id(flair_template._id):
            jquery('#%s' % flair_template._id).parent().remove()
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_delete_template')

    @require_oauth2_scope("modflair")
    @validatedForm(
        VSrModerator(perms='flair'),
        VModhash(),
        VNotInTimeout(),
        flair_type=VOneOf('flair_type', (USER_FLAIR, LINK_FLAIR),
            default=USER_FLAIR),
    )
    @api_doc(api_section.flair, uses_site=True)
    def POST_clearflairtemplates(self, form, jquery, flair_type):
        FlairTemplateBySubredditIndex.clear(c.site._id, flair_type=flair_type)
        jquery.refresh()
        ModAction.create(c.site, c.user, action='editflair',
                         details='flair_clear_template')

    @csrf_exempt
    @require_oauth2_scope("flair")
    @validate(
        VUser(),
        user=VFlairAccount('name'),
        link=VFlairLink('link'),
    )
    @api_doc(api_section.flair, uses_site=True)
    def POST_flairselector(self, user, link):
        """Return information about a users's flair options.

        If `link` is given, return link flair options.
        Otherwise, return user flair options for this subreddit.

        The logged in user's flair is also returned.
        Subreddit moderators may give a user by `name` to instead
        retrieve that user's flair.

        """

        if link:
            if not (c.user_is_admin or link.can_flair_slow(c.user)):
                abort(403)

            site = link.subreddit_slow
            user = c.user
            return FlairSelector(user, site, link).render()
        else:
            if isinstance(c.site, FakeSubreddit):
                abort(403)
            else:
                site = c.site

            if user:
                if (user != c.user and
                        not c.user_is_admin and
                        not site.is_moderator_with_perms(c.user, 'flair')):
                    abort(403)
            else:
                user = c.user

            if user._deleted:
                abort(403)

            return FlairSelector(user, site).render()

    @require_oauth2_scope("flair")
    @validatedForm(VUser(),
                   VModhash(),
                   user = VFlairAccount('name'),
                   link = VFlairLink('link'),
                   flair_template_id = nop('flair_template_id'),
                   text = VFlairText('text'))
    @api_doc(api_section.flair, uses_site=True)
    def POST_selectflair(self, form, jquery, user, link, flair_template_id,
                         text):
        if link:
            flair_type = LINK_FLAIR
            subreddit = link.subreddit_slow
            if not (c.user_is_admin or link.can_flair_slow(c.user)):
                abort(403)
        elif user:
            flair_type = USER_FLAIR
            subreddit = c.site
            if not (c.user_is_admin or user.can_flair_in_sr(c.user, subreddit)):
                abort(403)
        else:
            return self.abort404()

        if flair_template_id:
            try:
                flair_template = FlairTemplateBySubredditIndex.get_template(
                    subreddit._id, flair_template_id, flair_type=flair_type)
            except NotFound:
                # TODO: serve error to client
                g.log.debug('invalid flair template for subreddit %s', subreddit._id)
                return

            text_editable = flair_template.text_editable

            # Ignore given text if user doesn't have permission to customize it.
            if not (text_editable or
                        c.user_is_admin or
                        subreddit.is_moderator_with_perms(c.user, "flair")):
                text = None

            if not text:
                text = flair_template.text

            css_class = flair_template.css_class
        else:
            flair_template = None
            text_editable = False
            text = None
            css_class = None

        if flair_type == LINK_FLAIR:
            VNotInTimeout().run(action_name="editflair", details_text="select",
                target=link)
            link.set_flair(text, css_class, set_by=c.user)

            # XXX: gross UI code
            # Push some client-side updates back to the browser.

            jquery('.id-%s' % link._fullname).removeLinkFlairClass()
            jquery('.id-%s .entry .linkflairlabel' % link._fullname).remove()
            title_path = '.id-%s .entry > .title > .title' % link._fullname

            # TODO: move this to a template
            if flair_template:
                classes = ' '.join('linkflair-' + c for c in css_class.split())
                jquery('.id-%s' % link._fullname).addClass('linkflair').addClass(classes)
                flair = format_html('<span class="linkflairlabel">%s</span>', text)

                if subreddit.link_flair_position == 'left':
                    jquery(title_path).before(flair)
                elif subreddit.link_flair_position == 'right':
                    jquery(title_path).after(flair)

            # TODO: close the selector popup more gracefully
            jquery('body').click()
        else:
            VNotInTimeout().run(action_name="editflair", details_text="select",
                target=user)
            user.set_flair(subreddit, text, css_class, set_by=c.user)

            # XXX: gross UI code
            # Push some client-side updates back to the browser.
            u = WrappedUser(user, force_show_flair=True,
                            flair_text_editable=text_editable,
                            include_flair_selector=True)
            flair = u.render(style='html')
            jquery('.tagline .flairselectable.id-%s'
                % user._fullname).parent().html(flair)
            jquery('#flairrow_%s input[name="text"]' % user._id36).data(
                'saved', text).val(text)
            jquery('#flairrow_%s input[name="css_class"]' % user._id36).data(
                'saved', css_class).val(css_class)

    @validatedForm(
        VUser(),
        VModhash(),
        sr_style_enabled=VBoolean("sr_style_enabled")
    )
    def POST_set_sr_style_enabled(self, form, jquery, sr_style_enabled):
        """Update enabling of individual sr themes; refresh the page style"""
        if feature.is_enabled('stylesheets_everywhere'):
            c.user.set_subreddit_style(c.site, sr_style_enabled)
            c.can_apply_styles = True
            sr = DefaultSR()

            if sr_style_enabled:
                sr = c.site
            elif (c.user.pref_default_theme_sr and
                    feature.is_enabled('stylesheets_everywhere')):
                sr = Subreddit._by_name(c.user.pref_default_theme_sr)
                if (not sr.can_view(c.user) or
                        not c.user.pref_enable_default_themes):
                    sr = DefaultSR()
            sr_stylesheet_url = Reddit.get_subreddit_stylesheet_url(sr)
            if not sr_stylesheet_url:
                sr_stylesheet_url = ""
                c.can_apply_styles = False

            jquery.apply_stylesheet_url(sr_stylesheet_url, sr_style_enabled)

            if not sr.header or header_url(sr.header) == g.default_header_url:
                jquery.remove_header_image();
            else:
                jquery.apply_header_image(header_url(sr.header),
                    sr.header_size, sr.header_title)

    @validatedForm(secret_used=VAdminOrAdminSecret("secret"),
                   award=VByName("fullname"),
                   description=VLength("description", max_length=1000),
                   url=VLength("url", max_length=1000),
                   recipient=VExistingUname("recipient"))
    def POST_givetrophy(self, form, jquery, secret_used, award, description,
                        url, recipient):
        if form.has_errors("recipient", errors.USER_DOESNT_EXIST,
                                        errors.NO_USER):
            pass

        if form.has_errors("fullname", errors.NO_TEXT, errors.NO_THING_ID):
            pass

        if secret_used and not award.api_ok:
            c.errors.add(errors.NO_API, field='secret')
            form.has_errors('secret', errors.NO_API)

        if form.has_error():
            return

        t = Trophy._new(recipient, award, description=description, url=url)

        form.set_text(".status", _('saved'))
        form._send_data(trophy_fn=t._id36)

    @validatedForm(secret_used=VAdminOrAdminSecret("secret"),
                   trophy = VTrophy("trophy_fn"))
    def POST_removetrophy(self, form, jquery, secret_used, trophy):
        if not trophy:
            return self.abort404()
        recipient = trophy._thing1
        award = trophy._thing2
        if secret_used and not award.api_ok:
            c.errors.add(errors.NO_API, field='secret')
            form.has_errors('secret', errors.NO_API)
        
        if form.has_error():
            return

        trophy._delete()
        Trophy.by_account(recipient, _update=True)
        Trophy.by_award(award, _update=True)

    @validatedForm(
        VAdmin(),
        VModhash(),
        recipient=VExistingUname("recipient"),
        num_creddits=VInt('num_creddits', num_default=0),
    )
    def POST_givecreddits(self, form, jquery, recipient, num_creddits):
        if form.has_errors("recipient",
                           errors.USER_DOESNT_EXIST, errors.NO_USER):
            return

        with creddits_lock(recipient):
            recipient.gold_creddits += num_creddits
            # make sure it doesn't go into the negative
            recipient.gold_creddits = max(0, recipient.gold_creddits)
            recipient._commit()

        form.set_text(".status", _('saved'))

    @validatedForm(
        VAdmin(),
        VModhash(),
        recipient=VExistingUname("recipient"),
        num_months=VInt('num_months', num_default=0),
    )
    def POST_givegold(self, form, jquery, recipient, num_months):
        if form.has_errors("recipient",
                           errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        
        if not recipient.gold and num_months < 0:
            form.set_text(".status", _('no gold to take'))
            return

        admintools.adjust_gold_expiration(recipient, months=num_months)
        form.set_text(".status", _('saved'))

    @noresponse(VUser(),
                VModhash(),
                ui_elem=VOneOf('id', ('organic',)))
    def POST_disable_ui(self, ui_elem):
        if ui_elem:
            pref = "pref_%s" % ui_elem
            if getattr(c.user, pref):
                setattr(c.user, "pref_" + ui_elem, False)
                c.user._commit()

    @noresponse(VUser(),
                VModhash(),
                show_nsfw_media=VBoolean("show_nsfw_media"))
    def POST_set_nsfw_media_pref(self, show_nsfw_media):
        changed = False

        if show_nsfw_media is not None:
            no_profanity = not show_nsfw_media
            if c.user.pref_no_profanity != no_profanity:
                c.user.pref_no_profanity = no_profanity
                changed = True

        if not c.user.nsfw_media_acknowledged:
            c.user.nsfw_media_acknowledged = True
            changed = True

        if changed:
            c.user._commit()

    @validatedForm(type = VOneOf('type', ('click'), default = 'click'),
                   links = VByName('ids', thing_cls = Link, multiple = True))
    def GET_gadget(self, form, jquery, type, links):
        if not links and type == 'click':
            # malformed cookie, clear it out
            set_user_cookie('recentclicks2', '')

        if not links:
            return

        content = ClickGadget(links).make_content()

        jquery('.gadget').show().find('.click-gadget').html(
            spaceCompress(content))

    @csrf_exempt
    @require_oauth2_scope("read")
    @json_validate(query=VPrintable('query', max_length=50),
                   include_over_18=VBoolean('include_over_18', default=True),
                   exact=VBoolean('exact', default=False))
    @api_doc(api_section.subreddits)
    def POST_search_reddit_names(self, responder, query, include_over_18, exact):
        """List subreddit names that begin with a query string.

        Subreddits whose names begin with `query` will be returned. If
        `include_over_18` is false, subreddits with over-18 content
        restrictions will be filtered from the results.

        If `exact` is true, only an exact match will be returned.
        """
        if query:
            query = sr_path_rx.sub('\g<name>', query.strip())

        names = []
        if query and exact:
            try:
                sr = Subreddit._by_name(query.strip())
            except NotFound:
                self.abort404()
            else:
                # not respecting include_over_18 for exact match
                names = [sr.name]
        elif query:
            names = search_reddits(query, include_over_18)

        return {'names': names}

    @validate(link=VByName('link_id', thing_cls=Link))
    def GET_expando(self, link):
        if not link:
            abort(404, 'not found')

        # pass through wrap_links/IDBuilder to ensure the user can view the link
        listing = wrap_links(link)
        try:
            wrapped_link = listing.things[0]
        except IndexError:
            wrapped_link = None

        if wrapped_link and wrapped_link.link_child:
            content = wrapped_link.link_child.content()
            return websafe(spaceCompress(content))
        else:
            abort(404, 'not found')

    @csrf_exempt
    def POST_expando(self):
        return self.GET_expando()

    @validatedForm(
        VUser(),
        VModhash(),
        VVerifyPassword("password", fatal=False),
        VOneTimePassword("otp",
                         required=not g.disable_require_admin_otp),
        remember=VBoolean("remember"),
        dest=VDestination(),
    )
    def POST_adminon(self, form, jquery, remember, dest):
        if c.user.name not in g.admins:
            self.abort403()

        if form.has_errors('password', errors.WRONG_PASSWORD):
            return

        if form.has_errors("otp", errors.WRONG_PASSWORD,
                                  errors.NO_OTP_SECRET,
                                  errors.RATELIMIT):
            return

        if remember:
            self.remember_otp(c.user)

        self.enable_admin_mode(c.user)
        form.redirect(dest)

    @validatedForm(
        VUser(),
        VModhash(),
        VVerifyPassword("password", fatal=False),
    )
    def POST_generate_otp_secret(self, form, jquery):
        if form.has_errors("password", errors.WRONG_PASSWORD):
            return

        if c.user.otp_secret:
            c.errors.add(errors.OTP_ALREADY_ENABLED, field="password")
            form.has_errors("password", errors.OTP_ALREADY_ENABLED)
            return

        secret = totp.generate_secret()
        g.gencache.set("otp:secret_" + c.user._id36, secret, time=300)
        jquery("body").make_totp_qrcode(secret)

    @validatedForm(VUser(),
                   VModhash(),
                   otp=nop("otp"))
    def POST_enable_otp(self, form, jquery, otp):
        if form.has_errors("password", errors.WRONG_PASSWORD):
            return

        if c.user.otp_secret:
            c.errors.add(errors.OTP_ALREADY_ENABLED, field="otp")
            form.has_errors("otp", errors.OTP_ALREADY_ENABLED)
            return

        secret = g.gencache.get("otp:secret_" + c.user._id36)
        if not secret:
            c.errors.add(errors.EXPIRED, field="otp")
            form.has_errors("otp", errors.EXPIRED)
            return

        if not VOneTimePassword.validate_otp(secret, otp):
            c.errors.add(errors.WRONG_PASSWORD, field="otp")
            form.has_errors("otp", errors.WRONG_PASSWORD)
            return

        c.user.otp_secret = secret
        c.user._commit()

        form.redirect("/prefs/security")

    @validatedForm(
        VUser(),
        VModhash(),
        VVerifyPassword("password", fatal=False),
        VOneTimePassword("otp", required=True),
    )
    def POST_disable_otp(self, form, jquery):
        if form.has_errors("password", errors.WRONG_PASSWORD):
            return

        if form.has_errors("otp", errors.WRONG_PASSWORD,
                                  errors.NO_OTP_SECRET,
                                  errors.RATELIMIT):
            return

        c.user.otp_secret = ""
        c.user._commit()
        form.redirect("/prefs/security")

    @require_oauth2_scope("read")
    @json_validate(query=VLength("query", max_length=50))
    @api_doc(api_section.subreddits)
    def GET_subreddits_by_topic(self, responder, query):
        """Return a list of subreddits that are relevant to a search query."""
        if not g.CLOUDSEARCH_SEARCH_API:
            return []

        query = query and query.strip()
        if not query or len(query) < 2:
            return []

        # http://en.wikipedia.org/wiki/Most_common_words_in_English
        common_english_words = {
            'the', 'be', 'to', 'of', 'and', 'in', 'that', 'have', 'it', 'for',
            'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but',
            'his', 'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or', 'an',
            'will', 'my', 'one', 'all', 'would', 'there', 'their', 'what', 'so',
            'up', 'out', 'if', 'about', 'who', 'get', 'which', 'go', 'me',
            'when', 'make', 'can', 'like', 'time', 'no', 'just', 'him', 'know',
            'take', 'people', 'into', 'year', 'your', 'good', 'some', 'could',
            'them', 'see', 'other', 'than', 'then', 'now', 'look', 'only',
            'come', 'its', 'over', 'think', 'also', 'back', 'after', 'use',
            'two', 'how', 'our', 'work', 'first', 'well', 'way', 'even', 'new',
            'want', 'because', 'any', 'these', 'give', 'day', 'most', 'us',
        }

        if query.lower() in common_english_words:
            return []

        exclude = Subreddit.default_subreddits()

        faceting = {"reddit":{"sort":"-sum(text_relevance)", "count":20}}
        try:
            results = g.search.SearchQuery(query, sort="relevance",
                                           faceting=faceting, num=0,
                                           syntax="plain").run()
        except g.search.SearchException:
            abort(500)

        sr_results = []
        for sr, count in results.subreddit_facets:
            if (sr._id in exclude or (sr.over_18 and not c.over18)
                  or sr.type == "archived"):
                continue

            sr_results.append({
                "name": sr.name,
            })

        return sr_results

    @noresponse(VUser(),
                VModhash(),
                client=VOAuth2ClientID())
    def POST_revokeapp(self, client):
        if client:
            client.revoke(c.user)

    @validatedForm(VUser(),
                   VModhash(),
                   name=VRequired('name', errors.NO_TEXT,
                                  docs=dict(name="a name for the app")),
                   about_url=VSanitizedUrl('about_url'),
                   icon_url=VSanitizedUrl('icon_url'),
                   redirect_uri=VRedirectUri('redirect_uri'),
                   app_type=VOneOf('app_type', OAuth2Client.APP_TYPES))
    def POST_updateapp(self, form, jquery, name, about_url, icon_url,
                       redirect_uri, app_type):
        if (form.has_errors('name', errors.NO_TEXT) |
            form.has_errors('redirect_uri', errors.BAD_URL) |
            form.has_errors('redirect_uri', errors.NO_URL) |
            form.has_errors('app_type', errors.INVALID_OPTION)):
            return

        # Web apps should be redirecting to web
        if app_type == 'web':
            parsed = urlparse(redirect_uri)
            if parsed.scheme not in ('http', 'https'):
                c.errors.add(errors.INVALID_SCHEME, field='redirect_uri',
                        msg_params={"schemes": "http, https"})
                form.has_errors('redirect_uri', errors.INVALID_SCHEME)
                return

        description = request.POST.get('description', '')

        client_id = request.POST.get('client_id')
        if client_id:
            # client_id was specified, updating existing OAuth2Client
            client = OAuth2Client.get_token(client_id)
            if client.is_first_party() and not c.user_is_admin:
                form.set_text('.status', _('this app can not be modified from this interface'))
                return
            if app_type != client.app_type:
                # App type cannot be changed after creation
                abort(400, "invalid request")
                return
            if not client:
                form.set_text('.status', _('invalid client id'))
                return
            if client.deleted:
                form.set_text('.status', _('cannot update deleted app'))
                return
            if not client.has_developer(c.user):
                form.set_text('.status', _('app does not belong to you'))
                return

            client.name = name
            client.description = description
            client.about_url = about_url or ''
            client.redirect_uri = redirect_uri
            client._commit()
            form.set_text('.status', _('application updated'))
            apps = PrefApps([], [client])
            jquery('#developed-app-%s' % client._id).replaceWith(
                apps.render_developed_app(client, collapsed=False))
        else:
            # client_id was omitted or empty, creating new OAuth2Client
            client = OAuth2Client._new(name=name,
                                       description=description,
                                       about_url=about_url or '',
                                       redirect_uri=redirect_uri,
                                       app_type=app_type)
            client._commit()
            client.add_developer(c.user)
            form.set_text('.status', _('application created'))
            apps = PrefApps([], [client])
            jquery('#developed-apps > h1').show()
            jquery('#developed-apps > ul').append(
                apps.render_developed_app(client, collapsed=False))

    @validatedForm(VUser(),
                   VModhash(),
                   client=VOAuth2ClientDeveloper(),
                   account=VExistingUname('name'))
    def POST_adddeveloper(self, form, jquery, client, account):
        if not client:
            return
        if form.has_errors('name', errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        if client.is_first_party() and not c.user_is_admin:
            c.errors.add(errors.DEVELOPER_FIRST_PARTY_APP, field='name')
            form.set_error(errors.DEVELOPER_FIRST_PARTY_APP, 'name')
            return
        if ((account.employee or account == Account.system_user()) and
           not c.user_is_admin):
            c.errors.add(errors.DEVELOPER_PRIVILEGED_ACCOUNT, field='name')
            form.set_error(errors.DEVELOPER_PRIVILEGED_ACCOUNT, 'name')
            return
        if client.has_developer(account):
            c.errors.add(errors.DEVELOPER_ALREADY_ADDED, field='name')
            form.set_error(errors.DEVELOPER_ALREADY_ADDED, 'name')
            return
        try:
            client.add_developer(account)
        except OverflowError:
            c.errors.add(errors.TOO_MANY_DEVELOPERS, field='')
            form.set_error(errors.TOO_MANY_DEVELOPERS, '')
            return

        form.set_text('.status', _('developer added'))
        apps = PrefApps([], [client])
        (jquery('#app-developer-%s input[name="name"]' % client._id).val('')
            .closest('.prefright').find('ul').append(
                apps.render_editable_developer(client, account)))

    @validatedForm(VUser(),
                   VModhash(),
                   client=VOAuth2ClientDeveloper(),
                   account=VExistingUname('name'))
    def POST_removedeveloper(self, form, jquery, client, account):
        if client.is_first_party() and not c.user_is_admin:
            c.errors.add(errors.DEVELOPER_FIRST_PARTY_APP, field='name')
            form.set_error(errors.DEVELOPER_FIRST_PARTY_APP, 'name')
            return
        if client and account and not form.has_errors('name'):
            client.remove_developer(account)
            if account._id == c.user._id:
                jquery('#developed-app-%s' % client._id).fadeOut()
            else:
                jquery('li#app-dev-%s-%s' % (client._id, account._id)).fadeOut()

    @noresponse(VUser(),
                VModhash(),
                client=VOAuth2ClientDeveloper())
    def POST_deleteapp(self, client):
        if client:
            client.deleted = True
            client._commit()

    @validatedMultipartForm(VUser(),
                            VModhash(),
                            client=VOAuth2ClientDeveloper(),
                            icon_file=VUploadLength(
                                'file', max_length=1024*128,
                                docs=dict(file="an icon (72x72)")))
    def POST_setappicon(self, form, jquery, client, icon_file):
        if not icon_file:
            form.set_error(errors.TOO_LONG, 'file')
        if not form.has_error():
            try:
                client.icon_url = media.upload_icon(icon_file, (72, 72))
            except IOError, ex:
                c.errors.add(errors.BAD_IMAGE,
                             msg_params=dict(message=ex.message),
                             field='file')
                form.set_error(errors.BAD_IMAGE, 'file')
            else:
                client._commit()
                form.set_text('.status', 'uploaded')
                jquery('#developed-app-%s .app-icon img'
                       % client._id).attr('src', make_url_protocol_relative(client.icon_url))
                jquery('#developed-app-%s .ajax-upload-form'
                       % client._id).hide()
                jquery('#developed-app-%s .edit-app-icon-button'
                       % client._id).toggleClass('collapsed')

    @json_validate(
        VUser(),
        VModhash(),
        thing=VByName("thing"),
        signed=VBoolean("signed")
    )
    def POST_generate_payment_blob(self, responder, thing, signed):
        if not thing:
            abort(400, "Bad Request")

        if thing._deleted:
            abort(403, "Forbidden")

        thing_sr = Subreddit._byID(thing.sr_id, data=True)
        if (not thing_sr.can_view(c.user) or
            not thing_sr.allow_gilding):
            abort(403, "Forbidden")

        try:
            recipient = Account._byID(thing.author_id, data=True)
        except NotFound:
            self.abort404()

        if recipient._deleted:
            self.abort404()

        VNotInTimeout().run(action_name="gild", target=thing)

        return generate_blob(dict(
            goldtype="gift",
            account_id=c.user._id,
            account_name=c.user.name,
            status="initialized",
            signed=signed,
            recipient=recipient.name,
            giftmessage=None,
            thing=thing._fullname,
        ))

    @json_validate(
        VUser(),
        VModhash(),
        code=nop("code"),
        signed=VBoolean("signed", default=False),
        message=nop("message")
    )
    def POST_modify_payment_blob(self, responder, code, signed, message):
        if c.user.gild_reveal_username != signed:
            c.user.gild_reveal_username = signed
            c.user._commit()

        updates = {}
        updates["signed"] = signed
        if message and message.strip() != "":
            updates["giftmessage"] = _force_utf8(message)

        update_blob(str(code), updates)

    def OPTIONS_request_promo(self):
        """Send CORS headers for request_promo requests."""
        if "Origin" in request.headers:
            origin = request.headers["Origin"]
            response.headers["Access-Control-Allow-Origin"] = "*"
            response.headers["Access-Control-Allow-Methods"] = "POST"
            response.headers["Access-Control-Allow-Headers"] = "Authorization, "
            response.headers["Access-Control-Allow-Credentials"] = "false"
            response.headers['Access-Control-Expose-Headers'] = \
                self.COMMON_REDDIT_HEADERS

    @csrf_exempt
    @validate(srnames=VPrintable("srnames", max_length=2100))
    def POST_request_promo(self, srnames):
        self.OPTIONS_request_promo()

        if not srnames:
            return

        srnames = srnames.split('+')
        try:
            srnames.remove(Frontpage.name)
            srnames.append('')
        except ValueError:
            pass

        promo_tuples = promote.lottery_promoted_links(srnames, n=10)
        builder = CampaignBuilder(promo_tuples,
                                  wrap=default_thing_wrapper(),
                                  keep_fn=promote.promo_keep_fn,
                                  num=1,
                                  skip=True)
        listing = LinkListing(builder, nextprev=False).listing()
        promote.add_trackers(listing.things, c.site)
        promote.update_served(listing.things)
        if listing.things:
            w = listing.things[0]
            w.num = ""
            return responsive(w.render(), space_compress=True)

    @json_validate(
        VUser(),
        VModhash(),
        collapsed=VBoolean('collapsed'),
    )
    def POST_set_left_bar_collapsed(self, responder, collapsed):
        c.user.pref_collapse_left_bar = collapsed
        c.user._commit()

    @require_oauth2_scope("read")
    @validate(srs=VSRByNames("srnames"),
              to_omit=VSRByNames("omit", required=False))
    @api_doc(api_section.subreddits, uri='/api/recommend/sr/{srnames}')
    def GET_subreddit_recommendations(self, srs, to_omit):
        """Return subreddits recommended for the given subreddit(s).

        Gets a list of subreddits recommended for `srnames`, filtering out any
        that appear in the optional `omit` param.

        """

        srs = [sr for sr in srs.values() if not isinstance(sr, FakeSubreddit)]
        to_omit = [sr for sr in to_omit.values() if not isinstance(sr, FakeSubreddit)]

        omit_id36s = [sr._id36 for sr in to_omit]
        rec_srs = recommender.get_recommendations(srs, to_omit=omit_id36s)
        sr_data = [{'sr_name': sr.name} for sr in rec_srs]
        return json.dumps(sr_data)


    @validatedForm(VUser(),
                   VModhash(),
                   action=VOneOf("type", FEEDBACK_ACTIONS),
                   srs=VSRByNames("srnames"))
    def POST_rec_feedback(self, form, jquery, action, srs):
        if form.has_errors("type", errors.INVALID_OPTION):
            return self.abort404()
        AccountSRFeedback.record_feedback(c.user, srs.values(), action)


    @validatedForm(
        VUser(),
        VModhash(),
        seconds_visibility=VOneOf(
            "seconds_visibility",
            ("public", "private"),
            default="private",
        ),
    )
    def POST_server_seconds_visibility(self, form, jquery, seconds_visibility):
        c.user.pref_public_server_seconds = seconds_visibility == "public"
        c.user._commit()

        hook = hooks.get_hook("server_seconds_visibility.change")
        hook.call(user=c.user, value=c.user.pref_public_server_seconds)

    @require_oauth2_scope("save")
    @noresponse(VGold(),
                VModhash(),
                links = VByName('links', thing_cls=Link, multiple=True,
                                limit=100))
    @api_doc(api_section.links_and_comments)
    def POST_store_visits(self, links):
        if not c.user.pref_store_visits or not links:
            return

        LinkVisitsByAccount._visit(c.user, links)

    @validatedForm(
        VAdmin(),
        VModhash(),
        system=VLength('system', 1024),
        subject=VLength('subject', 1024),
        note=VLength('note', 10000),
        author=VLength('author', 1024),
    )
    def POST_add_admin_note(self, form, jquery, system, subject, note, author):
        if form.has_errors(('system', 'subject', 'note', 'author'),
                           errors.TOO_LONG):
            return

        if note:
            from r2.models.admin_notes import AdminNotesBySystem
            AdminNotesBySystem.add(system, subject, note, author)
        form.refresh()

    @require_oauth2_scope("modconfig")
    @validatedForm(
        VSrModerator(perms="config"),
        VModhash(),
        short_name=VAvailableSubredditRuleName("short_name"),
        description=VMarkdownLength("description", max_length=500),
        kind=VOneOf('kind', ['link', 'comment', 'all']),
    )
    def POST_add_subreddit_rule(self, form, jquery, short_name, description,
            kind):
        if not feature.is_enabled("subreddit_rules", subreddit=c.site.name):
            abort(404)
        if form.has_errors("short_name", errors.TOO_SHORT, errors.NO_TEXT,
                errors.TOO_LONG, errors.SR_RULE_EXISTS, errors.SR_RULE_TOO_MANY):
            return
        if form.has_errors("description", errors.TOO_LONG):
            return
        if form.has_errors("kind", errors.INVALID_OPTION):
            return

        SubredditRules.create(c.site, short_name, description, kind)
        ModAction.create(c.site, c.user, 'createrule', details=short_name)

        if description:
            description_html = safemarkdown(description, wrap=False)
            form._send_data(description_html=description_html)

        form.refresh()

    @require_oauth2_scope("modconfig")
    @validatedForm(
        VSrModerator(perms="config"),
        VModhash(),
        rule=VSubredditRule("old_short_name"),
        short_name=VAvailableSubredditRuleName("short_name", updating=True),
        description=VMarkdownLength('description', max_length=500),
        kind=VOneOf('kind', ['link', 'comment', 'all']),
    )
    def POST_update_subreddit_rule(self, form, jquery, rule,
            short_name, description, kind):
        if not feature.is_enabled("subreddit_rules", subreddit=c.site.name):
            abort(404)
        if form.has_errors("old_short_name", errors.SR_RULE_DOESNT_EXIST):
            return
        if form.has_errors("short_name", errors.TOO_SHORT, errors.NO_TEXT,
                errors.TOO_LONG, errors.SR_RULE_TOO_MANY):
            return
        # if the short_name is changing and the new short_name already exists
        if rule["short_name"] != short_name:
            if form.has_errors("short_name", errors.SR_RULE_EXISTS):
                return
        if form.has_errors("description", errors.TOO_LONG):
            return
        if form.has_errors("kind", errors.INVALID_OPTION):
            return

        SubredditRules.update(c.site, rule["short_name"], short_name,
            description, kind)
        ModAction.create(c.site, c.user, 'editrule', details=short_name)

        if description:
            description_html = safemarkdown(description, wrap=False)
            form._send_data(description_html=description_html)

        form.refresh()

    @require_oauth2_scope("modconfig")
    @validatedForm(
        VSrModerator(perms="config"),
        VModhash(),
        rule=VSubredditRule("short_name"),
    )
    def POST_remove_subreddit_rule(self, form, jquery, rule):
        if not feature.is_enabled("subreddit_rules", subreddit=c.site.name):
            abort(404)
        if form.has_errors("rule", errors.SR_RULE_DOESNT_EXIST):
            return
        short_name = rule["short_name"]
        SubredditRules.remove_rule(c.site, short_name)
        ModAction.create(c.site, c.user, 'deleterule', details=short_name)
        form.refresh()

    @validatedForm(
        VUser(),
        VModhash(),
        thing=VByName('thing'),
    )
    def GET_report_form(self, form, jquery, thing):
        """Return information about report reasons for the thing."""
        if feature.is_enabled("new_report_dialog"):
            if request.params.get("api_type") == "json":
                style = "json"
                filter_by_kind = False
            else:
                style = "html"
                filter_by_kind = True

            report_form = SubredditReportForm(thing, filter_by_kind=filter_by_kind)

            if style == "json":
                form._send_data(
                    rules=report_form.rules,
                    sr_name=report_form.sr_name,
                )
            return report_form.render(style=style)
        else:
            return ReportForm(thing).render(style="html")
        abort(404, 'not found')


    @validatedForm(VModhashIfLoggedIn())
    def POST_hide_locationbar(self, form, jquery):
        c.user.pref_hide_locationbar = True
        c.user._commit()
        jquery(".locationbar").hide()

    @validatedForm(VModhashIfLoggedIn())
    def POST_use_global_defaults(self, form, jquery):
        c.user.pref_use_global_defaults = True
        c.user._commit()
        jquery.refresh()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import hmac

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.controllers.util import abort

from r2.controllers.reddit_base import MinimalController
from r2.lib.pages import MediaEmbedBody
from r2.lib.media import get_media_embed
from r2.lib.utils import constant_time_compare
from r2.lib.validator import validate, VLink, nop
from r2.models import Subreddit


class MediaembedController(MinimalController):
    @validate(
        link=VLink('link'),
        credentials=nop('credentials'),
    )
    def GET_mediaembed(self, link, credentials):
        if request.host != g.media_domain:
            # don't serve up untrusted content except on our
            # specifically untrusted domain
            abort(404)

        if link.subreddit_slow.type in Subreddit.private_types:
            expected_mac = hmac.new(g.secrets["media_embed"], link._id36,
                                    hashlib.sha1).hexdigest()
            if not constant_time_compare(credentials or "", expected_mac):
                abort(404)

        if not c.secure:
            media_object = link.media_object
        else:
            media_object = link.secure_media_object

        if not media_object:
            abort(404)
        elif isinstance(media_object, dict):
            # otherwise it's the new style, which is a dict(type=type, **args)
            media_embed = get_media_embed(media_object)
            content = media_embed.content

        c.allow_framing = True

        return MediaEmbedBody(body = content).render()


class AdController(MinimalController):
    def GET_ad(self):
        return "This is a placeholder ad."
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
from urllib import urlencode
import base64
import simplejson

from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.config.extensions import set_extension
from r2.lib.base import abort
from reddit_base import RedditController, MinimalController, require_https
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import NotFound
from r2.lib.pages import RedditError
from r2.lib.strings import strings
from r2.models import Account, admintools, create_gift_gold, send_system_message
from r2.models.token import (
    OAuth2Client, OAuth2AuthorizationCode, OAuth2AccessToken,
    OAuth2RefreshToken, OAuth2Scope)
from r2.lib.errors import BadRequestError, ForbiddenError, errors
from r2.lib.pages import OAuth2AuthorizationPage
from r2.lib.require import RequirementException, require, require_split
from r2.lib.utils import constant_time_compare, parse_http_basic, UrlParser
from r2.lib.validator import (
    nop,
    validate,
    VRequired,
    VThrottledLogin,
    VOneOf,
    VUser,
    VModhash,
    VOAuth2ClientID,
    VOAuth2Scope,
    VOAuth2RefreshToken,
    VRatelimit,
    VLength,
)


def _update_redirect_uri(base_redirect_uri, params, as_fragment=False):
    parsed = UrlParser(base_redirect_uri)
    if as_fragment:
        parsed.fragment = urlencode(params)
    else:
        parsed.update_query(**params)
    return parsed.unparse()


def get_device_id(client):
    if client.is_first_party():
        return request.POST.get('device_id')


class OAuth2FrontendController(RedditController):
    def check_for_bearer_token(self):
        pass

    def pre(self):
        RedditController.pre(self)
        require_https()

    def _abort_oauth_error(self, error):
        g.stats.simple_event('oauth2.errors.%s' % error)
        abort(BadRequestError(error))

    def _check_redirect_uri(self, client, redirect_uri):
        if (errors.OAUTH2_INVALID_CLIENT, 'client_id') in c.errors:
            self._abort_oauth_error(errors.OAUTH2_INVALID_CLIENT)

        if not redirect_uri or redirect_uri != client.redirect_uri:
            self._abort_oauth_error(errors.OAUTH2_INVALID_REDIRECT_URI)

    def _check_response_type_and_scope(self, response_type, scope):
        if (errors.INVALID_OPTION, 'response_type') in c.errors:
            self._abort_oauth_error(errors.OAUTH2_INVALID_RESPONSE_TYPE)

        if (errors.OAUTH2_INVALID_SCOPE, 'scope') in c.errors:
            self._abort_oauth_error(errors.OAUTH2_INVALID_SCOPE)

    def _check_client_type_and_duration(self, response_type, client, duration):
        if response_type == "token" and client.is_confidential():
            # Prevent "confidential" clients from distributing tokens
            # in a non-confidential manner
            self._abort_oauth_error(errors.OAUTH2_CONFIDENTIAL_TOKEN)

        if response_type == "token" and duration != "temporary":
            # implicit grant -> No refresh tokens allowed
            self._abort_oauth_error(errors.OAUTH2_NO_REFRESH_TOKENS_ALLOWED)

    def _error_response(self, state, redirect_uri, as_fragment=False):
        """Return an error redirect."""
        resp = {"state": state}

        if (errors.OAUTH2_ACCESS_DENIED, "authorize") in c.errors:
            resp["error"] = "access_denied"
        elif (errors.INVALID_MODHASH, None) in c.errors:
            resp["error"] = "access_denied"
        else:
            resp["error"] = "invalid_request"

        final_redirect = _update_redirect_uri(redirect_uri, resp, as_fragment)
        return self.redirect(final_redirect, code=302)

    def _check_employee_grants(self, client, scope):
        if not c.user.employee or not client or not scope:
            return
        if client._id in g.employee_approved_clients:
            return
        if client._id in g.mobile_auth_allowed_clients:
            return
        # The identity scope doesn't leak much, and we don't mind if employees
        # prove their identity to some external service
        if scope.scopes == {"identity"}:
            return
        error_page = RedditError(
            title=_('this app has not been approved for use with employee accounts'),
            message="",
        )
        request.environ["usable_error_content"] = error_page.render()
        self.abort403()

    @validate(VUser(),
              response_type = VOneOf("response_type", ("code", "token")),
              client = VOAuth2ClientID(),
              redirect_uri = VRequired("redirect_uri", errors.OAUTH2_INVALID_REDIRECT_URI),
              scope = VOAuth2Scope(),
              state = VRequired("state", errors.NO_TEXT),
              duration = VOneOf("duration", ("temporary", "permanent"),
                                default="temporary"))
    def GET_authorize(self, response_type, client, redirect_uri, scope, state,
                      duration):
        """
        First step in [OAuth 2.0](http://oauth.net/2/) authentication.
        End users will be prompted for their credentials (username/password)
        and asked if they wish to authorize the application identified by
        the **client_id** parameter with the permissions specified by the
        **scope** parameter.  They are then redirected to the endpoint on
        the client application's side specified by **redirect_uri**.

        If the user granted permission to the application, the response will
        contain a **code** parameter with a temporary authorization code
        which can be exchanged for an access token at
        [/api/v1/access_token](#api_method_access_token).

        **redirect_uri** must match the URI configured for the client in the
        [app preferences](/prefs/apps).  All errors will show a 400 error
        page along with some information on what option was wrong.
        """
        self._check_employee_grants(client, scope)

        # Check redirect URI first; it will ensure client exists
        self._check_redirect_uri(client, redirect_uri)

        self._check_response_type_and_scope(response_type, scope)

        self._check_client_type_and_duration(response_type, client, duration)

        if not c.errors:
            return OAuth2AuthorizationPage(client, redirect_uri, scope, state,
                                           duration, response_type).render()
        else:
            self._abort_oauth_error(errors.INVALID_OPTION)

    @validate(VUser(),
              VModhash(fatal=False),
              client = VOAuth2ClientID(),
              redirect_uri = VRequired("redirect_uri", errors.OAUTH2_INVALID_REDIRECT_URI),
              scope = VOAuth2Scope(),
              state = VRequired("state", errors.NO_TEXT),
              duration = VOneOf("duration", ("temporary", "permanent"),
                                default="temporary"),
              authorize = VRequired("authorize", errors.OAUTH2_ACCESS_DENIED),
              response_type = VOneOf("response_type", ("code", "token"),
                                     default="code"))
    def POST_authorize(self, authorize, client, redirect_uri, scope, state,
                       duration, response_type):
        """Endpoint for OAuth2 authorization."""

        self._check_employee_grants(client, scope)

        self._check_redirect_uri(client, redirect_uri)

        self._check_response_type_and_scope(response_type, scope)

        self._check_client_type_and_duration(response_type, client, duration)

        if c.errors:
            return self._error_response(state, redirect_uri,
                                        as_fragment=(response_type == "token"))

        if response_type == "code":
            code = OAuth2AuthorizationCode._new(client._id, redirect_uri,
                                            c.user._id36, scope,
                                            duration == "permanent")
            resp = {"code": code._id, "state": state}
            final_redirect = _update_redirect_uri(redirect_uri, resp)
            g.stats.simple_event('oauth2.POST_authorize.authorization_code_create')
        elif response_type == "token":
            device_id = get_device_id(client)
            token = OAuth2AccessToken._new(
                client_id=client._id,
                user_id=c.user._id36,
                scope=scope,
                device_id=device_id,
            )
            resp = OAuth2AccessController._make_new_token_response(token)
            resp["state"] = state
            final_redirect = _update_redirect_uri(redirect_uri, resp, as_fragment=True)
            g.stats.simple_event('oauth2.POST_authorize.access_token_create')

        # If this is the first time the user is logging in with an official
        # mobile app, gild them
        if (g.live_config.get('mobile_gild_first_login') and
                not c.user.has_used_mobile_app and
                client._id in g.mobile_auth_gild_clients):
            buyer = Account.system_user()
            admintools.adjust_gold_expiration(
                c.user, days=g.mobile_auth_gild_time)
            create_gift_gold(
                buyer._id, c.user._id, g.mobile_auth_gild_time,
                datetime.now(g.tz), signed=True, note='first_mobile_auth')
            subject = 'Let there be gold! Reddit just sent you Reddit gold!'
            message = (
                "Thank you for using the Reddit mobile app!  As a thank you "
                "for logging in during launch week, you've been gifted %s of "
                "Reddit Gold.\n\n"
                "Reddit Gold is Reddit's premium membership program, which "
                "grants you: \n"
                "An ads-free experience in Reddit's mobile apps, and\n"
                "Extra site features on desktop\n\n"
                "Discuss and get help on the features and perks at "
                "r/goldbenefits."
            ) % g.mobile_auth_gild_message
            message += '\n\n' + strings.gold_benefits_msg
            send_system_message(c.user, subject, message, add_to_sent=False)
            c.user.has_used_mobile_app = True
            c.user._commit()

        return self.redirect(final_redirect, code=302)

class OAuth2AccessController(MinimalController):
    handles_csrf = True

    def pre(self):
        if g.disallow_db_writes:
            abort(403)

        set_extension(request.environ, "json")
        MinimalController.pre(self)
        require_https()
        if request.method != "OPTIONS":
            c.oauth2_client = self._get_client_auth()

    def _get_client_auth(self):
        auth = request.headers.get("Authorization")
        try:
            client_id, client_secret = parse_http_basic(auth)
            require(client_id)
            client = OAuth2Client.get_token(client_id)
            require(client)
            if client.is_confidential():
                require(client_secret)
                require(constant_time_compare(client.secret, client_secret))
            return client
        except RequirementException:
            abort(401, headers=[("WWW-Authenticate", 'Basic realm="reddit"')])

    def OPTIONS_access_token(self):
        """Send CORS headers for access token requests

        * Allow all origins
        * Only POST requests allowed to /api/v1/access_token
        * No ambient credentials
        * Authorization header required to identify the client
        * Expose common reddit headers

        """
        if "Origin" in request.headers:
            response.headers["Access-Control-Allow-Origin"] = "*"
            response.headers["Access-Control-Allow-Methods"] = \
                "POST"
            response.headers["Access-Control-Allow-Headers"] = \
                    "Authorization, "
            response.headers["Access-Control-Allow-Credentials"] = "false"
            response.headers['Access-Control-Expose-Headers'] = \
                self.COMMON_REDDIT_HEADERS

    @validate(
        grant_type=VOneOf("grant_type",
            (
                 "authorization_code",
                 "refresh_token",
                 "password",
                 "client_credentials",
                 "https://oauth.reddit.com/grants/installed_client",
            )
        ),
    )
    def POST_access_token(self, grant_type):
        """
        Exchange an [OAuth 2.0](http://oauth.net/2/) authorization code
        or refresh token (from [/api/v1/authorize](#api_method_authorize)) for
        an access token.

        On success, returns a URL-encoded dictionary containing
        **access_token**, **token_type**, **expires_in**, and **scope**.
        If an authorization code for a permanent grant was given, a
        **refresh_token** will be included. If there is a problem, an **error**
        parameter will be returned instead.

        Must be called using SSL, and must contain a HTTP `Authorization:`
        header which contains the application's client identifier as the
        username and client secret as the password.  (The client id and secret
        are visible on the [app preferences page](/prefs/apps).)

        Per the OAuth specification, **grant_type** must be one of:

        * ``authorization_code`` for the initial access token ("standard" OAuth2 flow)
        * ``refresh_token`` for renewing the access token.
        * ``password`` for script-type apps using password auth
        * ``client_credentials`` for application-only (signed out) access - confidential clients
        * ``https://oauth.reddit.com/grants/installed_client`` extension grant for application-only (signed out)
                access - non-confidential (installed) clients

        **redirect_uri** must exactly match the value that was used in the call
        to [/api/v1/authorize](#api_method_authorize) that created this grant.

        See reddit's [OAuth2 wiki](https://github.com/reddit/reddit/wiki/OAuth2) for
        more information.

        """
        self.OPTIONS_access_token()
        if grant_type == "authorization_code":
            return self._access_token_code()
        elif grant_type == "refresh_token":
            return self._access_token_refresh()
        elif grant_type == "password":
            return self._access_token_password()
        elif grant_type == "client_credentials":
            return self._access_token_client_credentials()
        elif grant_type == "https://oauth.reddit.com/grants/installed_client":
            return self._access_token_extension_client_credentials()
        else:
            resp = {"error": "unsupported_grant_type"}
            return self.api_wrapper(resp)

    def _check_for_errors(self):
        resp = {}
        if (errors.INVALID_OPTION, "scope") in c.errors:
            resp["error"] = "invalid_scope"
        else:
            resp["error"] = "invalid_request"
        return resp

    @classmethod
    def _make_new_token_response(cls, access_token, refresh_token=None):
        if not access_token:
            return {"error": "invalid_grant"}
        expires_in = int(access_token._ttl) if access_token._ttl else None
        resp = {
            "access_token": access_token._id,
            "token_type": access_token.token_type,
            "expires_in": expires_in,
            "scope": access_token.scope,
        }
        if refresh_token:
            resp["refresh_token"] = refresh_token._id

        if access_token.device_id:
            resp['device_id'] = access_token.device_id

        return resp

    @validate(code=nop("code"),
              redirect_uri=VRequired("redirect_uri",
                                     errors.OAUTH2_INVALID_REDIRECT_URI))
    def _access_token_code(self, code, redirect_uri):
        if not code:
            c.errors.add("NO_TEXT", field="code")
        if c.errors:
            return self.api_wrapper(self._check_for_errors())

        access_token = None
        refresh_token = None

        client = c.oauth2_client
        auth_token = OAuth2AuthorizationCode.use_token(code, client._id, redirect_uri)

        if auth_token:
            if auth_token.refreshable:
                refresh_token = OAuth2RefreshToken._new(
                    client_id=auth_token.client_id,
                    user_id=auth_token.user_id,
                    scope=auth_token.scope,
                )
                g.stats.simple_event(
                    'oauth2.access_token_code.refresh_token_create')

            device_id = get_device_id(client)
            access_token = OAuth2AccessToken._new(
                client_id=auth_token.client_id,
                user_id=auth_token.user_id,
                scope=auth_token.scope,
                refresh_token=refresh_token._id if refresh_token else "",
                device_id=device_id,
            )
            g.stats.simple_event(
                'oauth2.access_token_code.access_token_create')

        resp = self._make_new_token_response(access_token, refresh_token)

        return self.api_wrapper(resp)

    @validate(refresh_token=VOAuth2RefreshToken("refresh_token"))
    def _access_token_refresh(self, refresh_token):
        access_token = None
        if refresh_token:
            if refresh_token.client_id == c.oauth2_client._id:
                access_token = OAuth2AccessToken._new(
                    refresh_token.client_id, refresh_token.user_id,
                    refresh_token.scope,
                    refresh_token=refresh_token._id)
                g.stats.simple_event(
                    'oauth2.access_token_refresh.access_token_create')
            else:
                g.stats.simple_event(
                    'oauth2.errors.OAUTH2_INVALID_REFRESH_TOKEN')
                c.errors.add(errors.OAUTH2_INVALID_REFRESH_TOKEN)
        else:
            g.stats.simple_event('oauth2.errors.NO_TEXT')
            c.errors.add("NO_TEXT", field="refresh_token")

        if c.errors:
            resp = self._check_for_errors()
            response.status = 400
        else:
            g.stats.simple_event('oauth2.access_token_refresh.success')
            resp = self._make_new_token_response(access_token)
        return self.api_wrapper(resp)

    @validate(user=VThrottledLogin(["username", "password"]),
              scope=nop("scope"))
    def _access_token_password(self, user, scope):
        # username:password auth via OAuth is only allowed for
        # private use scripts
        client = c.oauth2_client
        if client.app_type != "script":
            g.stats.simple_event('oauth2.errors.PASSWORD_UNAUTHORIZED_CLIENT')
            return self.api_wrapper({"error": "unauthorized_client",
                "error_description": "Only script apps may use password auth"})
        dev_ids = client._developer_ids
        if not user or user._id not in dev_ids:
            g.stats.simple_event('oauth2.errors.INVALID_GRANT')
            return self.api_wrapper({"error": "invalid_grant"})
        if c.errors:
            return self.api_wrapper(self._check_for_errors())

        if scope:
            scope = OAuth2Scope(scope)
            if not scope.is_valid():
                g.stats.simple_event('oauth2.errors.PASSWORD_INVALID_SCOPE')
                c.errors.add(errors.INVALID_OPTION, "scope")
                return self.api_wrapper({"error": "invalid_scope"})
        else:
            scope = OAuth2Scope(OAuth2Scope.FULL_ACCESS)

        device_id = get_device_id(client)
        access_token = OAuth2AccessToken._new(
            client_id=client._id,
            user_id=user._id36,
            scope=scope,
            device_id=device_id,
        )
        g.stats.simple_event(
            'oauth2.access_token_password.access_token_create')
        resp = self._make_new_token_response(access_token)
        return self.api_wrapper(resp)

    @validate(
        scope=nop("scope"),
    )
    def _access_token_client_credentials(self, scope):
        client = c.oauth2_client
        if not client.is_confidential():
            g.stats.simple_event(
                'oauth2.errors.CLIENT_CREDENTIALS_UNAUTHORIZED_CLIENT')
            return self.api_wrapper({"error": "unauthorized_client",
                "error_description": "Only confidential clients may use client_credentials auth"})
        if scope:
            scope = OAuth2Scope(scope)
            if not scope.is_valid():
                g.stats.simple_event(
                    'oauth2.errors.CLIENT_CREDENTIALS_INVALID_SCOPE')
                c.errors.add(errors.INVALID_OPTION, "scope")
                return self.api_wrapper({"error": "invalid_scope"})
        else:
            scope = OAuth2Scope(OAuth2Scope.FULL_ACCESS)

        device_id = get_device_id(client)
        access_token = OAuth2AccessToken._new(
            client_id=client._id,
            user_id="",
            scope=scope,
            device_id=device_id,
        )
        g.stats.simple_event(
            'oauth2.access_token_client_credentials.access_token_create')
        resp = self._make_new_token_response(access_token)
        return self.api_wrapper(resp)

    @validate(
        scope=nop("scope"),
        device_id=VLength("device_id", 50, min_length=20),
    )
    def _access_token_extension_client_credentials(self, scope, device_id):
        if ((errors.NO_TEXT, "device_id") in c.errors or
                (errors.TOO_SHORT, "device_id") in c.errors or
                (errors.TOO_LONG, "device_id") in c.errors):
            g.stats.simple_event('oauth2.errors.BAD_DEVICE_ID')
            return self.api_wrapper({
                "error": "invalid_request",
                "error_description": "bad device_id",
            })

        client = c.oauth2_client
        if scope:
            scope = OAuth2Scope(scope)
            if not scope.is_valid():
                g.stats.simple_event(
                    'oauth2.errors.EXTENSION_CLIENT_CREDENTIALS_INVALID_SCOPE')
                c.errors.add(errors.INVALID_OPTION, "scope")
                return self.api_wrapper({"error": "invalid_scope"})
        else:
            scope = OAuth2Scope(OAuth2Scope.FULL_ACCESS)

        access_token = OAuth2AccessToken._new(
            client_id=client._id,
            user_id="",
            scope=scope,
            device_id=device_id,
        )
        g.stats.simple_event(
            'oauth2.access_token_extension_client_credentials.'
            'access_token_create')
        resp = self._make_new_token_response(access_token)
        return self.api_wrapper(resp)

    def OPTIONS_revoke_token(self):
        """Send CORS headers for token revocation requests

        * Allow all origins
        * Only POST requests allowed to /api/v1/revoke_token
        * No ambient credentials
        * Authorization header required to identify the client
        * Expose common reddit headers

        """
        if "Origin" in request.headers:
            response.headers["Access-Control-Allow-Origin"] = "*"
            response.headers["Access-Control-Allow-Methods"] = \
                "POST"
            response.headers["Access-Control-Allow-Headers"] = \
                    "Authorization, "
            response.headers["Access-Control-Allow-Credentials"] = "false"
            response.headers['Access-Control-Expose-Headers'] = \
                self.COMMON_REDDIT_HEADERS

    @validate(
        VRatelimit(rate_user=False, rate_ip=True, prefix="rate_revoke_token_"),
        token_id=nop("token"),
        token_hint=VOneOf("token_type_hint", ("access_token", "refresh_token")),
    )
    def POST_revoke_token(self, token_id, token_hint):
        '''Revoke an OAuth2 access or refresh token.

        token_type_hint is optional, and hints to the server
        whether the passed token is a refresh or access token.

        A call to this endpoint is considered a success if
        the passed `token_id` is no longer valid. Thus, if an invalid
        `token_id` was passed in, a successful 204 response will be returned.

        See [RFC7009](http://tools.ietf.org/html/rfc7009)

        '''
        self.OPTIONS_revoke_token()
        # In success cases, this endpoint returns no data.
        response.status = 204

        if not token_id:
            return

        types = (OAuth2AccessToken, OAuth2RefreshToken)
        if token_hint == "refresh_token":
            types = reversed(types)

        for token_type in types:
            try:
                token = token_type._byID(token_id)
            except tdb_cassandra.NotFound:
                g.stats.simple_event(
                    'oauth2.POST_revoke_token.cass_not_found.%s'
                    % token_type.__name__)
                continue
            else:
                break
        else:
            # No Token found. The given token ID is already gone
            # or never existed. Either way, from the client's perspective,
            # the passed in token is no longer valid.
            return

        if constant_time_compare(token.client_id, c.oauth2_client._id):
            token.revoke()
        else:
            # RFC 7009 is not clear on how to handle this case.
            # Given that a malicious client could do much worse things
            # with a valid token then revoke it, returning an error
            # here is best as it may help certain clients debug issues
            response.status = 400
            g.stats.simple_event(
                'oauth2.errors.REVOKE_TOKEN_UNAUTHORIZED_CLIENT')
            return self.api_wrapper({"error": "unauthorized_client"})


def require_oauth2_scope(*scopes):
    def oauth2_scope_wrap(fn):
        fn.oauth2_perms = {"required_scopes": scopes, "oauth2_allowed": True}
        return fn
    return oauth2_scope_wrap


def allow_oauth2_access(fn):
    fn.oauth2_perms = {"required_scopes": [], "oauth2_allowed": True}
    return fn
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from reddit_base import RedditController, UnloggedUser
from r2.lib.pages import (ButtonLite, ButtonDemoPanel, WidgetDemoPanel,
                          BoringPage)
from r2.lib.pages.things import wrap_links
from r2.models import *
from r2.lib.validator import *
from pylons import request, response
from pylons import tmpl_context as c
from pylons.i18n import _

class ButtonsController(RedditController):
    def get_wrapped_link(self, url, link = None, wrapper = None):
        try:
            links = []
            if link:
                links = [link]
            else:
                sr = None if isinstance(c.site, FakeSubreddit) else c.site
                try:
                    links = Link._by_url(url, sr)
                except NotFound:
                    pass

            if links:
                kw = {}
                if wrapper:
                    links = wrap_links(links, wrapper = wrapper)
                else:
                    links = wrap_links(links)
                links = list(links)
                links = max(links, key = lambda x: x._score) if links else None
            if not links and wrapper:
                return wrapper(None)
            return links
            # note: even if _by_url successed or a link was passed in,
            # it is possible link_listing.things is empty if the
            # link(s) is/are members of a private reddit
            # return the link with the highest score (if more than 1)
        except:
            #we don't want to return 500s in other people's pages.
            import traceback
            g.log.debug("FULLPATH: get_link error in buttons code")
            g.log.debug(traceback.format_exc())
            if wrapper:
                return wrapper(None)

    @validate(buttontype = VInt('t', 1, 5))
    def GET_button_embed(self, buttontype):
        if not buttontype:
            abort(404)

        return self.redirect('/static/button/button%s.js' % buttontype,
                             code=301)

    @validate(buttonimage = VInt('i', 0, 14),
              title = nop('title'),
              url = VSanitizedUrl('url'),
              newwindow = VBoolean('newwindow', default = False),
              styled = VBoolean('styled', default=True))
    def GET_button_lite(self, buttonimage, title, url, styled, newwindow):
        c.user = UnloggedUser([c.lang])
        c.user_is_loggedin = False
        c.render_style = 'js'

        if not url:
            url = request.referer

        def builder_wrapper(thing = None):
            kw = {}
            if not thing:
                kw['url'] = url
                kw['title'] = title
            return ButtonLite(thing,
                              image = 1 if buttonimage is None else buttonimage,
                              target = "_new" if newwindow else "_parent",
                              styled = styled, **kw)

        bjs = self.get_wrapped_link(url, wrapper = builder_wrapper)
        response.content_type = "text/javascript"
        return bjs.render()

    def GET_button_demo_page(self):
        # no buttons for domain listings -> redirect to top level
        if isinstance(c.site, DomainSR):
            return self.redirect('/buttons')
        return BoringPage(_("reddit buttons"),
                          show_sidebar = False, 
                          content=ButtonDemoPanel()).render()

    def GET_widget_demo_page(self):
        return BoringPage(_("reddit widget"),
                          show_sidebar = False, 
                          content=WidgetDemoPanel()).render()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import os

import pylibmc
from pylons import request, response
from pylons import app_globals as g
from pylons.controllers.util import abort

from r2.controllers.reddit_base import MinimalController
from r2.lib import promote, cache


class HealthController(MinimalController):
    def pre(self):
        pass

    def post(self):
        pass

    def GET_health(self):
        if os.path.exists("/var/opt/reddit/quiesce"):
            request.environ["usable_error_content"] = "No thanks, I'm full."
            abort(503)

        response.content_type = "application/json"
        return json.dumps(g.versions, sort_keys=True, indent=4)

    def GET_promohealth(self):
        response.content_type = "application/json"
        return json.dumps(promote.health_check())

    def GET_cachehealth(self):
        results = {}
        behaviors = {
            # Passed on to poll(2) in milliseconds
            "connect_timeout": 1000,
            # Passed on to setsockopt(2) in microseconds
            "receive_timeout": int(1e6),
            "send_timeout": int(1e6),
        }
        for server in cache._CACHE_SERVERS:
            try:
                if server.startswith("udp:"):
                    # libmemcached doesn't support UDP get/fetch operations
                    continue
                mc = pylibmc.Client([server], behaviors=behaviors)
                # it's ok that not all caches are mcrouter, we'll just ignore
                # the miss either way
                mc.get("__mcrouter__.version")
                results[server] = "OK"
            except pylibmc.Error as e:
                g.log.warning("Health check for %s FAILED: %s", server, e)
                results[server] = "FAILED %s" % e
        return json.dumps(results)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import request, response
from pylons import app_globals as g

from r2.controllers.reddit_base import MinimalController
from r2.lib.base import abort
from r2.lib.pages import Robots, CrossDomain
from r2.lib import utils


class RobotsController(MinimalController):
    def pre(self):
        pass

    def post(self):
        pass

    def on_crawlable_domain(self):
        # This ensures we don't have the port included.
        requested_domain = utils.domain(request.host)

        # If someone CNAMEs myspammysite.com to reddit.com or something, we
        # don't want search engines to index that.
        if not utils.is_subdomain(requested_domain, g.domain):
            return False

        # Only allow the canonical desktop site and mobile subdomains, since
        # we have canonicalization set up appropriately for them.
        # Note: in development, DomainMiddleware needs to be temporarily
        # modified to not skip assignment of reddit-domain-extension on
        # localhost for this to work properly.
        return (requested_domain == g.domain or
                request.environ.get('reddit-domain-extension') in
                    ('mobile', 'compact'))

    def GET_robots(self):
        response.content_type = "text/plain"
        if self.on_crawlable_domain():
            return Robots().render(style='txt')
        else:
            return "User-Agent: *\nDisallow: /\n"

    def GET_crossdomain(self):
        # Our middleware is weird and won't let us add a route for just
        # '/crossdomain.xml'. Just 404 for other extensions.
        if request.environ.get('extension', None) != 'xml':
            abort(404)
        response.content_type = "text/x-cross-domain-policy"
        return CrossDomain().render(style='xml')
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _
from BeautifulSoup import BeautifulSoup, Tag

from r2.lib.base import abort
from r2.controllers.reddit_base import RedditController
from r2.models.subreddit import Frontpage
from r2.models.wiki import WikiPage, WikiRevision, WikiBadRevision
from r2.lib.db import tdb_cassandra
from r2.lib.filters import unsafe, wikimarkdown, generate_table_of_contents
from r2.lib.validator import validate, nop
from r2.lib.pages import PolicyPage, PolicyView


class PoliciesController(RedditController):
    @validate(requested_rev=nop('v'))
    def GET_policy_page(self, page, requested_rev):
        if c.render_style == 'compact':
            self.redirect('/wiki/' + page)
        if page == 'privacypolicy':
            wiki_name = g.wiki_page_privacy_policy
            pagename = _('privacy policy')
        elif page == 'useragreement':
            wiki_name = g.wiki_page_user_agreement
            pagename = _('user agreement')
        elif page == 'contentpolicy':
            wiki_name = g.wiki_page_content_policy
            pagename = _('content policy')
        else:
            abort(404)

        wp = WikiPage.get(Frontpage, wiki_name)

        revs = list(wp.get_revisions())

        # collapse minor edits into revisions with reasons
        rev_info = []
        last_edit = None
        for rev in revs:
            if rev.is_hidden:
                continue

            if not last_edit:
                last_edit = rev

            if rev._get('reason'):
                rev_info.append({
                    'id': str(last_edit._id),
                    'title': rev._get('reason'),
                })
                last_edit = None

        if requested_rev:
            try:
                display_rev = WikiRevision.get(requested_rev, wp._id)
            except (tdb_cassandra.NotFound, WikiBadRevision):
                abort(404)
        else:
            display_rev = revs[0]

        doc_html = wikimarkdown(display_rev.content, include_toc=False)
        soup = BeautifulSoup(doc_html.decode('utf-8'))
        toc = generate_table_of_contents(soup, prefix='section')
        self._number_sections(soup)
        self._linkify_headings(soup)

        content = PolicyView(
            body_html=unsafe(soup),
            toc_html=unsafe(toc),
            revs=rev_info,
            display_rev=str(display_rev._id),
        )
        return PolicyPage(
            pagename=pagename,
            content=content,
        ).render()

    def _number_sections(self, soup):
        count = 1
        for para in soup.find('div', 'md').findAll(['p'], recursive=False):
            a = Tag(soup, 'a', [
                ('class', 'p-anchor'),
                ('id', 'p_%d' % count),
                ('href', '#p_%d' % count),
            ])
            a.append(str(count))
            para.insert(0, a)
            para.insert(1, ' ')
            count += 1

    def _linkify_headings(self, soup):
        md_el = soup.find('div', 'md')
        for heading in md_el.findAll(['h1', 'h2', 'h3'], recursive=False):
            heading_a = Tag(soup, "a", [('href', '#%s' % heading['id'])])
            heading_a.contents = heading.contents
            heading.contents = []
            heading.append(heading_a)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from reddit_base import RedditController
from r2.controllers.oauth2 import require_oauth2_scope
from r2.lib.utils import url_links_builder
from reddit_base import paginated_listing
from r2.models.wiki import (
    ContentLengthError,
    modactions,
    WikiPage,
    WikiPageExists,
    WikiRevision,
)
from r2.models.subreddit import Subreddit
from r2.models.modaction import ModAction
from r2.models.builder import WikiRevisionBuilder, WikiRecentRevisionBuilder

from r2.lib.template_helpers import join_urls


from r2.lib.validator import (
    nop,
    validate,
    VAdmin,
    VBoolean,
    VExistingUname,
    VInt,
    VMarkdown,
    VModhash,
    VNotInTimeout,
    VOneOf,
    VPrintable,
    VRatelimit,
)

from r2.lib.validator.wiki import (
    VWikiPage,
    VWikiPageAndVersion,
    VWikiModerator,
    VWikiPageRevise,
    this_may_revise,
    this_may_view,
    VWikiPageName,
)
from r2.controllers.api_docs import api_doc, api_section
from r2.lib.pages.wiki import (WikiPageView, WikiNotFound, WikiRevisions,
                              WikiEdit, WikiSettings, WikiRecent,
                              WikiListing, WikiDiscussions,
                              WikiCreate)

from r2.config.extensions import set_extension
from r2.lib.template_helpers import add_sr
from r2.lib.db import tdb_cassandra
from r2.models.listing import WikiRevisionListing
from r2.lib.pages.things import default_thing_wrapper
from r2.lib.pages import BoringPage, CssError
from reddit_base import base_listing
from r2.models import IDBuilder, LinkListing, DefaultSR
from r2.lib.merge import ConflictException, make_htmldiff
from pylons.i18n import _
from r2.lib.pages import PaneStack
from r2.lib.utils import timesince
from r2.config import extensions
from r2.lib.base import abort
from r2.lib.errors import reddit_http_error
from r2.lib.automoderator import Ruleset

import json

page_descriptions = {
    "config/stylesheet": _("This page is the subreddit stylesheet, changes here apply to the subreddit css"),
    "config/submit_text": _("The contents of this page appear on the submit page"),
    "config/sidebar": _("The contents of this page appear on the subreddit sidebar"),
    "config/description": _("The contents of this page appear in the public subreddit description and when the user does not have access to the subreddit"),
    "config/automoderator": _("This page is used to configure AutoModerator for the subreddit, please see [the full documentation](/wiki/automoderator/full-documentation) for information"),
}

ATTRIBUTE_BY_PAGE = {"config/sidebar": "description",
                     "config/submit_text": "submit_text",
                     "config/description": "public_description"}
RENDERERS_BY_PAGE = {
    "config/automoderator": "automoderator",
    "config/description": "reddit",
    "config/sidebar": "reddit",
    "config/stylesheet": "stylesheet",
    "config/submit_text": "reddit",
    "toolbox": "rawcode",
    "usernotes": "rawcode",
}

class WikiController(RedditController):
    allow_stylesheets = True

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/{page}', uses_site=True)
    @validate(pv=VWikiPageAndVersion(('page', 'v', 'v2'),
                                     required=False,
                                     restricted=False,
                                     allow_hidden_revision=False),
              page_name=VWikiPageName('page',
                                      error_on_name_normalized=True))
    def GET_wiki_page(self, pv, page_name):
        """Return the content of a wiki page

        If `v` is given, show the wiki page as it was at that version
        If both `v` and `v2` are given, show a diff of the two

        """
        message = None

        if c.errors.get(('PAGE_NAME_NORMALIZED', 'page')):
            url = join_urls(c.wiki_base_url, page_name)
            return self.redirect(url)

        page, version, version2 = pv

        if not page:
            is_api = c.render_style in extensions.API_TYPES
            if this_may_revise():
                if is_api:
                    self.handle_error(404, 'PAGE_NOT_CREATED')
                errorpage = WikiNotFound(page=page_name)
                request.environ['usable_error_content'] = errorpage.render()
            elif is_api:
                self.handle_error(404, 'PAGE_NOT_FOUND')
            self.abort404()

        if version:
            edit_by = version.get_author()
            edit_date = version.date
        else:
            edit_by = page.get_author()
            edit_date = page._get('last_edit_date')

        diffcontent = None
        if not version:
            content = page.content
            if c.is_wiki_mod and page.name in page_descriptions:
                message = page_descriptions[page.name]
        else:
            message = _("viewing revision from %s") % timesince(version.date)
            if version2:
                t1 = timesince(version.date)
                t2 = timesince(version2.date)
                timestamp1 = _("%s ago") % t1
                timestamp2 = _("%s ago") % t2
                message = _("comparing revisions from %(date_1)s and %(date_2)s") \
                          % {'date_1': t1, 'date_2': t2}
                diffcontent = make_htmldiff(version.content, version2.content, timestamp1, timestamp2)
                content = version2.content
            else:
                message = _("viewing revision from %s ago") % timesince(version.date)
                content = version.content

        renderer = RENDERERS_BY_PAGE.get(page.name, 'wiki') 

        return WikiPageView(content, alert=message, v=version, diff=diffcontent,
                            may_revise=this_may_revise(page), edit_by=edit_by,
                            edit_date=edit_date, page=page.name,
                            renderer=renderer).render()

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/revisions/{page}', uses_site=True)
    @paginated_listing(max_page_size=100, backend='cassandra')
    @validate(page=VWikiPage(('page'), restricted=False))
    def GET_wiki_revisions(self, num, after, reverse, count, page):
        """Retrieve a list of revisions of this wiki `page`"""
        revisions = page.get_revisions()
        wikiuser = c.user if c.user_is_loggedin else None
        builder = WikiRevisionBuilder(revisions, user=wikiuser, sr=c.site,
                                      num=num, reverse=reverse, count=count,
                                      after=after, skip=not c.is_wiki_mod,
                                      wrap=default_thing_wrapper(),
                                      page=page)
        listing = WikiRevisionListing(builder).listing()
        return WikiRevisions(listing, page=page.name, may_revise=this_may_revise(page)).render()

    @validate(wp=VWikiPageRevise('page'),
              page=VWikiPageName('page'))
    def GET_wiki_create(self, wp, page):
        api = c.render_style in extensions.API_TYPES
        error = c.errors.get(('WIKI_CREATE_ERROR', 'page'))
        if error:
            error = error.msg_params
        if wp[0]:
            VNotInTimeout().run(action_name="wikirevise",
                details_text="create", target=page)
            return self.redirect(join_urls(c.wiki_base_url, wp[0].name))
        elif api:
            if error:
                self.handle_error(403, **error)
            else:
                self.handle_error(404, 'PAGE_NOT_CREATED')
        elif error:
            error_msg = ''
            if error['reason'] == 'PAGE_NAME_LENGTH':
                error_msg = _("this wiki cannot handle page names of that magnitude!  please select a page name shorter than %d characters") % error['max_length']
            elif error['reason'] == 'PAGE_CREATED_ELSEWHERE':
                error_msg = _("this page is a special page, please go into the subreddit settings and save the field once to create this special page")
            elif error['reason'] == 'PAGE_NAME_MAX_SEPARATORS':
                error_msg = _('a max of %d separators "/" are allowed in a wiki page name.') % error['max_separators']
            return BoringPage(_("Wiki error"), infotext=error_msg).render()
        else:
            VNotInTimeout().run(action_name="wikirevise",
                details_text="create")
            return WikiCreate(page=page, may_revise=True).render()

    @validate(wp=VWikiPageRevise('page', restricted=True, required=True))
    def GET_wiki_revise(self, wp, page, message=None, **kw):
        wp = wp[0]
        VNotInTimeout().run(action_name="wikirevise", details_text="revise",
            target=wp)
        error = c.errors.get(('MAY_NOT_REVISE', 'page'))
        if error:
            self.handle_error(403, **(error.msg_params or {}))
        
        previous = kw.get('previous', wp._get('revision'))
        content = kw.get('content', wp.content)
        if not message and wp.name in page_descriptions:
            message = page_descriptions[wp.name]
        return WikiEdit(content, previous, alert=message, page=wp.name,
                        may_revise=True).render()

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/revisions', uses_site=True)
    @paginated_listing(max_page_size=100, backend='cassandra')
    def GET_wiki_recent(self, num, after, reverse, count):
        """Retrieve a list of recently changed wiki pages in this subreddit"""
        revisions = WikiRevision.get_recent(c.site)
        wikiuser = c.user if c.user_is_loggedin else None
        builder = WikiRecentRevisionBuilder(revisions,  num=num, count=count,
                                            reverse=reverse, after=after,
                                            wrap=default_thing_wrapper(),
                                            skip=not c.is_wiki_mod,
                                            user=wikiuser, sr=c.site)
        listing = WikiRevisionListing(builder).listing()
        return WikiRecent(listing).render()

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/pages', uses_site=True)
    def GET_wiki_listing(self):
        """Retrieve a list of wiki pages in this subreddit"""
        def check_hidden(page):
            return page.listed and this_may_view(page)
        pages, linear_pages = WikiPage.get_listing(c.site, filter_check=check_hidden)
        return WikiListing(pages, linear_pages).render()

    def GET_wiki_redirect(self, page='index'):
        return self.redirect(str("%s/%s" % (c.wiki_base_url, page)), code=301)

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/discussions/{page}', uses_site=True)
    @base_listing
    @validate(page=VWikiPage('page', restricted=True))
    def GET_wiki_discussions(self, page, num, after, reverse, count):
        """Retrieve a list of discussions about this wiki `page`"""
        page_url = add_sr("%s/%s" % (c.wiki_base_url, page.name))
        builder = url_links_builder(page_url, num=num, after=after,
                                    reverse=reverse, count=count)
        listing = LinkListing(builder).listing()
        return WikiDiscussions(listing, page=page.name,
                               may_revise=this_may_revise(page)).render()

    @require_oauth2_scope("modwiki")
    @api_doc(api_section.wiki, uri='/wiki/settings/{page}', uses_site=True)
    @validate(page=VWikiPage('page', restricted=True, modonly=True))
    def GET_wiki_settings(self, page):
        """Retrieve the current permission settings for `page`"""
        settings = {'permlevel': page._get('permlevel', 0),
                    'listed': page.listed}
        VNotInTimeout().run(action_name="pageview",
                details_text="wikisettings", target=page)
        mayedit = page.get_editor_accounts()
        restricted = (not page.special) and page.restricted
        show_editors = not restricted
        return WikiSettings(settings, mayedit, show_settings=not page.special,
                            page=page.name, show_editors=show_editors,
                            restricted=restricted,
                            may_revise=True).render()

    @require_oauth2_scope("modwiki")
    @api_doc(api_section.wiki, uri='/wiki/settings/{page}', uses_site=True)
    @validate(
        VModhash(),
        page=VWikiPage('page', restricted=True, modonly=True),
        permlevel=VInt('permlevel'),
        listed=VBoolean('listed'),
    )
    def POST_wiki_settings(self, page, permlevel, listed):
        """Update the permissions and visibility of wiki `page`"""
        oldpermlevel = page.permlevel
        if oldpermlevel != permlevel:
            VNotInTimeout().run(action_name="wikipermlevel",
                details_text="edit", target=page)
        if page.listed != listed:
            VNotInTimeout().run(action_name="wikipagelisted",
                details_text="edit", target=page)

        try:
            page.change_permlevel(permlevel)
        except ValueError:
            self.handle_error(403, 'INVALID_PERMLEVEL')
        if page.listed != listed:
            page.listed = listed
            page._commit()
            verb = 'Relisted' if listed else 'Delisted'
            description = '%s page %s' % (verb, page.name)
            ModAction.create(c.site, c.user, 'wikipagelisted',
                             description=description)
        if oldpermlevel != permlevel:
            description = 'Page: %s, Changed from %s to %s' % (
                page.name, oldpermlevel, permlevel
            )
            ModAction.create(c.site, c.user, 'wikipermlevel',
                             description=description)
        return self.GET_wiki_settings(page=page.name)

    def on_validation_error(self, error):
        RedditController.on_validation_error(self, error)
        if error.code:
            self.handle_error(error.code, error.name)

    def handle_error(self, code, reason=None, **data):
        abort(reddit_http_error(code, reason, **data))

    def pre(self):
        RedditController.pre(self)
        if g.disable_wiki and not c.user_is_admin:
            self.handle_error(403, 'WIKI_DOWN')
        if not c.site._should_wiki:
            self.handle_error(404, 'NOT_WIKIABLE')  # /r/mod for an example
        frontpage = isinstance(c.site, DefaultSR)
        c.wiki_base_url = join_urls(c.site.path, 'wiki')
        c.wiki_api_url = join_urls(c.site.path, '/api/wiki')
        c.wiki_id = g.default_sr if frontpage else c.site.name
        self.editconflict = False
        c.is_wiki_mod = (
            c.user_is_admin or c.site.is_moderator_with_perms(c.user, 'wiki')
            ) if c.user_is_loggedin else False
        c.wikidisabled = False

        mode = c.site.wikimode
        if not mode or mode == 'disabled':
            if not c.is_wiki_mod:
                self.handle_error(403, 'WIKI_DISABLED')
            else:
                c.wikidisabled = True

    # Redirects from the old wiki
    def GET_faq(self):
        return self.GET_wiki_redirect(page='faq')

    GET_help = GET_wiki_redirect


class WikiApiController(WikiController):
    @require_oauth2_scope("wikiedit")
    @validate(VModhash(),
              pageandprevious=VWikiPageRevise(('page', 'previous'), restricted=True),
              content=nop(('content')),
              page_name=VWikiPageName('page'),
              reason=VPrintable('reason', 256, empty_error=None))
    @api_doc(api_section.wiki, uri='/api/wiki/edit', uses_site=True)
    def POST_wiki_edit(self, pageandprevious, content, page_name, reason):
        """Edit a wiki `page`"""
        page, previous = pageandprevious

        if c.user._spam:
            error = _("You are doing that too much, please try again later.")
            self.handle_error(415, 'SPECIAL_ERRORS', special_errors=[error])

        if not page:
            error = c.errors.get(('WIKI_CREATE_ERROR', 'page'))
            if error:
                self.handle_error(403, **(error.msg_params or {}))

            VNotInTimeout().run(action_name="wikirevise", details_text="create")
            try:
                page = WikiPage.create(c.site, page_name)
            except WikiPageExists:
                self.handle_error(400, 'WIKI_CREATE_ERROR')

        else:
            VNotInTimeout().run(action_name="wikirevise", details_text="edit",
                target=page)
            error = c.errors.get(('MAY_NOT_REVISE', 'page'))
            if error:
                self.handle_error(403, **(error.msg_params or {}))

        renderer = RENDERERS_BY_PAGE.get(page.name, 'wiki')
        if renderer in ('wiki', 'reddit'):
            content = VMarkdown(('content'), renderer=renderer).run(content)

        # Use the raw POST value as we need to tell the difference between
        # None/Undefined and an empty string.  The validators use a default
        # value with both of those cases and would need to be changed.
        # In order to avoid breaking functionality, this was done instead.
        previous = previous._id if previous else request.POST.get('previous')
        try:
            # special validation methods
            if page.name == 'config/stylesheet':
                css_errors, parsed = c.site.parse_css(content, verify=False)
                if g.css_killswitch:
                    self.handle_error(403, 'STYLESHEET_EDIT_DENIED')
                if css_errors:
                    error_items = [CssError(x).message for x in css_errors]
                    self.handle_error(415, 'SPECIAL_ERRORS', special_errors=error_items)
            elif page.name == "config/automoderator":
                try:
                    rules = Ruleset(content)
                except ValueError as e:
                    error_items = [e.message]
                    self.handle_error(415, "SPECIAL_ERRORS", special_errors=error_items)

            # special saving methods
            if page.name == "config/stylesheet":
                c.site.change_css(content, parsed, previous, reason=reason)
            else:
                try:
                    page.revise(content, previous, c.user._id36, reason=reason)
                except ContentLengthError as e:
                    self.handle_error(403, 'CONTENT_LENGTH_ERROR', max_length=e.max_length)

                # continue storing the special pages as data attributes on the subreddit
                # object. TODO: change this to minimize subreddit get sizes.
                if page.special and page.name in ATTRIBUTE_BY_PAGE:
                    setattr(c.site, ATTRIBUTE_BY_PAGE[page.name], content)
                    c.site._commit()

                if page.special or c.is_wiki_mod:
                    description = modactions.get(page.name, 'Page %s edited' % page.name)
                    ModAction.create(c.site, c.user, "wikirevise",
                        details=description, description=reason)
        except ConflictException as e:
            self.handle_error(409, 'EDIT_CONFLICT', newcontent=e.new, newrevision=page.revision, diffcontent=e.htmldiff)
        return json.dumps({})

    @require_oauth2_scope("modwiki")
    @validate(VModhash(),
              VWikiModerator(),
              page=VWikiPage('page'),
              act=VOneOf('act', ('del', 'add')),
              user=VExistingUname('username'))
    @api_doc(api_section.wiki, uri='/api/wiki/alloweditor/{act}',
             uses_site=True,
             uri_variants=['/api/wiki/alloweditor/%s' % act for act in ('del', 'add')])
    def POST_wiki_allow_editor(self, act, page, user):
        """Allow/deny `username` to edit this wiki `page`"""
        if not user:
            self.handle_error(404, 'UNKNOWN_USER')
        elif act == 'del':
            VNotInTimeout().run(action_name="wikipermlevel",
                details_text="del_editor", target=user)
            page.remove_editor(user._id36)
        elif act == 'add':
            VNotInTimeout().run(action_name="wikipermlevel",
                details_text="allow_editor", target=user)
            page.add_editor(user._id36)
        else:
            self.handle_error(400, 'INVALID_ACTION')
        return json.dumps({})

    @validate(
        VModhash(),
        VAdmin(),
        pv=VWikiPageAndVersion(('page', 'revision')),
        deleted=VBoolean('deleted'),
    )
    def POST_wiki_revision_delete(self, pv, deleted):
        page, revision = pv
        if not revision:
            self.handle_error(400, 'INVALID_REVISION')
        if deleted and page.revision == str(revision._id):
            self.handle_error(400, 'REVISION_IS_CURRENT')
        revision.admin_deleted = deleted
        revision._commit()
        return json.dumps({'status': revision.admin_deleted})

    @require_oauth2_scope("modwiki")
    @validate(VModhash(),
              VWikiModerator(),
              pv=VWikiPageAndVersion(('page', 'revision')))
    @api_doc(api_section.wiki, uri='/api/wiki/hide', uses_site=True)
    def POST_wiki_revision_hide(self, pv):
        """Toggle the public visibility of a wiki page revision"""
        page, revision = pv
        if not revision:
            self.handle_error(400, 'INVALID_REVISION')

        VNotInTimeout().run(action_name="wikirevise",
                details_text="revision_hide", target=page)
        return json.dumps({'status': revision.toggle_hide()})

    @require_oauth2_scope("modwiki")
    @validate(VModhash(),
              VWikiModerator(),
              pv=VWikiPageAndVersion(('page', 'revision')))
    @api_doc(api_section.wiki, uri='/api/wiki/revert', uses_site=True)
    def POST_wiki_revision_revert(self, pv):
        """Revert a wiki `page` to `revision`"""
        page, revision = pv
        if not revision:
            self.handle_error(400, 'INVALID_REVISION')
        VNotInTimeout().run(action_name="wikirevise",
                details_text="revision_revert", target=page)
        content = revision.content
        reason = 'reverted back %s' % timesince(revision.date)
        if page.name == 'config/stylesheet':
            css_errors, parsed = c.site.parse_css(content)
            if css_errors:
                self.handle_error(403, 'INVALID_CSS')
            c.site.change_css(content, parsed, prev=None, reason=reason, force=True)
        else:
            try:
                page.revise(content, author=c.user._id36, reason=reason, force=True)

                # continue storing the special pages as data attributes on the subreddit
                # object. TODO: change this to minimize subreddit get sizes.
                if page.name in ATTRIBUTE_BY_PAGE:
                    setattr(c.site, ATTRIBUTE_BY_PAGE[page.name], content)
                    c.site._commit()
            except ContentLengthError as e:
                self.handle_error(403, 'CONTENT_LENGTH_ERROR', max_length=e.max_length)
        return json.dumps({})

    def pre(self):
        WikiController.pre(self)
        c.render_style = 'api'
        set_extension(request.environ, 'json')

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import hmac
import re
import time

from pylons import app_globals as g
from pylons import request

from r2.config import feature
from r2.controllers.reddit_base import RedditController
from r2.lib.base import abort
from r2.lib.csrf import csrf_exempt
from r2.lib.db import queries
from r2.lib.filters import markdown_souptest
from r2.lib.message_to_email import (
    parse_and_validate_reply_to_address,
    queue_blocked_muted_email,
)
from r2.lib.souptest import SoupError
from r2.lib.utils import constant_time_compare
from r2.models import (
    Account,
    Message,
    Subreddit,
)


MAX_TIMESTAMP_DEVIATION = 600
ZENDESK_PREFIX = "##- Please type your reply above this line -##"


def validate_mailgun_webhook(timestamp, token, signature):
    """Check whether this is a valid webhook sent by Mailgun.

    See https://documentation.mailgun.com/user_manual.html#securing-webhooks

    NOTE:
    A single Mailgun account is used for both outbound email (Mailgun HTTP API)
    and inbound email (Mailgun Routes + MailgunWebhookController). As a result
    the `mailgun_api_key` is used by both.

    """

    message = ''.join((timestamp, token))
    expected_mac = hmac.new(
        g.secrets['mailgun_api_key'], message, hashlib.sha256).hexdigest()
    if not constant_time_compare(expected_mac, signature):
        g.stats.simple_event("mailgun.incoming.bad_signature")
        return False

    if abs(int(timestamp) - time.time()) > MAX_TIMESTAMP_DEVIATION:
        g.stats.simple_event("mailgun.incoming.bad_timestamp")
        return False

    return True


class MailgunWebhookController(RedditController):
    """Handle POST requests from Mailgun generated by inbound emails."""

    @csrf_exempt
    def POST_zendeskreply(self):
        request_body = request.POST
        recipient = request_body["recipient"]
        sender_email = request_body["sender"]
        from_ = request_body["from"]
        subject = request_body["subject"]
        body_plain = request_body["body-plain"]
        stripped_text = request_body["stripped-text"]
        timestamp = request_body["timestamp"]
        token = request_body["token"]
        signature = request_body["signature"]
        email_id = request_body["Message-Id"]

        if not validate_mailgun_webhook(timestamp, token, signature):
            # per Mailgun docs send a 406 so the message won't be retried
            abort(406, "invalid signature")

        message_id36 = parse_and_validate_reply_to_address(recipient)

        if not message_id36:
            # per Mailgun docs send a 406 so the message won't be retried
            abort(406, "invalid message")

        parent = Message._byID36(message_id36, data=True)
        to = Account._byID(parent.author_id, data=True)
        sr = Subreddit._byID(parent.sr_id, data=True)

        if stripped_text.startswith(ZENDESK_PREFIX):
            stripped_text = stripped_text[len(ZENDESK_PREFIX):].lstrip()

        if len(stripped_text) > 10000:
            body = stripped_text[:10000] + "\n\n--snipped--"
        else:
            body = stripped_text

        try:
            markdown_souptest(body)
        except SoupError:
            g.log.warning("bad markdown in modmail email: %s", body)
            abort(406, "invalid body")

        if parent.get_muted_user_in_conversation():
            queue_blocked_muted_email(sr, parent, sender_email, email_id)
            return

        # keep the subject consistent
        message_subject = parent.subject
        if not message_subject.startswith("re: "):
            message_subject = "re: " + message_subject

        # from_ is like '"NAME (GROUP)" <something@domain.zendesk.com>'
        match = re.search("\"(?P<name>\w+) [\w ()]*\"", from_)
        from_sr = True
        author = Account.system_user()

        if match and match.group("name") in g.live_config['modmail_account_map']:
            zendesk_name = match.group("name")
            moderator_name = g.live_config['modmail_account_map'][zendesk_name]
            moderator = Account._by_name(moderator_name)
            if sr.is_moderator_with_perms(moderator, "mail"):
                author = moderator
                from_sr = False

        message, inbox_rel = Message._new(
            author=author,
            to=to,
            subject=message_subject,
            body=body,
            ip='0.0.0.0',
            parent=parent,
            sr=sr,
            from_sr=from_sr,
            can_send_email=False,
            sent_via_email=True,
            email_id=email_id,
        )
        message._commit()
        queries.new_message(message, inbox_rel)
        g.stats.simple_event("mailgun.incoming.success")
        g.stats.simple_event("modmail_email.incoming_email")
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime, timedelta

import json

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _
from sqlalchemy.exc import IntegrityError
import stripe

from r2.controllers.reddit_base import RedditController
from r2.lib.base import abort
from r2.lib.csrf import csrf_exempt
from r2.lib.emailer import _system_email
from r2.lib.errors import MessageError
from r2.lib.filters import _force_unicode, _force_utf8
from r2.lib.hooks import get_hook
from r2.lib.pages import GoldGiftCodeEmail
from r2.lib.strings import strings
from r2.lib.utils import constant_time_compare, randstr, timeago
from r2.lib.validator import (
    nop,
    textresponse,
    validatedForm,
    VByName,
    VDecimal,
    VFloat,
    VInt,
    VLength,
    VModhash,
    VOneOf,
    VPrintable,
    VUser,
)
from r2.models import (
    Account,
    account_by_payingid,
    account_from_stripe_customer_id,
    accountid_from_subscription,
    admintools,
    append_random_bottlecap_phrase,
    cancel_subscription,
    Comment,
    creddits_lock,
    create_claimed_gold,
    create_gift_gold,
    create_gold_code,
    Email,
    get_discounted_price,
    has_prev_subscr_payments,
    Link,
    make_gold_message,
    NotFound,
    retrieve_gold_transaction,
    send_system_message,
    Thing,
    update_gold_transaction,
    generate_token,
)

BLOB_TTL = 86400 * 30
stripe.api_key = g.secrets['stripe_secret_key']


def generate_blob(data):
    passthrough = generate_token(15)

    g.hardcache.set("payment_blob-" + passthrough,
                    data, BLOB_TTL)
    g.log.info("just set payment_blob-%s", passthrough)
    return passthrough


def get_blob(code):
    key = "payment_blob-" + code
    with g.make_lock("payment_blob", "payment_blob_lock-" + code):
        blob = g.hardcache.get(key)
        if not blob:
            raise NotFound("No payment_blob-" + code)
        if blob.get('status', None) != 'initialized':
            raise ValueError("payment_blob %s has status = %s" %
                             (code, blob.get('status', None)))
        blob['status'] = "locked"
        g.hardcache.set(key, blob, BLOB_TTL)
    return key, blob


def update_blob(code, updates=None):
    blob = g.hardcache.get("payment_blob-%s" % code)
    if not blob:
        raise NotFound("No payment_blob-" + code)
    if blob.get('account_id', None) != c.user._id:
        raise ValueError("%s doesn't have access to payment_blob %s" %
                         (c.user._id, code))

    for item, value in updates.iteritems():
        blob[item] = value
    g.hardcache.set("payment_blob-%s" % code, blob, BLOB_TTL)


def has_blob(custom):
    if not custom:
        return False

    blob = g.hardcache.get('payment_blob-%s' % custom)
    return bool(blob)

def dump_parameters(parameters):
    for k, v in parameters.iteritems():
        g.log.info("IPN: %r = %r" % (k, v))

def check_payment_status(payment_status):
    if payment_status is None:
        payment_status = ''

    psl = payment_status.lower()

    if psl == 'completed':
        return (None, psl)
    elif psl == 'refunded':
        return ("Ok", psl)
    elif psl == 'pending':
        return ("Ok", psl)
    elif psl == 'reversed':
        return ("Ok", psl)
    elif psl == 'canceled_reversal':
        return ("Ok", psl)
    elif psl == 'failed':
        return ("Ok", psl)
    elif psl == 'denied':
        return ("Ok", psl)
    elif psl == '':
        return (None, psl)
    else:
        raise ValueError("Unknown IPN status: %r" % payment_status)

def check_txn_type(txn_type, psl):
    if txn_type == 'subscr_signup':
        return ("Ok", None)
    elif txn_type == 'subscr_cancel':
        return ("Ok", "cancel")
    elif txn_type == 'subscr_eot':
        return ("Ok", None)
    elif txn_type == 'subscr_failed':
        return ("Ok", None)
    elif txn_type == 'subscr_modify':
        return ("Ok", None)
    elif txn_type == 'send_money':
        return ("Ok", None)
    elif txn_type in ('new_case',
        'recurring_payment_suspended_due_to_max_failed_payment'):
        return ("Ok", None)
    elif txn_type == 'subscr_payment' and psl == 'completed':
        return (None, "new")
    elif txn_type == 'web_accept' and psl == 'completed':
        return (None, None)
    elif txn_type == "paypal_here":
        return ("Ok", None)
    else:
        raise ValueError("Unknown IPN txn_type / psl %r" %
                         ((txn_type, psl),))


def existing_subscription(subscr_id, paying_id, custom):
    if subscr_id is None:
        return None

    account_id = accountid_from_subscription(subscr_id)

    if not account_id and has_blob(custom):
        # New subscription contains the user info in hardcache
        return None

    should_set_subscriber = False
    if account_id is None:
        # Payment from legacy subscription (subscr_id not set), fall back
        # to guessing the user from the paying_id
        account_id = account_by_payingid(paying_id)
        should_set_subscriber = True
        if account_id is None:
            return None

    try:
        account = Account._byID(account_id, data=True)

        if account._deleted:
            g.log.info("IPN renewal for deleted account %d (%s)", account_id,
                       subscr_id)
            return "deleted account"

        if should_set_subscriber:
            if hasattr(account, "gold_subscr_id") and account.gold_subscr_id:
                g.log.warning("Attempted to set subscr_id (%s) for account (%d) "
                              "that already has one." % (subscr_id, account_id))
                return None

            account.gold_subscr_id = subscr_id
            account._commit()
    except NotFound:
        g.log.info("Just got IPN renewal for non-existent account #%d" % account_id)

    return account

def months_and_days_from_pennies(pennies, discount=False):
    if discount:
        year_pennies = get_discounted_price(g.gold_year_price).pennies
        month_pennies = get_discounted_price(g.gold_month_price).pennies
    else:
        year_pennies = g.gold_year_price.pennies
        month_pennies = g.gold_month_price.pennies

    if pennies >= year_pennies:
        years = pennies / year_pennies
        months = 12 * years
        days  = 366 * years
    else:
        months = pennies / month_pennies
        days   = 31 * months
    return (months, days)

def send_gift(buyer, recipient, months, days, signed, giftmessage,
              thing_fullname, note=None):
    admintools.adjust_gold_expiration(recipient, days=days)

    # increment num_gildings for all types of gildings not to themselves
    if buyer != recipient:
        buyer._incr("num_gildings")

    if thing_fullname:
        thing = Thing._by_fullname(thing_fullname, data=True)
        thing._gild(buyer)
        if isinstance(thing, Comment):
            gilding_type = 'comment gild'
        else:
            gilding_type = 'post gild'
    else:
        thing = None
        get_hook('user.gild').call(recipient=recipient, gilder=buyer)
        gilding_type = 'user gild'

    if signed:
        sender = buyer.name
        md_sender = "/u/%s" % sender
        repliable = True
    else:
        sender = "An anonymous redditor"
        md_sender = "An anonymous redditor"

        if buyer.name in g.live_config["proxy_gilding_accounts"]:
            repliable = False
        else:    
            repliable = True

    create_gift_gold(buyer._id, recipient._id, days, c.start_time, signed, note, gilding_type)

    if months == 1:
        amount = "a month"
    else:
        amount = "%d months" % months

    if not thing:
        subject = 'Let there be gold! %s just sent you reddit gold!' % sender
        message = strings.youve_got_gold % dict(sender=md_sender, amount=amount)
    else:
        url = thing.make_permalink_slow()
        if isinstance(thing, Comment):
            subject = 'Your comment has been gilded!'
            message = strings.youve_been_gilded_comment 
            message %= {'sender': md_sender, 'url': url}
        else:
            subject = 'Your submission has been gilded!'
            message = strings.youve_been_gilded_link 
            message %= {'sender': md_sender, 'url': url}

    if giftmessage and giftmessage.strip():
        message += ("\n\n" + strings.giftgold_note + 
                    _force_unicode(giftmessage) + '\n\n----')

    message += '\n\n' + strings.gold_benefits_msg
    if g.lounge_reddit:
        message += '\n\n' + strings.lounge_msg
    message = append_random_bottlecap_phrase(message)

    if not signed:
        if not repliable:
            message += '\n\n' + strings.unsupported_respond_to_gilder
        else:
            message += '\n\n' + strings.respond_to_anonymous_gilder

    try:
        send_system_message(recipient, subject, message, author=buyer,
                            distinguished='gold-auto', repliable=repliable,
                            signed=signed)
    except MessageError:
        g.log.error('send_gift: could not send system message')

    g.log.info("%s gifted %s to %s" % (buyer.name, amount, recipient.name))
    return thing


def send_gold_code(buyer, months, days,
                   trans_id=None, payer_email='', pennies=0, buyer_email=None):
    if buyer:
        paying_id = buyer._id
        buyer_name = buyer.name
    else:
        paying_id = buyer_email
        buyer_name = buyer_email
    code = create_gold_code(trans_id, payer_email,
                            paying_id, pennies, days, c.start_time)
    # format the code so it's easier to read (XXXXX-XXXXX)
    split_at = len(code) / 2
    code = code[:split_at] + '-' + code[split_at:]

    if months == 1:
        amount = "a month"
    else:
        amount = "%d months" % months

    subject = _('Your gold gift code has been generated!')
    message = _('Here is your gift code for %(amount)s of reddit gold:\n\n'
                '%(code)s\n\nThe recipient (or you!) can enter it at '
                'https://www.reddit.com/gold or go directly to '
                'https://www.reddit.com/thanks/%(code)s to claim it.'
              ) % {'amount': amount, 'code': code}

    if buyer:
        # bought by a logged-in user, send a reddit PM
        message = append_random_bottlecap_phrase(message)
        send_system_message(buyer, subject, message, distinguished='gold-auto')
    else:
        # bought by a logged-out user, send an email
        contents = GoldGiftCodeEmail(message=message).render(style='email')
        _system_email(buyer_email, contents, Email.Kind.GOLD_GIFT_CODE)
                      
    g.log.info("%s bought a gold code for %s", buyer_name, amount)
    return code


class IpnController(RedditController):
    # Used when buying gold with creddits
    @validatedForm(VUser(),
                   VModhash(),
                   months = VInt("months"),
                   passthrough = VPrintable("passthrough", max_length=50))
    def POST_spendcreddits(self, form, jquery, months, passthrough):
        if months is None or months < 1:
            form.set_text(".status", _("nice try."))
            return

        days = months * 31

        if not passthrough:
            raise ValueError("/spendcreddits got no passthrough?")

        blob_key, payment_blob = get_blob(passthrough)
        if payment_blob["goldtype"] not in ("gift", "code", "onetime"):
            raise ValueError("/spendcreddits payment_blob %s has goldtype %s" %
                             (passthrough, payment_blob["goldtype"]))

        if payment_blob["account_id"] != c.user._id:
            fmt = ("/spendcreddits payment_blob %s has userid %d " +
                   "but c.user._id is %d")
            raise ValueError(fmt % passthrough,
                             payment_blob["account_id"],
                             c.user._id)

        if payment_blob["goldtype"] == "gift":
            signed = payment_blob["signed"]
            giftmessage = _force_unicode(payment_blob["giftmessage"])
            recipient_name = payment_blob["recipient"]

            try:
                recipient = Account._by_name(recipient_name)
            except NotFound:
                raise ValueError("Invalid username %s in spendcreddits, buyer = %s"
                                 % (recipient_name, c.user.name))

            if recipient._deleted:
                form.set_text(".status", _("that user has deleted their account"))
                return

        redirect_to_spent = False
        thing = None

        with creddits_lock(c.user):
            if not c.user.employee and c.user.gold_creddits < months:
                msg = "%s is trying to sneak around the creddit check"
                msg %= c.user.name
                raise ValueError(msg)

            if payment_blob["goldtype"] == "gift":
                thing_fullname = payment_blob.get("thing")
                thing = send_gift(c.user, recipient, months, days, signed,
                                  giftmessage, thing_fullname)
                form.set_text(".status", _("the gold has been delivered!"))
            elif payment_blob["goldtype"] == "code":
                try:
                    send_gold_code(c.user, months, days)
                except MessageError:
                    msg = _("there was an error creating a gift code. "
                            "please try again later, or contact %(email)s "
                            "for assistance.") % {'email': g.goldsupport_email}
                    form.set_text(".status", msg)
                    return
                form.set_text(".status",
                              _("the gift code has been messaged to you!"))
            elif payment_blob["goldtype"] == "onetime":
                admintools.adjust_gold_expiration(c.user, days=days)
                form.set_text(".status", _("the gold has been delivered!"))

            redirect_to_spent = True

            if not c.user.employee:
                c.user.gold_creddits -= months
                c.user._commit()

        form.find("button").hide()

        payment_blob["status"] = "processed"
        g.hardcache.set(blob_key, payment_blob, BLOB_TTL)

        if thing:
            gilding_message = make_gold_message(thing, user_gilded=True)
            jquery.gild_thing(thing_fullname, gilding_message, thing.gildings)
        elif redirect_to_spent:
            form.redirect("/gold/thanks?v=spent-creddits")

    @csrf_exempt
    @textresponse(paypal_secret = VPrintable('secret', 50),
                  payment_status = VPrintable('payment_status', 20),
                  txn_id = VPrintable('txn_id', 20),
                  paying_id = VPrintable('payer_id', 50),
                  payer_email = VPrintable('payer_email', 250),
                  mc_currency = VPrintable('mc_currency', 20),
                  mc_gross = VDecimal('mc_gross'),
                  custom = VPrintable('custom', 50))
    def POST_ipn(self, paypal_secret, payment_status, txn_id, paying_id,
                 payer_email, mc_currency, mc_gross, custom):

        parameters = request.POST.copy()

        # Make sure it's really PayPal
        if not constant_time_compare(paypal_secret,
                                     g.secrets['paypal_webhook']):
            raise ValueError

        # Return early if it's an IPN class we don't care about
        response, psl = check_payment_status(payment_status)
        if response:
            return response

        # Return early if it's a txn_type we don't care about
        response, subscription = check_txn_type(parameters['txn_type'], psl)
        if subscription is None:
            subscr_id = None
        elif subscription == "new":
            subscr_id = parameters['subscr_id']
        elif subscription == "cancel":
            cancel_subscription(parameters['subscr_id'])
        else:
            raise ValueError("Weird subscription: %r" % subscription)

        if response:
            return response

        if mc_currency != 'USD':
            raise ValueError("Somehow got non-USD IPN %r" % mc_currency)

        if not (txn_id and paying_id and payer_email and mc_gross):
            dump_parameters(parameters)
            raise ValueError("Got incomplete IPN")

        pennies = int(mc_gross * 100)
        months, days = months_and_days_from_pennies(pennies)

        # Special case: autorenewal payment
        existing = existing_subscription(subscr_id, paying_id, custom)
        if existing:
            if existing != "deleted account":
                try:
                    create_claimed_gold ("P" + txn_id, payer_email, paying_id,
                                         pennies, days, None, existing._id,
                                         c.start_time, subscr_id)
                except IntegrityError:
                    return "Ok"
                admintools.adjust_gold_expiration(existing, days=days)

                subject, message = subscr_pm(pennies, months, new_subscr=False)
                message = append_random_bottlecap_phrase(message)
                send_system_message(existing.name, subject, message,
                                    distinguished='gold-auto')

                g.log.info("Just applied IPN renewal for %s, %d days" %
                           (existing.name, days))
            return "Ok"

        # More sanity checks that all non-autorenewals should pass:

        if not custom:
            dump_parameters(parameters)
            raise ValueError("Got IPN with txn_id=%s and no custom"
                             % txn_id)

        self.finish(parameters, "P" + txn_id,
                    payer_email, paying_id, subscr_id,
                    custom, pennies, months, days)

    def finish(self, parameters, txn_id,
               payer_email, paying_id, subscr_id,
               custom, pennies, months, days):

        try:
            blob_key, payment_blob = get_blob(custom)
        except ValueError:
            g.log.error("whoops, %s was locked", custom)
            return

        buyer = None
        buyer_email = None
        buyer_id = payment_blob.get('account_id', None)
        if buyer_id:
            try:
                buyer = Account._byID(buyer_id, data=True)
            except NotFound:
                dump_parameters(parameters)
                raise ValueError("Invalid buyer_id %d in IPN with custom='%s'"
                                 % (buyer_id, custom))
        else:
            buyer_email = payment_blob.get('email')
            if not buyer_email:
                dump_parameters(parameters)
                error = "No buyer_id or email in IPN with custom='%s'" % custom
                raise ValueError(error)

        if subscr_id:
            buyer.gold_subscr_id = subscr_id

        instagift = False
        if payment_blob['goldtype'] == 'onetime':
            admintools.adjust_gold_expiration(buyer, days=days)

            subject = _("Eureka! Thank you for investing in reddit gold!")
            message = _("Thank you for buying reddit gold. Your patronage "
                        "supports the site and makes future development "
                        "possible. For example, one month of reddit gold "
                        "pays for 5 instance hours of reddit's servers.")
            message += "\n\n" + strings.gold_benefits_msg
            if g.lounge_reddit:
                message += "\n\n" + strings.lounge_msg
        elif payment_blob['goldtype'] == 'autorenew':
            admintools.adjust_gold_expiration(buyer, days=days)
            subject, message = subscr_pm(pennies, months, new_subscr=True)
        elif payment_blob['goldtype'] == 'creddits':
            buyer._incr("gold_creddits", months)
            buyer._commit()
            subject = _("Eureka! Thank you for investing in reddit gold "
                        "creddits!")

            message = _("Thank you for buying creddits. Your patronage "
                        "supports the site and makes future development "
                        "possible. To spend your creddits and spread reddit "
                        "gold, visit [/gold](/gold) or your favorite "
                        "person's user page.")
            message += "\n\n" + strings.gold_benefits_msg + "\n\n"
            message += _("Thank you again for your support, and have fun "
                         "spreading gold!")
        elif payment_blob['goldtype'] == 'gift':
            recipient_name = payment_blob.get('recipient', None)
            try:
                recipient = Account._by_name(recipient_name)
            except NotFound:
                dump_parameters(parameters)
                raise ValueError("Invalid recipient_name %s in IPN/GC with custom='%s'"
                                 % (recipient_name, custom))
            signed = payment_blob.get("signed", False)
            giftmessage = _force_unicode(payment_blob.get("giftmessage", ""))
            thing_fullname = payment_blob.get("thing")
            send_gift(buyer, recipient, months, days, signed, giftmessage,
                      thing_fullname)
            instagift = True
            subject = _("Thanks for giving the gift of reddit gold!")
            message = _("Your classy gift to %s has been delivered.\n\n"
                        "Thank you for gifting reddit gold. Your patronage "
                        "supports the site and makes future development "
                        "possible.") % recipient.name
            message += "\n\n" + strings.gold_benefits_msg + "\n\n"
            message += _("Thank you again for your support, and have fun "
                         "spreading gold!")
        elif payment_blob['goldtype'] == 'code':
            pass
        else:
            dump_parameters(parameters)
            raise ValueError("Got status '%s' in IPN/GC" % payment_blob['status'])

        if payment_blob['goldtype'] == 'code':
            send_gold_code(buyer, months, days, txn_id, payer_email,
                           pennies, buyer_email)
        else:
            # Reuse the old "secret" column as a place to record the goldtype
            # and "custom", just in case we need to debug it later or something
            secret = payment_blob['goldtype'] + "-" + custom

            if instagift:
                status="instagift"
            else:
                status="processed"

            create_claimed_gold(txn_id, payer_email, paying_id, pennies, days,
                                secret, buyer_id, c.start_time,
                                subscr_id, status=status)

            message = append_random_bottlecap_phrase(message)

            try:
                send_system_message(buyer, subject, message,
                                    distinguished='gold-auto')
            except MessageError:
                g.log.error('finish: could not send system message')

        payment_blob["status"] = "processed"
        g.hardcache.set(blob_key, payment_blob, BLOB_TTL)


class Webhook(object):
    def __init__(self, passthrough=None, transaction_id=None, subscr_id=None,
                 pennies=None, months=None, payer_email='', payer_id='',
                 goldtype=None, buyer=None, recipient=None, signed=False,
                 giftmessage=None, thing=None, buyer_email=None):
        self.passthrough = passthrough
        self.transaction_id = transaction_id
        self.subscr_id = subscr_id
        self.pennies = pennies
        self.months = months
        self.payer_email = payer_email
        self.payer_id = payer_id
        self.goldtype = goldtype
        self.buyer = buyer
        self.buyer_email = buyer_email
        self.recipient = recipient
        self.signed = signed
        self.giftmessage = giftmessage
        self.thing = thing

    def load_blob(self):
        payment_blob = validate_blob(self.passthrough)
        self.goldtype = payment_blob['goldtype']
        self.buyer = payment_blob.get('buyer')
        self.buyer_email = payment_blob.get('email')
        self.recipient = payment_blob.get('recipient')
        self.signed = payment_blob.get('signed', False)
        self.giftmessage = payment_blob.get('giftmessage')
        thing = payment_blob.get('thing')
        self.thing = thing._fullname if thing else None

    def __repr__(self):
        return '<%s: transaction %s>' % (self.__class__.__name__, self.transaction_id)


class GoldPaymentController(RedditController):
    name = ''
    webhook_secret = ''
    event_type_mappings = {}
    abort_on_error = True

    @csrf_exempt
    @textresponse(secret=VPrintable('secret', 50))
    def POST_goldwebhook(self, secret):
        self.validate_secret(secret)
        status, webhook = self.process_response()

        try:
            event_type = self.event_type_mappings[status]
        except KeyError:
            g.log.error('%s %s: unknown status %s' % (self.name,
                                                      webhook,
                                                      status))
            if self.abort_on_error:
                self.abort403()
            else:
                return
        self.process_webhook(event_type, webhook)

    def validate_secret(self, secret):
        if not constant_time_compare(secret, self.webhook_secret):
            g.log.error('%s: invalid webhook secret from %s' % (self.name,
                                                                request.ip))
            self.abort403() 

    @classmethod
    def process_response(cls):
        """Extract status and webhook."""
        raise NotImplementedError

    def process_webhook(self, event_type, webhook):
        if event_type == 'noop':
            return

        existing = retrieve_gold_transaction(webhook.transaction_id)
        if not existing and webhook.passthrough:
            try:
                webhook.load_blob()
            except GoldException as e:
                g.log.error('%s: payment_blob %s', webhook.transaction_id, e)
                if self.abort_on_error:
                    self.abort403()
                else:
                    return
        msg = None

        if event_type == 'cancelled':
            subject = _('reddit gold payment cancelled')
            msg = _('Your reddit gold payment has been cancelled, contact '
                    '%(gold_email)s for details') % {'gold_email':
                                                     g.goldsupport_email}
            if existing:
                # note that we don't check status on existing, probably
                # should update gold_table when a cancellation happens
                reverse_gold_purchase(webhook.transaction_id)
        elif event_type == 'succeeded':
            if (existing and
                    existing.status in ('processed', 'unclaimed', 'claimed')):
                g.log.info('POST_goldwebhook skipping %s' % webhook.transaction_id)
                return

            self.complete_gold_purchase(webhook)
        elif event_type == 'failed':
            subject = _('reddit gold payment failed')
            msg = _('Your reddit gold payment has failed, contact '
                    '%(gold_email)s for details') % {'gold_email':
                                                     g.goldsupport_email}
        elif event_type == 'deleted_subscription':
            # the subscription may have been deleted directly by the user using
            # POST_delete_subscription, in which case gold_subscr_id is already
            # unset and we don't need to message them
            if webhook.buyer and webhook.buyer.gold_subscr_id:
                subject = _('reddit gold subscription cancelled')
                msg = _('Your reddit gold subscription has been cancelled '
                        'because your credit card could not be charged. '
                        'Contact %(gold_email)s for details')
                msg %= {'gold_email': g.goldsupport_email}
                webhook.buyer.gold_subscr_id = None
                webhook.buyer._commit()
        elif event_type == 'refunded':
            if not (existing and existing.status == 'processed'):
                return

            subject = _('reddit gold refund')
            msg = _('Your reddit gold payment has been refunded, contact '
                   '%(gold_email)s for details') % {'gold_email':
                                                    g.goldsupport_email}
            reverse_gold_purchase(webhook.transaction_id)

        if msg:
            if existing:
                buyer = Account._byID(int(existing.account_id), data=True)
            elif webhook.buyer:
                buyer = webhook.buyer
            else:
                return

            try:
                send_system_message(buyer, subject, msg)
            except MessageError:
                g.log.error('process_webhook: send_system_message error')

    @classmethod
    def complete_gold_purchase(cls, webhook):
        """After receiving a message from a payment processor, apply gold.

        Shared endpoint for all payment processing systems. Validation of gold
        purchase (sender, recipient, etc.) should happen before hitting this.

        """

        secret = webhook.passthrough
        transaction_id = webhook.transaction_id
        payer_email = webhook.payer_email
        payer_id = webhook.payer_id
        subscr_id = webhook.subscr_id
        pennies = webhook.pennies
        months = webhook.months
        goldtype = webhook.goldtype
        buyer = webhook.buyer
        buyer_email = webhook.buyer_email
        recipient = webhook.recipient
        signed = webhook.signed
        giftmessage = webhook.giftmessage
        thing = webhook.thing

        days = days_from_months(months)

        # locking isn't necessary for code purchases
        if goldtype == 'code':
            send_gold_code(buyer, months, days, transaction_id,
                           payer_email, pennies, buyer_email)
            # the rest of the function isn't needed for a code purchase
            return

        gold_recipient = recipient or buyer
        with gold_recipient.get_read_modify_write_lock() as lock:
            gold_recipient.update_from_cache(lock)

            secret_pieces = [goldtype]
            if goldtype == 'gift':
                secret_pieces.append(recipient.name)
            secret_pieces.append(secret or transaction_id)
            secret = '-'.join(secret_pieces)

            if goldtype in ('onetime', 'autorenew'):
                admintools.adjust_gold_expiration(buyer, days=days)
                if goldtype == 'onetime':
                    subject = "thanks for buying reddit gold!"
                    if g.lounge_reddit:
                        message = strings.lounge_msg
                    else:
                        message = ":)"
                else:
                    if has_prev_subscr_payments(subscr_id):
                        secret = None
                        subject, message = subscr_pm(pennies, months, new_subscr=False)
                    else:
                        subject, message = subscr_pm(pennies, months, new_subscr=True)

            elif goldtype == 'creddits':
                buyer._incr('gold_creddits', months)
                subject = "thanks for buying creddits!"
                message = ("To spend them, visit %s://%s/gold or your "
                           "favorite person's userpage." % (g.default_scheme,
                                                            g.domain))

            elif goldtype == 'gift':
                send_gift(buyer, recipient, months, days, signed, giftmessage,
                          thing)
                subject = "thanks for giving reddit gold!"
                message = "Your gift to %s has been delivered." % recipient.name

            try:
                create_claimed_gold(transaction_id, payer_email, payer_id,
                                    pennies, days, secret, buyer._id,
                                    c.start_time, subscr_id=subscr_id,
                                    status='processed')
            except IntegrityError:
                g.log.error('gold: got duplicate gold transaction')

            try:
                message = append_random_bottlecap_phrase(message)
                send_system_message(buyer, subject, message,
                                    distinguished='gold-auto')
            except MessageError:
                g.log.error('complete_gold_purchase: send_system_message error')


def handle_stripe_error(fn):
    def wrapper(cls, form, *a, **kw):
        try:
            return fn(cls, form, *a, **kw)
        except stripe.CardError as e:
            form.set_text('.status',
                          _('error: %(error)s') % {'error': e.message})
        except stripe.InvalidRequestError as e:
            form.set_text('.status', _('invalid request'))
        except stripe.APIConnectionError as e:
            form.set_text('.status', _('api error'))
        except stripe.AuthenticationError as e:
            form.set_text('.status', _('connection error'))
        except stripe.StripeError as e:
            form.set_text('.status', _('error'))
            g.log.error('stripe error: %s' % e)
        except:
            raise
        form.find('.stripe-submit').removeAttr('disabled').end()
    return wrapper


class StripeController(GoldPaymentController):
    name = 'stripe'
    webhook_secret = g.secrets['stripe_webhook']
    event_type_mappings = {
        'charge.succeeded': 'succeeded',
        'charge.failed': 'failed',
        'charge.refunded': 'refunded',
        'charge.dispute.created': 'noop',
        'charge.dispute.updated': 'noop',
        'charge.dispute.closed': 'noop',
        'charge.dispute.funds_withdrawn': 'noop',
        'charge.updated': 'noop',
        'customer.created': 'noop',
        'customer.card.created': 'noop',
        'customer.card.updated': 'noop',
        'customer.card.deleted': 'noop',
        'customer.source.updated': 'noop',
        'transfer.created': 'noop',
        'transfer.paid': 'noop',
        'balance.available': 'noop',
        'invoice.created': 'noop',
        'invoice.updated': 'noop',
        'invoice.payment_succeeded': 'noop',
        'invoice.payment_failed': 'noop',
        'invoiceitem.deleted': 'noop',
        'customer.subscription.created': 'noop',
        'customer.deleted': 'noop',
        'customer.updated': 'noop',
        'customer.subscription.deleted': 'deleted_subscription',
        'customer.subscription.trial_will_end': 'noop',
        'customer.subscription.updated': 'noop',
        'review.opened': 'noop',
        'dummy': 'noop',
    }

    @classmethod
    def process_response(cls):
        event_dict = json.loads(request.body)
        stripe_secret = g.secrets['stripe_secret_key']
        event = stripe.Event.construct_from(event_dict, stripe_secret)
        status = event.type

        if status == 'invoice.created':
            # sent 1 hr before a subscription is charged or immediately for
            # a new subscription
            invoice = event.data.object
            customer_id = invoice.customer
            account = account_from_stripe_customer_id(customer_id)
            # if the charge hasn't been attempted (meaning this is 1 hr before
            # the charge) check that the account can receive the gold
            if (not invoice.attempted and
                (not account or (account and account._banned))):
                # there's no associated account - delete the subscription
                # to cancel the charge
                g.log.error('no account for stripe invoice: %s', invoice)
                try:
                    cancel_stripe_subscription(customer_id)
                except stripe.InvalidRequestError:
                    pass
        elif status == 'customer.subscription.deleted':
            subscription = event.data.object
            customer_id = subscription.customer
            buyer = account_from_stripe_customer_id(customer_id)
            webhook = Webhook(subscr_id=customer_id, buyer=buyer)
            return status, webhook

        event_type = cls.event_type_mappings.get(status)
        if not event_type:
            raise ValueError('Stripe: unrecognized status %s' % status)
        elif event_type == 'noop':
            return status, None

        charge = event.data.object
        description = charge.description
        invoice_id = charge.invoice
        transaction_id = 'S%s' % charge.id
        pennies = charge.amount
        months, days = months_and_days_from_pennies(pennies)

        if status == 'charge.failed' and invoice_id:
            # we'll get an additional failure notification event of
            # "invoice.payment_failed", don't double notify
            return 'dummy', None
        elif status == 'charge.failed' and not description:
            # create_customer can POST successfully but fail to create a
            # customer because the card is declined. This will trigger a
            # 'charge.failed' notification but without description so we can't
            # do anything with it
            return 'dummy', None
        elif invoice_id:
            # subscription charge - special handling
            customer_id = charge.customer
            buyer = account_from_stripe_customer_id(customer_id)
            if not buyer and status == 'charge.refunded':
                # refund may happen after the subscription has been cancelled
                # and removed from the user's account. the refund process will
                # be able to find the user from the transaction record
                webhook = Webhook(transaction_id=transaction_id)
                return status, webhook
            elif not buyer:
                charge_date = datetime.fromtimestamp(charge.created, tz=g.tz)

                # don't raise exception if charge date is within the past hour
                # db replication lag may cause the account lookup to fail
                if charge_date < timeago('1 hour'):
                    raise ValueError('no buyer for charge: %s' % charge.id)
                else:
                    abort(404, "not found")
            webhook = Webhook(transaction_id=transaction_id,
                              subscr_id=customer_id, pennies=pennies,
                              months=months, goldtype='autorenew',
                              buyer=buyer)
            return status, webhook
        else:
            try:
                passthrough = description[:20]
            except (AttributeError, ValueError):
                g.log.error('stripe_error on charge: %s', charge)
                raise

            webhook = Webhook(passthrough=passthrough,
                transaction_id=transaction_id, pennies=pennies, months=months)
            return status, webhook

    @classmethod
    @handle_stripe_error
    def create_customer(cls, form, token, description):
        customer = stripe.Customer.create(card=token, description=description)

        if (customer['active_card']['address_line1_check'] == 'fail' or
            customer['active_card']['address_zip_check'] == 'fail'):
            form.set_text('.status',
                          _('error: address verification failed'))
            form.find('.stripe-submit').removeAttr('disabled').end()
            return None
        elif customer['active_card']['cvc_check'] == 'fail':
            form.set_text('.status', _('error: cvc check failed'))
            form.find('.stripe-submit').removeAttr('disabled').end()
            return None
        else:
            return customer

    @classmethod
    @handle_stripe_error
    def charge_customer(cls, form, customer, pennies, passthrough,
                        description):
        charge = stripe.Charge.create(
            amount=pennies,
            currency="usd",
            customer=customer['id'],
            description='%s-%s' % (passthrough, description),
        )
        return charge

    @classmethod
    @handle_stripe_error
    def set_creditcard(cls, form, user, token):
        if not user.has_stripe_subscription:
            return

        customer = stripe.Customer.retrieve(user.gold_subscr_id)
        customer.card = token
        customer.save()
        return customer

    @classmethod
    @handle_stripe_error
    def set_subscription(cls, form, customer, plan_id):
        subscription = customer.update_subscription(plan=plan_id)
        return subscription

    @classmethod
    @handle_stripe_error
    def cancel_subscription(cls, form, user):
        if not user.has_stripe_subscription:
            return

        customer = cancel_stripe_subscription(user.gold_subscr_id)

        user.gold_subscr_id = None
        user._commit()
        subject = _('your gold subscription has been cancelled')
        message = _('if you have any questions please email %(email)s')
        message %= {'email': g.goldsupport_email}
        send_system_message(user, subject, message)
        return customer

    @csrf_exempt
    @validatedForm(token=nop('stripeToken'),
                   passthrough=VPrintable("passthrough", max_length=50),
                   pennies=VInt('pennies'),
                   months=VInt("months"),
                   period=VOneOf("period", ("monthly", "yearly")))
    def POST_goldcharge(self, form, jquery, token, passthrough, pennies, months,
                        period):
        """
        Submit charge to stripe.

        Called by GoldPayment form. This submits the charge to stripe, and gold
        will be applied once we receive a webhook from stripe.

        """

        try:
            payment_blob = validate_blob(passthrough)
        except GoldException as e:
            # This should never happen. All fields in the payment_blob
            # are validated on creation
            form.set_text('.status',
                          _('something bad happened, try again later'))
            g.log.debug('POST_goldcharge: %s' % e.message)
            return

        if period:
            plan_id = (g.STRIPE_MONTHLY_GOLD_PLAN if period == 'monthly'
                       else g.STRIPE_YEARLY_GOLD_PLAN)
            if c.user.has_gold_subscription:
                form.set_text('.status',
                              _('your account already has a gold subscription'))
                return
        else:
            plan_id = None
            penny_months, days = months_and_days_from_pennies(pennies)
            if not months or months != penny_months:
                form.set_text('.status', _('stop trying to trick the form'))
                return

        if c.user_is_loggedin:
            description = c.user.name
        else:
            description = payment_blob["email"]
        customer = self.create_customer(form, token, description)
        if not customer:
            return

        if period:
            subscription = self.set_subscription(form, customer, plan_id)
            if not subscription:
                return

            c.user.gold_subscr_id = customer.id
            c.user._commit()

            status = _('subscription created')
            subject = _('reddit gold subscription')
            body = _('Your subscription is being processed and reddit gold '
                     'will be delivered shortly.')
        else:
            charge = self.charge_customer(form, customer, pennies,
                                          passthrough, description)
            if not charge:
                return

            status = _('payment submitted')
            subject = _('reddit gold payment')
            body = _('Your payment is being processed and reddit gold '
                     'will be delivered shortly.')

        form.set_text('.status', status)
        if c.user_is_loggedin:
            body = append_random_bottlecap_phrase(body)
            send_system_message(c.user, subject, body, distinguished='gold-auto')
            form.redirect("/gold/thanks?v=stripe")

    @validatedForm(VUser(),
                   VModhash(),
                   token=nop('stripeToken'))
    def POST_modify_subscription(self, form, jquery, token):
        customer = self.set_creditcard(form, c.user, token)
        if not customer:
            return

        form.set_text('.status', _('your payment details have been updated'))

    @validatedForm(VUser(),
                   VModhash(),
                   user=VByName('user'))
    def POST_cancel_subscription(self, form, jquery, user):
        if user != c.user and not c.user_is_admin:
            self.abort403()
        customer = self.cancel_subscription(form, user)
        if not customer:
            return

        form.set_text(".status", _("your subscription has been cancelled"))

class CoinbaseController(GoldPaymentController):
    name = 'coinbase'
    webhook_secret = g.secrets['coinbase_webhook']
    event_type_mappings = {
        'completed': 'succeeded',
        'cancelled': 'cancelled',
        'mispaid': 'noop',
        'expired': 'noop',
        'payout': 'noop',
    }
    abort_on_error = False

    @classmethod
    def process_response(cls):
        event_dict = json.loads(request.body)

        # handle non-payment events we can ignore
        if 'payout' in event_dict:
            return 'payout', None

        order = event_dict['order']
        transaction_id = 'C%s' % order['id']
        status = order['status']    # new/completed/cancelled
        pennies = int(order['total_native']['cents'])
        months, days = months_and_days_from_pennies(pennies, discount=True)
        passthrough = order['custom']
        webhook = Webhook(passthrough=passthrough,
            transaction_id=transaction_id, pennies=pennies, months=months)
        return status, webhook


class RedditGiftsController(GoldPaymentController):
    """Handle notifications of gold purchases from reddit gifts.

    Payment is handled by reddit gifts. Once an order is complete they can hit
    this route to apply gold to a user's account.

    The post should include data in the form:
    {
        'transaction_id', transaction_id,
        'goldtype': goldtype,
        'buyer': buyer name,
        'pennies': pennies,
        'months': months,
        ['recipient': recipient name,]
        ['giftmessage': message,]
        ['signed': bool,]
    }

    """

    name = 'redditgifts'
    webhook_secret = g.secrets['redditgifts_webhook']
    event_type_mappings = {'succeeded': 'succeeded'}

    def process_response(self):
        data = request.POST

        transaction_id = 'RG%s' % data['transaction_id']
        pennies = int(data['pennies'])
        months = int(data['months'])
        status = 'succeeded'

        goldtype = data['goldtype']
        buyer = Account._by_name(data['buyer'])

        if goldtype == 'gift':
            gift_kw = {
                'recipient': Account._by_name(data['recipient']),
                'giftmessage': _force_unicode(data.get('giftmessage', None)),
                'signed': data.get('signed') == 'True',
            }
        else:
            gift_kw = {}

        webhook = Webhook(transaction_id=transaction_id, pennies=pennies,
                          months=months, goldtype=goldtype, buyer=buyer,
                          **gift_kw)
        return status, webhook


class GoldException(Exception): pass


def validate_blob(custom):
    """Validate payment_blob and return a dict with everything looked up."""
    ret = {}

    if not custom:
        raise GoldException('no custom')

    payment_blob = g.hardcache.get('payment_blob-%s' % str(custom))
    if not payment_blob:
        raise GoldException('no payment_blob')

    if 'account_id' in payment_blob and 'account_name' in payment_blob:
        try:
            buyer = Account._byID(payment_blob['account_id'], data=True)
            ret['buyer'] = buyer
        except NotFound:
            raise GoldException('bad account_id')

        if not buyer.name.lower() == payment_blob['account_name'].lower():
            raise GoldException('buyer mismatch')
    elif 'email' in payment_blob:
        ret['email'] = payment_blob['email']
    else:
        raise GoldException('no account_id or email')

    goldtype = payment_blob['goldtype']
    ret['goldtype'] = goldtype

    if goldtype == 'gift':
        recipient_name = payment_blob.get('recipient', None)
        if not recipient_name:
            raise GoldException('gift missing recpient')
        try:
            recipient = Account._by_name(recipient_name)
            ret['recipient'] = recipient
        except NotFound:
            raise GoldException('bad recipient')
        thing_fullname = payment_blob.get('thing', None)
        if thing_fullname:
            try:
                ret['thing'] = Thing._by_fullname(thing_fullname)
            except NotFound:
                raise GoldException('bad thing')
        ret['signed'] = payment_blob.get('signed', False)
        giftmessage = payment_blob.get('giftmessage')
        giftmessage = _force_unicode(giftmessage) if giftmessage else None
        ret['giftmessage'] = giftmessage
    elif goldtype not in ('onetime', 'autorenew', 'creddits', 'code'):
        raise GoldException('bad goldtype')

    return ret


def days_from_months(months):
    if months >= 12:
        assert months % 12 == 0
        years = months / 12
        days = years * 366
    else:
        days = months * 31
    return days


def subtract_gold_days(user, days):
    user.gold_expiration -= timedelta(days=days)
    if user.gold_expiration < datetime.now(g.display_tz):
        admintools.degolden(user)
    user._commit()


def subtract_gold_creddits(user, num):
    user._incr('gold_creddits', -num)


def reverse_gold_purchase(transaction_id):
    transaction = retrieve_gold_transaction(transaction_id)

    if not transaction:
        raise GoldException('gold_table %s not found' % transaction_id)

    buyer = Account._byID(int(transaction.account_id), data=True)
    recipient = None
    days = transaction.days
    months = days / 31

    if transaction.subscr_id:
        goldtype = 'autorenew'
    else:
        secret = transaction.secret
        pieces = secret.split('-')
        goldtype = pieces[0]

    if goldtype == 'gift':
        recipient_name, secret = pieces[1:]
        recipient = Account._by_name(recipient_name)

    gold_recipient = recipient or buyer
    with gold_recipient.get_read_modify_write_lock() as lock:
        gold_recipient.update_from_cache(lock)

        if goldtype in ('onetime', 'autorenew'):
            subtract_gold_days(buyer, days)

        elif goldtype == 'creddits':
            subtract_gold_creddits(buyer, months)

        elif goldtype == 'gift':
            subtract_gold_days(recipient, days)
            subject = 'your gifted gold has been reversed'
            message = 'sorry, but the payment was reversed'
            send_system_message(recipient, subject, message)
    update_gold_transaction(transaction_id, 'reversed')


def cancel_stripe_subscription(customer_id):
    customer = stripe.Customer.retrieve(customer_id)
    if hasattr(customer, 'deleted'):
        return customer
    customer.delete()
    return customer


def subscr_pm(pennies, months, new_subscr=True):
    price = "$%0.2f" % (pennies/100.0)
    if new_subscr:
        if months % 12 == 0:
            message = _("You have created a yearly Reddit Gold subscription "
                "for %(price)s per year.\n\nThis subscription will renew "
                "automatically yearly until you cancel. You may cancel your "
                "subscription at any time by visiting %(subscr_url)s.\n\n")
        else:
            message = _("You have created a monthly Reddit Gold subscription "
                "for %(price)s per month.\n\nThis subscription will renew "
                "automatically monthly until you cancel. You may cancel your "
                "subscription at any time by visiting %(subscr_url)s.\n\n")
    else:
        if months == 1:
            message = _("Your Reddit Gold subscription has been renewed "
                "for 1 month for %(price)s.\n\nThis subscription will renew "
                "automatically monthly until you cancel. You may cancel your "
                "subscription at any time by visiting %(subscr_url)s.\n\n")
        else:
            message = _("Your Reddit Gold subscription has been renewed "
                "for 1 year for %(price)s.\n\nThis subscription will renew "
                "automatically yearly until you cancel. You may cancel your "
                "subscription at any time by visiting %(subscr_url)s.\n\n")

    subject = _("Reddit Gold Subscription")
    message += _("If you cancel, you will not be billed for any additional "
        "months of service, and service will continue until the end of the "
        "billing period. If you cancel, you will not receive a refund for any "
        "service already paid for.\n\nIf you have any questions, please "
        "contact %(gold_email)s.")

    message %= {
        "price": price,
        "subscr_url": "https://www.reddit.com/gold/subscription",
        "gold_email": g.goldsupport_email,
    }
    return subject, message
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.controllers.reddit_base import RedditController
from r2.lib.pages import Newsletter


class NewsletterController(RedditController):
    def GET_newsletter(self):
        return Newsletter().render()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons import url
from pylons.controllers.util import redirect
from pylons.i18n import _

from r2.lib.pages import *
from reddit_base import set_over18_cookie, delete_over18_cookie
from api import ApiController
from r2.lib.utils import query_string, UrlParser
from r2.lib.emailer import opt_in, opt_out
from r2.lib.validator import *
from r2.lib.validator.preferences import (
    filter_prefs,
    PREFS_VALIDATORS,
    set_prefs,
)
from r2.lib.csrf import csrf_exempt
from r2.models.recommend import ExploreSettings
from r2.controllers.login import handle_login, handle_register
from r2.models import *
from r2.config import feature

class PostController(ApiController):
    @csrf_exempt
    @validate(pref_lang = VLang('lang'),
              all_langs = VOneOf('all-langs', ('all', 'some'), default='all'))
    def POST_unlogged_options(self, all_langs, pref_lang):
        prefs = {"pref_lang": pref_lang}
        set_prefs(c.user, prefs)
        c.user._commit()
        return self.redirect(request.referer)

    @validate(VUser(), VModhash(),
              all_langs=VOneOf('all-langs', ('all', 'some'), default='all'),
              **PREFS_VALIDATORS)
    def POST_options(self, all_langs, **prefs):
        if feature.is_enabled("autoexpand_media_previews"):
            validator = VOneOf('media_preview', ('on', 'off', 'subreddit'))
            value = request.params.get('media_preview')
            prefs["pref_media_preview"] = validator.run(value)

        u = UrlParser(c.site.path + "prefs")

        filter_prefs(prefs, c.user)
        if c.errors.errors:
            for error in c.errors.errors:
                if error[1] == 'stylesheet_override':
                    u.update_query(error_style_override=error[0])
                else:
                    u.update_query(generic_error=error[0])
            return self.redirect(u.unparse())

        set_prefs(c.user, prefs)
        c.user._commit()
        u.update_query(done='true')
        return self.redirect(u.unparse())

    def GET_over18(self):
        return InterstitialPage(
            _("over 18?"),
            content=Over18Interstitial(),
        ).render()

    @validate(
        dest=VDestination(default='/'),
    )
    def GET_quarantine(self, dest):
        sr = UrlParser(dest).get_subreddit()

        # if dest doesn't include a quarantined subreddit,
        # redirect to the homepage or the original destination
        if not sr:
            return self.redirect('/')
        elif isinstance(sr, FakeSubreddit) or sr.is_exposed(c.user):
            return self.redirect(dest)

        errpage = InterstitialPage(
            _("quarantined"),
            content=QuarantineInterstitial(
                sr_name=sr.name,
                logged_in=c.user_is_loggedin,
                email_verified=c.user_is_loggedin and c.user.email_verified,
            ),
        )
        request.environ['usable_error_content'] = errpage.render()
        self.abort403()

    @csrf_exempt
    @validate(
        over18=nop('over18'),
        dest=VDestination(default='/'),
    )
    def POST_over18(self, over18, dest):
        if over18 == 'yes':
            if c.user_is_loggedin and not c.errors:
                c.user.pref_over_18 = True
                c.user._commit()
            else:
                set_over18_cookie()
            return self.redirect(dest)
        else:
            if c.user_is_loggedin and not c.errors:
                c.user.pref_over_18 = False
                c.user._commit()
            else:
                delete_over18_cookie()
            return self.redirect('/')

    @validate(
        VModhash(fatal=False),
        sr=VSRByName('sr_name'),
        accept=VBoolean('accept'),
        dest=VDestination(default='/'),
    )
    def POST_quarantine(self, sr, accept, dest):
        can_opt_in = c.user_is_loggedin and c.user.email_verified

        if accept and can_opt_in and not c.errors:
            QuarantinedSubredditOptInsByAccount.opt_in(c.user, sr)
            g.events.quarantine_event('quarantine_opt_in', sr,
                request=request, context=c)
            return self.redirect(dest)
        else:
            if c.user_is_loggedin and not c.errors:
                QuarantinedSubredditOptInsByAccount.opt_out(c.user, sr)
            g.events.quarantine_event('quarantine_interstitial_dismiss', sr,
                request=request, context=c)
            return self.redirect('/')

    @csrf_exempt
    @validate(msg_hash = nop('x'))
    def POST_optout(self, msg_hash):
        email, sent = opt_out(msg_hash)
        if not email:
            return self.abort404()
        return BoringPage(_("opt out"),
                          content = OptOut(email = email, leave = True,
                                           sent = True,
                                           msg_hash = msg_hash)).render()

    @csrf_exempt
    @validate(msg_hash = nop('x'))
    def POST_optin(self, msg_hash):
        email, sent = opt_in(msg_hash)
        if not email:
            return self.abort404()
        return BoringPage(_("welcome back"),
                          content = OptOut(email = email, leave = False,
                                           sent = True,
                                           msg_hash = msg_hash)).render()


    @csrf_exempt
    @validate(dest = VDestination(default = "/"))
    def POST_login(self, dest, *a, **kw):
        super(PostController, self).POST_login(*a, **kw)
        c.render_style = "html"
        response.content_type = "text/html"

        if not c.user_is_loggedin:
            return LoginPage(user_login = request.POST.get('user'),
                             dest = dest).render()

        return self.redirect(dest)

    @csrf_exempt
    @validate(dest = VDestination(default = "/"))
    def POST_reg(self, dest, *a, **kw):
        super(PostController, self).POST_register(*a, **kw)
        c.render_style = "html"
        response.content_type = "text/html"

        if not c.user_is_loggedin:
            return LoginPage(user_reg = request.POST.get('user'),
                             dest = dest).render()

        return self.redirect(dest)

    def GET_login(self, *a, **kw):
        return self.redirect('/login' + query_string(dict(dest="/")))

    @validatedForm(
        VUser(),
        VModhash(),
        personalized=VBoolean('pers', default=False),
        discovery=VBoolean('disc', default=False),
        rising=VBoolean('ris', default=False),
        nsfw=VBoolean('nsfw', default=False),
    )
    def POST_explore_settings(self,
                              form,
                              jquery,
                              personalized,
                              discovery,
                              rising,
                              nsfw):
        ExploreSettings.record_settings(
            c.user,
            personalized=personalized,
            discovery=discovery,
            rising=rising,
            nsfw=nsfw,
        )
        return redirect(url(controller='front', action='explore'))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.config.extensions import set_extension
from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.reddit_base import RedditController, abort_with_error
from r2.controllers.oauth2 import require_oauth2_scope
from r2.models.account import Account
from r2.models.subreddit import (
    FakeSubreddit,
    Subreddit,
    LabeledMulti,
    TooManySubredditsError,
)
from r2.lib.db import tdb_cassandra
from r2.lib.validator import (
    validate,
    VAccountByName,
    VBoolean,
    VColor,
    VLength,
    VMarkdownLength,
    VModhash,
    VMultiPath,
    VMultiByPath,
    VOneOf,
    VSubredditName,
    VSRByName,
    VUser,
    VValidatedJSON,
)
from r2.lib.pages.things import wrap_things
from r2.lib.jsontemplates import (
    LabeledMultiJsonTemplate,
    LabeledMultiDescriptionJsonTemplate,
)
from r2.lib.errors import RedditError


multi_sr_data_json_spec = VValidatedJSON.Object({
    'name': VSubredditName('name', allow_language_srs=True),
})

MAX_DESC = 10000
MAX_DISP_NAME = 50
WRITABLE_MULTI_FIELDS = ('visibility', 'description_md', 'display_name',
                         'key_color', 'weighting_scheme')

multi_json_spec = VValidatedJSON.PartialObject({
    'description_md': VMarkdownLength('description_md', max_length=MAX_DESC,
                                      empty_error=None),
    'display_name': VLength('display_name', max_length=MAX_DISP_NAME),
    'icon_name': VOneOf('icon_name', g.multi_icons + ("", None)),
    'key_color': VColor('key_color'),
    'visibility': VOneOf('visibility', ('private', 'public', 'hidden')),
    'weighting_scheme': VOneOf('weighting_scheme', ('classic', 'fresh')),
    'subreddits': VValidatedJSON.ArrayOf(multi_sr_data_json_spec),
})


multi_description_json_spec = VValidatedJSON.Object({
    'body_md': VMarkdownLength('body_md', max_length=MAX_DESC, empty_error=None),
})


class MultiApiController(RedditController):
    def on_validation_error(self, error):
        abort_with_error(error, error.code or 400)

    def pre(self):
        set_extension(request.environ, "json")
        RedditController.pre(self)

    def _format_multi_list(self, multis, viewer, expand_srs):
        templ = LabeledMultiJsonTemplate(expand_srs)
        resp = [templ.render(multi).finalize() for multi in multis
                if multi.can_view(viewer)]
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(
        user=VAccountByName("username"),
        expand_srs=VBoolean("expand_srs"),
    )
    @api_doc(api_section.multis, uri="/api/multi/user/{username}")
    def GET_list_multis(self, user, expand_srs):
        """Fetch a list of public multis belonging to `username`"""
        multis = LabeledMulti.by_owner(user)
        return self._format_multi_list(multis, c.user, expand_srs)

    @require_oauth2_scope("read")
    @validate(VUser(), expand_srs=VBoolean("expand_srs"))
    @api_doc(api_section.multis, uri="/api/multi/mine")
    def GET_my_multis(self, expand_srs):
        """Fetch a list of multis belonging to the current user."""
        multis = LabeledMulti.by_owner(c.user)
        return self._format_multi_list(multis, c.user, expand_srs)

    def _format_multi(self, multi, expand_sr_info=False):
        multi_info = LabeledMultiJsonTemplate(expand_sr_info).render(multi)
        return self.api_wrapper(multi_info.finalize())

    @require_oauth2_scope("read")
    @validate(
        multi=VMultiByPath("multipath", require_view=True),
        expand_srs=VBoolean("expand_srs"),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}",
        uri_variants=['/api/filter/{filterpath}'],
    )
    def GET_multi(self, multi, expand_srs):
        """Fetch a multi's data and subreddit list by name."""
        return self._format_multi(multi, expand_srs)

    def _check_new_multi_path(self, path_info):
        if path_info['owner'].lower() != c.user.name.lower():
            raise RedditError('MULTI_CANNOT_EDIT', code=403,
                              fields='multipath')
        return c.user

    def _add_multi_srs(self, multi, sr_datas):
        srs = Subreddit._by_name(sr_data['name'] for sr_data in sr_datas)

        for sr in srs.itervalues():
            if isinstance(sr, FakeSubreddit):
                raise RedditError('MULTI_SPECIAL_SUBREDDIT',
                                  msg_params={'path': sr.path},
                                  code=400)

        sr_props = {}
        for sr_data in sr_datas:
            try:
                sr = srs[sr_data['name']]
            except KeyError:
                raise RedditError('SUBREDDIT_NOEXIST', code=400)
            else:
                # name is passed in via the API data format, but should not be
                # stored on the model.
                del sr_data['name']
                sr_props[sr] = sr_data

        try:
            multi.add_srs(sr_props)
        except TooManySubredditsError as e:
            raise RedditError('MULTI_TOO_MANY_SUBREDDITS', code=409)

        return sr_props

    def _write_multi_data(self, multi, data):
        srs = data.pop('subreddits', None)
        if srs is not None:
            multi.clear_srs()
            try:
                self._add_multi_srs(multi, srs)
            except:
                multi._revert()
                raise

        if 'icon_name' in data:
            try:
                multi.set_icon_by_name(data.pop('icon_name'))
            except:
                multi._revert()
                raise

        for key, val in data.iteritems():
            if key in WRITABLE_MULTI_FIELDS:
                setattr(multi, key, val)

        multi._commit()
        return multi

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        path_info=VMultiPath("multipath", required=False),
        data=VValidatedJSON("model", multi_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi)
    def POST_multi(self, path_info, data):
        """Create a multi. Responds with 409 Conflict if it already exists."""

        if not path_info and "path" in data:
            path_info = VMultiPath("").run(data["path"])
        elif 'display_name' in data:
            # if path not provided, create multi for user
            path = LabeledMulti.slugify(c.user, data['display_name'])
            path_info = VMultiPath("").run(path)

        if not path_info:
            raise RedditError('BAD_MULTI_PATH', code=400)

        owner = self._check_new_multi_path(path_info)

        try:
            LabeledMulti._byID(path_info['path'])
        except tdb_cassandra.NotFound:
            multi = LabeledMulti.create(path_info['path'], owner)
            response.status = 201
        else:
            raise RedditError('MULTI_EXISTS', code=409, fields='multipath')

        self._write_multi_data(multi, data)
        return self._format_multi(multi)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        path_info=VMultiPath("multipath"),
        data=VValidatedJSON("model", multi_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi)
    def PUT_multi(self, path_info, data):
        """Create or update a multi."""

        owner = self._check_new_multi_path(path_info)

        try:
            multi = LabeledMulti._byID(path_info['path'])
        except tdb_cassandra.NotFound:
            multi = LabeledMulti.create(path_info['path'], owner)
            response.status = 201

        self._write_multi_data(multi, data)
        return self._format_multi(multi)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
    )
    @api_doc(api_section.multis, extends=GET_multi)
    def DELETE_multi(self, multi):
        """Delete a multi."""
        multi.delete()

    def _copy_multi(self, from_multi, to_path_info, rename=False):
        """Copy a multi to a user account."""

        to_owner = self._check_new_multi_path(to_path_info)

        # rename requires same owner
        if rename and from_multi.owner != to_owner:
            raise RedditError('MULTI_CANNOT_EDIT', code=400)

        try:
            LabeledMulti._byID(to_path_info['path'])
        except tdb_cassandra.NotFound:
            to_multi = LabeledMulti.copy(to_path_info['path'], from_multi,
                                         owner=to_owner)
        else:
            raise RedditError('MULTI_EXISTS', code=409, fields='multipath')

        return to_multi

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        from_multi=VMultiByPath("from", require_view=True, kinds='m'),
        to_path_info=VMultiPath("to", required=False,
            docs={"to": "destination multireddit url path"},
        ),
        display_name=VLength("display_name", max_length=MAX_DISP_NAME,
                             empty_error=None),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/copy",
    )
    def POST_multi_copy(self, from_multi, to_path_info, display_name):
        """Copy a multi.

        Responds with 409 Conflict if the target already exists.

        A "copied from ..." line will automatically be appended to the
        description.

        """
        if not to_path_info:
            if display_name:
                # if path not provided, copy multi to same owner
                path = LabeledMulti.slugify(from_multi.owner, display_name)
                to_path_info = VMultiPath("").run(path)
            else:
                raise RedditError('BAD_MULTI_PATH', code=400)

        to_multi = self._copy_multi(from_multi, to_path_info)

        from_path = from_multi.path
        to_multi.copied_from = from_path
        if to_multi.description_md:
            to_multi.description_md += '\n\n'
        to_multi.description_md += _('copied from %(source)s') % {
            # force markdown linking since /user/foo is not autolinked
            'source': '[%s](%s)' % (from_path, from_path)
        }
        to_multi.visibility = 'private'
        if display_name:
            to_multi.display_name = display_name
        to_multi._commit()

        return self._format_multi(to_multi)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        from_multi=VMultiByPath("from", require_edit=True, kinds='m'),
        to_path_info=VMultiPath("to", required=False,
            docs={"to": "destination multireddit url path"},
        ),
        display_name=VLength("display_name", max_length=MAX_DISP_NAME,
                             empty_error=None),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/rename",
    )
    def POST_multi_rename(self, from_multi, to_path_info, display_name):
        """Rename a multi."""
        if not to_path_info:
            if display_name:
                path = LabeledMulti.slugify(from_multi.owner, display_name)
                to_path_info = VMultiPath("").run(path)
            else:
                raise RedditError('BAD_MULTI_PATH', code=400)

        to_multi = self._copy_multi(from_multi, to_path_info, rename=True)

        if display_name:
            to_multi.display_name = display_name
            to_multi._commit()
        from_multi.delete()

        return self._format_multi(to_multi)

    def _get_multi_subreddit(self, multi, sr):
        resp = LabeledMultiJsonTemplate.sr_props(multi, [sr])[0]
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(
        VUser(),
        multi=VMultiByPath("multipath", require_view=True),
        sr=VSRByName('srname'),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}/r/{srname}",
        uri_variants=['/api/filter/{filterpath}/r/{srname}'],
    )
    def GET_multi_subreddit(self, multi, sr):
        """Get data about a subreddit in a multi."""
        return self._get_multi_subreddit(multi, sr)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
        sr_name=VSubredditName('srname', allow_language_srs=True),
        data=VValidatedJSON("model", multi_sr_data_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi_subreddit)
    def PUT_multi_subreddit(self, multi, sr_name, data):
        """Add a subreddit to a multi."""

        new = not any(sr.name.lower() == sr_name.lower() for sr in multi.srs)

        data['name'] = sr_name
        sr_props = self._add_multi_srs(multi, [data])
        sr = sr_props.items()[0][0]
        multi._commit()

        if new:
            response.status = 201

        return self._get_multi_subreddit(multi, sr)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
        sr=VSRByName('srname'),
    )
    @api_doc(api_section.multis, extends=GET_multi_subreddit)
    def DELETE_multi_subreddit(self, multi, sr):
        """Remove a subreddit from a multi."""
        multi.del_srs(sr)
        multi._commit()

    def _format_multi_description(self, multi):
        resp = LabeledMultiDescriptionJsonTemplate().render(multi).finalize()
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(
        VUser(),
        multi=VMultiByPath("multipath", require_view=True, kinds='m'),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}/description",
    )
    def GET_multi_description(self, multi):
        """Get a multi's description."""
        return self._format_multi_description(multi)

    @require_oauth2_scope("read")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True, kinds='m'),
        data=VValidatedJSON('model', multi_description_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi_description)
    def PUT_multi_description(self, multi, data):
        """Change a multi's markdown description."""
        multi.description_md = data['body_md']
        multi._commit()
        return self._format_multi_description(multi)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import json
import re
import simplejson
import socket
import itertools

from Cookie import CookieError
from copy import copy
from datetime import datetime, timedelta
from functools import wraps
from hashlib import sha1, md5
from urllib import quote, unquote
from urlparse import urlparse

import babel.core
import pylibmc

from mako.filters import url_escape
from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _
from pylons.i18n.translation import LanguageError

from r2.config import feature
from r2.config.extensions import is_api, set_extension
from r2.lib import (
    baseplate_integration,
    filters,
    geoip,
    hooks,
    pages,
    ratelimit,
    utils,
)
from r2.lib.base import BaseController, abort
from r2.lib.cookies import (
    change_user_cookie_security,
    Cookies,
    Cookie,
    delete_secure_session_cookie,
    have_secure_session_cookie,
    upgrade_cookie_security,
    NEVER,
    DELETE,
)
from r2.lib.errors import (
    ErrorSet,
    BadRequestError,
    ForbiddenError,
    errors,
    reddit_http_error,
)
from r2.lib.filters import _force_utf8, _force_unicode, scriptsafe_dumps
from r2.lib.loid import LoId
from r2.lib.require import RequirementException, require, require_split
from r2.lib.strings import strings
from r2.lib.template_helpers import add_sr, JSPreload
from r2.lib.tracking import encrypt, decrypt, get_pageview_pixel_url
from r2.lib.translation import set_lang
from r2.lib.utils import (
    SimpleSillyStub,
    UniqueIterator,
    extract_subdomain,
    http_utils,
    is_subdomain,
    is_throttled,
    tup,
    UrlParser,
)
from r2.lib.validator import (
    build_arg_list,
    fullname_regex,
    valid_jsonp_callback,
    validate,
    VBoolean,
    VByName,
    VCount,
    VLang,
    VLength,
    VLimit,
    VTarget,
)
from r2.models import (
    Account,
    All,
    AllFiltered,
    AllMinus,
    DefaultSR,
    DomainSR,
    FakeAccount,
    FakeSubreddit,
    Friends,
    Frontpage,
    LabeledMulti,
    Link,
    Mod,
    ModFiltered,
    ModMinus,
    MultiReddit,
    NotFound,
    OAuth2AccessToken,
    OAuth2Client,
    OAuth2Scope,
    Random,
    RandomNSFW,
    RandomSubscription,
    Subreddit,
    valid_admin_cookie,
    valid_feed,
    valid_otp_cookie,
)
from r2.lib.db import tdb_cassandra


# Cookies which may be set in a response without making it uncacheable
CACHEABLE_COOKIES = ()


class UnloggedUser(FakeAccount):
    COOKIE_NAME = "_options"
    allowed_prefs = {
        "pref_lang": VLang.validate_lang,
        "pref_hide_locationbar": bool,
        "pref_use_global_defaults": bool,
    }

    def __init__(self, browser_langs, *a, **kw):
        FakeAccount.__init__(self, *a, **kw)
        lang = browser_langs[0] if browser_langs else g.lang
        self._defaults = self._defaults.copy()
        self._defaults['pref_lang'] = lang
        self._defaults['pref_hide_locationbar'] = False
        self._defaults['pref_use_global_defaults'] = False
        self._t.update(self._from_cookie())

    @property
    def name(self):
        raise NotImplementedError

    def _decode_json(self, json_blob):
        data = json.loads(json_blob)
        validated = {}
        for k, v in data.iteritems():
            validator = self.allowed_prefs.get(k)
            if validator:
                try:
                    validated[k] = validator(v)
                except ValueError:
                    pass  # don't override defaults for bad data
        return validated

    def _from_cookie(self):
        cookie = c.cookies.get(self.COOKIE_NAME)
        if not cookie:
            return {}

        try:
            return self._decode_json(cookie.value)
        except ValueError:
            # old-style _options cookies are encrypted
            try:
                plaintext = decrypt(cookie.value)
                values = self._decode_json(plaintext)
            except (TypeError, ValueError):
                # this cookie is totally invalid, delete it
                c.cookies[self.COOKIE_NAME] = Cookie(value="", expires=DELETE)
                return {}
            else:
                self._to_cookie(values)  # upgrade the cookie
                return values

    def _to_cookie(self, data):
        allowed_data = {k: v for k, v in data.iteritems()
                        if k in self.allowed_prefs}
        jsonified = json.dumps(allowed_data, sort_keys=True)
        c.cookies[self.COOKIE_NAME] = Cookie(value=jsonified)

    def _subscribe(self, sr):
        pass

    def _unsubscribe(self, sr):
        pass

    def _commit(self):
        if self._dirty:
            for k, (oldv, newv) in self._dirties.iteritems():
                self._t[k] = newv
            self._to_cookie(self._t)

def read_user_cookie(name):
    uname = c.user.name if c.user_is_loggedin else ""
    cookie_name = uname + '_' + name
    if cookie_name in c.cookies:
        return c.cookies[cookie_name].value
    else:
        return ''

def set_user_cookie(name, val, **kwargs):
    uname = c.user.name if c.user_is_loggedin else ""
    c.cookies[uname + '_' + name] = Cookie(value=val, **kwargs)


valid_click_cookie = fullname_regex(Link, True).match
def set_recent_clicks():
    c.recent_clicks = []
    if not c.user_is_loggedin:
        return

    click_cookie = read_user_cookie('recentclicks2')
    if click_cookie:
        if valid_click_cookie(click_cookie):
            names = [ x for x in UniqueIterator(click_cookie.split(',')) if x ]

            if len(names) > 5:
                names = names[:5]
                set_user_cookie('recentclicks2', ','.join(names))
            #eventually this will look at the user preference
            names = names[:5]

            try:
                c.recent_clicks = Link._by_fullname(names, data=True,
                                                    return_dict=False)
            except NotFound:
                # clear their cookie because it's got bad links in it
                set_user_cookie('recentclicks2', '')
        else:
            #if the cookie wasn't valid, clear it
            set_user_cookie('recentclicks2', '')

def delete_obsolete_cookies():
    for cookie_name in c.cookies:
        if cookie_name.endswith(("_last_thing", "_mod")):
            c.cookies[cookie_name] = Cookie("", expires=DELETE)

def over18():
    if c.user_is_loggedin:
        return c.user.pref_over_18 or c.user_is_admin
    else:
        if 'over18' in c.cookies:
            cookie = c.cookies['over18'].value
            if cookie == "1":
                return True
            else:
                delete_over18_cookie()


def set_over18_cookie():
    c.cookies.add("over18", "1")


def delete_over18_cookie():
    c.cookies["over18"] = Cookie(value="", expires=DELETE)


def set_obey_over18():
    "querystring parameter for API to obey over18 filtering rules"
    c.obey_over18 = request.GET.get("obey_over18") == "true"

valid_ascii_domain = re.compile(r'\A(\w[-\w]*\.)+[\w]+\Z')
def set_subreddit():
    #the r parameter gets added by javascript for API requests so we
    #can reference c.site in api.py
    sr_name = request.environ.get("subreddit", request.params.get('r'))
    domain = request.environ.get("domain")

    can_stale = request.method.upper() in ('GET', 'HEAD')

    c.site = Frontpage
    if not sr_name:
        #check for cnames
        cname = request.environ.get('legacy-cname')
        if cname:
            sr = Subreddit._by_domain(cname) or Frontpage
            domain = g.domain
            if g.domain_prefix:
                domain = ".".join((g.domain_prefix, domain))
            path = '%s://%s%s' % (g.default_scheme, domain, sr.path)
            abort(301, location=BaseController.format_output_url(path))
    elif '+' in sr_name:
        name_filter = lambda name: Subreddit.is_valid_name(name,
            allow_language_srs=True)
        sr_names = filter(name_filter, sr_name.split('+'))
        srs = Subreddit._by_name(sr_names, stale=can_stale).values()
        if All in srs:
            c.site = All
        elif Friends in srs:
            c.site = Friends
        else:
            srs = [sr for sr in srs if not isinstance(sr, FakeSubreddit)]
            if len(srs) == 1:
                c.site = srs[0]
            elif srs:
                found = {sr.name.lower() for sr in srs}
                sr_names = filter(lambda name: name.lower() in found, sr_names)
                sr_name = '+'.join(sr_names)
                multi_path = '/r/' + sr_name
                c.site = MultiReddit(multi_path, srs)
            elif not c.error_page:
                abort(404)
    elif '-' in sr_name:
        sr_names = sr_name.split('-')
        base_sr_name, exclude_sr_names = sr_names[0], sr_names[1:]
        srs = Subreddit._by_name(sr_names, stale=can_stale)
        base_sr = srs.pop(base_sr_name, None)
        exclude_srs = [sr for sr in srs.itervalues()
                          if not isinstance(sr, FakeSubreddit)]

        if base_sr == All:
            if exclude_srs:
                c.site = AllMinus(exclude_srs)
            else:
                c.site = All
        elif base_sr == Mod:
            if exclude_srs:
                c.site = ModMinus(exclude_srs)
            else:
                c.site = Mod
        else:
            path = "/subreddits/search?q=%s" % sr_name
            abort(302, location=BaseController.format_output_url(path))
    else:
        try:
            c.site = Subreddit._by_name(sr_name, stale=can_stale)
        except NotFound:
            if Subreddit.is_valid_name(sr_name):
                path = "/subreddits/search?q=%s" % sr_name
                abort(302, location=BaseController.format_output_url(path))
            elif not c.error_page and not request.path.startswith("/api/login/") :
                abort(404)

    #if we didn't find a subreddit, check for a domain listing
    if not sr_name and isinstance(c.site, DefaultSR) and domain:
        # Redirect IDN to their IDNA name if necessary
        try:
            idna = _force_unicode(domain).encode("idna")
            if idna != domain:
                path_info = request.environ["PATH_INFO"]
                path = "/domain/%s%s" % (idna, path_info)
                abort(302, location=BaseController.format_output_url(path))
        except UnicodeError:
            domain = ''  # Ensure valid_ascii_domain fails
        if not c.error_page and not valid_ascii_domain.match(domain):
            abort(404)
        c.site = DomainSR(domain)

    if isinstance(c.site, FakeSubreddit):
        c.default_sr = True

_FILTER_SRS = {"mod": ModFiltered, "all": AllFiltered}
def set_multireddit():
    routes_dict = request.environ["pylons.routes_dict"]
    if "multipath" in routes_dict or ("m" in request.GET and is_api()):
        fullpath = routes_dict.get("multipath", "").lower()
        multipaths = fullpath.split("+")
        multi_ids = None
        logged_in_username = c.user.name.lower() if c.user_is_loggedin else None
        multiurl = None

        if c.user_is_loggedin and routes_dict.get("my_multi"):
            multi_ids = ["/user/%s/m/%s" % (logged_in_username, multipath)
                         for multipath in multipaths]
            multiurl = "/me/m/" + fullpath
        elif "username" in routes_dict:
            username = routes_dict["username"].lower()

            if c.user_is_loggedin:
                # redirect /user/foo/m/... to /me/m/... for user foo.
                if username == logged_in_username and not is_api():
                    # trim off multi id
                    url_parts = request.path_qs.split("/")[5:]
                    url_parts.insert(0, "/me/m/%s" % fullpath)
                    path = "/".join(url_parts)
                    abort(302, location=BaseController.format_output_url(path))

            multiurl = "/user/" + username + "/m/" + fullpath
            multi_ids = ["/user/%s/m/%s" % (username, multipath)
                        for multipath in multipaths]
        elif "m" in request.GET and is_api():
            # Only supported via API as we don't have a valid non-query
            # parameter equivalent for cross-user multis, which means
            # we can't generate proper links to /new, /top, etc in HTML
            multi_ids = [m.lower() for m in request.GET.getall("m") if m]
            multiurl = ""

        if multi_ids is not None:
            multis = LabeledMulti._byID(multi_ids, return_dict=False) or []
            multis = [m for m in multis if m.can_view(c.user)]
            if not multis:
                abort(404)
            elif len(multis) == 1:
                c.site = multis[0]
            else:
                sr_ids = Subreddit.random_reddits(
                    logged_in_username,
                    list(set(itertools.chain.from_iterable(
                        multi.sr_ids for multi in multis
                    ))),
                    LabeledMulti.MAX_SR_COUNT,
                )
                srs = Subreddit._byID(sr_ids, data=True, return_dict=False)
                c.site = MultiReddit(multiurl, srs)
                if any(m.weighting_scheme == "fresh" for m in multis):
                    c.site.weighting_scheme = "fresh"

    elif "filtername" in routes_dict:
        if not c.user_is_loggedin:
            abort(404)
        filtername = routes_dict["filtername"].lower()
        filtersr = _FILTER_SRS.get(filtername)
        if not filtersr:
            abort(404)
        c.site = filtersr()


def set_content_type():
    e = request.environ
    c.render_style = e['render_style']
    response.content_type = e['content_type']

    if e.has_key('extension'):
        c.extension = ext = e['extension']
        if ext in ('embed', 'widget'):
            wrapper = request.params.get("callback", "document.write")
            wrapper = filters._force_utf8(wrapper)
            if not valid_jsonp_callback(wrapper):
                abort(BadRequestError(errors.BAD_JSONP_CALLBACK))

            # force logged-out state since these can be accessed cross-domain
            c.user = UnloggedUser(get_browser_langs())
            c.user_is_loggedin = False
            c.forced_loggedout = True

            def to_js(content):
                # Add a comment to the beginning to prevent the "Rosetta Flash"
                # XSS when an attacker controls the beginning of a resource
                return "/**/" + wrapper + "(" + utils.string2js(content) + ");"

            c.response_wrapper = to_js
        if ext in ("rss", "api", "json") and request.method.upper() == "GET":
            user = valid_feed(request.GET.get("user"),
                              request.GET.get("feed"),
                              request.path)
            if user and not g.read_only_mode:
                c.user = user
                c.user_is_loggedin = True
        if ext in ("mobile", "m") and not request.GET.get("keep_extension"):
            try:
                if request.cookies['reddit_mobility'] == "compact":
                    c.extension = "compact"
                    c.render_style = "compact"
            except (ValueError, KeyError):
                c.suggest_compact = True
        if ext in ("mobile", "m", "compact"):
            if request.GET.get("keep_extension"):
                c.cookies['reddit_mobility'] = Cookie(ext, expires=NEVER)

    # allow content and api calls to set an loid
    if is_api() or c.render_style in ("html", "mobile", "compact"):
        c.loid = LoId.load(request, c)

    # allow JSONP requests to generate callbacks, but do not allow
    # the user to be logged in for these
    callback = request.GET.get("jsonp")
    if is_api() and request.method.upper() == "GET" and callback:
        if not valid_jsonp_callback(callback):
            abort(BadRequestError(errors.BAD_JSONP_CALLBACK))
        c.allowed_callback = callback
        c.user = UnloggedUser(get_browser_langs())
        c.user_is_loggedin = False
        c.forced_loggedout = True
        response.content_type = "application/javascript"

def get_browser_langs():
    browser_langs = []
    langs = request.environ.get('HTTP_ACCEPT_LANGUAGE')
    if langs:
        langs = langs.split(',')
        browser_langs = []
        seen_langs = set()
        # extract languages from browser string
        for l in langs:
            if ';' in l:
                l = l.split(';')[0]
            if l not in seen_langs and l in g.languages:
                browser_langs.append(l)
                seen_langs.add(l)
            if '-' in l:
                l = l.split('-')[0]
            if l not in seen_langs and l in g.languages:
                browser_langs.append(l)
                seen_langs.add(l)
    return browser_langs

def set_iface_lang():
    host_lang = request.environ.get('reddit-prefer-lang')
    lang = host_lang or c.user.pref_lang

    if getattr(g, "lang_override") and lang == "en":
        lang = g.lang_override

    c.lang = lang

    try:
        set_lang(lang, fallback_lang=g.lang)
    except LanguageError:
        lang = g.lang
        set_lang(lang, graceful_fail=True)

    try:
        c.locale = babel.core.Locale.parse(lang, sep='-')
    except (babel.core.UnknownLocaleError, ValueError):
        c.locale = babel.core.Locale.parse(g.lang, sep='-')


def set_colors():
    theme_rx = re.compile(r'')
    color_rx = re.compile(r'\A([a-fA-F0-9]){3}(([a-fA-F0-9]){3})?\Z')
    c.theme = None
    if color_rx.match(request.GET.get('bgcolor') or ''):
        c.bgcolor = request.GET.get('bgcolor')
    if color_rx.match(request.GET.get('bordercolor') or ''):
        c.bordercolor = request.GET.get('bordercolor')


def ratelimit_agent(agent, limit=10, slice_size=10):

    # Ensure the agent regex is a valid memcached key
    h = md5()
    h.update(agent)
    hashed_agent = h.hexdigest()

    slice_size = min(slice_size, 60)
    time_slice = ratelimit.get_timeslice(slice_size)
    usage = ratelimit.record_usage("rl-agent-" + hashed_agent, time_slice)
    if usage > limit:
        request.environ['retry_after'] = time_slice.remaining
        abort(429)


appengine_re = re.compile(r'AppEngine-Google; \(\+http://code.google.com/appengine; appid: (?:dev|s)~([a-z0-9-]{6,30})\)\Z')
def ratelimit_agents():
    user_agent = request.user_agent

    if not user_agent:
        return

    # parse out the appid for appengine apps
    appengine_match = appengine_re.search(user_agent)
    if appengine_match:
        appid = appengine_match.group(1)
        ratelimit_agent(appid)
        return

    # Search anywhere in the useragent for the given regex
    for agent_re, limit in g.user_agent_ratelimit_regexes.iteritems():
        if agent_re.search(user_agent):
            ratelimit_agent(agent_re.pattern, limit)
            return


def ratelimit_throttled():
    ip = request.ip.strip()
    if is_throttled(ip):
        abort(429)


def paginated_listing(default_page_size=25, max_page_size=100, backend='sql'):
    def decorator(fn):
        @validate(num=VLimit('limit', default=default_page_size,
                             max_limit=max_page_size),
                  after=VByName('after', backend=backend),
                  before=VByName('before', backend=backend),
                  count=VCount('count'),
                  target=VTarget("target"),
                  sr_detail=VBoolean(
                      "sr_detail", docs={"sr_detail": "(optional) expand subreddits"}),
                  show=VLength('show', 3, empty_error=None,
                               docs={"show": "(optional) the string `all`"}),
        )
        @wraps(fn)
        def new_fn(self, before, **env):
            if c.render_style == "htmllite":
                c.link_target = env.get("target")
            elif "target" in env:
                del env["target"]

            if "show" in env and env['show'] == 'all':
                c.ignore_hide_rules = True
            kw = build_arg_list(fn, env)

            #turn before into after/reverse
            kw['reverse'] = False
            if before:
                kw['after'] = before
                kw['reverse'] = True

            return fn(self, **kw)

        if hasattr(fn, "_api_doc"):
            notes = fn._api_doc["notes"] or []
            if paginated_listing.doc_note not in notes:
                notes.append(paginated_listing.doc_note)
            fn._api_doc["notes"] = notes

        return new_fn
    return decorator

paginated_listing.doc_note = "*This endpoint is [a listing](#listings).*"

#TODO i want to get rid of this function. once the listings in front.py are
#moved into listingcontroller, we shouldn't have a need for this
#anymore
def base_listing(fn):
    return paginated_listing()(fn)

def is_trusted_origin(origin):
    try:
        origin = urlparse(origin)
    except ValueError:
        return False

    return any(is_subdomain(origin.hostname, domain) for domain in g.trusted_domains)

def cross_domain(origin_check=is_trusted_origin, **options):
    """Set up cross domain validation and hoisting for a request handler."""
    def cross_domain_wrap(fn):
        cors_perms = {
            "origin_check": origin_check,
            "allow_credentials": bool(options.get("allow_credentials"))
        }

        @wraps(fn)
        def cross_domain_handler(self, *args, **kwargs):
            if request.params.get("hoist") == "cookie":
                # Cookie polling response
                if cors_perms["origin_check"](g.origin):
                    name = request.environ["pylons.routes_dict"]["action_name"]
                    resp = fn(self, *args, **kwargs)
                    c.cookies.add('hoist_%s' % name, ''.join(tup(resp)))
                    response.content_type = 'text/html'
                    return ""
                else:
                    abort(403)
            else:
                self.check_cors()
                return fn(self, *args, **kwargs)

        cross_domain_handler.cors_perms = cors_perms
        return cross_domain_handler
    return cross_domain_wrap


def make_url_https(url):
    """Turn a possibly relative URL into a fully-qualified HTTPS URL."""
    new_url = UrlParser(url)
    new_url.scheme = "https"
    if not new_url.hostname:
        new_url.hostname = request.host.lower()
    return new_url.unparse()


def generate_modhash():
    # OAuth clients should never receive a modhash of any kind as they could
    # use it in a CSRF attack to bypass their permitted OAuth scopes
    if c.oauth_user:
        return None

    modhash = hooks.get_hook("modhash.generate").call_until_return()
    if modhash is not None:
        return modhash

    # if no plugins generate a modhash, just use the user name
    return c.user.name


def enforce_https():
    """Enforce policy for forced usage of HTTPS."""

    # OAuth HTTPS enforcement is dealt with elsewhere
    if c.oauth_user:
        return

    # This is likely a cross-domain request, the initiator has no way of
    # respecting the user's HTTPS preferences and redirecting them is unlikely
    # to stop them from making future requests via HTTP.
    if c.forced_loggedout or c.render_style == "js":
        return

    # It's not possible to redirect inside the error handler
    if request.environ.get('pylons.error_call', False):
        return

    is_api_request = is_api() or request.path.startswith("/api/")
    redirect_url = None

    # This is likely a request from an API client. Redirecting them or giving
    # them an HSTS grant is unlikely to stop them from making requests to HTTP.
    if is_api_request and not c.secure:
        # Record the violation so we know who to talk to to get this fixed.
        # This is preferable to redirecting insecure API reqs right away
        # because a lot of clients just break on redirect, it would create two
        # requests for every request, and it wouldn't increase security.
        ua = request.user_agent
        g.stats.count_string('https.security_violation', ua)
        # It's especially bad to send credentials over HTTP
        if c.user_is_loggedin:
            g.stats.count_string('https.loggedin_security_violation', ua)

        # They didn't send a login cookie, but their cookies indicate they won't
        # be authed properly unless we redirect them to the secure version.
        if have_secure_session_cookie() and not c.user_is_loggedin:
            redirect_url = make_url_https(request.fullurl)

    # These are all safe to redirect to HTTPS
    if c.render_style in {"html", "compact", "mobile"} and not is_api_request:
        want_redirect = (feature.is_enabled("force_https") or
                         feature.is_enabled("https_redirect"))
        if not c.secure and want_redirect:
            redirect_url = make_url_https(request.fullurl)

    if redirect_url:
        headers = {"Cache-Control": "private, no-cache", "Pragma": "no-cache"}
        # Browsers change the method to GET on 301, and 308 is ill-supported.
        status_code = 301 if request.method == "GET" else 307
        abort(status_code, location=redirect_url, headers=headers)


def require_https():
    if not c.secure:
        abort(ForbiddenError(errors.HTTPS_REQUIRED))


def require_domain(required_domain):
    if not is_subdomain(request.host, required_domain):
        abort(ForbiddenError(errors.WRONG_DOMAIN))


def disable_subreddit_css():
    def wrap(f):
        @wraps(f)
        def no_funny_business(*args, **kwargs):
            c.allow_styles = False
            return f(*args, **kwargs)
        return no_funny_business
    return wrap


def request_timer_name(action):
    return "service_time.web." + action


def flatten_response(content):
    """Convert a content iterable to a string, properly handling unicode."""
    # TODO: it would be nice to replace this with response.body someday
    # once unicode issues are ironed out.
    return "".join(_force_utf8(x) for x in tup(content) if x)


def abort_with_error(error, code=None):
    if not code and not error.code:
        raise ValueError('Error %r missing status code' % error)

    abort(reddit_http_error(
        code=code or error.code,
        error_name=error.name,
        explanation=error.message,
        fields=error.fields,
    ))


class MinimalController(BaseController):

    allow_stylesheets = False
    defer_ratelimiting = False

    def run_sitewide_ratelimits(self):
        """Ratelimit users and add ratelimit headers to the response.

        Headers added are:
        X-Ratelimit-Used: Number of requests used in this period
        X-Ratelimit-Remaining: Number of requests left to use
        X-Ratelimit-Reset: Approximate number of seconds to end of period

        This function only has an effect if one of
        g.RL_SITEWIDE_ENABLED or g.RL_OAUTH_SITEWIDE_ENABLED
        are set to 'true' in the app configuration

        If the ratelimit is exceeded, a 429 response will be sent,
        unless the app configuration has g.ENFORCE_RATELIMIT off.
        Headers will be sent even on aborted requests.

        """
        if c.error_page:
            # ErrorController is re-running pre, don't double ratelimit
            return

        if c.oauth_user and g.RL_OAUTH_SITEWIDE_ENABLED:
            type_ = "oauth"
            period = g.RL_OAUTH_RESET_SECONDS
            max_reqs = c.oauth2_client._max_reqs
            # Convert client_id to ascii str for use as memcache key
            client_id = c.oauth2_access_token.client_id.encode("ascii")
            # OAuth2 ratelimits are per user-app combination
            key = 'siterl-oauth-' + c.user._id36 + ":" + client_id
        elif c.cdn_cacheable:
            type_ = "cdn"
        elif not is_api():
            type_ = "web"
        elif g.RL_SITEWIDE_ENABLED:
            type_ = "api"
            max_reqs = g.RL_MAX_REQS
            period = g.RL_RESET_SECONDS
            # API (non-oauth) limits are per-ip
            key = 'siterl-api-' + request.ip
        else:
            type_ = "none"

        g.stats.event_count("ratelimit.type", type_, sample_rate=0.01)
        if type_ in ("cdn", "web", "none"):
            # No ratelimiting or headers for:
            # * Web requests (HTML)
            # * CDN requests (logged out via www.reddit.com)
            return

        time_slice = ratelimit.get_timeslice(period)

        try:
            recent_reqs = ratelimit.record_usage(key, time_slice)
        except ratelimit.RatelimitError as e:
            # Ratelimiting is non-critical; if the system is
            # having issues, just skip adding the headers
            g.log.info("ratelimit error: %s", e)
            return
        reqs_remaining = max(0, max_reqs - recent_reqs)

        c.ratelimit_headers = {
            "X-Ratelimit-Used": str(recent_reqs),
            "X-Ratelimit-Reset": str(time_slice.remaining),
            "X-Ratelimit-Remaining": str(reqs_remaining),
        }

        event_type = None

        if reqs_remaining <= 0:
            if recent_reqs > (2 * max_reqs):
                event_type = "hyperbolic"
            else:
                event_type = "over"
            if g.ENFORCE_RATELIMIT:
                # For non-abort situations, the headers will be added in post()
                request.environ['retry_after'] = time_slice.remaining
                response.headers.update(c.ratelimit_headers)
                abort(429)
        elif reqs_remaining < (0.1 * max_reqs):
            event_type = "close"

        if event_type is not None:
            g.stats.event_count("ratelimit.exceeded", event_type)
            if type_ == "oauth":
                g.stats.count_string("oauth.{}".format(event_type), client_id)

    def pre(self):
        action = request.environ["pylons.routes_dict"].get("action")
        if action:
            if not self._get_action_handler():
                action = 'invalid'
            controller = request.environ["pylons.routes_dict"]["controller"]
            key = "{}.{}".format(controller, action)
            c.request_timer = g.stats.get_timer(request_timer_name(key))
        else:
            c.request_timer = SimpleSillyStub()

        baseplate_integration.make_server_span(span_name=key).start()

        c.response_wrapper = None
        c.start_time = datetime.now(g.tz)
        c.request_timer.start()
        g.reset_caches()

        c.domain_prefix = request.environ.get("reddit-domain-prefix",
                                              g.domain_prefix)
        c.secure = request.environ["wsgi.url_scheme"] == "https"
        c.request_origin = request.host_url

        #check if user-agent needs a dose of rate-limiting
        if not c.error_page:
            ratelimit_throttled()
            ratelimit_agents()

        # Allow opting out of the `websafe_json` madness
        if "WANT_RAW_JSON" not in request.environ:
            want_raw_json = request.params.get("raw_json", "") == "1"
            request.environ["WANT_RAW_JSON"] = want_raw_json

        c.allow_framing = False

        # According to http://www.w3.org/TR/2014/WD-referrer-policy-20140807/
        # we really want "origin-when-crossorigin", but that isn't widely
        # supported yet.
        c.referrer_policy = "origin"

        c.cdn_cacheable = (request.via_cdn and
                           g.login_cookie not in request.cookies)

        c.extension = request.environ.get('extension')
        # the domain has to be set before Cookies get initialized
        set_subreddit()
        c.subdomain = extract_subdomain()
        c.errors = ErrorSet()
        c.cookies = Cookies()
        # if an rss feed, this will also log the user in if a feed=
        # GET param is included
        set_content_type()

        c.request_timer.intermediate("minimal-pre")
        # True/False forces. None updates for most non-POST requests
        c.update_last_visit = None

        if is_subdomain(request.host, g.oauth_domain):
            self.check_cors()

        if not self.defer_ratelimiting:
            self.run_sitewide_ratelimits()
            c.request_timer.intermediate("minimal-ratelimits")

        hooks.get_hook("reddit.request.minimal_begin").call()

    def post(self):
        c.request_timer.intermediate("action")

        # if the action raised an HTTPException (i.e. it aborted) then pylons
        # will have replaced response with the exception itself.
        c.is_exception_response = getattr(response, "_exception", False)

        if c.response_wrapper and not c.is_exception_response:
            content = flatten_response(response.content)
            wrapped_content = c.response_wrapper(content)
            response.content = wrapped_content

        # we need to not add X-Frame-Options to requests (such as media embeds)
        # that intend to allow framing.
        if not c.allow_framing:
            response.headers["X-Frame-Options"] = "SAMEORIGIN"

        # set some headers related to client security
        response.headers['X-Content-Type-Options'] = 'nosniff'
        response.headers['X-XSS-Protection'] = '1; mode=block'

        if (feature.is_enabled("force_https")
                and feature.is_enabled("upgrade_cookies")):
            upgrade_cookie_security()

        # Don't poison the cache with uncacheable cookies
        dirty_cookies = (k for k, v in c.cookies.iteritems() if v.dirty)
        would_poison = any((k not in CACHEABLE_COOKIES) for k in dirty_cookies)

        if c.user_is_loggedin or would_poison:
            # Based off logged in <https://en.wikipedia.org/>,
            # must-revalidate might not be necessary, but should force
            # similar behaviour to no-cache (in theory.)
            # Normally you'd prefer `no-store`, but many of reddit's
            # listings are ephemeral, and the content might not even
            # exist anymore if you force a refresh when hitting back.
            cache_control = (
                'private',
                's-maxage=0',
                'max-age=0',
                'must-revalidate',
            )
            response.headers['Expires'] = '-1'
            response.headers['Cache-Control'] = ', '.join(cache_control)

        if c.ratelimit_headers:
            response.headers.update(c.ratelimit_headers)

        # write loid cookie if necessary
        if c.loid:
            c.loid.save(domain=g.domain)

        # send cookies
        secure_cookies = feature.is_enabled("force_https")
        for k, v in c.cookies.iteritems():
            if v.dirty:
                v_secure = v.secure if v.secure is not None else secure_cookies
                response.set_cookie(key=k,
                                    value=quote(v.value),
                                    domain=v.domain,
                                    expires=v.expires,
                                    secure=v_secure,
                                    httponly=getattr(v, 'httponly', False))


        if not isinstance(c.site, FakeSubreddit) and not g.disallow_db_writes:
            if c.user_is_loggedin:
                c.site.record_visitor_activity("logged_in", c.user._fullname)

        if self.should_update_last_visit():
            c.user.update_last_visit(c.start_time)

        hooks.get_hook("reddit.request.end").call()

        # this thread is probably going to be reused, but it could be
        # a while before it is. So we might as well dump the cache in
        # the mean time so that we don't have dead objects hanging
        # around taking up memory
        g.reset_caches()

        c.request_timer.intermediate("post")

        # add tags to the trace
        c.trace.set_tag("user", c.user._fullname if c.user_is_loggedin else None)
        c.trace.set_tag("render_style", c.render_style)

        # push data to statsd
        baseplate_integration.finish_server_span()
        c.request_timer.stop()
        g.stats.flush()

    def on_validation_error(self, error):
        if error.name == errors.USER_REQUIRED:
            self.intermediate_redirect('/login')
        elif error.name == errors.VERIFIED_USER_REQUIRED:
            self.intermediate_redirect('/verify')

    def abort404(self):
        abort(404, "not found")

    def abort403(self):
        abort(403, "forbidden")

    COMMON_REDDIT_HEADERS = ", ".join((
        "X-Ratelimit-Used",
        "X-Ratelimit-Remaining",
        "X-Ratelimit-Reset",
        "X-Moose",
    ))

    def check_cors(self):
        origin = request.headers.get("Origin")
        if c.cors_checked or not origin:
            return

        method = request.method
        if method == 'OPTIONS':
            # preflight request
            method = request.headers.get("Access-Control-Request-Method")
            if not method:
                self.abort403()

        via_oauth = is_subdomain(request.host, g.oauth_domain)
        if via_oauth:
            response.headers["Access-Control-Allow-Origin"] = "*"
            response.headers["Access-Control-Allow-Methods"] = \
                "GET, POST, PUT, PATCH, DELETE"
            response.headers["Access-Control-Allow-Headers"] = \
                "Authorization, "
            response.headers["Access-Control-Allow-Credentials"] = "false"
            response.headers['Access-Control-Expose-Headers'] = \
                self.COMMON_REDDIT_HEADERS
        else:
            action = request.environ["pylons.routes_dict"]["action_name"]

            handler = self._get_action_handler(action, method)
            cors = handler and getattr(handler, "cors_perms", None)

            if cors and cors["origin_check"](origin):
                response.headers["Access-Control-Allow-Origin"] = origin
                if cors.get("allow_credentials"):
                    response.headers["Access-Control-Allow-Credentials"] = "true"
        c.cors_checked = True

    def OPTIONS(self):
        """Return empty responses for CORS preflight requests"""
        self.check_cors()

    def update_qstring(self, dict):
        merged = copy(request.GET)
        merged.update(dict)
        return request.path + utils.query_string(merged)

    def api_wrapper(self, kw):
        if request.environ.get("WANT_RAW_JSON"):
            return scriptsafe_dumps(kw)
        return filters.websafe_json(simplejson.dumps(kw))

    def should_update_last_visit(self):
        if g.disallow_db_writes:
            return False

        if not c.user_is_loggedin:
            return False

        if c.update_last_visit is not None:
            return c.update_last_visit

        return request.method.upper() != "POST"


class OAuth2ResourceController(MinimalController):
    defer_ratelimiting = True

    def authenticate_with_token(self):
        set_extension(request.environ, "json")
        set_content_type()
        require_https()
        require_domain(g.oauth_domain)

        try:
            access_token = OAuth2AccessToken.get_token(self._get_bearer_token())
            require(access_token)
            require(access_token.check_valid())
            c.oauth2_access_token = access_token
            if access_token.user_id:
                account = Account._byID36(access_token.user_id, data=True)
                require(account)
                require(not account._deleted)
                c.user = c.oauth_user = account
                c.user_is_loggedin = True
            else:
                c.user = UnloggedUser(get_browser_langs())
                c.user_is_loggedin = False
            c.oauth2_client = OAuth2Client._byID(access_token.client_id)
        except RequirementException:
            self._auth_error(401, "invalid_token")

        handler = self._get_action_handler()
        if handler:
            oauth2_perms = getattr(handler, "oauth2_perms", {})
            if oauth2_perms.get("oauth2_allowed", False):
                grant = OAuth2Scope(access_token.scope)
                required = set(oauth2_perms['required_scopes'])
                if not grant.has_access(c.site.name, required):
                    self._auth_error(403, "insufficient_scope")
                c.oauth_scope = grant
            else:
                self._auth_error(400, "invalid_request")

    def check_for_bearer_token(self):
        if self._get_bearer_token(strict=False):
            self.authenticate_with_token()

    def _auth_error(self, code, error):
        abort(code, headers=[("WWW-Authenticate", 'Bearer realm="reddit", error="%s"' % error)])

    def _get_bearer_token(self, strict=True):
        auth = request.headers.get("Authorization")
        if not auth:
            return None
        try:
            auth_scheme, bearer_token = require_split(auth, 2)
            require(auth_scheme.lower() == "bearer")
            return bearer_token
        except RequirementException:
            if strict:
                self._auth_error(400, "invalid_request")
            else:
                return None

    def set_up_user_context(self):
        if c.user.inbox_count > 0:
            c.have_messages = True
        c.have_mod_messages = bool(c.user.modmsgtime)

        c.user_special_distinguish = c.user.special_distinguish()


class OAuth2OnlyController(OAuth2ResourceController):
    """Base controller for endpoints that may only be accessed via OAuth 2"""

    # OAuth2 doesn't rely on ambient credentials for authentication,
    # so CSRF prevention is unnecessary.
    handles_csrf = True

    def pre(self):
        OAuth2ResourceController.pre(self)
        if request.method != "OPTIONS":
            self.authenticate_with_token()
            self.set_up_user_context()
            self.run_sitewide_ratelimits()

    def on_validation_error(self, error):
        abort_with_error(error, error.code or 400)


class RedditController(OAuth2ResourceController):

    @staticmethod
    def login(user, rem=False):
        # This can't be handled in post() due to PRG and ErrorController fun.
        user.update_last_visit(c.start_time)
        force_https = feature.is_enabled("force_https", user)
        c.cookies[g.login_cookie] = Cookie(value=user.make_cookie(),
                                           expires=NEVER if rem else None,
                                           httponly=True,
                                           secure=force_https)
        # Make sure user-specific cookies get the secure flag set properly
        change_user_cookie_security(secure=force_https, remember=rem)

    @staticmethod
    def logout():
        c.cookies[g.login_cookie] = Cookie(value='', expires=DELETE)
        delete_secure_session_cookie()

    @staticmethod
    def enable_admin_mode(user, first_login=None):
        # no expiration time so the cookie dies with the browser session
        admin_cookie = user.make_admin_cookie(first_login=first_login)
        c.cookies[g.admin_cookie] = Cookie(
            value=admin_cookie,
            httponly=True,
            secure=feature.is_enabled("force_https"),
        )

    @staticmethod
    def remember_otp(user):
        cookie = user.make_otp_cookie()
        expiration = datetime.utcnow() + timedelta(seconds=g.OTP_COOKIE_TTL)
        set_user_cookie(g.otp_cookie,
                        cookie,
                        secure=True,
                        httponly=True,
                        expires=expiration)

    @staticmethod
    def disable_admin_mode(user):
        c.cookies[g.admin_cookie] = Cookie(value='', expires=DELETE)

    def pre(self):
        record_timings = g.admin_cookie in request.cookies or g.debug
        admin_bar_eligible = response.content_type == 'text/html'
        if admin_bar_eligible and record_timings:
            g.stats.start_logging_timings()

        # set up stuff needed in base templates at error time here.
        c.js_preload = JSPreload()

        MinimalController.pre(self)

        # Set IE to always use latest rendering engine
        response.headers["X-UA-Compatible"] = "IE=edge"

        # populate c.cookies unless we're on the unsafe media_domain
        if request.host != g.media_domain or g.media_domain == g.domain:
            cookie_counts = collections.Counter()
            for k, v in request.cookies.iteritems():
                # minimalcontroller can still set cookies
                if k not in c.cookies:
                    # we can unquote even if it's not quoted
                    c.cookies[k] = Cookie(value=unquote(v), dirty=False)
                    cookie_counts[Cookie.classify(k)] += 1

            for cookietype, count in cookie_counts.iteritems():
                g.stats.simple_event("cookie.%s" % cookietype, count)

        delete_obsolete_cookies()

        # the user could have been logged in via one of the feeds
        maybe_admin = False
        is_otpcookie_valid = False

        self.check_for_bearer_token()

        # no logins for RSS feed unless valid_feed has already been called
        if not c.user:
            if c.extension != "rss":
                if not g.read_only_mode:
                    c.user = g.auth_provider.get_authenticated_account()

                    if c.user and c.user._deleted:
                        c.user = None
                else:
                    c.user = None
                c.user_is_loggedin = bool(c.user)

                admin_cookie = c.cookies.get(g.admin_cookie)
                if c.user_is_loggedin and admin_cookie:
                    maybe_admin, first_login = valid_admin_cookie(admin_cookie.value)

                    if maybe_admin:
                        self.enable_admin_mode(c.user, first_login=first_login)
                    else:
                        self.disable_admin_mode(c.user)

                otp_cookie = read_user_cookie(g.otp_cookie)
                if c.user_is_loggedin and otp_cookie:
                    is_otpcookie_valid = valid_otp_cookie(otp_cookie)

            if not c.user:
                c.user = UnloggedUser(get_browser_langs())
                # patch for fixing mangled language preferences
                if not isinstance(c.user.pref_lang, basestring):
                    c.user.pref_lang = g.lang
                    c.user._commit()

        if c.user_is_loggedin:
            self.set_up_user_context()
            c.modhash = generate_modhash()
            c.user_is_admin = maybe_admin and c.user.name in g.admins
            c.user_is_sponsor = c.user_is_admin or c.user.name in g.sponsors
            c.otp_cached = is_otpcookie_valid

        enforce_https()

        c.request_timer.intermediate("base-auth")

        self.run_sitewide_ratelimits()
        c.request_timer.intermediate("base-ratelimits")

        c.over18 = over18()
        set_obey_over18()

        # looking up the multireddit requires c.user.
        set_multireddit()

        #set_browser_langs()
        set_iface_lang()
        set_recent_clicks()
        # used for HTML-lite templates
        set_colors()

        # set some environmental variables in case we hit an abort
        if not isinstance(c.site, FakeSubreddit):
            request.environ['REDDIT_NAME'] = c.site.name

        # random reddit trickery
        if c.site == Random:
            c.site = Subreddit.random_reddit(user=c.user)
            site_path = c.site.path.strip('/')
            path = "/" + site_path + request.path_qs
            abort(302, location=self.format_output_url(path))
        elif c.site == RandomSubscription:
            if not c.user.gold:
                abort(302, location=self.format_output_url('/gold/about'))
            c.site = Subreddit.random_subscription(c.user)
            site_path = c.site.path.strip('/')
            path = '/' + site_path + request.path_qs
            abort(302, location=self.format_output_url(path))
        elif c.site == RandomNSFW:
            c.site = Subreddit.random_reddit(over18=True, user=c.user)
            site_path = c.site.path.strip('/')
            path = '/' + site_path + request.path_qs
            abort(302, location=self.format_output_url(path))

        if not request.path.startswith("/api/login/"):
            # is the subreddit banned?
            if c.site.spammy() and not c.user_is_admin and not c.error_page:
                ban_info = getattr(c.site, "ban_info", {})
                if "message" in ban_info and ban_info['message']:
                    message = ban_info['message']
                else:
                    message = None

                errpage = pages.InterstitialPage(
                    _("banned"),
                    content=pages.BannedInterstitial(
                        message=message,
                        ban_time=ban_info.get("banned_at"),
                    ),
                )

                request.environ['usable_error_content'] = errpage.render()
                self.abort404()

            # check if the user has access to this subreddit
            # Allow OPTIONS requests through, as no response body
            # is sent in those cases - just a set of headers
            if (not c.site.can_view(c.user) and not c.error_page and
                    request.method != "OPTIONS"):
                allowed_to_view = c.site.is_allowed_to_view(c.user)

                if isinstance(c.site, LabeledMulti):
                    # do not leak the existence of multis via 403.
                    self.abort404()
                elif not allowed_to_view and c.site.type == 'gold_only':
                    errpage = pages.InterstitialPage(
                        _("gold members only"),
                        content=pages.GoldOnlyInterstitial(
                            sr_name=c.site.name,
                            sr_description=c.site.public_description,
                        ),
                    )
                    request.environ['usable_error_content'] = errpage.render()
                    self.abort403()
                elif not allowed_to_view:
                    errpage = pages.InterstitialPage(
                        _("private"),
                        content=pages.PrivateInterstitial(
                            sr_name=c.site.name,
                            sr_description=c.site.public_description,
                        ),
                    )
                    request.environ['usable_error_content'] = errpage.render()
                    self.abort403()
                else:
                    if c.render_style != 'html':
                        self.abort403()
                    g.events.quarantine_event('quarantine_interstitial_view', c.site,
                        request=request, context=c)
                    return self.intermediate_redirect("/quarantine", sr_path=False)

            # check over 18
            if (
                c.site.over_18 and not c.over18 and
                request.path != "/over18" and
                c.render_style == 'html' and
                not request.parsed_agent.bot
            ):
                return self.intermediate_redirect("/over18", sr_path=False)

        #check whether to allow custom styles
        c.allow_styles = True
        c.can_apply_styles = self.allow_stylesheets

        # use override stylesheet if one exists and:
        #   this page has no custom stylesheet
        #   or the user disabled the stylesheet for this sr (indiv or global)
        has_style_override = (c.user.pref_default_theme_sr and
                feature.is_enabled('stylesheets_everywhere') and
                Subreddit._by_name(c.user.pref_default_theme_sr).can_view(c.user))
        sr_stylesheet_enabled = c.user.use_subreddit_style(c.site)

        if (not sr_stylesheet_enabled and
                not has_style_override):
            c.can_apply_styles = False

        c.bare_content = request.GET.pop('bare', False)

        c.show_admin_bar = admin_bar_eligible and (c.user_is_admin or g.debug)
        if not c.show_admin_bar:
            g.stats.end_logging_timings()

        hooks.get_hook("reddit.request.begin").call()

        c.request_timer.intermediate("base-pre")

    def post(self):
        MinimalController.post(self)
        if response.content_type == "text/html":
            self._embed_html_timing_data()

        # allow logged-out JSON requests to be read cross-domain
        if (not c.cors_checked and request.method.upper() == "GET" and
                not c.user_is_loggedin and c.render_style == "api"):
            response.headers["Access-Control-Allow-Origin"] = "*"

            request_origin = request.headers.get('Origin')
            if request_origin and request_origin != g.origin:
                g.stats.simple_event('cors.api_request')
                g.stats.count_string('origins', request_origin)

        if g.tracker_url and request.method.upper() == "GET" and is_api():
            tracking_url = make_url_https(get_pageview_pixel_url())
            response.headers["X-Reddit-Tracking"] = tracking_url

    def _embed_html_timing_data(self):
        timings = g.stats.end_logging_timings()

        if not timings or not c.show_admin_bar or c.is_exception_response:
            return

        timings = [{
            "key": timing.key,
            "start": round(timing.start, 4),
            "end": round(timing.end, 4),
        } for timing in timings]

        content = flatten_response(response.content)
        # inject stats script tag at the end of the <body>
        body_parts = list(content.rpartition("</body>"))
        if body_parts[1]:
            script = ('<script type="text/javascript">'
                      'window.r = window.r || {};'
                      'r.timings = %s'
                      '</script>') % simplejson.dumps(timings)
            body_parts.insert(1, script)
            response.content = "".join(body_parts)

    def search_fail(self, exception):
        errpage = pages.RedditError(_("search failed"),
                                    strings.search_failed)

        request.environ['usable_error_content'] = errpage.render()
        request.environ['retry_after'] = 60
        abort(503)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons.i18n import _, ungettext
from r2.controllers.reddit_base import (
    base_listing,
    disable_subreddit_css,
    paginated_listing,
    RedditController,
    require_https,
)
from r2 import config
from r2.models import *
from r2.models.recommend import ExploreSettings
from r2.config import feature
from r2.config.extensions import is_api, API_TYPES, RSS_TYPES
from r2.lib import hooks, recommender, embeds, pages
from r2.lib.pages import *
from r2.lib.pages.things import hot_links_by_url_listing
from r2.lib.pages import trafficpages
from r2.lib.menus import *
from r2.lib.csrf import csrf_exempt
from r2.lib.utils import to36, sanitize_url, title_to_url
from r2.lib.utils import query_string, UrlParser, url_links_builder
from r2.lib.template_helpers import get_domain
from r2.lib.filters import unsafe, _force_unicode, _force_utf8
from r2.lib.emailer import Email, generate_notification_email_unsubscribe_token
from r2.lib.db.operators import desc
from r2.lib.db import queries
from r2.lib.db.tdb_cassandra import MultiColumnQuery
from r2.lib.strings import strings
from r2.lib.validator import *
from r2.lib import jsontemplates
import r2.lib.db.thing as thing
from r2.lib.errors import errors, ForbiddenError
from listingcontroller import ListingController
from oauth2 import require_oauth2_scope
from api_docs import api_doc, api_section

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.models.token import EmailVerificationToken
from r2.controllers.ipn import generate_blob, validate_blob, GoldException

from operator import attrgetter
import string
import random as rand
import re
from urllib import quote_plus

class FrontController(RedditController):

    allow_stylesheets = True

    @validate(link=VLink('link_id'))
    def GET_link_id_redirect(self, link):
        if not link:
            abort(404)
        elif not link.subreddit_slow.can_view(c.user):
            # don't disclose the subreddit/title of a post via the redirect url
            abort(403)
        else:
            redirect_url = link.make_permalink_slow(force_domain=True)

        query_params = dict(request.GET)
        if query_params:
            url = UrlParser(redirect_url)
            url.update_query(**query_params)
            redirect_url = url.unparse()

        return self.redirect(redirect_url, code=301)

    @validate(article=VLink('article'),
              comment=VCommentID('comment'))
    def GET_oldinfo(self, article, type, dest, rest=None, comment=''):
        """Legacy: supporting permalink pages from '06,
           and non-search-engine-friendly links"""
        if not (dest in ('comments','details')):
                dest = 'comments'
        if type == 'ancient':
            #this could go in config, but it should never change
            max_link_id = 10000000
            new_id = max_link_id - int(article._id)
            return self.redirect('/info/' + to36(new_id) + '/' + rest)
        if type == 'old':
            if not article.subreddit_slow.can_view(c.user):
                self.abort403()

            new_url = "/%s/%s/%s" % \
                      (dest, article._id36,
                       quote_plus(title_to_url(article.title).encode('utf-8')))
            if not c.default_sr:
                new_url = "/r/%s%s" % (c.site.name, new_url)
            if comment:
                new_url = new_url + "/%s" % comment._id36
            if c.extension:
                new_url = new_url + "/.%s" % c.extension

            new_url = new_url + query_string(request.GET)

            # redirect should be smarter and handle extensions, etc.
            return self.redirect(new_url, code=301)

    @require_oauth2_scope("read")
    @api_doc(api_section.listings, uses_site=True)
    def GET_random(self):
        """The Serendipity button"""
        sort = rand.choice(('new','hot'))
        q = c.site.get_links(sort, 'all')
        if isinstance(q, thing.Query):
            q._limit = g.num_serendipity
            names = [link._fullname for link in q]
        else:
            names = list(q)[:g.num_serendipity]

        rand.shuffle(names)

        def keep_fn(item):
            return (
                item.fresh and
                item.keep_item(item) and
                item.subreddit.discoverable
            )

        builder = IDBuilder(names, skip=True, keep_fn=keep_fn, num=1)
        links, first, last, before, after = builder.get_items()

        if links:
            redirect_url = links[0].make_permalink_slow(force_domain=True)
            return self.redirect(redirect_url)
        else:
            return self.redirect(add_sr('/'))

    @disable_subreddit_css()
    @validate(
        VAdmin(),
        thing=VByName('article'),
        oldid36=nop('article'),
        after=nop('after'),
        before=nop('before'),
        count=VCount('count'),
        listing_only=VBoolean('listing_only'),
    )
    def GET_details(self, thing, oldid36, after, before, count, listing_only):
        """The (now deprecated) details page.  Content on this page
        has been subsubmed by the presence of the LinkInfoBar on the
        rightbox, so it is only useful for Admin-only wizardry."""
        if not thing:
            try:
                link = Link._byID36(oldid36)
                return self.redirect('/details/' + link._fullname)
            except (NotFound, ValueError):
                abort(404)

        kw = {
            'count': count,
            'listing_only': listing_only,
        }
        if before:
            kw['after'] = before
            kw['reverse'] = True
        else:
            kw['after'] = after
            kw['reverse'] = False
        c.referrer_policy = "always"
        page = DetailsPage(thing=thing, expand_children=False, **kw)
        if listing_only:
            return page.details.listing.listing().render()
        return page.render()

    @validate(VUser())
    def GET_explore(self):
        settings = ExploreSettings.for_user(c.user)
        recs = recommender.get_recommended_content_for_user(c.user,
                                                            settings,
                                                            record_views=True)
        content = ExploreItemListing(recs, settings)
        return BoringPage(_("explore"),
                          show_sidebar=True,
                          show_chooser=True,
                          page_classes=['explore-page'],
                          content=content).render()

    @validate(article=VLink('article'))
    def GET_shirt(self, article):
        if not can_view_link_comments(article):
            abort(403, 'forbidden')
        return self.abort404()

    @require_oauth2_scope("read")
    @validate(article=VLink('article',
                  docs={"article": "ID36 of a link"}),
              comment=VCommentID('comment',
                  docs={"comment": "(optional) ID36 of a comment"}),
              context=VInt('context', min=0, max=8),
              sort=VOneOf('sort', CommentSortMenu._options),
              limit=VInt('limit',
                  docs={"limit": "(optional) an integer"}),
              depth=VInt('depth',
                  docs={"depth": "(optional) an integer"}),
              showedits=VBoolean("showedits", default=True),
              showmore=VBoolean("showmore", default=True),
              sr_detail=VBoolean(
                  "sr_detail", docs={"sr_detail": "(optional) expand subreddits"}),
              )
    @api_doc(api_section.listings,
             uri='/comments/{article}',
             uses_site=True,
             supports_rss=True)
    def GET_comments(
        self, article, comment, context, sort, limit, depth,
            showedits=True, showmore=True, sr_detail=False):
        """Get the comment tree for a given Link `article`.

        If supplied, `comment` is the ID36 of a comment in the comment tree for
        `article`. This comment will be the (highlighted) focal point of the
        returned view and `context` will be the number of parents shown.

        `depth` is the maximum depth of subtrees in the thread.

        `limit` is the maximum number of comments to return.

        See also: [/api/morechildren](#GET_api_morechildren) and
        [/api/comment](#POST_api_comment).

        """
        if not sort:
            sort = c.user.pref_default_comment_sort

            # hot sort no longer exists but might still be set as a preference
            if sort == "hot":
                sort = "confidence"

        if comment and comment.link_id != article._id:
            return self.abort404()

        sr = Subreddit._byID(article.sr_id, True)

        if sr.name == g.takedown_sr:
            request.environ['REDDIT_TAKEDOWN'] = article._fullname
            return self.abort404()

        if not c.default_sr and c.site._id != sr._id:
            return self.abort404()

        if not can_view_link_comments(article):
            abort(403, 'forbidden')

        # check over 18
        if (
            article.is_nsfw and
            not c.over18 and
            c.render_style == 'html' and
            not request.parsed_agent.bot
        ):
            return self.intermediate_redirect("/over18", sr_path=False)

        canonical_link = article.make_canonical_link(sr)

        # Determine if we should show the embed link for comments
        c.can_embed = bool(comment) and article.is_embeddable

        is_embed = embeds.prepare_embed_request()
        if is_embed and comment:
            embeds.set_up_comment_embed(sr, comment, showedits=showedits)

        # Temporary hook until IAMA app "OP filter" is moved from partners
        # Not to be open-sourced
        page = hooks.get_hook("comments_page.override").call_until_return(
            controller=self,
            article=article,
            limit=limit,
        )
        if page:
            return page

        # If there is a focal comment, communicate down to
        # comment_skeleton.html who that will be. Also, skip
        # comment_visits check
        previous_visits = None
        if comment:
            c.focal_comment = comment._id36
        elif (c.user_is_loggedin and
                (c.user.gold or sr.is_moderator(c.user)) and
                c.user.pref_highlight_new_comments):
            timer = g.stats.get_timer("gold.comment_visits")
            timer.start()
            previous_visits = CommentVisitsByUser.get_and_update(
                c.user, article, c.start_time)
            timer.stop()

        # check if we just came from the submit page
        infotext = None
        infotext_class = None
        infotext_show_icon = False
        if request.GET.get('already_submitted'):
            submit_url = request.GET.get('submit_url') or article.url
            submit_title = request.GET.get('submit_title') or ""
            resubmit_url = Link.resubmit_link(submit_url, submit_title)
            if c.user_is_loggedin and c.site.can_submit(c.user):
                resubmit_url = add_sr(resubmit_url)
            infotext = strings.already_submitted % resubmit_url
        elif article.archived_slow:
            infotext = strings.archived_post_message
            infotext_class = 'archived-infobar'
            infotext_show_icon = True
        elif article.locked:
            infotext = strings.locked_post_message
            infotext_class = 'locked-infobar'
            infotext_show_icon = True

        if not c.user.pref_num_comments:
            num = g.num_comments
        elif c.user_is_loggedin and (c.user.gold or sr.is_moderator(c.user)):
            num = min(c.user.pref_num_comments, g.max_comments_gold)
        else:
            num = min(c.user.pref_num_comments, g.max_comments)

        kw = {}
        # allow depth to be reset (I suspect I'll turn the VInt into a
        # validator on my next pass of .compact)
        if depth is not None and 0 < depth < MAX_RECURSION:
            kw['max_depth'] = depth
        elif c.render_style == "compact":
            kw['max_depth'] = 5

        kw["edits_visible"] = showedits
        kw["load_more"] = kw["continue_this_thread"] = showmore
        kw["show_deleted"] = embeds.is_embed()

        displayPane = PaneStack()

        # allow the user's total count preferences to be overwritten
        # (think of .embed as the use case together with depth=1)

        if limit and limit > 0:
            num = limit

        if c.user_is_loggedin and (c.user.gold or sr.is_moderator(c.user)):
            if num > g.max_comments_gold:
                displayPane.append(InfoBar(message =
                                           strings.over_comment_limit_gold
                                           % max(0, g.max_comments_gold)))
                num = g.max_comments_gold
        elif num > g.max_comments:
            if limit:
                displayPane.append(InfoBar(message =
                                       strings.over_comment_limit
                                       % dict(max=max(0, g.max_comments),
                                              goldmax=max(0,
                                                   g.max_comments_gold))))
            num = g.max_comments

        page_classes = ['comments-page']

        # if permalink page, add that message first to the content
        if comment:
            displayPane.append(PermalinkMessage(article.make_permalink_slow()))
            page_classes.append('comment-permalink-page')

        displayPane.append(LinkCommentSep())

        # insert reply box only for logged in user
        if (not is_api() and
                c.user_is_loggedin and
                article.can_comment_slow(c.user)):
            # no comment box for permalinks
            display = not comment

            # show geotargeting notice only if user is able to comment
            if article.promoted:
                geotargeted, city_target = promote.is_geotargeted_promo(article)
                if geotargeted:
                    displayPane.append(GeotargetNotice(city_target=city_target))

            data_attrs = {'type': 'link', 'event-action': 'comment'}

            displayPane.append(UserText(item=article, creating=True,
                                        post_form='comment',
                                        display=display,
                                        cloneable=True,
                                        data_attrs=data_attrs))

        if previous_visits:
            displayPane.append(CommentVisitsBox(previous_visits))

        if c.site.allows_referrers:
            c.referrer_policy = "always"

        suggested_sort_active = False
        if not c.user.pref_ignore_suggested_sort:
            suggested_sort = article.sort_if_suggested()
        else:
            suggested_sort = None

        # Special override: if the suggested sort is Q&A, and a responder of
        # the thread is viewing it, we don't want to suggest to them to view
        # the thread in Q&A mode (as it hides many unanswered questions)
        if (suggested_sort == "qa" and
                c.user_is_loggedin and
                c.user._id in article.responder_ids):
            suggested_sort = None

        if article.contest_mode:
            if c.user_is_loggedin and sr.is_moderator(c.user):
                # Default to top for contest mode to make determining winners
                # easier, but allow them to override it for moderation
                # purposes.
                if 'sort' not in request.params:
                    sort = "top"
            else:
                sort = "random"
        elif suggested_sort and 'sort' not in request.params:
            sort = suggested_sort
            suggested_sort_active = True

        # finally add the comment listing
        displayPane.append(CommentPane(article, CommentSortMenu.operator(sort),
                                       comment, context, num, **kw))

        subtitle_buttons = []
        disable_comments = article.promoted and article.disable_comments

        if (c.focal_comment or
            context is not None or
            disable_comments):
            subtitle = None
        elif article.num_comments == 0:
            subtitle = _("no comments (yet)")
        elif article.num_comments <= num:
            subtitle = _("all %d comments") % article.num_comments
        else:
            subtitle = _("top %d comments") % num

            if g.max_comments > num:
                self._add_show_comments_link(subtitle_buttons, article, num,
                                             g.max_comments, gold=False)

            if (c.user_is_loggedin and
                    (c.user.gold or sr.is_moderator(c.user)) and
                    article.num_comments > g.max_comments):
                self._add_show_comments_link(subtitle_buttons, article, num,
                                             g.max_comments_gold, gold=True)

        sort_menu = CommentSortMenu(
            default=sort,
            css_class='suggested' if suggested_sort_active else '',
            suggested_sort=suggested_sort,
        )

        link_settings = LinkCommentsSettings(
            article,
            sort=sort,
            suggested_sort=suggested_sort,
        )

        # Check for click urls on promoted links
        click_url = None
        campaign_fullname = None
        if article.promoted and not article.is_self:
            campaign_fullname = request.GET.get("campaign", None)
            click_url = request.GET.get("click_url", None)
            click_hash = request.GET.get("click_hash", "")

            if (click_url and not promote.is_valid_click_url(
                    link=article,
                    click_url=click_url,
                    click_hash=click_hash)):
                click_url = None

        # event target for screenviews
        if comment:
            event_target = {
                'target_type': 'comment',
                'target_fullname': comment._fullname,
                'target_id': comment._id,
            }
        elif article.is_self:
            event_target = {
                'target_type': 'self',
                'target_fullname': article._fullname,
                'target_id': article._id,
                'target_sort': sort,
            }
        else:
            event_target = {
                'target_type': 'link',
                'target_fullname': article._fullname,
                'target_id': article._id,
                'target_url': article.url,
                'target_url_domain': article.link_domain(),
                'target_sort': sort,
            }
        extra_js_config = {'event_target': event_target}

        res = LinkInfoPage(
            link=article,
            comment=comment,
            disable_comments=disable_comments,
            content=displayPane,
            page_classes=page_classes,
            subtitle=subtitle,
            subtitle_buttons=subtitle_buttons,
            nav_menus=[sort_menu, link_settings],
            infotext=infotext,
            infotext_class=infotext_class,
            infotext_show_icon=infotext_show_icon,
            sr_detail=sr_detail,
            campaign_fullname=campaign_fullname,
            click_url=click_url,
            canonical_link=canonical_link,
            extra_js_config=extra_js_config,
        )

        return res.render()

    def _add_show_comments_link(self, array, article, num, max_comm, gold=False):
        if num == max_comm:
            return
        elif article.num_comments <= max_comm:
            link_text = _("show all %d") % article.num_comments
        else:
            link_text = _("show %d") % max_comm

        limit_param = "?limit=%d" % max_comm

        if gold:
            link_class = "gold"
        else:
            link_class = ""

        more_link = article.make_permalink_slow() + limit_param
        array.append( (link_text, more_link, link_class) )

    @validate(VUser(),
              name=nop('name'))
    def GET_newreddit(self, name):
        """Create a subreddit form"""
        VNotInTimeout().run(action_name="pageview", details_text="newreddit")
        title = _('create a subreddit')
        captcha = Captcha() if c.user.needs_captcha() else None
        content = CreateSubreddit(name=name or '', captcha=captcha)
        res = FormPage(_("create a subreddit"),
                       content=content,
                       captcha=captcha,
                       ).render()
        return res

    @require_oauth2_scope("modconfig")
    @api_doc(api_section.moderation, uses_site=True)
    def GET_stylesheet(self):
        """Redirect to the subreddit's stylesheet if one exists.

        See also: [/api/subreddit_stylesheet](#POST_api_subreddit_stylesheet).

        """
        # de-stale the subreddit object so we don't poison downstream caches
        if not isinstance(c.site, FakeSubreddit):
            c.site = Subreddit._byID(c.site._id, data=True, stale=False)

        url = Reddit.get_subreddit_stylesheet_url(c.site)
        if url:
            return self.redirect(url)
        else:
            self.abort404()

    def GET_share_close(self):
        """Render a page that closes itself.

        Intended for use as a redirect target for facebook sharing.
        """
        return ShareClose().render()

    def _make_moderationlog(self, srs, num, after, reverse, count, mod=None, action=None):
        query = Subreddit.get_modactions(srs, mod=mod, action=action)
        builder = ModActionBuilder(
            query, num=num, after=after, count=count, reverse=reverse,
            wrap=default_thing_wrapper())
        listing = ModActionListing(builder)
        pane = listing.listing()
        return pane

    modname_splitter = re.compile('[ ,]+')

    @require_oauth2_scope("modlog")
    @disable_subreddit_css()
    @paginated_listing(max_page_size=500, backend='cassandra')
    @validate(
        mod=nop('mod', docs={"mod": "(optional) a moderator filter"}),
        action=VOneOf('type', ModAction.actions),
    )
    @api_doc(api_section.moderation, uses_site=True,
             uri="/about/log", supports_rss=True)
    def GET_moderationlog(self, num, after, reverse, count, mod, action):
        """Get a list of recent moderation actions.

        Moderator actions taken within a subreddit are logged. This listing is
        a view of that log with various filters to aid in analyzing the
        information.

        The optional `mod` parameter can be a comma-delimited list of moderator
        names to restrict the results to, or the string `a` to restrict the
        results to admin actions taken within the subreddit.

        The `type` parameter is optional and if sent limits the log entries
        returned to only those of the type specified.

        """
        if not c.user_is_loggedin or not (c.user_is_admin or
                                          c.site.is_moderator(c.user)):
            return self.abort404()

        VNotInTimeout().run(action_name="pageview", details_text="modlog")
        if mod:
            if mod == 'a':
                modnames = g.admins
            else:
                modnames = self.modname_splitter.split(mod)
            mod = []
            for name in modnames:
                try:
                    mod.append(Account._by_name(name, allow_deleted=True))
                except NotFound:
                    continue
            mod = mod or None

        if isinstance(c.site, (MultiReddit, ModSR)):
            srs = Subreddit._byID(c.site.sr_ids, return_dict=False)

            # grab all moderators
            mod_ids = set(Subreddit.get_all_mod_ids(srs))
            mods = Account._byID(mod_ids, data=True)

            pane = self._make_moderationlog(srs, num, after, reverse, count,
                                            mod=mod, action=action)
        elif isinstance(c.site, FakeSubreddit):
            return self.abort404()
        else:
            mod_ids = c.site.moderators
            mods = Account._byID(mod_ids, data=True)

            pane = self._make_moderationlog(c.site, num, after, reverse, count,
                                            mod=mod, action=action)

        panes = PaneStack()
        panes.append(pane)

        action_buttons = [QueryButton(_('all'), None, query_param='type',
                                      css_class='primary')]
        for a in ModAction.actions:
            button = QueryButton(ModAction._menu[a], a, query_param='type')
            action_buttons.append(button)

        mod_buttons = [QueryButton(_('all'), None, query_param='mod',
                                   css_class='primary')]
        for mod_id in mod_ids:
            mod = mods[mod_id]
            mod_buttons.append(QueryButton(mod.name, mod.name,
                                           query_param='mod'))
        # add a choice for the automoderator account if it's not a mod
        if (g.automoderator_account and
                all(mod.name != g.automoderator_account
                    for mod in mods.values())):
            automod_button = QueryButton(
                g.automoderator_account,
                g.automoderator_account,
                query_param="mod",
            )
            mod_buttons.append(automod_button)
        mod_buttons.append(QueryButton(_('admins*'), 'a', query_param='mod'))
        base_path = request.path
        menus = [NavMenu(action_buttons, base_path=base_path,
                         title=_('filter by action'), type='lightdrop', css_class='modaction-drop'),
                NavMenu(mod_buttons, base_path=base_path,
                        title=_('filter by moderator'), type='lightdrop')]
        extension_handling = "private" if c.user.pref_private_feeds else False
        return EditReddit(content=panes,
                          nav_menus=menus,
                          location="log",
                          extension_handling=extension_handling).render()

    def _make_spamlisting(self, location, only, num, after, reverse, count):
        include_links, include_comments = True, True
        if only == 'links':
            include_comments = False
        elif only == 'comments':
            include_links = False

        if location == 'reports':
            query = c.site.get_reported(include_links=include_links,
                                        include_comments=include_comments)
        elif location == 'spam':
            query = c.site.get_spam(include_links=include_links,
                                    include_comments=include_comments)
        elif location == 'modqueue':
            query = c.site.get_modqueue(include_links=include_links,
                                        include_comments=include_comments)
        elif location == 'unmoderated':
            query = c.site.get_unmoderated()
        elif location == 'edited':
            query = c.site.get_edited(include_links=include_links,
                                      include_comments=include_comments)
        else:
            raise ValueError

        if isinstance(query, thing.Query):
            builder_cls = QueryBuilder
        elif isinstance (query, list):
            builder_cls = QueryBuilder
        else:
            builder_cls = IDBuilder

        def keep_fn(x):
            # no need to bother mods with banned users, or deleted content
            if x._deleted:
                return False
            if getattr(x,'author',None) == c.user and c.user._spam:
                return False

            if location == "reports":
                return x.reported > 0 and not x._spam
            elif location == "spam":
                return x._spam
            elif location == "modqueue":
                if x.reported > 0 and not x._spam:
                    return True # reported but not banned
                if x.author._spam and x.subreddit.exclude_banned_modqueue:
                    # banned user, don't show if subreddit pref excludes
                    return False

                verdict = getattr(x, "verdict", None)
                if verdict is None:
                    return True # anything without a verdict
                if x._spam:
                    ban_info = getattr(x, "ban_info", {})
                    if ban_info.get("auto", True):
                        return True # spam, unless banned by a moderator
                return False
            elif location == "unmoderated":
                # banned user, don't show if subreddit pref excludes
                if x.author._spam and x.subreddit.exclude_banned_modqueue:
                    return False
                if x._spam:
                    ban_info = getattr(x, "ban_info", {})
                    if ban_info.get("auto", True):
                        return True
                return not getattr(x, 'verdict', None)
            elif location == "edited":
                return bool(getattr(x, "editted", False))
            else:
                raise ValueError

        builder = builder_cls(query,
                              skip=True,
                              num=num, after=after,
                              keep_fn=keep_fn,
                              count=count, reverse=reverse,
                              wrap=ListingController.builder_wrapper,
                              spam_listing=True)
        listing = LinkListing(builder)
        pane = listing.listing()

        # Indicate that the comment tree wasn't built for comments
        for i in pane.things:
            if hasattr(i, 'body'):
                i.child = None

        return pane

    def _edit_normal_reddit(self, location, created):
        if (location == 'edit' and
                c.user_is_loggedin and
                (c.user_is_admin or
                    c.site.is_moderator_with_perms(c.user, 'config'))):
            pane = PaneStack()

            if created == 'true':
                infobar_message = strings.sr_created
                pane.append(InfoBar(message=infobar_message))

            c.allow_styles = True
            c.site = Subreddit._byID(c.site._id, data=True, stale=False)
            pane.append(CreateSubreddit(site=c.site))
        elif (location == 'stylesheet'
              and c.site.can_change_stylesheet(c.user)
              and not g.css_killswitch):
            stylesheet_contents = c.site.fetch_stylesheet_source()
            c.allow_styles = True
            pane = SubredditStylesheet(site=c.site,
                                       stylesheet_contents=stylesheet_contents)
        elif (location == 'stylesheet'
              and c.site.can_view(c.user)
              and not g.css_killswitch):
            stylesheet = c.site.fetch_stylesheet_source()
            pane = SubredditStylesheetSource(stylesheet_contents=stylesheet)
        elif (location == 'traffic' and
              (c.site.public_traffic or
               (c.user_is_loggedin and
                (c.site.is_moderator(c.user) or c.user.employee)))):
            pane = trafficpages.SubredditTraffic()
        elif (location == "about") and is_api():
            return self.redirect(add_sr('about.json'), code=301)
        else:
            return self.abort404()

        return EditReddit(content=pane,
                          location=location,
                          extension_handling=False).render()

    @require_oauth2_scope("read")
    @base_listing
    @disable_subreddit_css()
    @validate(
        VSrModerator(perms='posts'),
        location=nop('location'),
        only=VOneOf('only', ('links', 'comments')),
        timeout=VNotInTimeout(),
    )
    @api_doc(
        api_section.moderation,
        uses_site=True,
        uri='/about/{location}',
        uri_variants=['/about/' + loc for loc in
                      ('reports', 'spam', 'modqueue', 'unmoderated', 'edited')],
    )
    def GET_spamlisting(self, location, only, num, after, reverse, count,
            timeout):
        """Return a listing of posts relevant to moderators.

        * reports: Things that have been reported.
        * spam: Things that have been marked as spam or otherwise removed.
        * modqueue: Things requiring moderator review, such as reported things
            and items caught by the spam filter.
        * unmoderated: Things that have yet to be approved/removed by a mod.
        * edited: Things that have been edited recently.

        Requires the "posts" moderator permission for the subreddit.

        """
        c.allow_styles = True
        c.profilepage = True
        panes = PaneStack()

        # We clone and modify this when a user clicks 'reply' on a comment.
        replyBox = UserText(item=None, display=False, cloneable=True,
                            creating=True, post_form='comment')
        panes.append(replyBox)

        spamlisting = self._make_spamlisting(location, only, num, after,
                                             reverse, count)
        panes.append(spamlisting)

        extension_handling = "private" if c.user.pref_private_feeds else False

        if location in ('reports', 'spam', 'modqueue', 'edited'):
            buttons = [
                QueryButton(_('posts and comments'), None, query_param='only'),
                QueryButton(_('posts'), 'links', query_param='only'),
                QueryButton(_('comments'), 'comments', query_param='only'),
            ]
            menus = [NavMenu(buttons, base_path=request.path, title=_('show'),
                             type='lightdrop')]
        else:
            menus = None
        return EditReddit(content=panes,
                          location=location,
                          nav_menus=menus,
                          extension_handling=extension_handling).render()

    @base_listing
    @disable_subreddit_css()
    @validate(
        VSrModerator(perms='flair'),
        name=nop('name'),
        timeout=VNotInTimeout(),
    )
    def GET_flairlisting(self, num, after, reverse, count, name, timeout):
        user = None
        if name:
            try:
                user = Account._by_name(name)
            except NotFound:
                c.errors.add(errors.USER_DOESNT_EXIST, field='name')

        c.allow_styles = True
        pane = FlairPane(num, after, reverse, name, user)
        return EditReddit(content=pane, location='flair').render()

    @require_oauth2_scope("modconfig")
    @disable_subreddit_css()
    @validate(location=nop('location'),
              created=VOneOf('created', ('true','false'),
                             default='false'))
    @api_doc(api_section.subreddits, uri="/r/{subreddit}/about/edit")
    def GET_editreddit(self, location, created):
        """Get the current settings of a subreddit.

        In the API, this returns the current settings of the subreddit as used
        by [/api/site_admin](#POST_api_site_admin).  On the HTML site, it will
        display a form for editing the subreddit.

        """
        c.profilepage = True
        if isinstance(c.site, FakeSubreddit):
            return self.abort404()
        else:
            VNotInTimeout().run(action_name="pageview",
                details_text="editreddit_%s" % location, target=c.site)
            return self._edit_normal_reddit(location, created)

    @require_oauth2_scope("read")
    @api_doc(api_section.subreddits, uri='/r/{subreddit}/about')
    def GET_about(self):
        """Return information about the subreddit.

        Data includes the subscriber count, description, and header image."""
        if not is_api() or isinstance(c.site, FakeSubreddit):
            return self.abort404()

        # we do this here so that item.accounts_active_count is only present on
        # this one endpoint, and not all the /subreddit listings etc. since
        # looking up activity across multiple subreddits is more work.
        accounts_active_count = None
        activity = c.site.count_activity()
        if activity:
            accounts_active_count = activity.logged_in.count

        item = Wrapped(c.site, accounts_active_count=accounts_active_count)
        Subreddit.add_props(c.user, [item])
        return Reddit(content=item).render()

    @require_oauth2_scope("read")
    @api_doc(api_section.subreddits, uses_site=True)
    def GET_sidebar(self):
        """Get the sidebar for the current subreddit"""
        usertext = UserText(c.site, c.site.description)
        return Reddit(content=usertext).render()

    @require_oauth2_scope("read")
    @api_doc(api_section.subreddits, uri='/r/{subreddit}/about/rules')
    def GET_rules(self):
        """Get the rules for the current subreddit"""
        if not feature.is_enabled("subreddit_rules", subreddit=c.site.name):
            abort(404)
        if isinstance(c.site, FakeSubreddit):
            abort(404)

        kind_labels = {
            "all": _("Posts & Comments"),
            "link": _("Posts only"),
            "comment": _("Comments only"),
        }
        title_string = _("Rules for r/%(subreddit)s") % { "subreddit" : c.site.name }
        content = Rules(
            title=title_string,
            kind_labels=kind_labels,
        )
        extra_js_config = {"kind_labels": kind_labels}
        return ModToolsPage(
            title=title_string,
            content=content,
            extra_js_config=extra_js_config,
        ).render()

    @require_oauth2_scope("read")
    @api_doc(api_section.subreddits, uses_site=True)
    @validate(
        num=VInt("num",
            min=1, max=Subreddit.MAX_STICKIES, num_default=1, coerce=True),
    )
    def GET_sticky(self, num):
        """Redirect to one of the posts stickied in the current subreddit

        The "num" argument can be used to select a specific sticky, and will
        default to 1 (the top sticky) if not specified.
        Will 404 if there is not currently a sticky post in this subreddit.

        """
        if not num or not c.site.sticky_fullnames:
            abort(404)

        try:
            fullname = c.site.sticky_fullnames[num-1]
        except IndexError:
            abort(404)
        sticky = Link._by_fullname(fullname, data=True)
        self.redirect(sticky.make_permalink_slow())

    def GET_awards(self):
        """The awards page."""
        return BoringPage(_("awards"), content=UserAwards()).render()

    @base_listing
    @require_oauth2_scope("read")
    @validate(article=VLink('article'))
    def GET_related(self, num, article, after, reverse, count):
        """Related page: removed, redirects to comments page."""
        if not can_view_link_comments(article):
            abort(403, 'forbidden')

        self.redirect(article.make_permalink_slow(), code=301)

    @base_listing
    @require_oauth2_scope("read")
    @validate(article=VLink('article'))
    @api_doc(
        api_section.listings,
        uri="/duplicates/{article}",
        supports_rss=True,
    )
    def GET_duplicates(self, article, num, after, reverse, count):
        """Return a list of other submissions of the same URL"""
        if not can_view_link_comments(article):
            abort(403, 'forbidden')

        builder = url_links_builder(article.url, exclude=article._fullname,
                                    num=num, after=after, reverse=reverse,
                                    count=count)
        if after and not builder.valid_after(after):
            g.stats.event_count("listing.invalid_after", "duplicates")
            self.abort403()
        num_duplicates = len(builder.get_items()[0])
        listing = LinkListing(builder).listing()

        res = LinkInfoPage(link=article,
                           comment=None,
                           num_duplicates=num_duplicates,
                           content=listing,
                           page_classes=['other-discussions-page'],
                           subtitle=_('other discussions')).render()
        return res

    @base_listing
    @require_oauth2_scope("read")
    @validate(query=nop('q', docs={"q": "a search query"}),
              sort=VMenu('sort', SubredditSearchSortMenu, remember=False))
    @api_doc(api_section.subreddits, uri='/subreddits/search', supports_rss=True)
    def GET_search_reddits(self, query, reverse, after, count, num, sort):
        """Search subreddits by title and description."""

        # trigger redirect to /over18
        if request.GET.get('over18') == 'yes':
            u = UrlParser(request.fullurl)
            del u.query_dict['over18']
            search_url = u.unparse()
            return self.intermediate_redirect('/over18', sr_path=False,
                                              fullpath=search_url)

        # show NSFW to API and RSS users unless obey_over18=true
        is_api_or_rss = (c.render_style in API_TYPES
                         or c.render_style in RSS_TYPES)
        if is_api_or_rss:
            include_over18 = not c.obey_over18 or c.over18
        elif feature.is_enabled('safe_search'):
            include_over18 = c.over18
        else:
            include_over18 = True

        if query:
            q = g.search.SubredditSearchQuery(query, sort=sort, faceting={},
                                              include_over18=include_over18)
            content = self._search(q, num=num, reverse=reverse,
                                   after=after, count=count,
                                   skip_deleted_authors=False)
        else:
            content = None

        # event target for screenviews (/subreddits/search)
        event_target = {}
        if after:
            event_target['target_count'] = count
            if reverse:
                event_target['target_before'] = after._fullname
            else:
                event_target['target_after'] = after._fullname
        extra_js_config = {'event_target': event_target}

        res = SubredditsPage(content=content,
                             prev_search=query,
                             page_classes=['subreddits-page'],
                             extra_js_config=extra_js_config,
                             # update if we ever add sorts
                             search_params={},
                             title=_("search results"),
                             simple=True).render()
        return res

    search_help_page = "/wiki/search"
    verify_langs_regex = re.compile(r"\A[a-z][a-z](,[a-z][a-z])*\Z")

    @base_listing
    @require_oauth2_scope("read")
    @validate(query=VLength('q', max_length=512),
              sort=VMenu('sort', SearchSortMenu, remember=False),
              recent=VMenu('t', TimeMenu, remember=False),
              restrict_sr=VBoolean('restrict_sr', default=False),
              include_facets=VBoolean('include_facets', default=False),
              result_types=VResultTypes('type'),
              syntax=VOneOf('syntax', options=g.search_syntaxes))
    @api_doc(api_section.search, supports_rss=True, uses_site=True)
    def GET_search(self, query, num, reverse, after, count, sort, recent,
                   restrict_sr, include_facets, result_types, syntax, sr_detail):
        """Search links page."""
        if c.site.login_required and not c.user_is_loggedin:
            raise UserRequiredException

        # trigger redirect to /over18
        if request.GET.get('over18') == 'yes':
            u = UrlParser(request.fullurl)
            del u.query_dict['over18']
            search_url = u.unparse()
            return self.intermediate_redirect('/over18', sr_path=False,
                                              fullpath=search_url)

        if query and '.' in query:
            url = sanitize_url(query, require_scheme=True)
            if url:
                return self.redirect("/submit" + query_string({'url':url}))

        if not restrict_sr:
            site = DefaultSR()
        else:
            site = c.site

        has_query = query or not isinstance(site, (DefaultSR, AllSR))

        if not syntax:
            syntax = g.search.SearchQuery.default_syntax

        # show NSFW to API and RSS users unless obey_over18=true
        is_api_or_rss = (c.render_style in API_TYPES
                         or c.render_style in RSS_TYPES)
        if is_api_or_rss:
            include_over18 = not c.obey_over18 or c.over18
        elif feature.is_enabled('safe_search'):
            include_over18 = c.over18
        else:
            include_over18 = True

        # do not request facets--they are not popular with users and result in
        # looking up unpopular subreddits (which is bad for site performance)
        faceting = {}

        # no subreddit results if fielded search or structured syntax
        if syntax == 'cloudsearch' or (query and ':' in query):
            result_types = result_types - {'sr'}

        # combined results on first page only
        if not after and not restrict_sr and result_types == {'link', 'sr'}:
            # hardcoded to 3 subreddits (or fewer)
            sr_num = min(3, int(num / 3))
            num = num - sr_num
        elif result_types == {'sr'}:
            sr_num = num
            num = 0
        else:
            sr_num = 0

        content = None
        subreddits = None
        nav_menus = None
        cleanup_message = None
        converted_data = None
        subreddit_facets = None
        legacy_render_class = feature.is_enabled('legacy_search') or c.user.pref_legacy_search

        if num > 0 and has_query:
            nav_menus = [SearchSortMenu(default=sort), TimeMenu(default=recent)]
            try:
                q = g.search.SearchQuery(query, site, sort=sort,
                                         faceting=faceting,
                                         include_over18=include_over18,
                                         recent=recent, syntax=syntax)
                content = self._search(q, num=num, after=after, reverse=reverse,
                                       count=count, sr_detail=sr_detail,
                                       heading=_('posts'), nav_menus=nav_menus,
                                       legacy_render_class=legacy_render_class)
                converted_data = q.converted_data
                subreddit_facets = content.subreddit_facets

            except g.search.InvalidQuery:
                g.stats.simple_event('cloudsearch.error.invalidquery')

                # Clean the search of characters that might be causing the
                # InvalidQuery exception. If the cleaned search boils down
                # to an empty string, the search code is expected to bail
                # out early with an empty result set.
                cleaned = re.sub("[^\w\s]+", " ", query)
                cleaned = cleaned.lower().strip()

                q = g.search.SearchQuery(cleaned, site, sort=sort,
                                         faceting=faceting,
                                         include_over18=include_over18,
                                         recent=recent)
                content = self._search(q, num=num, after=after, reverse=reverse,
                                       count=count, heading=_('posts'), nav_menus=nav_menus,
                                       legacy_render_class=legacy_render_class)
                converted_data = q.converted_data
                subreddit_facets = content.subreddit_facets

                if cleaned:
                    cleanup_message = strings.invalid_search_query % {
                                                        "clean_query": cleaned
                                                                      }
                    cleanup_message += " "
                    cleanup_message += strings.search_help % {
                                          "search_help": self.search_help_page
                                                              }
                else:
                    cleanup_message = strings.completely_invalid_search_query

        # extra search request for subreddit results
        if sr_num > 0 and has_query:
            sr_q = g.search.SubredditSearchQuery(query, sort='relevance',
                                                 faceting={},
                                                 include_over18=include_over18)
            subreddits = self._search(sr_q, num=sr_num, reverse=reverse,
                                      after=after, count=count, type='sr',
                                      skip_deleted_authors=False, heading=_('subreddits'),
                                      legacy_render_class=legacy_render_class)

            # backfill with facets if no subreddit search results
            if subreddit_facets and not subreddits.things:
                names = [sr._fullname for sr, count in subreddit_facets]
                builder = IDBuilder(names, num=sr_num)
                listing = SearchListing(builder, nextprev=False)
                subreddits = listing.listing(
                    legacy_render_class=legacy_render_class)

            # ensure response is not list for subreddit only result type
            if is_api() and not content:
                content = subreddits
                subreddits = None

        # event target for screenviews (/search)
        event_target = {
            'target_sort': sort,
            'target_filter_time': recent,
        }
        if after:
            event_target['target_count'] = count
            if reverse:
                event_target['target_before'] = after._fullname
            else:
                event_target['target_after'] = after._fullname
        extra_js_config = {'event_target': event_target}

        res = SearchPage(_('search results'), query,
                         content=content,
                         subreddits=subreddits,
                         nav_menus=nav_menus,
                         search_params=dict(sort=sort, t=recent),
                         infotext=cleanup_message,
                         simple=False, site=c.site,
                         restrict_sr=restrict_sr,
                         syntax=syntax,
                         converted_data=converted_data,
                         facets=subreddit_facets,
                         sort=sort,
                         recent=recent,
                         extra_js_config=extra_js_config,
                         ).render()

        return res

    def _search_builder_wrapper(self, q):
        query = q.query
        recent = str(q.recent) if q.recent else None
        sort = q.sort
        def wrapper_fn(thing):
            w = Wrapped(thing)
            w.prev_search = query
            w.recent = recent
            w.sort = sort

            if isinstance(thing, Link):
                w.render_class = SearchResultLink
            elif isinstance(thing, Subreddit):
                w.render_class = SearchResultSubreddit
            return w
        return wrapper_fn

    def _legacy_search_builder_wrapper(self):
        default_wrapper = default_thing_wrapper()
        def wrapper_fn(thing):
            w = default_wrapper(thing)
            if isinstance(thing, Link):
                w.render_class = LegacySearchResultLink
            return w
        return wrapper_fn

    def _search(self, query_obj, num, after, reverse, count=0, type=None,
                skip_deleted_authors=True, sr_detail=False,
                heading=None, nav_menus=None, legacy_render_class=True):
        """Helper function for interfacing with search.  Basically a
           thin wrapper for SearchBuilder."""

        if legacy_render_class:
            builder_wrapper = self._legacy_search_builder_wrapper()
        else:
            builder_wrapper = self._search_builder_wrapper(query_obj)

        builder = SearchBuilder(query_obj,
                                after=after, num=num, reverse=reverse,
                                count=count,
                                wrap=builder_wrapper,
                                skip_deleted_authors=skip_deleted_authors,
                                sr_detail=sr_detail)
        if after and not builder.valid_after(after):
            g.stats.event_count("listing.invalid_after", "search")
            self.abort403()

        params = request.GET.copy()
        if type:
            params['type'] = type

        listing = SearchListing(builder, show_nums=True, params=params,
                                heading=heading, nav_menus=nav_menus)

        try:
            res = listing.listing(legacy_render_class)
        except g.search.SearchException as e:
            return self.search_fail(e)

        return res

    @validate(VAdmin(),
              comment=VCommentByID('comment_id'))
    def GET_comment_by_id(self, comment):
        href = comment.make_permalink_slow(context=5, anchor=True)
        return self.redirect(href)

    @validate(url=VRequired('url', None),
              title=VRequired('title', None),
              text=VRequired('text', None),
              selftext=VRequired('selftext', None))
    def GET_submit(self, url, title, text, selftext):
        """Submit form."""
        resubmit = request.GET.get('resubmit')
        url = sanitize_url(url)

        if url and not resubmit:
            # check to see if the url has already been submitted

            def keep_fn(item):
                # skip promoted links
                would_keep = item.keep_item(item)
                return would_keep and getattr(item, "promoted", None) is None

            listing = hot_links_by_url_listing(
                url, sr=c.site, num=100, skip=True, keep_fn=keep_fn)
            links = listing.things

            if links and len(links) == 1:
                # redirect the user to the existing link's comments
                existing_submission_url = links[0].already_submitted_link(
                    url, title)
                return self.redirect(existing_submission_url)
            elif links:
                # show the user a listing of all the other links with this url
                # an infotext to resubmit it
                resubmit_url = Link.resubmit_link(url, title)
                sr_resubmit_url = add_sr(resubmit_url)
                infotext = strings.multiple_submitted % sr_resubmit_url
                res = BoringPage(
                    _("seen it"), content=listing, infotext=infotext).render()
                return res

        if not c.user_is_loggedin:
            raise UserRequiredException

        if not (c.default_sr or c.site.can_submit(c.user)):
            abort(403, "forbidden")

        target = c.site if not isinstance(c.site, FakeSubreddit) else None
        VNotInTimeout().run(action_name="pageview", details_text="submit",
            target=target)

        captcha = Captcha() if c.user.needs_captcha() else None

        extra_subreddits = []
        if isinstance(c.site, MultiReddit):
            extra_subreddits.append((
                _('%s subreddits') % c.site.name,
                c.site.srs
            ))

        newlink = NewLink(
            url=url or '',
            title=title or '',
            text=text or '',
            selftext=selftext or '',
            captcha=captcha,
            resubmit=resubmit,
            default_sr=c.site if not c.default_sr else None,
            extra_subreddits=extra_subreddits,
            show_link=c.default_sr or c.site.can_submit_link(c.user),
            show_self=((c.default_sr or c.site.can_submit_text(c.user))
                      and not request.GET.get('no_self')),
        )

        return FormPage(_("submit"),
                        show_sidebar=True,
                        page_classes=['submit-page'],
                        content=newlink).render()

    def GET_catchall(self):
        return self.abort404()

    @require_oauth2_scope("modtraffic")
    @validate(VSponsor('link'),
              link=VLink('link'),
              campaign=VPromoCampaign('campaign'),
              before=VDate('before', format='%Y%m%d%H'),
              after=VDate('after', format='%Y%m%d%H'))
    def GET_traffic(self, link, campaign, before, after):
        if link and campaign and link._id != campaign.link_id:
            return self.abort404()

        if c.render_style == 'csv':
            return trafficpages.PromotedLinkTraffic.as_csv(campaign or link)

        content = trafficpages.PromotedLinkTraffic(link, campaign, before,
                                                   after)
        return LinkInfoPage(link=link,
                            page_classes=["promoted-traffic"],
                            show_sidebar=False, comment=None,
                            show_promote_button=True, content=content).render()

    @validate(VEmployee())
    def GET_site_traffic(self):
        return trafficpages.SitewideTrafficPage().render()

    @validate(VEmployee())
    def GET_lang_traffic(self, langcode):
        return trafficpages.LanguageTrafficPage(langcode).render()

    @validate(VEmployee())
    def GET_advert_traffic(self, code):
        return trafficpages.AdvertTrafficPage(code).render()

    @validate(VEmployee())
    def GET_subreddit_traffic_report(self):
        content = trafficpages.SubredditTrafficReport()

        if c.render_style == 'csv':
            return content.as_csv()
        return trafficpages.TrafficPage(content=content).render()

    @validate(VUser())
    def GET_account_activity(self):
        return AccountActivityPage().render()

    def GET_contact_us(self):
        return BoringPage(_("contact us"), show_sidebar=False,
                          content=ContactUs(), page_classes=["contact-us-page"]
                          ).render()

    @validate(vendor=VOneOf("v", ("claimed-gold", "claimed-creddits",
                                  "spent-creddits", "paypal", "coinbase",
                                  "stripe"),
                            default="claimed-gold"))
    def GET_goldthanks(self, vendor):
        vendor_url = None
        lounge_md = None

        if vendor == "claimed-gold":
            claim_msg = _("Claimed! Enjoy your reddit gold membership.")
            if g.lounge_reddit:
                lounge_md = strings.lounge_msg
        elif vendor == "claimed-creddits":
            claim_msg = _("Your gold creddits have been claimed! Now go to "
                          "someone's userpage and give them a present!")
        elif vendor == "spent-creddits":
            claim_msg = _("Thanks for buying reddit gold! Your transaction "
                          "has been completed.")
        elif vendor == "paypal":
            claim_msg = _("Thanks for buying reddit gold! Your transaction "
                          "has been completed and emailed to you. You can "
                          "check the details by signing into your account "
                          "at:")
            vendor_url = "https://www.paypal.com/us"
        elif vendor in {"coinbase", "stripe"}:  # Pending vendors
            claim_msg = _("Thanks for buying reddit gold! Your transaction is "
                          "being processed. If you have any questions please "
                          "email us at %(gold_email)s")
            claim_msg = claim_msg % {'gold_email': g.goldsupport_email}
        else:
            abort(404)

        return BoringPage(_("thanks"), show_sidebar=False,
                          content=GoldThanks(claim_msg=claim_msg,
                                             vendor_url=vendor_url,
                                             lounge_md=lounge_md),
                          page_classes=["gold-page-ga-tracking"]
                         ).render()

    @validate(VUser(),
              token=VOneTimeToken(AwardClaimToken, "code"))
    def GET_confirm_award_claim(self, token):
        if not token:
            abort(403)

        award = Award._by_fullname(token.awardfullname)
        trophy = FakeTrophy(c.user, award, token.description, token.url)
        content = ConfirmAwardClaim(trophy=trophy, user=c.user.name,
                                    token=token)
        return BoringPage(_("claim this award?"), content=content).render()

    @validate(VUser(),
              VModhash(),
              token=VOneTimeToken(AwardClaimToken, "code"))
    def POST_claim_award(self, token):
        if not token:
            abort(403)

        token.consume()

        award = Award._by_fullname(token.awardfullname)
        trophy, preexisting = Trophy.claim(c.user, token.uid, award,
                                           token.description, token.url)
        redirect = '/awards/received?trophy=' + trophy._id36
        if preexisting:
            redirect += '&duplicate=true'
        self.redirect(redirect)

    @validate(trophy=VTrophy('trophy'),
              preexisting=VBoolean('duplicate'))
    def GET_received_award(self, trophy, preexisting):
        content = AwardReceived(trophy=trophy, preexisting=preexisting)
        return BoringPage(_("award claim"), content=content).render()

    def GET_gilding(self):
        return BoringPage(
            _("gilding"),
            show_sidebar=False,
            content=Gilding(),
            page_classes=["gold-page", "gilding"],
        ).render()

    @csrf_exempt
    @validate(dest=VDestination(default='/'))
    def _modify_hsts_grant(self, dest):
        """Endpoint subdomains can redirect through to update HSTS grants."""
        # TODO: remove this once it stops getting hit
        from r2.lib.base import abort
        require_https()
        if request.host != g.domain:
            abort(ForbiddenError(errors.WRONG_DOMAIN))

        # We can't send the user back to http: if they're forcing HTTPS
        dest_parsed = UrlParser(dest)
        dest_parsed.scheme = "https"
        dest = dest_parsed.unparse()

        return self.redirect(dest, code=307)

    POST_modify_hsts_grant = _modify_hsts_grant
    GET_modify_hsts_grant = _modify_hsts_grant
    DELETE_modify_hsts_grant = _modify_hsts_grant
    PUT_modify_hsts_grant = _modify_hsts_grant


class FormsController(RedditController):

    def GET_password(self):
        """The 'what is my password' page"""
        return BoringPage(_("password"), content=Password()).render()

    @validate(VUser(),
              dest=VDestination(),
              reason=nop('reason'))
    def GET_verify(self, dest, reason):
        if c.user.email_verified:
            content = InfoBar(message=strings.email_verified)
            if dest:
                return self.redirect(dest)
        else:
            if reason == "submit":
                infomsg = strings.verify_email_submit
            else:
                infomsg = strings.verify_email

            content = PaneStack(
                [InfoBar(message=infomsg),
                 PrefUpdate(email=True, verify=True,
                            password=False, dest=dest)])
        return BoringPage(_("verify email"), content=content).render()

    @validate(VUser(),
              token=VOneTimeToken(EmailVerificationToken, "key"),
              dest=VDestination(default="/prefs/update?verified=true"))
    def GET_verify_email(self, token, dest):
        fail_msg = None
        if token and token.user_id != c.user._fullname:
            fail_msg = strings.email_verify_wrong_user
        elif c.user.email_verified:
            # they've already verified.
            if token:
                # consume and ignore this token (if not already consumed).
                token.consume()
            return self.redirect(dest)
        elif token and token.valid_for_user(c.user):
            # successful verification!
            token.consume()
            c.user.email_verified = True
            c.user._commit()
            Award.give_if_needed("verified_email", c.user)
            return self.redirect(dest)

        # failure. let 'em know.
        content = PaneStack(
            [InfoBar(message=fail_msg or strings.email_verify_failed),
             PrefUpdate(email=True,
                        verify=True,
                        password=False)])
        return BoringPage(_("verify email"), content=content).render()

    @validate(token=VOneTimeToken(PasswordResetToken, "key"),
              key=nop("key"))
    def GET_resetpassword(self, token, key):
        """page hit once a user has been sent a password reset email
        to verify their identity before allowing them to update their
        password."""

        done = False
        if not key and request.referer:
            referer_path = request.referer.split(g.domain)[-1]
            done = referer_path.startswith(request.fullpath)
        elif not token:
            return self.redirect("/password?expired=true")

        token_user = Account._by_fullname(token.user_id, data=True)

        return BoringPage(
            _("reset password"),
            content=ResetPassword(
                key=key,
                done=done,
                username=token_user.name,
            )
        ).render()

    @validate(
        user_id36=nop('user'),
        provided_mac=nop('key')
    )
    def GET_unsubscribe_emails(self, user_id36, provided_mac):
        from r2.lib.utils import constant_time_compare

        expected_mac = generate_notification_email_unsubscribe_token(user_id36)
        if not constant_time_compare(provided_mac or '', expected_mac):
            error_page = pages.RedditError(
                title=_('incorrect message token'),
                message='',
            )
            request.environ["usable_error_content"] = error_page.render()
            self.abort404()
        user = Account._byID36(user_id36, data=True)
        user.pref_email_messages = False
        user._commit()

        return BoringPage(_('emails unsubscribed'),
                          content=MessageNotificationEmailsUnsubscribe()).render()

    @disable_subreddit_css()
    @validate(VUser(),
              location=nop("location"),
              verified=VBoolean("verified"))
    def GET_prefs(self, location='', verified=False):
        """Preference page"""
        content = None
        infotext = None
        if not location or location == 'options':
            content = PrefOptions(
                done=request.GET.get('done'),
                error_style_override=request.GET.get('error_style_override'),
                generic_error=request.GET.get('generic_error'),
            )
        elif location == 'update':
            if verified:
                infotext = strings.email_verified
            content = PrefUpdate()
        elif location == 'apps':
            content = PrefApps(my_apps=OAuth2Client._by_user_grouped(c.user),
                               developed_apps=OAuth2Client._by_developer(c.user))
        elif location == 'feeds' and c.user.pref_private_feeds:
            content = PrefFeeds()
        elif location == 'deactivate':
            content = PrefDeactivate()
        elif location == 'delete':
            return self.redirect('/prefs/deactivate', code=301)
        elif location == 'security':
            if c.user.name not in g.admins:
                return self.redirect('/prefs/')
            content = PrefSecurity()
        else:
            return self.abort404()

        return PrefsPage(content=content, infotext=infotext).render()

    @validate(dest=VDestination())
    def GET_login(self, dest):
        """The /login form.  No link to this page exists any more on
        the site (all actions invoking it now go through the login
        cover).  However, this page is still used for logging the user
        in during submission or voting from the bookmarklets."""

        if (c.user_is_loggedin and
            not request.environ.get('extension') == 'embed'):
            return self.redirect(dest)
        return LoginPage(dest=dest).render()


    @validate(dest=VDestination())
    def GET_register(self, dest):
        if (c.user_is_loggedin and
            not request.environ.get('extension') == 'embed'):
            return self.redirect(dest)
        return RegisterPage(dest=dest).render()

    @validate(VUser(),
              VModhash(),
              dest=VDestination())
    def GET_logout(self, dest):
        return self.redirect(dest)

    @validate(VUser(),
              VModhash(),
              dest=VDestination())
    def POST_logout(self, dest):
        """wipe login cookie and redirect to referer."""
        self.logout()
        self.redirect(dest)

    @validate(VUser(),
              dest=VDestination())
    def GET_adminon(self, dest):
        """Enable admin interaction with site"""
        #check like this because c.user_is_admin is still false
        if not c.user.name in g.admins:
            return self.abort404()

        return InterstitialPage(
            _("turn admin on"),
            content=AdminInterstitial(dest=dest)).render()

    @validate(VAdmin(),
              dest=VDestination())
    def GET_adminoff(self, dest):
        """disable admin interaction with site."""
        if not c.user.name in g.admins:
            return self.abort404()
        self.disable_admin_mode(c.user)
        return self.redirect(dest)

    def _render_opt_in_out(self, msg_hash, leave):
        """Generates the form for an optin/optout page"""
        email = Email.handler.get_recipient(msg_hash)
        if not email:
            return self.abort404()
        sent = (has_opted_out(email) == leave)
        return BoringPage(_("opt out") if leave else _("welcome back"),
                          content=OptOut(email=email, leave=leave,
                                           sent=sent,
                                           msg_hash=msg_hash)).render()

    @validate(msg_hash=nop('x'))
    def GET_optout(self, msg_hash):
        """handles /mail/optout to add an email to the optout mailing
        list.  The actual email addition comes from the user posting
        the subsequently rendered form and is handled in
        ApiController.POST_optout."""
        return self._render_opt_in_out(msg_hash, True)

    @validate(msg_hash=nop('x'))
    def GET_optin(self, msg_hash):
        """handles /mail/optin to remove an email address from the
        optout list. The actual email removal comes from the user
        posting the subsequently rendered form and is handled in
        ApiController.POST_optin."""
        return self._render_opt_in_out(msg_hash, False)

    @validate(dest=VDestination("dest"))
    def GET_try_compact(self, dest):
        c.render_style = "compact"
        return TryCompact(dest=dest).render()

    @validate(VUser(),
              secret=VPrintable("secret", 50))
    def GET_claim(self, secret):
        """The page to claim reddit gold trophies"""
        return BoringPage(_("thanks"), content=Thanks(secret)).render()

    @validate(VUser(),
              passthrough=nop('passthrough'))
    def GET_creditgild(self, passthrough):
        """Used only for setting up credit card payments for gilding."""
        try:
            payment_blob = validate_blob(passthrough)
        except GoldException:
            self.abort404()

        if c.user != payment_blob['buyer']:
            self.abort404()

        if not payment_blob['goldtype'] == 'gift':
            self.abort404()

        recipient = payment_blob['recipient']
        thing = payment_blob.get('thing')
        if not thing:
            thing = payment_blob['comment']
        if (not thing or
            thing._deleted or
            not thing.subreddit_slow.can_view(c.user)):
            self.abort404()

        if isinstance(thing, Comment):
            summary = strings.gold_summary_gilding_page_comment
        else:
            summary = strings.gold_summary_gilding_page_link
        summary = summary % {'recipient': recipient.name}
        months = 1
        price = g.gold_month_price * months

        if isinstance(thing, Comment):
            desc = thing.body
        else:
            desc = thing.markdown_link_slow()

        content = CreditGild(
            summary=summary,
            price=price,
            months=months,
            stripe_key=g.secrets['stripe_public_key'],
            passthrough=passthrough,
            description=desc,
            period=None,
        )

        return BoringPage(_("reddit gold"),
                          show_sidebar=False,
                          content=content,
                          page_classes=["gold-page-ga-tracking"]
                         ).render()

    @validate(is_payment=VBoolean("is_payment"),
              goldtype=VOneOf("goldtype",
                              ("autorenew", "onetime", "creddits", "gift",
                               "code")),
              period=VOneOf("period", ("monthly", "yearly")),
              months=VInt("months"),
              num_creddits=VInt("num_creddits"),
              # variables below are just for gifts
              signed=VBoolean("signed", default=True),
              recipient=VExistingUname("recipient", default=None),
              thing=VByName("thing"),
              giftmessage=VLength("giftmessage", 10000),
              email=ValidEmail("email"),
              edit=VBoolean("edit", default=False),
    )
    def GET_gold(self, is_payment, goldtype, period, months, num_creddits,
                 signed, recipient, giftmessage, thing, email, edit):
        VNotInTimeout().run(action_name="pageview", details_text="gold",
            target=thing)
        if thing:
            thing_sr = Subreddit._byID(thing.sr_id, data=True)
            if (thing._deleted or
                    thing._spam or
                    not thing_sr.can_view(c.user) or
                    not thing_sr.allow_gilding):
                thing = None

        start_over = False

        if edit:
            start_over = True

        if not c.user_is_loggedin:
            if goldtype != "code":
                start_over = True
            elif months is None or months < 1:
                start_over = True
            elif not email:
                start_over = True
        elif goldtype == "autorenew":
            if period is None:
                start_over = True
            elif c.user.has_gold_subscription:
                return self.redirect("/gold/subscription")
        elif goldtype in ("onetime", "code"):
            if months is None or months < 1:
                start_over = True
        elif goldtype == "creddits":
            if num_creddits is None or num_creddits < 1:
                start_over = True
            else:
                months = num_creddits
        elif goldtype == "gift":
            if months is None or months < 1:
                start_over = True

            if thing:
                recipient = Account._byID(thing.author_id, data=True)
                if recipient._deleted:
                    thing = None
                    recipient = None
                    start_over = True
            elif not recipient:
                start_over = True
        else:
            goldtype = ""
            start_over = True

        if start_over:
            # If we have a form that didn't validate, and we're on the payment
            # page, redirect to the form, passing all of our form fields
            # (which are currently GET parameters).
            if is_payment:
                g.stats.simple_event("gold.checkout_redirects.to_form")
                qs = query_string(request.GET)
                return self.redirect('/gold' + qs)

            can_subscribe = (c.user_is_loggedin and
                             not c.user.has_gold_subscription)
            if not can_subscribe and goldtype == "autorenew":
                self.redirect("/creddits", code=302)

            return BoringPage(_("reddit gold"),
                              show_sidebar=False,
                              content=Gold(goldtype, period, months, signed,
                                           email, recipient,
                                           giftmessage,
                                           can_subscribe=can_subscribe,
                                           edit=edit),
                              page_classes=["gold-page", "gold-signup", "gold-page-ga-tracking"],
                              ).render()
        else:
            # If we have a validating form, and we're not yet on the payment
            # page, redirect to it, passing all of our form fields
            # (which are currently GET parameters).
            if not is_payment:
                g.stats.simple_event("gold.checkout_redirects.to_payment")
                qs = query_string(request.GET)
                return self.redirect('/gold/payment' + qs)

            payment_blob = dict(goldtype=goldtype,
                                status="initialized")
            if c.user_is_loggedin:
                payment_blob["account_id"] = c.user._id
                payment_blob["account_name"] = c.user.name
            else:
                payment_blob["email"] = email

            if goldtype == "gift":
                payment_blob["signed"] = signed
                payment_blob["recipient"] = recipient.name
                payment_blob["giftmessage"] = _force_utf8(giftmessage)
                if thing:
                    payment_blob["thing"] = thing._fullname

            passthrough = generate_blob(payment_blob)

            page_classes = ["gold-page", "gold-payment", "gold-page-ga-tracking"]
            if goldtype == "creddits":
                page_classes.append("creddits-payment")

            return BoringPage(_("reddit gold"),
                              show_sidebar=False,
                              content=GoldPayment(goldtype, period, months,
                                                  signed, recipient,
                                                  giftmessage, passthrough,
                                                  thing),
                              page_classes=page_classes,
                              ).render()

    def GET_creddits(self):
        return BoringPage(_("purchase creddits"),
                          show_sidebar=False,
                          content=Creddits(),
                          page_classes=["gold-page", "creddits-purchase", "gold-page-ga-tracking"],
                          ).render()

    @validate(VUser())
    def GET_subscription(self):
        user = c.user
        content = GoldSubscription(user)
        return BoringPage(_("reddit gold subscription"),
                          show_sidebar=False,
                          content=content,
                          page_classes=["gold-page-ga-tracking"]
                         ).render()


class FrontUnstyledController(FrontController):
    allow_stylesheets = False
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import app_globals as g
from pylons import tmpl_context as c
from pylons import request

from r2.controllers.reddit_base import MinimalController
from r2.lib.pages import (
    GoogleTagManagerJail,
    GoogleTagManager,
)
from r2.lib.validator import (
    validate,
    VGTMContainerId,
)


class GoogleTagManagerController(MinimalController):
    def pre(self):
        if request.host != g.media_domain:
            # don't serve up untrusted content except on our
            # specifically untrusted domain
            self.abort404()

        MinimalController.pre(self)

        c.allow_framing = True

    @validate(
        container_id=VGTMContainerId("id")
    )
    def GET_jail(self, container_id):
        return GoogleTagManagerJail(container_id=container_id).render()

    @validate(
        container_id=VGTMContainerId("id")
    )
    def GET_gtm(self, container_id):
        return GoogleTagManager(container_id=container_id).render()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import app_globals as g
from reddit_base import RedditController
from r2.lib.pages import AdminPage, AdminAwards
from r2.lib.pages import AdminAwardGive, AdminAwardWinners
from r2.lib.validator import *

class AwardsController(RedditController):

    @validate(VAdmin())
    def GET_index(self):
        res = AdminPage(content = AdminAwards(),
                        title = 'awards').render()
        return res

    @validate(VAdmin(),
              award = VAwardByCodename('awardcn'),
              recipient = nop('recipient'),
              desc = nop('desc'),
              url = nop('url'),
              hours = nop('hours'))
    def GET_give(self, award, recipient, desc, url, hours):
        if award is None:
            abort(404, 'page not found')

        res = AdminPage(content = AdminAwardGive(award, recipient, desc,
                                                 url, hours),
                        title='give an award').render()
        return res

    @validate(VAdmin(),
              award = VAwardByCodename('awardcn'))
    def GET_winners(self, award):
        if award is None:
            abort(404, 'page not found')

        res = AdminPage(content = AdminAwardWinners(award),
                        title='award winners').render()
        return res
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from webob.exc import HTTPBadRequest
from r2.controllers import api_docs
from r2.controllers.oauth2 import allow_oauth2_access
from r2.controllers.reddit_base import RedditController
from r2.lib.base import abort
from r2.lib.validator import validate, nop
from r2.models import OAuth2Scope


class APIv1ScopesController(RedditController):
    THREE_SIXTY = OAuth2Scope.FULL_ACCESS

    @allow_oauth2_access
    @validate(
        scope_str=nop("scopes",
                   docs={"scopes": "(optional) An OAuth2 scope string"}),
    )
    @api_docs.api_doc(api_docs.api_section.misc)
    def GET_scopes(self, scope_str):
        """Retrieve descriptions of reddit's OAuth2 scopes.

        If no scopes are given, information on all scopes are returned.

        Invalid scope(s) will result in a 400 error with body that indicates
        the invalid scope(s).

        """
        scopes = OAuth2Scope(scope_str or self.THREE_SIXTY)
        if scope_str and not scopes.is_valid():
            invalid = [s for s in scopes.scopes if s not in scopes.scope_info]
            error = {"error": "invalid_scopes", "invalid_scopes": invalid}
            http_err = HTTPBadRequest()
            http_err.error_data = error
            abort(http_err)
        return self.api_wrapper({k: v for k, v in scopes.details() if k})
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import request
from pylons import tmpl_context as c

from r2.config.extensions import set_extension
from r2.controllers.reddit_base import RedditController, generate_modhash
from r2.controllers.login import handle_login, handle_register
from r2.lib.csrf import csrf_exempt
from r2.lib.validator import (
    json_validate,
    ValidEmail,
    VPasswordChange,
    VRatelimit,
    VSigned,
    VThrottledLogin,
    VUname,
)


class APIv1LoginController(RedditController):

    def pre(self):
        super(APIv1LoginController, self).pre()
        c.extension = "json"
        set_extension(request.environ, "json")

    @csrf_exempt
    @json_validate(
        VRatelimit(rate_ip=True, prefix="rate_register_"),
        signature=VSigned(),
        name=VUname(['user']),
        email=ValidEmail("email"),
        password=VPasswordChange(['passwd', 'passwd2']),
    )
    def POST_register(self, responder, name, email, password, **kwargs):
        kwargs.update(dict(
            controller=self,
            form=responder("noop"),
            responder=responder,
            name=name,
            email=email,
            password=password,
        ))
        return handle_register(**kwargs)

    @csrf_exempt
    @json_validate(
        signature=VSigned(),
        user=VThrottledLogin(['user', 'passwd']),
    )
    def POST_login(self, responder, user, **kwargs):
        kwargs.update(dict(
            controller=self,
            form=responder("noop"),
            responder=responder,
            user=user,
        ))
        return handle_login(**kwargs)

    def _login(self, responder, user, rem=None):
        """Login the user.

        AJAX login handler, used by both login and register to set the
        user cookie and send back a redirect.
        """
        c.user = user
        c.user_is_loggedin = True
        self.login(user, rem=rem)

        responder._send_data(modhash=generate_modhash())
        responder._send_data(cookie=user.make_cookie())
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import response
from pylons import tmpl_context as c

from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.oauth2 import require_oauth2_scope
from r2.controllers.reddit_base import OAuth2OnlyController
from r2.lib.jsontemplates import (
    FriendTableItemJsonTemplate,
    get_usertrophies,
    IdentityJsonTemplate,
    KarmaListJsonTemplate,
    PrefsJsonTemplate,
)
from r2.lib.pages import FriendTableItem
from r2.lib.validator import (
    validate,
    VAccountByName,
    VFriendOfMine,
    VLength,
    VList,
    VUser,
    VValidatedJSON,
)
from r2.models import Account, Trophy
import r2.lib.errors as errors
import r2.lib.validator.preferences as vprefs


PREFS_JSON_SPEC = VValidatedJSON.PartialObject({
    k[len("pref_"):]: v for k, v in
    vprefs.PREFS_VALIDATORS.iteritems()
})


class APIv1UserController(OAuth2OnlyController):
    @require_oauth2_scope("identity")
    @validate(
        VUser(),
    )
    @api_doc(api_section.account)
    def GET_me(self):
        "Returns the identity of the user currently authenticated via OAuth."
        resp = IdentityJsonTemplate().data(c.oauth_user)
        return self.api_wrapper(resp)

    @require_oauth2_scope("identity")
    @validate(
        VUser(),
        fields=VList(
            "fields",
            choices=PREFS_JSON_SPEC.spec.keys(),
            error=errors.errors.NON_PREFERENCE,
        ),
    )
    @api_doc(api_section.account, uri='/api/v1/me/prefs')
    def GET_prefs(self, fields):
        """Return the preference settings of the logged in user"""
        resp = PrefsJsonTemplate(fields).data(c.oauth_user)
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(
        user=VAccountByName('username'),
    )
    @api_doc(
        section=api_section.users,
        uri='/api/v1/user/{username}/trophies',
    )
    def GET_usertrophies(self, user):
        """Return a list of trophies for the a given user."""
        return self.api_wrapper(get_usertrophies(user))

    @require_oauth2_scope("identity")
    @validate(
        VUser(),
    )
    @api_doc(
        section=api_section.account,
        uri='/api/v1/me/trophies',
    )
    def GET_trophies(self):
        """Return a list of trophies for the current user."""
        return self.api_wrapper(get_usertrophies(c.oauth_user))

    @require_oauth2_scope("mysubreddits")
    @validate(
        VUser(),
    )
    @api_doc(
        section=api_section.account,
        uri='/api/v1/me/karma',
    )
    def GET_karma(self):
        """Return a breakdown of subreddit karma."""
        karmas = c.oauth_user.all_karmas(include_old=False)
        resp = KarmaListJsonTemplate().render(karmas)
        return self.api_wrapper(resp.finalize())

    PREFS_JSON_VALIDATOR = VValidatedJSON("json", PREFS_JSON_SPEC,
                                          body=True)

    @require_oauth2_scope("account")
    @validate(
        VUser(),
        validated_prefs=PREFS_JSON_VALIDATOR,
    )
    @api_doc(api_section.account, json_model=PREFS_JSON_VALIDATOR,
             uri='/api/v1/me/prefs')
    def PATCH_prefs(self, validated_prefs):
        user_prefs = c.user.preferences()
        for short_name, new_value in validated_prefs.iteritems():
            pref_name = "pref_" + short_name
            user_prefs[pref_name] = new_value
        vprefs.filter_prefs(user_prefs, c.user)
        vprefs.set_prefs(c.user, user_prefs)
        c.user._commit()
        return self.api_wrapper(PrefsJsonTemplate().data(c.user))

    FRIEND_JSON_SPEC = VValidatedJSON.PartialObject({
        "name": VAccountByName("name"),
        "note": VLength("note", 300),
    })
    FRIEND_JSON_VALIDATOR = VValidatedJSON("json", spec=FRIEND_JSON_SPEC,
                                           body=True)
    @require_oauth2_scope('subscribe')
    @validate(
        VUser(),
        friend=VAccountByName('username'),
        notes_json=FRIEND_JSON_VALIDATOR,
    )
    @api_doc(api_section.users, json_model=FRIEND_JSON_VALIDATOR,
             uri='/api/v1/me/friends/{username}')
    def PUT_friends(self, friend, notes_json):
        """Create or update a "friend" relationship.

        This operation is idempotent. It can be used to add a new
        friend, or update an existing friend (e.g., add/change the
        note on that friend)

        """
        err = None
        if 'name' in notes_json and notes_json['name'] != friend:
            # The 'name' in the JSON is optional, but if present, must
            # match the username from the URL
            err = errors.RedditError('BAD_USERNAME', fields='name')
        if 'note' in notes_json and not c.user.gold:
            err = errors.RedditError('GOLD_REQUIRED', fields='note')
        if err:
            self.on_validation_error(err)

        # See if the target is already an existing friend.
        # If not, create the friend relationship.
        friend_rel = Account.get_friend(c.user, friend)
        rel_exists = bool(friend_rel)
        if not friend_rel:
            friend_rel = c.user.add_friend(friend)
            response.status = 201

        if 'note' in notes_json:
            note = notes_json['note'] or ''
            if not rel_exists:
                # If this is a newly created friend relationship,
                # the cache needs to be updated before a note can
                # be applied
                c.user.friend_rels_cache(_update=True)
            c.user.add_friend_note(friend, note)
        rel_view = FriendTableItem(friend_rel)
        return self.api_wrapper(FriendTableItemJsonTemplate().data(rel_view))

    @require_oauth2_scope('mysubreddits')
    @validate(
        VUser(),
        friend_rel=VFriendOfMine('username'),
    )
    @api_doc(api_section.users, uri='/api/v1/me/friends/{username}')
    def GET_friends(self, friend_rel):
        """Get information about a specific 'friend', such as notes."""
        rel_view = FriendTableItem(friend_rel)
        return self.api_wrapper(FriendTableItemJsonTemplate().data(rel_view))

    @require_oauth2_scope('subscribe')
    @validate(
        VUser(),
        friend_rel=VFriendOfMine('username'),
    )
    @api_doc(api_section.users, uri='/api/v1/me/friends/{username}')
    def DELETE_friends(self, friend_rel):
        """Stop being friends with a user."""
        c.user.remove_friend(friend_rel._thing2)
        if c.user.gold:
            c.user.friend_rels_cache(_update=True)
        response.status = 204
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.oauth2 import require_oauth2_scope
from r2.controllers.reddit_base import OAuth2OnlyController
from r2.controllers.ipn import send_gift
from r2.lib.errors import RedditError
from r2.lib.validator import (
    validate,
    VAccountByName,
    VByName,
    VInt,
    VNotInTimeout,
)
from r2.models import Account, Comment, Link, NotFound
from r2.models.gold import creddits_lock
from r2.lib.validator import VUser


class APIv1GoldController(OAuth2OnlyController):
    def _gift_using_creddits(self, recipient, months=1, thing_fullname=None,
            proxying_for=None):
        with creddits_lock(c.user):
            if not c.user.employee and c.user.gold_creddits < months:
                err = RedditError("INSUFFICIENT_CREDDITS")
                self.on_validation_error(err)

            note = None
            buyer = c.user
            if c.user.name.lower() in g.live_config["proxy_gilding_accounts"]:
                note = "proxy-%s" % c.user.name
                if proxying_for:
                    try:
                        buyer = Account._by_name(proxying_for)
                    except NotFound:
                        pass

            send_gift(
                buyer=buyer,
                recipient=recipient,
                months=months,
                days=months * 31,
                signed=False,
                giftmessage=None,
                thing_fullname=thing_fullname,
                note=note,
            )

            if not c.user.employee:
                c.user.gold_creddits -= months
                c.user._commit()

    @require_oauth2_scope("creddits")
    @validate(
        VUser(),
        target=VByName("fullname"),
    )
    @api_doc(
        api_section.gold,
        uri="/api/v1/gold/gild/{fullname}",
    )
    def POST_gild(self, target):
        if not isinstance(target, (Comment, Link)):
            err = RedditError("NO_THING_ID")
            self.on_validation_error(err)

        if target.subreddit_slow.quarantine:
            err = RedditError("GILDING_NOT_ALLOWED")
            self.on_validation_error(err)
        VNotInTimeout().run(target=target, subreddit=target.subreddit_slow)

        self._gift_using_creddits(
            recipient=target.author_slow,
            thing_fullname=target._fullname,
            proxying_for=request.POST.get("proxying_for"),
        )

    @require_oauth2_scope("creddits")
    @validate(
        VUser(),
        user=VAccountByName("username"),
        months=VInt("months", min=1, max=36),
        timeout=VNotInTimeout(),
    )
    @api_doc(
        api_section.gold,
        uri="/api/v1/gold/give/{username}",
    )
    def POST_give(self, user, months, timeout):
        self._gift_using_creddits(
            recipient=user,
            months=months,
            proxying_for=request.POST.get("proxying_for"),
        )
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib import amqp
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import NotFound
from r2.lib.errors import MessageError
from r2.lib.utils import tup, fetch_things2
from r2.lib.filters import websafe
from r2.lib.hooks import HookRegistrar
from r2.models import (
    Account,
    Comment,
    Link,
    Message,
    NotFound,
    Report,
    Subreddit,
)
from r2.models.award import Award
from r2.models.gold import append_random_bottlecap_phrase, creddits_lock
from r2.models.token import AwardClaimToken
from r2.models.wiki import WikiPage

from _pylibmc import MemcachedError
from pylons import config
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from datetime import datetime, timedelta
from copy import copy

admintools_hooks = HookRegistrar()

class AdminTools(object):

    def spam(self, things, auto=True, moderator_banned=False,
             banner=None, date=None, train_spam=True, **kw):
        from r2.lib.db import queries

        all_things = tup(things)
        new_things = [x for x in all_things if not x._spam]

        Report.accept(all_things, True)

        for t in all_things:
            if getattr(t, "promoted", None) is not None:
                g.log.debug("Refusing to mark promotion %r as spam" % t)
                continue

            if not t._spam and train_spam:
                note = 'spam'
            elif not t._spam and not train_spam:
                note = 'remove not spam'
            elif t._spam and not train_spam:
                note = 'confirm spam'
            elif t._spam and train_spam:
                note = 'reinforce spam'

            t._spam = True

            if moderator_banned:
                t.verdict = 'mod-removed'
            elif not auto:
                t.verdict = 'admin-removed'

            ban_info = copy(getattr(t, 'ban_info', {}))
            if isinstance(banner, dict):
                ban_info['banner'] = banner[t._fullname]
            else:
                ban_info['banner'] = banner
            ban_info.update(auto=auto,
                            moderator_banned=moderator_banned,
                            banned_at=date or datetime.now(g.tz),
                            **kw)
            ban_info['note'] = note

            t.ban_info = ban_info
            t._commit()

            if auto:
                amqp.add_item("auto_removed", t._fullname)

        if not auto:
            self.author_spammer(new_things, True)
            self.set_last_sr_ban(new_things)

        queries.ban(all_things, filtered=auto)

        for t in all_things:
            if auto:
                amqp.add_item("auto_removed", t._fullname)

            if isinstance(t, Comment):
                amqp.add_item("removed_comment", t._fullname)
            elif isinstance(t, Link):
                amqp.add_item("removed_link", t._fullname)

    def unspam(self, things, moderator_unbanned=True, unbanner=None,
               train_spam=True, insert=True):
        from r2.lib.db import queries

        things = tup(things)

        # We want to make unban-all moderately efficient, so when
        # mass-unbanning, we're going to skip the code below on links that
        # are already not banned.  However, when someone manually clicks
        # "approve" on an unbanned link, and there's just one, we want do
        # want to run the code below. That way, the little green checkmark
        # will have the right mouseover details, the reports will be
        # cleared, etc.

        if len(things) > 1:
            things = [x for x in things if x._spam]

        Report.accept(things, False)
        for t in things:
            ban_info = copy(getattr(t, 'ban_info', {}))
            ban_info['unbanned_at'] = datetime.now(g.tz)
            if unbanner:
                ban_info['unbanner'] = unbanner
            if ban_info.get('reset_used', None) == None:
                ban_info['reset_used'] = False
            else:
                ban_info['reset_used'] = True
            t.ban_info = ban_info
            t._spam = False
            if moderator_unbanned:
                t.verdict = 'mod-approved'
            else:
                t.verdict = 'admin-approved'
            t._commit()

            if isinstance(t, Comment):
                amqp.add_item("approved_comment", t._fullname)
            elif isinstance(t, Link):
                amqp.add_item("approved_link", t._fullname)

        self.author_spammer(things, False)
        self.set_last_sr_ban(things)
        queries.unban(things, insert)
    
    def report(self, thing):
        pass

    def author_spammer(self, things, spam):
        """incr/decr the 'spammer' field for the author of every
           passed thing"""
        by_aid = {}
        for thing in things:
            if (hasattr(thing, 'author_id')
                and not getattr(thing, 'ban_info', {}).get('auto',True)):
                # only decrement 'spammer' for items that were not
                # autobanned
                by_aid.setdefault(thing.author_id, []).append(thing)

        if by_aid:
            authors = Account._byID(by_aid.keys(), data=True, return_dict=True)

            for aid, author_things in by_aid.iteritems():
                author = authors[aid]
                author._incr('spammer', len(author_things) if spam else -len(author_things))

    def set_last_sr_ban(self, things):
        by_srid = {}
        for thing in things:
            if getattr(thing, 'sr_id', None) is not None:
                by_srid.setdefault(thing.sr_id, []).append(thing)

        if by_srid:
            srs = Subreddit._byID(by_srid.keys(), data=True, return_dict=True)
            for sr_id, sr_things in by_srid.iteritems():
                sr = srs[sr_id]

                sr.last_mod_action = datetime.now(g.tz)
                sr._commit()
                sr._incr('mod_actions', len(sr_things))

    def adjust_gold_expiration(self, account, days=0, months=0, years=0):
        now = datetime.now(g.display_tz)
        if months % 12 == 0:
            years += months / 12
        else:
            days += months * 31
        days += years * 366

        existing_expiration = getattr(account, "gold_expiration", None)
        if existing_expiration is None or existing_expiration < now:
            existing_expiration = now
        account.gold_expiration = existing_expiration + timedelta(days)
        
        if account.gold_expiration > now and not account.gold:
            self.engolden(account)
        elif account.gold_expiration <= now and account.gold:
            self.degolden(account)

        account._commit()     

    def engolden(self, account):
        now = datetime.now(g.display_tz)
        account.gold = True
        description = "Since " + now.strftime("%B %Y")
        
        trophy = Award.give_if_needed("reddit_gold", account,
                                     description=description,
                                     url="/gold/about")
        if trophy and trophy.description.endswith("Member Emeritus"):
            trophy.description = description
            trophy._commit()

        account._commit()
        account.friend_rels_cache(_update=True)

    def degolden(self, account):
        Award.take_away("reddit_gold", account)
        account.gold = False
        account._commit()

    def admin_list(self):
        return list(g.admins)

    def create_award_claim_code(self, unique_award_id, award_codename,
                                description, url):
        '''Create a one-time-use claim URL for a user to claim a trophy.

        `unique_award_id` - A string that uniquely identifies the kind of
                            Trophy the user would be claiming.
                            See: token.py:AwardClaimToken.uid
        `award_codename` - The codename of the Award the user will claim
        `description` - The description the Trophy will receive
        `url` - The URL the Trophy will receive

        '''
        award = Award._by_codename(award_codename)
        token = AwardClaimToken._new(unique_award_id, award, description, url)
        return token.confirm_url()

admintools = AdminTools()

def cancel_subscription(subscr_id):
    q = Account._query(Account.c.gold_subscr_id == subscr_id, data=True)
    l = list(q)
    if len(l) != 1:
        g.log.warning("Found %d matches for canceled subscription %s"
                      % (len(l), subscr_id))
    for account in l:
        account.gold_subscr_id = None
        account._commit()
        g.log.info("%s canceled their recurring subscription %s" %
                   (account.name, subscr_id))

def all_gold_users():
    q = Account._query(Account.c.gold == True, Account.c._spam == (True, False),
                       data=True, sort="_id")
    return fetch_things2(q)

def accountid_from_subscription(subscr_id):
    if subscr_id is None:
        return None

    q = Account._query(Account.c.gold_subscr_id == subscr_id,
                       Account.c._spam == (True, False),
                       Account.c._deleted == (True, False), data=False)
    l = list(q)
    if l:
        return l[0]._id
    else:
        return None

def update_gold_users():
    now = datetime.now(g.display_tz)
    warning_days = 3
    renew_msg = _("[Click here for details on how to set up an "
                  "automatically-renewing subscription or to renew.]"
                  "(/gold) If you have any thoughts, complaints, "
                  "rants, suggestions about reddit gold, please write "
                  "to us at %(gold_email)s. Your feedback would be "
                  "much appreciated.\n\nThank you for your past "
                  "patronage.") % {'gold_email': g.goldsupport_email}

    for account in all_gold_users():
        days_left = (account.gold_expiration - now).days
        if days_left < 0:
            if account.pref_creddit_autorenew:
                with creddits_lock(account):
                    if account.gold_creddits > 0:
                        admintools.adjust_gold_expiration(account, days=31)
                        account.gold_creddits -= 1
                        account._commit()
                        continue

            admintools.degolden(account)

            subject = _("Your reddit gold subscription has expired.")
            message = _("Your subscription to reddit gold has expired.")
            message += "\n\n" + renew_msg
            message = append_random_bottlecap_phrase(message)

            send_system_message(account, subject, message,
                                distinguished='gold-auto')
        elif days_left <= warning_days and not account.gold_will_autorenew:
            hc_key = "gold_expiration_notice-" + account.name
            already_warned = g.hardcache.get(hc_key)
            if not already_warned:
                g.hardcache.set(hc_key, True, 86400 * (warning_days + 1))
                
                subject = _("Your reddit gold subscription is about to "
                            "expire!")
                message = _("Your subscription to reddit gold will be "
                            "expiring soon.")
                message += "\n\n" + renew_msg
                message = append_random_bottlecap_phrase(message)

                send_system_message(account, subject, message,
                                    distinguished='gold-auto')


def is_banned_domain(dom):
    return None

def is_shamed_domain(dom):
    return False, None, None

def bans_for_domain_parts(dom):
    return []


def apply_updates(user, timer):
    pass


def ip_span(ip):
    ip = websafe(ip)
    return '<!-- %s -->' % ip


def wiki_template(template_slug, sr=None):
    """Pull content from a subreddit's wiki page for internal use."""
    if not sr:
        try:
            sr = Subreddit._by_name(g.default_sr)
        except NotFound:
            return None

    try:
        wiki = WikiPage.get(sr, "templates/%s" % template_slug)
    except tdb_cassandra.NotFound:
        return None

    return wiki._get("content")


@admintools_hooks.on("account.registered")
def send_welcome_message(user):
    welcome_title = wiki_template("welcome_title")
    welcome_message = wiki_template("welcome_message")

    if not welcome_title or not welcome_message:
        g.log.warning("Unable to send welcome message: invalid wiki templates.")
        return

    welcome_title = welcome_title.format(username=user.name)
    welcome_message = welcome_message.format(username=user.name)

    return send_system_message(user, welcome_title, welcome_message)


def send_system_message(user, subject, body, system_user=None,
                        distinguished='admin', repliable=False,
                        add_to_sent=True, author=None, signed=False):
    from r2.lib.db import queries

    if system_user is None:
        system_user = Account.system_user()
    if not system_user:
        g.log.warning("Can't send system message "
                      "- invalid system_user or g.system_user setting")
        return
    if not author:
        author = system_user

    item, inbox_rel = Message._new(author, user, subject, body,
                                   ip='0.0.0.0')
    item.distinguished = distinguished
    item.repliable = repliable
    item.display_author = system_user._id
    item.signed = signed
    item._commit()

    try:
        queries.new_message(item, inbox_rel, add_to_sent=add_to_sent)
    except MemcachedError:
        raise MessageError('reddit_inbox')


if config['r2.import_private']:
    from r2admin.models.admintools import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
from datetime import datetime

from pycassa.system_manager import UTF8_TYPE, TIME_UUID_TYPE
from pycassa.util import convert_uuid_to_time
from pylons import app_globals as g

from r2.lib.db import tdb_cassandra


class AdminNotesBySystem(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.ONE
    _compare_with = TIME_UUID_TYPE
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "default_validation_class": UTF8_TYPE,
    }

    @classmethod
    def add(cls, system_name, subject, note, author, when=None):
        if not when:
            when = datetime.now(g.tz)
        jsonpacked = json.dumps({"note": note, "author": author})
        updatedict = {when: jsonpacked}
        key = cls._rowkey(system_name, subject)
        cls._set_values(key, updatedict)

    @classmethod
    def in_display_order(cls, system_name, subject):
        key = cls._rowkey(system_name, subject)
        try:
            query = cls._cf.get(key, column_reversed=True)
        except tdb_cassandra.NotFoundException:
            return []
        result = []
        for uuid, json_blob in query.iteritems():
            when = datetime.fromtimestamp(convert_uuid_to_time(uuid), tz=g.tz)
            payload = json.loads(json_blob)
            payload['when'] = when
            result.append(payload)
        return result

    @classmethod
    def _rowkey(cls, system_name, subject):
        return "%s:%s" % (system_name, subject)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import timedelta
import itertools
from uuid import UUID

from pycassa.system_manager import TIME_UUID_TYPE
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup


class ModAction(tdb_cassandra.UuidThing):
    """
    Columns:
    sr_id - Subreddit id36
    mod_id - Account id36 of moderator
    action - specific name of action, must be in ModAction.actions
    target_fullname - optional fullname of the target of the action
    details - subcategory available for some actions, must show up in 
    description - optional user
    """

    _read_consistency_level = tdb_cassandra.CL.ONE
    _use_db = True
    _connection_pool = 'main'
    _ttl = timedelta(days=120)
    _str_props = ('sr_id36', 'mod_id36', 'target_fullname', 'action', 'details', 
                  'description')
    _defaults = {}

    actions = ('banuser', 'unbanuser', 'removelink', 'approvelink', 
               'removecomment', 'approvecomment', 'addmoderator',
               'invitemoderator', 'uninvitemoderator', 'acceptmoderatorinvite',
               'removemoderator', 'addcontributor', 'removecontributor',
               'editsettings', 'editflair', 'distinguish', 'marknsfw',
               'wikibanned', 'wikicontributor', 'wikiunbanned', 'wikipagelisted',
               'removewikicontributor', 'wikirevise', 'wikipermlevel',
               'ignorereports', 'unignorereports', 'setpermissions',
               'setsuggestedsort', 'sticky', 'unsticky', 'setcontestmode',
               'unsetcontestmode', 'lock', 'unlock', 'muteuser', 'unmuteuser',
               'createrule', 'editrule', 'deleterule')

    _menu = {'banuser': _('ban user'),
             'unbanuser': _('unban user'),
             'removelink': _('remove post'),
             'approvelink': _('approve post'),
             'removecomment': _('remove comment'),
             'approvecomment': _('approve comment'),
             'addmoderator': _('add moderator'),
             'removemoderator': _('remove moderator'),
             'invitemoderator': _('invite moderator'),
             'uninvitemoderator': _('uninvite moderator'),
             'acceptmoderatorinvite': _('accept moderator invite'),
             'addcontributor': _('add contributor'),
             'removecontributor': _('remove contributor'),
             'editsettings': _('edit settings'),
             'editflair': _('edit flair'),
             'distinguish': _('distinguish'),
             'marknsfw': _('mark nsfw'),
             'wikibanned': _('ban from wiki'),
             'wikiunbanned': _('unban from wiki'),
             'wikicontributor': _('add wiki contributor'),
             'wikipagelisted': _('delist/relist wiki pages'),
             'removewikicontributor': _('remove wiki contributor'),
             'wikirevise': _('wiki revise page'),
             'wikipermlevel': _('wiki page permissions'),
             'ignorereports': _('ignore reports'),
             'unignorereports': _('unignore reports'),
             'setpermissions': _('permissions'),
             'setsuggestedsort': _('set suggested sort'),
             'sticky': _('sticky post'),
             'unsticky': _('unsticky post'),
             'setcontestmode': _('set contest mode'),
             'unsetcontestmode': _('unset contest mode'),
             'lock': _('lock post'),
             'unlock': _('unlock post'),
             'muteuser': _('mute user'),
             'unmuteuser': _('unmute user'),
             'createrule': _('create rule'),
             'editrule': _('edit rule'),
             'deleterule': _('delete rule'),
            }

    _text = {'banuser': _('banned'),
             'wikibanned': _('wiki banned'),
             'wikiunbanned': _('unbanned from wiki'),
             'wikicontributor': _('added wiki contributor'),
             'removewikicontributor': _('removed wiki contributor'),
             'unbanuser': _('unbanned'),
             'removelink': _('removed'),
             'approvelink': _('approved'),
             'removecomment': _('removed'),
             'approvecomment': _('approved'),
             'addmoderator': _('added moderator'),
             'removemoderator': _('removed moderator'),
             'invitemoderator': _('invited moderator'),
             'uninvitemoderator': _('uninvited moderator'),
             'acceptmoderatorinvite': _('accepted moderator invitation'),
             'addcontributor': _('added approved contributor'),
             'removecontributor': _('removed approved contributor'),
             'editsettings': _('edited settings'),
             'editflair': _('edited flair'),
             'wikirevise': _('edited wiki page'),
             'wikipermlevel': _('changed wiki page permission level'),
             'wikipagelisted': _('changed wiki page listing preference'),
             'distinguish': _('distinguished'),
             'marknsfw': _('marked nsfw'),
             'ignorereports': _('ignored reports'),
             'unignorereports': _('unignored reports'),
             'setpermissions': _('changed permissions on'),
             'setsuggestedsort': _('set suggested sort'),
             'sticky': _('stickied'),
             'unsticky': _('unstickied'),
             'setcontestmode': _('set contest mode on'),
             'unsetcontestmode': _('unset contest mode on'),
             'lock': _('locked'),
             'unlock': _('unlocked'),
             'muteuser': _('muted'),
             'unmuteuser': _('unmuted'),
             'createrule': _('created rule'),
             'editrule': _('edited rule'),
             'deleterule': _('deleted rule'),
            }

    _details_text = {# approve comment/link
                     'unspam': _('unspam'),
                     'confirm_ham': _('approved'),
                     # remove comment/link
                     'confirm_spam': _('confirmed spam'),
                     'remove': _('removed not spam'),
                     'spam': _('removed spam'),
                     # removemoderator
                     'remove_self': _('removed self'),
                     # editsettings
                     'title': _('title'),
                     'public_description': _('description'),
                     'description': _('sidebar'),
                     'lang': _('language'),
                     'type': _('type'),
                     'link_type': _('link type'),
                     'submit_link_label': _('submit link button label'),
                     'submit_text_label': _('submit text post button label'),
                     'comment_score_hide_mins': _('comment score hide period'),
                     'over_18': _('toggle viewers must be over 18'),
                     'allow_top': _('toggle allow in default/trending lists'),
                     'show_media': _('toggle show thumbnail images of content'),
                     'public_traffic': _('toggle public traffic stats page'),
                     'collapse_deleted_comments': _('toggle collapse deleted/removed comments'),
                     'exclude_banned_modqueue': _('toggle exclude banned users\' posts from modqueue'),
                     'domain': _('domain'),
                     'show_cname_sidebar': _('toggle show sidebar from cname'),
                     'css_on_cname': _('toggle custom CSS from cname'),
                     'header_title': _('header title'),
                     'stylesheet': _('stylesheet'),
                     'del_header': _('delete header image'),
                     'del_image': _('delete image'),
                     'del_icon': _('delete icon image'),
                     'del_banner': _('delete banner image'),
                     'upload_image_header': _('upload header image'),
                     'upload_image_icon': _('upload icon image'),
                     'upload_image_banner': _('upload banner image'),
                     'upload_image': _('upload image'),
                     # editflair
                     'flair_edit': _('add/edit flair'),
                     'flair_delete': _('delete flair'),
                     'flair_csv': _('edit by csv'),
                     'flair_enabled': _('toggle flair enabled'),
                     'flair_position': _('toggle user flair position'),
                     'link_flair_position': _('toggle link flair position'),
                     'flair_self_enabled': _('toggle user assigned flair enabled'),
                     'link_flair_self_enabled': _('toggle submitter assigned link flair enabled'),
                     'flair_template': _('add/edit flair templates'),
                     'flair_delete_template': _('delete flair template'),
                     'flair_clear_template': _('clear flair templates'),
                     # distinguish/nsfw
                     'remove': _('remove'),
                     'ignore_reports': _('ignore reports'),
                     # permissions
                     'permission_moderator': _('set permissions on moderator'),
                     'permission_moderator_invite': _('set permissions on moderator invitation')}

    # NOTE: Wrapped ModAction objects are not cachable because wrapped_cache_key
    # is not defined

    @classmethod
    def create(cls, sr, mod, action, details=None, target=None, description=None):
        from r2.models import DefaultSR

        if not action in cls.actions:
            raise ValueError("Invalid ModAction: %s" % action)

        # Front page should insert modactions into the base sr
        sr = sr._base if isinstance(sr, DefaultSR) else sr

        kw = dict(sr_id36=sr._id36, mod_id36=mod._id36, action=action)

        if target:
            kw['target_fullname'] = target._fullname
        if details:
            kw['details'] = details
        if description:
            kw['description'] = description

        ma = cls(**kw)
        ma._commit()

        g.events.mod_event(
            modaction=ma,
            subreddit=sr,
            mod=mod,
            target=target,
            request=request if c.user_is_loggedin else None,
            context=c if c.user_is_loggedin else None,
        )

        return ma

    def _on_create(self):
        """
        Update all Views.
        """

        views = (ModActionBySR, ModActionBySRMod, ModActionBySRAction, 
                 ModActionBySRActionMod)

        for v in views:
            v.add_object(self)

    @classmethod
    def get_actions(cls, srs, mod=None, action=None, after=None, reverse=False, count=1000):
        """
        Get a ColumnQuery that yields ModAction objects according to
        specified criteria.
        """
        if after and isinstance(after, basestring):
            after = cls._byID(UUID(after))
        elif after and isinstance(after, UUID):
            after = cls._byID(after)

        if not isinstance(after, cls):
            after = None

        srs = tup(srs)

        if not mod and not action:
            rowkeys = [sr._id36 for sr in srs]
            q = ModActionBySR.query(rowkeys, after=after, reverse=reverse, count=count)
        elif mod:
            mods = tup(mod)
            key = '%s_%s' if not action else '%%s_%%s_%s' % action
            rowkeys = itertools.product([sr._id36 for sr in srs],
                [mod._id36 for mod in mods])
            rowkeys = [key % (sr, mod) for sr, mod in rowkeys]
            view = ModActionBySRActionMod if action else ModActionBySRMod
            q = view.query(rowkeys, after=after, reverse=reverse, count=count)
        else:
            rowkeys = ['%s_%s' % (sr._id36, action) for sr in srs]
            q = ModActionBySRAction.query(rowkeys, after=after, reverse=reverse, count=count)

        return q

    @property
    def details_text(self):
        text = ""
        if getattr(self, "details", None):
            text += self._details_text.get(self.details, self.details)
        if getattr(self, "description", None):
            if text:
                text += ": "
            text += self.description
        return text

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.db.thing import Thing
        from r2.lib.menus import QueryButton
        from r2.lib.pages import WrappedUser
        from r2.models import (
            Account,
            Link,
            ModSR,
            MultiReddit,
            Subreddit,
        )

        target_names = {item.target_fullname for item in wrapped
                            if hasattr(item, "target_fullname")}
        targets = Thing._by_fullname(target_names, data=True)

        # get moderators
        moderators = Account._byID36({item.mod_id36 for item in wrapped},
                                     data=True)

        # get authors for targets that are Links or Comments
        target_author_names = {target.author_id for target in targets.values()
                                    if hasattr(target, "author_id")}
        target_authors = Account._byID(target_author_names, data=True)

        # get parent links for targets that are Comments
        parent_link_names = {target.link_id for target in targets.values()
                                    if hasattr(target, "link_id")}
        parent_links = Link._byID(parent_link_names, data=True)

        # get subreddits
        srs = Subreddit._byID36({item.sr_id36 for item in wrapped}, data=True)

        for item in wrapped:
            item.moderator = moderators[item.mod_id36]
            item.subreddit = srs[item.sr_id36]
            item.text = cls._text.get(item.action, '')
            item.target = None
            item.target_author = None

            if hasattr(item, "target_fullname") and item.target_fullname:
                item.target = targets[item.target_fullname]

                if hasattr(item.target, "author_id"):
                    author_name = item.target.author_id
                    item.target_author = target_authors[author_name]

                if hasattr(item.target, "link_id"):
                    parent_link_name = item.target.link_id
                    item.parent_link = parent_links[parent_link_name]

                if isinstance(item.target, Account):
                    item.target_author = item.target

        if c.render_style == "html":
            request_path = request.path

            # make wrapped users for targets that are accounts
            user_targets = filter(lambda target: isinstance(target, Account),
                                  targets.values())
            wrapped_user_targets = {user._fullname: WrappedUser(user)
                                    for user in user_targets}

            for item in wrapped:
                if isinstance(item.target, Account):
                    user_name = item.target._fullname
                    item.wrapped_user_target = wrapped_user_targets[user_name]

                css_class = 'modactions %s' % item.action
                action_button = QueryButton(
                    '', item.action, query_param='type', css_class=css_class)
                action_button.build(base_path=request_path)
                item.action_button = action_button

                mod_button = QueryButton(
                    item.moderator.name, item.moderator.name, query_param='mod')
                mod_button.build(base_path=request_path)
                item.mod_button = mod_button

                if isinstance(c.site, ModSR) or isinstance(c.site, MultiReddit):
                    rgb = item.subreddit.get_rgb()
                    item.bgcolor = 'rgb(%s,%s,%s)' % rgb
                    item.is_multi = True
                else:
                    item.bgcolor = "rgb(255,255,255)"
                    item.is_multi = False


class ModActionBySR(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return ma.sr_id36

class ModActionBySRMod(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return '%s_%s' % (ma.sr_id36, ma.mod_id36)

class ModActionBySRActionMod(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return '%s_%s_%s' % (ma.sr_id36, ma.mod_id36, ma.action)

class ModActionBySRAction(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return '%s_%s' % (ma.sr_id36, ma.action)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import uuid

from pylons import app_globals as g

from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import Relation
from r2.lib.db.userrel import UserRel
from r2.lib.utils import to36
from r2.models import Account, Subreddit


USER_FLAIR = 'USER_FLAIR'
LINK_FLAIR = 'LINK_FLAIR'


class Flair(Relation(Subreddit, Account)):
    _cache = g.thingcache

    @classmethod
    def _cache_prefix(cls):
        return "flair:"


Subreddit.__bases__ += (
    UserRel(
        name='flair',
        relation=Flair,
        disable_ids_fn=True,
        disable_reverse_ids_fn=True,
    ),
)


class FlairTemplate(tdb_cassandra.Thing):
    """A template for some flair."""
    _defaults = dict(text='',
                     css_class='',
                     text_editable=False,
                    )

    _bool_props = ('text_editable',)

    _use_db = True
    _connection_pool = 'main'

    @classmethod
    def _new(cls, text='', css_class='', text_editable=False):
        if text is None:
            text = ''
        if css_class is None:
            css_class = ''
        ft = cls(text=text, css_class=css_class, text_editable=text_editable)
        ft._commit()
        return ft

    def _commit(self, *a, **kw):
        # Make sure an _id is always assigned before committing.
        if not self._id:
            self._id = str(uuid.uuid1())
        return tdb_cassandra.Thing._commit(self, *a, **kw)

    def covers(self, other_template):
        """Returns true if other_template is a subset of this one.

        The value for other_template may be another FlairTemplate, or a tuple
        of (text, css_class). The latter case is treated like a FlairTemplate
        that doesn't permit editable text.

        For example, if self permits editable text, then this method will return
        True as long as just the css_classes match. On the other hand, if self
        doesn't permit editable text but other_template does, this method will
        return False.
        """
        if isinstance(other_template, FlairTemplate):
            text_editable = other_template.text_editable
            text, css_class = other_template.text, other_template.css_class
        else:
            text_editable = False
            text, css_class = other_template

        if self.css_class != css_class:
            return False
        return self.text_editable or (not text_editable and self.text == text)


class FlairTemplateBySubredditIndex(tdb_cassandra.Thing):
    """Lists of FlairTemplate IDs for a subreddit.

    The FlairTemplate references are stored as an arbitrary number of attrs.
    The lexicographical ordering of these attr names gives the ordering for
    flair templates within the subreddit.
    """

    MAX_FLAIR_TEMPLATES = 350

    _int_props = ('sr_id',)
    _use_db = True
    _connection_pool = 'main'

    _key_prefixes = {
        USER_FLAIR: 'ft_',
        LINK_FLAIR: 'link_ft_',
    }

    @classmethod
    def _new(cls, sr_id, flair_type=USER_FLAIR):
        idx = cls(_id=to36(sr_id), sr_id=sr_id)
        idx._commit()
        return idx

    @classmethod
    def by_sr(cls, sr_id, create=False):
        try:
            return cls._byID(to36(sr_id))
        except tdb_cassandra.NotFound:
            if create:
                return cls._new(sr_id)
            raise

    @classmethod
    def create_template(cls, sr_id, text='', css_class='', text_editable=False,
                        flair_type=USER_FLAIR):
        idx = cls.by_sr(sr_id, create=True)

        if len(idx._index_keys(flair_type)) >= cls.MAX_FLAIR_TEMPLATES:
            raise OverflowError

        ft = FlairTemplate._new(text=text, css_class=css_class,
                                text_editable=text_editable)
        idx.insert(ft._id, flair_type=flair_type)
        return ft

    @classmethod
    def get_template_ids(cls, sr_id, flair_type=USER_FLAIR):
        try:
            return list(cls.by_sr(sr_id).iter_template_ids(flair_type))
        except tdb_cassandra.NotFound:
            return []

    @classmethod
    def get_template(cls, sr_id, ft_id, flair_type=None):
        if flair_type:
            flair_types = [flair_type]
        else:
            flair_types = [USER_FLAIR, LINK_FLAIR]
        for flair_type in flair_types:
            if ft_id in cls.get_template_ids(sr_id, flair_type=flair_type):
                return FlairTemplate._byID(ft_id)
        return None

    @classmethod
    def clear(cls, sr_id, flair_type=USER_FLAIR):
        try:
            idx = cls.by_sr(sr_id)
        except tdb_cassandra.NotFound:
            # Everything went better than expected.
            return

        for k in idx._index_keys(flair_type):
            del idx[k]
            # TODO: delete the orphaned FlairTemplate row

        idx._commit()

    def _index_keys(self, flair_type):
        keys = set(self._dirties.iterkeys())
        keys |= frozenset(self._orig.iterkeys())
        keys -= self._deletes
        key_prefix = self._key_prefixes[flair_type]
        return [k for k in keys if k.startswith(key_prefix)]

    @classmethod
    def _make_index_key(cls, position, flair_type):
        return '%s%08d' % (cls._key_prefixes[flair_type], position)

    def iter_template_ids(self, flair_type):
        return (getattr(self, key)
                for key in sorted(self._index_keys(flair_type)))

    def insert(self, ft_id, position=None, flair_type=USER_FLAIR):
        """Insert template reference into index at position.

        A position value of None means to simply append.
        """
        ft_ids = list(self.iter_template_ids(flair_type))
        if position is None:
            position = len(ft_ids)
        if position < 0 or position > len(ft_ids):
            raise IndexError(position)
        ft_ids.insert(position, ft_id)

        # Rewrite ALL the things.
        for k in self._index_keys(flair_type):
            del self[k]
        for i, ft_id in enumerate(ft_ids):
            setattr(self, self._make_index_key(i, flair_type), ft_id)
        self._commit()

    def delete_by_id(self, ft_id, flair_type=None):
        if flair_type:
            flair_types = [flair_type]
        else:
            flair_types = [USER_FLAIR, LINK_FLAIR]
        for flair_type in flair_types:
            if self._delete_by_id(ft_id, flair_type):
                return True
        g.log.debug("couldn't find %s to delete", ft_id)
        return False

    def _delete_by_id(self, ft_id, flair_type):
        for key in self._index_keys(flair_type):
            ft = getattr(self, key)
            if ft == ft_id:
                # TODO: delete the orphaned FlairTemplate row
                g.log.debug('deleting ft %s (%s)', ft, key)
                del self[key]
                self._commit()
                return True
        return False
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime

from pycassa.util import convert_uuid_to_time
from pylons import app_globals as g

from r2.models.trylater import TryLaterBySubject


class UserTempBan(object):
    @classmethod
    def schedule(cls, victim, duration):
        TryLaterBySubject.schedule(
            cls.cancel_rowkey(),
            cls.cancel_colkey(victim.name),
            victim._fullname,
            duration,
        )

    @classmethod
    def unschedule(cls, victim):
        TryLaterBySubject.unschedule(
            cls.cancel_rowkey(),
            cls.cancel_colkey(victim.name),
            cls.schedule_rowkey(),
        )

    @classmethod
    def search(cls, subjects):
        results = TryLaterBySubject.search(cls.cancel_rowkey(), subjects)

        def convert_uuid_to_datetime(uu):
            return datetime.fromtimestamp(convert_uuid_to_time(uu), g.tz)
        return {
            name: convert_uuid_to_datetime(uu)
                for name, uu in results.iteritems()
        }

    @classmethod
    def cancel_colkey(cls, name):
        return name


class TempTimeout(UserTempBan):
    @classmethod
    def cancel_rowkey(cls):
        return "untimeout"

    @classmethod
    def schedule_rowkey(cls):
        return "untimeout"
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""A delayed execution system.

The ``trylater`` module provides tools for performing an action at a set time
in the future.  To use it, you must do two things.

First, make a scheduling call::

    from datetime import timedelta

    from r2.models.trylater import TryLater

    def make_breakfast(spam):
        breakfast = cook(spam)
        later = timedelta(minutes=45)
        # The storage layer only likes strings.
        data = json.dumps(breakfast)
        TryLater.schedule('wash_dishes', data, later)

Then, write the delayed code and decorate it with a hook, using the same
identifier as you used when you scheduled it::

    from r2.lib import hooks
    trylater_hooks = hooks.HookRegistrar()

    @trylater_hooks.on('trylater.wash_dishes')
    def on_dish_washing(data):
        # data is an ordered dictionary of timeuuid -> data pairs.
        for datum in data.values():
            meal = json.loads(datum)
            for dish in meal.dishes:
                dish.wash()

Note: once you've scheduled a ``TryLater`` task, there's no stopping it!  If
you might need to cancel your jobs later, use ``TryLaterBySubject``, which uses
almost the exact same semantics, but has a useful ``unschedule`` method.
"""

from collections import OrderedDict
from datetime import datetime, timedelta
import uuid

from pycassa.system_manager import TIME_UUID_TYPE, UTF8_TYPE
from pycassa.util import convert_uuid_to_time
from pylons import app_globals as g

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup


class TryLater(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _compare_with = TIME_UUID_TYPE

    @classmethod
    def process_ready_items(cls, rowkey, ready_fn):
        cutoff = datetime.now(g.tz)

        columns = cls._cf.xget(rowkey, include_timestamp=True)
        ready_items = OrderedDict()
        ready_timestamps = []
        unripe_timestamps = []

        for ready_time_uuid, (data, timestamp) in columns:
            ready_time = convert_uuid_to_time(ready_time_uuid)
            ready_datetime = datetime.fromtimestamp(ready_time, tz=g.tz)
            if ready_datetime <= cutoff:
                ready_items[ready_time_uuid] = data
                ready_timestamps.append(timestamp)
            else:
                unripe_timestamps.append(timestamp)

        g.stats.simple_event(
            "trylater.{system}.ready".format(system=rowkey),
            delta=len(ready_items),
        )
        g.stats.simple_event(
            "trylater.{system}.pending".format(system=rowkey),
            delta=len(unripe_timestamps),
        )

        if not ready_items:
            return

        try:
            ready_fn(ready_items)
        except:
            g.stats.simple_event(
                "trylater.{system}.failed".format(system=rowkey),
            )

        cls.cleanup(rowkey, ready_items, ready_timestamps, unripe_timestamps)

    @classmethod
    def cleanup(cls, rowkey, ready_items, ready_timestamps, unripe_timestamps):
        """Remove ALL ready items from the C* row"""
        if (not unripe_timestamps or
                min(unripe_timestamps) > max(ready_timestamps)):
            # do a row/timestamp delete to avoid generating column
            # tombstones
            cls._cf.remove(rowkey, timestamp=max(ready_timestamps))
            g.stats.simple_event(
                "trylater.{system}.row_delete".format(system=rowkey),
                delta=len(ready_items),
            )
        else:
            # the columns weren't created with a fixed delay and there are some
            # unripe items with older (lower) timestamps than the items we want
            # to delete. fallback to deleting specific columns.
            cls._cf.remove(rowkey, ready_items.keys())
            g.stats.simple_event(
                "trylater.{system}.column_delete".format(system=rowkey),
                delta=len(ready_items),
            )

    @classmethod
    def run(cls):
        """Run all ready items through their processing hook."""
        from r2.lib import amqp
        from r2.lib.hooks import all_hooks

        for hook_name, hook in all_hooks().items():
            if hook_name.startswith("trylater."):
                rowkey = hook_name[len("trylater."):]

                def ready_fn(ready_items):
                    return hook.call(data=ready_items)

                g.log.info("Trying %s", rowkey)
                cls.process_ready_items(rowkey, ready_fn)

        amqp.worker.join()
        g.stats.flush()

    @classmethod
    def search(cls, rowkey, when):
        if isinstance(when, uuid.UUID):
            when = convert_uuid_to_time(when)
        try:
            return cls._cf.get(rowkey, column_start=when, column_finish=when)
        except tdb_cassandra.NotFoundException:
            return {}

    @classmethod
    def schedule(cls, system, data, delay=None):
        """Schedule code for later execution.

        system:  an string identifying the hook to be executed
        data:    passed to the hook as an argument
        delay:   (optional) a datetime.timedelta indicating the desired
                 execution time
        """
        if delay is None:
            delay = timedelta(minutes=60)
        key = datetime.now(g.tz) + delay
        scheduled = {key: data}
        cls._set_values(system, scheduled)
        return scheduled

    @classmethod
    def unschedule(cls, rowkey, column_keys):
        column_keys = tup(column_keys)
        return cls._cf.remove(rowkey, column_keys)


class TryLaterBySubject(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _compare_with = UTF8_TYPE
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "default_validation_class": TIME_UUID_TYPE,
    }
    _value_type = 'date'

    @classmethod
    def schedule(cls, system, subject, data, delay, trylater_rowkey=None):
        if trylater_rowkey is None:
            trylater_rowkey = system
        scheduled = TryLater.schedule(trylater_rowkey, data, delay)
        when = scheduled.keys()[0]

        # TTL 10 minutes after the TryLater runs just in case TryLater
        # is running late.
        ttl = (delay + timedelta(minutes=10)).total_seconds()
        coldict = {subject: when}
        cls._set_values(system, coldict, ttl=ttl)
        return scheduled

    @classmethod
    def search(cls, rowkey, subjects=None):
        try:
            if subjects:
                subjects = tup(subjects)
                return cls._cf.get(rowkey, subjects)
            else:
                return cls._cf.get(rowkey)
        except tdb_cassandra.NotFoundException:
            return {}

    @classmethod
    def unschedule(cls, rowkey, colkey, schedule_rowkey):
        colkey = tup(colkey)
        victims = cls.search(rowkey, colkey)
        for uu in victims.itervalues():
            keys = TryLater.search(schedule_rowkey, uu).keys()
            TryLater.unschedule(schedule_rowkey, keys)
        cls._cf.remove(rowkey, colkey)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json

from pycassa import NotFoundException
from pycassa.system_manager import UTF8_TYPE

from r2.lib.db.tdb_cassandra import ThingMeta

NoDefault = object()

class KeyValueStore(object):
    __metaclass__ = ThingMeta

    _use_db = False
    _cf_name = None
    _type_prefix = None
    _compare_with = UTF8_TYPE

    _extra_schema_creation_args = dict(
        key_validation_class=UTF8_TYPE,
        default_validation_class=UTF8_TYPE,
    )

    @classmethod
    def get(cls, key, default=NoDefault):
        try:
            return json.loads(cls._cf.get(key)["data"])
        except NotFoundException:
            if default is not NoDefault:
                return default
            else:
                raise

    @classmethod
    def set(cls, key, data):
        cls._cf.insert(key, {"data": json.dumps(data)})


class NamedGlobals(KeyValueStore):
    _use_db = True

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from __future__ import with_statement

import base64
import collections
import datetime
import itertools
import json
import re
import struct

from pycassa import types
from pycassa.util import convert_uuid_to_time
from pycassa.system_manager import ASCII_TYPE, DATE_TYPE, FLOAT_TYPE, UTF8_TYPE
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, N_
from thrift.protocol.TProtocol import TProtocolException
from thrift.Thrift import TApplicationException
from thrift.transport.TTransport import TTransportException

from r2.config import feature
from r2.lib.db.thing import Thing, Relation, NotFound
from account import (
    Account,
    FakeAccount,
    QuarantinedSubredditOptInsByAccount,
)
from printable import Printable
from r2.lib.db.userrel import UserRel, MigratingUserRel
from r2.lib.db.operators import lower, or_, and_, not_, desc
from r2.lib.errors import RedditError
from r2.lib.geoip import get_request_location
from r2.lib.memoize import memoize
from r2.lib.permissions import ModeratorPermissionSet
from r2.lib.utils import (
    UrlParser,
    in_chunks,
    summarize_markdown,
    timeago,
    to36,
    tup,
    unicode_title_to_ascii,
)
from r2.lib.cache import MemcachedError
from r2.lib.sgm import sgm
from r2.lib.strings import strings, Score
from r2.lib.filters import _force_unicode
from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_sql import CreationError
from r2.models.wiki import WikiPage, ImagesByWikiPage
from r2.models.trylater import TryLater, TryLaterBySubject
from r2.lib.merge import ConflictException
from r2.lib.cache import CL_ONE
from r2.lib import hooks
from r2.models.query_cache import MergedCachedQuery
from r2.models.rules import SubredditRules
import pycassa

from r2.models.keyvalue import NamedGlobals
from r2.models.wiki import WikiPage
import os.path
import random

trylater_hooks = hooks.HookRegistrar()


def get_links_sr_ids(sr_ids, sort, time):
    from r2.lib.db import queries

    if not sr_ids:
        return []

    results = [queries._get_links(sr_id, sort, time) for sr_id in sr_ids]
    return queries.merge_results(*results)


def get_user_location():
    """Determine country of origin for the current user

    This is provided via a call to geoip.get_request_location unless the
    user has opted into the global default location.
    """
    # The default location is just the unset one
    if c.user and c.user.pref_use_global_defaults:
        return ""

    # this call has the side effect of memoizing on c.location
    return get_request_location(request, c)


subreddit_rx = re.compile(r"\A[A-Za-z0-9][A-Za-z0-9_]{2,20}\Z")
language_subreddit_rx = re.compile(r"\A[a-z]{2}\Z")
time_subreddit_rx = re.compile(r"\At:[A-Za-z0-9][A-Za-z0-9_]{2,22}\Z")


class BaseSite(object):
    _defaults = dict(
        static_path=g.static_path,
        header=None,
        header_title='',
        login_required=False,
        sticky_fullnames=None,
    )

    def __getattr__(self, name):
        if name in self._defaults:
            return self._defaults[name]
        raise AttributeError

    @property
    def path(self):
        return "/r/%s/" % self.name

    @property
    def user_path(self):
        return self.path

    @property
    def analytics_name(self):
        return self.name

    @property
    def allows_referrers(self):
        return True

    def is_moderator_with_perms(self, user, *perms):
        rel = self.is_moderator(user)
        if rel:
            return all(rel.has_permission(perm) for perm in perms)

    def is_limited_moderator(self, user):
        rel = self.is_moderator(user)
        return bool(rel and not rel.is_superuser())

    def is_unlimited_moderator(self, user):
        rel = self.is_moderator(user)
        return bool(rel and rel.is_superuser())

    def get_links(self, sort, time):
        from r2.lib.db import queries
        return queries.get_links(self, sort, time)

    def get_spam(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_spam(self, user=c.user, include_links=include_links,
                                include_comments=include_comments)

    def get_reported(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_reported(self, user=c.user,
                                    include_links=include_links,
                                    include_comments=include_comments)

    def get_modqueue(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_modqueue(self, user=c.user,
                                    include_links=include_links,
                                    include_comments=include_comments)

    def get_unmoderated(self):
        from r2.lib.db import queries
        return queries.get_unmoderated(self, user=c.user)

    def get_edited(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_edited(self, user=c.user,
                                  include_links=include_links,
                                  include_comments=include_comments)

    def get_all_comments(self):
        from r2.lib.db import queries
        return queries.get_sr_comments(self)

    def get_gilded(self):
        from r2.lib.db import queries
        return queries.get_gilded(self._id)

    @classmethod
    def get_modactions(cls, srs, mod=None, action=None):
        # Get a query that will yield ModAction objects with mod and action
        from r2.models import ModAction
        return ModAction.get_actions(srs, mod=mod, action=action)

    def get_live_promos(self):
        raise NotImplementedError


class SubredditExists(Exception): pass


class Subreddit(Thing, Printable, BaseSite):
    _cache = g.thingcache

    # Note: As of 2010/03/18, nothing actually overrides the static_path
    # attribute, even on a cname. So c.site.static_path should always be
    # the same as g.static_path.
    _defaults = dict(BaseSite._defaults,
        stylesheet_url="",
        stylesheet_url_http="",
        stylesheet_url_https="",
        header_size=None,
        allow_top=False, # overridden in "_new"
        reported=0,
        valid_votes=0,
        show_media=False,
        show_media_preview=True,
        domain=None,
        suggested_comment_sort=None,
        wikimode="disabled",
        wiki_edit_karma=100,
        wiki_edit_age=0,
        over_18=False,
        exclude_banned_modqueue=False,
        mod_actions=0,
        # do we allow self-posts, links only, or any?
        link_type='any', # one of ('link', 'self', 'any')
        sticky_fullnames=None,
        submit_link_label='',
        submit_text_label='',
        comment_score_hide_mins=0,
        flair_enabled=True,
        flair_position='right', # one of ('left', 'right')
        link_flair_position='', # one of ('', 'left', 'right')
        flair_self_assign_enabled=False,
        link_flair_self_assign_enabled=False,
        use_quotas=True,
        description="",
        public_description="",
        submit_text="",
        public_traffic=False,
        spam_links='high',
        spam_selfposts='high',
        spam_comments='low',
        archive_age=g.ARCHIVE_AGE,
        gilding_server_seconds=0,
        contest_mode_upvotes_only=False,
        collapse_deleted_comments=False,
        icon_img='',
        icon_size=None,
        banner_img='',
        banner_size=None,
        key_color='',
        hide_ads=False,
        ban_count=0,
        quarantine=False,
    )

    # special attributes that shouldn't set Thing data attributes because they
    # have special setters that set other data attributes
    _derived_attrs = (
        'related_subreddits',
    )

    _essentials = ('type', 'name', 'lang')
    _data_int_props = Thing._data_int_props + ('mod_actions', 'reported',
                                               'wiki_edit_karma', 'wiki_edit_age',
                                               'gilding_server_seconds',
                                               'ban_count')

    sr_limit = 50
    gold_limit = 100
    DEFAULT_LIMIT = object()

    ICON_EXACT_SIZE = (256, 256)
    BANNER_MIN_SIZE = (640, 192)
    BANNER_MAX_SIZE = (1280, 384)
    BANNER_ASPECT_RATIO = 10.0 / 3

    valid_types = {
        'archived',
        'employees_only',
        'gold_only',
        'gold_restricted',
        'private',
        'public',
        'restricted',
    }

    # this holds the subreddit types where content is not accessible
    # unless you are a contributor or mod
    private_types = {
        'employees_only',
        'gold_only',
        'private',
    }

    KEY_COLORS = collections.OrderedDict([
        ('#ea0027', N_('red')),
        ('#ff4500', N_('orangered')),
        ('#ff8717', N_('orange')),
        ('#ffb000', N_('mango')),
        ('#94e044', N_('lime')),
        ('#46d160', N_('green')),
        ('#0dd3bb', N_('mint')),
        ('#25b79f', N_('teal')),
        ('#24a0ed', N_('blue')),
        ('#0079d3', N_('alien blue')),
        ('#ff66ac', N_('pink')),
        ('#7e53c1', N_('purple')),
        ('#ddbd37', N_('gold')),
        ('#a06a42', N_('brown')),
        ('#efefed', N_('pale grey')),
        ('#a5a4a4', N_('grey')),
        ('#545452', N_('dark grey')),
        ('#222222', N_('semi black')),
    ])
    ACCENT_COLORS = (
        '#f44336', # red
        '#9c27b0', # purple
        '#3f51b5', # indigo
        '#03a9f4', # light blue
        '#009688', # teal
        '#8bc34a', # light green
        '#ffeb3b', # yellow
        '#ff9800', # orange
        '#795548', # brown
        '#607d8b', # blue grey
        '#e91e63', # pink
        '#673ab7', # deep purple
        '#2196f3', # blue
        '#00bcd4', # cyan
        '#4caf50', # green
        '#cddc39', # lime
        '#ffc107', # amber
        '#ff5722', # deep orange
        '#9e9e9e', # grey
    )

    MAX_STICKIES = 2

    @classmethod
    def _cache_prefix(cls):
        return "sr:"

    def __setattr__(self, attr, val, make_dirty=True):
        if attr in self._derived_attrs:
            object.__setattr__(self, attr, val)
        else:
            Thing.__setattr__(self, attr, val, make_dirty=make_dirty)

    # note: for purposely unrenderable reddits (like promos) set author_id = -1
    @classmethod
    def _new(cls, name, title, author_id, ip, lang = g.lang, type = 'public',
             over_18 = False, **kw):
        if not cls.is_valid_name(name):
            raise ValueError("bad subreddit name")
        with g.make_lock("create_sr", 'create_sr_' + name.lower()):
            try:
                sr = Subreddit._by_name(name)
                raise SubredditExists
            except NotFound:
                if "allow_top" not in kw:
                    kw['allow_top'] = True
                sr = Subreddit(name = name,
                               title = title,
                               lang = lang,
                               type = type,
                               over_18 = over_18,
                               author_id = author_id,
                               ip = ip,
                               **kw)
                sr._commit()

                #clear cache
                Subreddit._by_name(name, _update = True)
                return sr

    @classmethod
    def is_valid_name(cls, name, allow_language_srs=False, allow_time_srs=False,
                      allow_reddit_dot_com=False):
        if not name:
            return False

        if allow_reddit_dot_com and name.lower() == "reddit.com":
            return True

        valid = bool(subreddit_rx.match(name))

        if not valid and allow_language_srs:
            valid = bool(language_subreddit_rx.match(name))

        if not valid and allow_time_srs:
            valid = bool(time_subreddit_rx.match(name))

        return valid

    _specials = {}

    SRNAME_NOTFOUND = "n"
    SRNAME_TTL = int(datetime.timedelta(hours=12).total_seconds())

    @classmethod
    def _by_name(cls, names, stale=False, _update = False):
        '''
        Usages:
        1. Subreddit._by_name('funny') # single sr name
        Searches for a single subreddit. Returns a single Subreddit object or
        raises NotFound if the subreddit doesn't exist.
        2. Subreddit._by_name(['aww','iama']) # list of sr names
        Searches for a list of subreddits. Returns a dict mapping srnames to
        Subreddit objects. Items that were not found are ommitted from the dict.
        If no items are found, an empty dict is returned.
        '''
        names, single = tup(names, True)

        to_fetch = {}
        ret = {}

        for name in names:
            try:
                ascii_only = str(name.decode("ascii", errors="ignore"))
            except UnicodeEncodeError:
                continue

            lname = ascii_only.lower()

            if lname in cls._specials:
                ret[name] = cls._specials[lname]
            else:
                valid_name = cls.is_valid_name(lname, allow_language_srs=True,
                                               allow_time_srs=True,
                                               allow_reddit_dot_com=True)
                if valid_name:
                    to_fetch[lname] = name
                else:
                    g.log.debug("Subreddit._by_name() ignoring invalid srname: %s", lname)

        if to_fetch:
            if not _update:
                srids_by_name = g.gencache.get_multi(
                    to_fetch.keys(), prefix='srid:', stale=True)
            else:
                srids_by_name = {}

            missing_srnames = set(to_fetch.keys()) - set(srids_by_name.keys())
            if missing_srnames:
                for srnames in in_chunks(missing_srnames, size=10):
                    q = cls._query(
                        lower(cls.c.name) == srnames,
                        cls.c._spam == (True, False),
                        # subreddits can't actually be deleted, but the combo
                        # of allowing for deletion and turning on optimize_rules
                        # gets rid of an unnecessary join on the thing table
                        cls.c._deleted == (True, False),
                        limit=len(srnames),
                        optimize_rules=True,
                        data=True,
                    )
                    with g.stats.get_timer('subreddit_by_name'):
                        fetched = {sr.name.lower(): sr._id for sr in q}
                    srids_by_name.update(fetched)

                    still_missing = set(srnames) - set(fetched)
                    fetched.update((name, cls.SRNAME_NOTFOUND) for name in still_missing)
                    try:
                        g.gencache.set_multi(
                            keys=fetched,
                            prefix='srid:',
                            time=cls.SRNAME_TTL,
                        )
                    except MemcachedError:
                        pass

            srs = {}
            srids = [v for v in srids_by_name.itervalues() if v != cls.SRNAME_NOTFOUND]
            if srids:
                srs = cls._byID(srids, data=True, return_dict=False, stale=stale)

            for sr in srs:
                ret[to_fetch[sr.name.lower()]] = sr

        if ret and single:
            return ret.values()[0]
        elif not ret and single:
            raise NotFound, 'Subreddit %s' % name
        else:
            return ret

    @classmethod
    @memoize('subreddit._by_domain')
    def _by_domain_cache(cls, name):
        q = cls._query(cls.c.domain == name,
                       limit = 1)
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_domain(cls, domain, _update = False):
        sr_id = cls._by_domain_cache(_force_unicode(domain).lower(),
                                     _update = _update)
        if sr_id:
            return cls._byID(sr_id, True)
        else:
            return None

    @property
    def allowed_types(self):
        if self.link_type == "any":
            return set(("link", "self"))
        return set((self.link_type,))

    @property
    def allows_referrers(self):
        return self.type in {'public', 'restricted',
                             'gold_restricted', 'archived'}

    @property
    def author_slow(self):
        if self.author_id:
            return Account._byID(self.author_id, data=True)
        else:
            return None

    def add_moderator(self, user, **kwargs):
        if not user.modmsgtime:
            user.modmsgtime = False
            user._commit()

        hook = hooks.get_hook("subreddit.add_moderator")
        hook.call(subreddit=self, user=user)

        return super(Subreddit, self).add_moderator(user, **kwargs)

    def remove_moderator(self, user, **kwargs):
        hook = hooks.get_hook("subreddit.remove_moderator")
        hook.call(subreddit=self, user=user)

        ret = super(Subreddit, self).remove_moderator(user, **kwargs)

        is_mod_somewhere = bool(Subreddit.reverse_moderator_ids(user))
        if not is_mod_somewhere:
            user.modmsgtime = None
            user._commit()

        return ret

    @property
    def moderators(self):
        return self.moderator_ids()

    def moderators_with_perms(self):
        return collections.OrderedDict(
            (r._thing2_id, r.get_permissions())
            for r in self.each_moderator())

    def moderator_invites_with_perms(self):
        return collections.OrderedDict(
            (r._thing2_id, r.get_permissions())
            for r in self.each_moderator_invite())

    def fetch_stylesheet_source(self):
        try:
            return WikiPage.get(self, 'config/stylesheet')._get('content','')
        except tdb_cassandra.NotFound:
            return ""

    @property
    def prev_stylesheet(self):
        try:
            return WikiPage.get(self, 'config/stylesheet')._get('revision','')
        except tdb_cassandra.NotFound:
            return ''

    @property
    def wikibanned(self):
        return self.wikibanned_ids()

    @property
    def wikicontributor(self):
        return self.wikicontributor_ids()

    @property
    def _should_wiki(self):
        return True

    @property
    def subscribers(self):
        return self.subscriber_ids()

    @property
    def wiki_use_subreddit_karma(self):
        return True

    @property
    def hide_subscribers(self):
        return self.name.lower() in g.hide_subscribers_srs

    @property
    def hide_contributors(self):
        return self.type in {'employees_only', 'gold_only'}

    @property
    def hide_num_users_info(self):
        return self.quarantine

    @property
    def _related_multipath(self):
        return '/r/%s/m/related' % self.name.lower()

    @property
    def related_subreddits(self):
        try:
            multi = LabeledMulti._byID(self._related_multipath)
        except tdb_cassandra.NotFound:
            multi = None
        return  [sr.name for sr in multi.srs] if multi else []

    @property
    def allow_ads(self):
        return not (self.hide_ads or self.quarantine)

    @property
    def discoverable(self):
        return self.allow_top and not self.quarantine

    @property
    def community_rules(self):
        return SubredditRules.get_rules(self)

    @related_subreddits.setter
    def related_subreddits(self, related_subreddits):
        try:
            multi = LabeledMulti._byID(self._related_multipath)
        except tdb_cassandra.NotFound:
            if not related_subreddits:
                return
            multi = LabeledMulti.create(self._related_multipath, self)

        if related_subreddits:
            srs = Subreddit._by_name(related_subreddits)
            try:
                sr_props = {srs[sr_name]: {} for sr_name in related_subreddits}
            except KeyError as e:
                raise NotFound, 'Subreddit %s' % e.args[0]

            multi.clear_srs()
            multi.add_srs(sr_props)
            multi._commit()
        else:
            multi.delete()

    activity_contexts = (
        "logged_in",
    )
    SubredditActivity = collections.namedtuple(
        "SubredditActivity", activity_contexts)

    def record_visitor_activity(self, context, visitor_id):
        """Record a visit to this subreddit in the activity service.

        This is used to show "here now" numbers. Multiple contexts allow us
        to bucket different kinds of visitors (logged-in vs. logged-out etc.)

        :param str context: The category of visitor. Must be one of
            Subreddit.activity_contexts.
        :param str visitor_id: A unique identifier for this visitor within the
            given context.

        """
        assert context in self.activity_contexts

        # we don't actually support other contexts yet
        assert self.activity_contexts == ("logged_in",)

        if not c.activity_service:
            return

        try:
            c.activity_service.record_activity(self._fullname, visitor_id)
        except (TApplicationException, TProtocolException, TTransportException):
            pass

    def count_activity(self):
        """Count activity in this subreddit in all known contexts.

        :returns: a named tuple of activity information for each context.

        """
        # we don't actually support other contexts yet
        assert self.activity_contexts == ("logged_in",)

        if not c.activity_service:
            return None

        try:
            # TODO: support batch lookup of multiple contexts (requires changes
            # to activity service)
            with c.activity_service.retrying(attempts=4, budget=0.1) as svc:
                activity = svc.count_activity(self._fullname)
            return self.SubredditActivity(activity)
        except (TApplicationException, TProtocolException, TTransportException):
            return None

    def spammy(self):
        return self._spam

    def is_contributor(self, user):
        if self.type == 'employees_only':
            return user.employee
        else:
            return super(Subreddit, self).is_contributor(user)

    def can_comment(self, user):
        if c.user_is_admin:
            return True

        override = hooks.get_hook("subreddit.can_comment").call_until_return(
                                                            sr=self, user=user)

        if override is not None:
            return override
        elif self.is_banned(user):
            return False
        elif self.type == 'gold_restricted' and user.gold:
            return True
        elif self.type in ('public','restricted'):
            return True
        elif self.is_moderator(user) or self.is_contributor(user):
            #private requires contributorship
            return True
        elif self.type == 'gold_only':
            return user.gold or user.gold_charter
        else:
            return False

    def wiki_can_submit(self, user):
        return self.can_submit(user)

    def can_submit(self, user, promotion=False):
        if c.user_is_admin:
            return True
        elif self.is_banned(user) and not promotion:
            return False
        elif self.spammy():
            return False
        elif self.type == 'public':
            return True
        elif self.is_moderator(user) or self.is_contributor(user):
            #restricted/private require contributorship
            return True
        elif self.type == 'gold_only':
            return user.gold or user.gold_charter
        elif self.type == 'gold_restricted' and user.gold:
            return True
        elif self.type == 'restricted' and promotion:
            return True
        else:
            return False

    def can_submit_link(self, user):
        if c.user_is_admin or self.is_moderator_with_perms(user, "posts"):
            return True
        return "link" in self.allowed_types

    def can_submit_text(self, user):
        if c.user_is_admin or self.is_moderator_with_perms(user, "posts"):
            return True
        return "self" in self.allowed_types

    def can_ban(self, user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator_with_perms(user, 'posts')))

    def can_mute(self, muter, user):
        return (user.is_mutable(self) and
            (c.user_is_admin or
                self.is_moderator_with_perms(muter, 'access', 'mail'))
        )

    def can_distinguish(self,user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator_with_perms(user, 'posts')))

    def can_change_stylesheet(self, user):
        if c.user_is_loggedin:
            return (
                c.user_is_admin or self.is_moderator_with_perms(user, 'config'))
        else:
            return False

    def parse_css(self, content, verify=True):
        from r2.lib import cssfilter
        from r2.lib.template_helpers import (
            make_url_protocol_relative,
            static,
        )

        if g.css_killswitch or (verify and not self.can_change_stylesheet(c.user)):
            return (None, None)

        if not content:
            return ([], "")

        # parse in regular old http mode
        images = ImagesByWikiPage.get_images(self, "config/stylesheet")

        if self.quarantine:
            images = {name: static('blank.png') for name, url in images.iteritems()}

        protocol_relative_images = {
            name: make_url_protocol_relative(url)
            for name, url in images.iteritems()}
        parsed, errors = cssfilter.validate_css(
            content,
            protocol_relative_images,
        )

        return (errors, parsed)

    def change_css(self, content, parsed, prev=None, reason=None, author=None, force=False):
        from r2.models import ModAction
        from r2.lib.media import upload_stylesheet

        if not author:
            author = c.user

        if content is None:
            content = ''
        try:
            wiki = WikiPage.get(self, 'config/stylesheet')
        except tdb_cassandra.NotFound:
            wiki = WikiPage.create(self, 'config/stylesheet')
        wr = wiki.revise(content, previous=prev, author=author._id36, reason=reason, force=force)

        if parsed:
            self.stylesheet_url = upload_stylesheet(parsed)
            self.stylesheet_url_http = ""
            self.stylesheet_url_https = ""
        else:
            self.stylesheet_url = ""
            self.stylesheet_url_http = ""
            self.stylesheet_url_https = ""
        self._commit()

        if wr:
            ModAction.create(self, author, action='wikirevise', details='Updated subreddit stylesheet')

        return wr

    def is_special(self, user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator(user)
                     or self.is_contributor(user)))

    def should_ratelimit(self, user, kind):
        if self.is_special(user):
            return False

        hook = hooks.get_hook("account.is_ratelimit_exempt")
        ratelimit_exempt = hook.call_until_return(account=c.user)
        if ratelimit_exempt:
            return False

        if kind == 'comment':
            rl_karma = g.MIN_RATE_LIMIT_COMMENT_KARMA
        else:
            rl_karma = g.MIN_RATE_LIMIT_KARMA

        return user.karma(kind, self) < rl_karma

    def can_view(self, user):
        if c.user_is_admin:
            return True

        if self.spammy() or not self.is_exposed(user):
            return False
        else:
            return self.is_allowed_to_view(user)

    def can_view_in_modlist(self, user):
        if c.user_is_admin:
            return True
        elif self.spammy():
            return False
        else:
            return self.is_allowed_to_view(user)

    def is_allowed_to_view(self, user):
        """Returns whether user can view based on permissions and settings"""
        if self.type in ('public', 'restricted',
                         'gold_restricted', 'archived'):
            return True
        elif c.user_is_loggedin:
            if self.type == 'gold_only':
                return (user.gold or
                    user.gold_charter or
                    self.is_moderator(user) or
                    self.is_moderator_invite(user))

            return (self.is_contributor(user) or
                    self.is_moderator(user) or
                    self.is_moderator_invite(user))

    def is_exposed(self, user):
        """Return whether user is opted in to the subreddit's content.

        If a subreddit is quarantined, users must opt-in before viewing its
        content. Logged out users cannot opt-in, and all users are considered
        opted-in to non-quarantined subreddits.
        """
        if not self.quarantine:
            return True
        elif not user:
            return False
        elif (user.email_verified and
              QuarantinedSubredditOptInsByAccount.is_opted_in(user, self)):
            return True

        return False

    @property
    def is_embeddable(self):
        return (self.type not in Subreddit.private_types and
                not self.over_18 and not self._spam and not self.quarantine)

    def can_demod(self, bully, victim):
        bully_rel = self.get_moderator(bully)
        if bully_rel is not None and bully == victim:
            # mods can always demod themselves
            return True
        victim_rel = self.get_moderator(victim)
        return (
            bully_rel is not None
            and victim_rel is not None
            and bully_rel.is_superuser()  # limited mods can't demod
            and bully_rel._date <= victim_rel._date)

    @classmethod
    def load_subreddits(cls, links, return_dict = True, stale=False):
        """returns the subreddits for a list of links. it also preloads the
        permissions for the current user."""
        srids = set(l.sr_id for l in links
                    if getattr(l, "sr_id", None) is not None)
        subreddits = {}
        if srids:
            subreddits = cls._byID(srids, data=True, stale=stale)

        if subreddits and c.user_is_loggedin:
            # dict( {Subreddit,Account,name} -> Relationship )
            SRMember._fast_query(subreddits.values(), (c.user,), ('moderator',),
                                 data=True)

        return subreddits if return_dict else subreddits.values()

    def keep_for_rising(self, sr_id):
        """Return whether or not to keep a thing in rising for this SR."""
        return sr_id == self._id

    @classmethod
    def get_sr_user_relations(cls, user, srs):
        """Return SubredditUserRelations for the user and subreddits.

        The SubredditUserRelation objects indicate whether the user is a
        moderator, contributor, subscriber, banned, or muted. This method
        batches the lookups of all the relations for all the subreddits.

        """

        moderator_srids = set()
        contributor_srids = set()
        banned_srids = set()
        muted_srids = set()
        subscriber_srids = cls.user_subreddits(user, limit=None)

        if user and c.user_is_loggedin:
            res = SRMember._fast_query(
                thing1s=srs,
                thing2s=user,
                name=["moderator", "contributor", "banned", "muted"],
            )
            # _fast_query returns a dict of {(t1, t2, name): rel}, with rel of
            # None if the relation doesn't exist
            rels = [rel for rel in res.itervalues() if rel]
            for rel in rels:
                rel_name = rel._name
                sr_id = rel._thing1_id

                if rel_name == "moderator":
                    moderator_srids.add(sr_id)
                elif rel_name == "contributor":
                    contributor_srids.add(sr_id)
                elif rel_name == "banned":
                    banned_srids.add(sr_id)
                elif rel_name == "muted":
                    muted_srids.add(sr_id)

        ret = {}
        for sr in srs:
            sr_id = sr._id
            ret[sr_id] = SubredditUserRelations(
                subscriber=sr_id in subscriber_srids,
                moderator=sr_id in moderator_srids,
                contributor=sr_id in contributor_srids,
                banned=sr_id in banned_srids,
                muted=sr_id in muted_srids,
            )
        return ret

    @classmethod
    def add_props(cls, user, wrapped):
        srs = {item.lookups[0] for item in wrapped}
        sr_user_relations = cls.get_sr_user_relations(user, srs)

        for item in wrapped:
            relations = sr_user_relations[item._id]
            item.subscriber = relations.subscriber
            item.moderator = relations.moderator
            item.contributor = relations.contributor
            item.banned = relations.banned
            item.muted = relations.muted

            if item.hide_subscribers and not c.user_is_admin:
                item._ups = 0

            item.score_hidden = (
                not item.can_view(user) or
                item.hide_num_users_info
            )

            item.score = item._ups

            # override "voting" score behavior (it will override the use of
            # item.score in builder.py to be ups-downs)
            item.likes = item.subscriber or None
            base_score = item.score - (1 if item.likes else 0)
            item.voting_score = [(base_score + x - 1) for x in range(3)]
            item.score_fmt = Score.subscribers

            #will seem less horrible when add_props is in pages.py
            from r2.lib.pages import UserText
            if item.public_description or item.description:
                text = (item.public_description or
                        summarize_markdown(item.description))
                item.public_description_usertext = UserText(item, text)
            else:
                item.public_description_usertext = None


        Printable.add_props(user, wrapped)

    cache_ignore = {
        "description",
        "public_description",
        "subscribers",
    }.union(Printable.cache_ignore)

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        return s

    @classmethod
    def default_subreddits(cls, ids=True):
        """Return the subreddits a user with no subscriptions would see."""
        location = get_user_location()
        srids = LocalizedDefaultSubreddits.get_defaults(location)

        srs = Subreddit._byID(srids, data=True, return_dict=False, stale=True)
        srs = filter(lambda sr: sr.allow_top, srs)

        if ids:
            return [sr._id for sr in srs]
        else:
            return srs

    @classmethod
    def featured_subreddits(cls):
        """Return the curated list of subreddits shown during onboarding."""
        location = get_user_location()
        srids = LocalizedFeaturedSubreddits.get_featured(location)

        srs = Subreddit._byID(srids, data=True, return_dict=False, stale=True)
        srs = filter(lambda sr: sr.discoverable, srs)

        return srs

    @classmethod
    @memoize('random_reddits', time = 1800)
    def random_reddits_cached(cls, user_name, sr_ids, limit):
        # First filter out any subreddits that don't have a new enough post
        # to be included in the front page (just doing this may remove enough
        # to get below the limit anyway)
        sr_ids = SubredditsActiveForFrontPage.filter_inactive_ids(sr_ids)
        if len(sr_ids) <= limit:
            return sr_ids

        return random.sample(sr_ids, limit)

    @classmethod
    def random_reddits(cls, user_name, sr_ids, limit):
        """Select a random subset from sr_ids.

        Used for limiting the number of subscribed subreddits shown on a user's
        front page. Selection is cached for a while so the front page doesn't
        jump around.

        """

        if not limit:
            return sr_ids

        # if the user is subscribed to them, the automatic subreddits should
        # always be in the front page set and not count towards the limit
        if g.automatic_reddits:
            automatics = Subreddit._by_name(
                g.automatic_reddits, stale=True).values()
            automatic_ids = [sr._id for sr in automatics if sr._id in sr_ids]
            sr_ids = [sr_id for sr_id in sr_ids if sr_id not in automatic_ids]
        else:
            automatic_ids = []

        if len(sr_ids) > limit:
            sr_ids = sorted(sr_ids)
            sr_ids = cls.random_reddits_cached(user_name, sr_ids, limit)

        return sr_ids + automatic_ids

    @classmethod
    def random_reddit(cls, over18=False, user=None):
        if over18:
            sr_ids = NamedGlobals.get("popular_over_18_sr_ids")
        else:
            sr_ids = NamedGlobals.get("popular_sr_ids")

        if user:
            excludes = set(cls.user_subreddits(user, limit=None))
            sr_ids = list(set(sr_ids) - excludes)

        if not sr_ids:
            return Subreddit._by_name(g.default_sr)

        sr_id = random.choice(sr_ids)
        sr = Subreddit._byID(sr_id, data=True)
        return sr

    @classmethod
    def update_popular_subreddits(cls, limit=5000):
        q = cls._query(cls.c.type == "public", sort=desc('_downs'), limit=limit,
                       data=True)
        srs = list(q)

        # split the list into two based on whether the subreddit is 18+ or not
        sr_ids = []
        over_18_sr_ids = []

        # /r/promos is public but has special handling to make it unviewable
        promo_sr_id = cls.get_promote_srid()

        for sr in srs:
            if not sr.discoverable:
                continue

            if sr._id == promo_sr_id:
                continue

            if not sr.over_18:
                sr_ids.append(sr._id)
            else:
                over_18_sr_ids.append(sr._id)

        NamedGlobals.set("popular_sr_ids", sr_ids)
        NamedGlobals.set("popular_over_18_sr_ids", over_18_sr_ids)

    @classmethod
    def random_subscription(cls, user):
        if user.has_subscribed:
            sr_ids = Subreddit.subscribed_ids_by_user(user)
        else:
            sr_ids = Subreddit.default_subreddits(ids=True)

        return (Subreddit._byID(random.choice(sr_ids), data=True)
                if sr_ids else Subreddit._by_name(g.default_sr))

    @classmethod
    def user_subreddits(cls, user, ids=True, limit=DEFAULT_LIMIT):
        """
        subreddits that appear in a user's listings. If the user has
        subscribed, returns the stored set of subscriptions.

        limit - if it's Subreddit.DEFAULT_LIMIT, limits to 50 subs
                (100 for gold users)
                if it's None, no limit is used
                if it's an integer, then that many subs will be returned

        Otherwise, return the default set.
        """
        # Limit the number of subs returned based on user status,
        # if no explicit limit was passed
        if limit is Subreddit.DEFAULT_LIMIT:
            if user and user.gold:
                # Goldies get extra subreddits
                limit = Subreddit.gold_limit
            else:
                limit = Subreddit.sr_limit

        # note: for user not logged in, the fake user account has
        # has_subscribed == False by default.
        if user and user.has_subscribed:
            sr_ids = Subreddit.subscribed_ids_by_user(user)
            sr_ids = cls.random_reddits(user.name, sr_ids, limit)

            return sr_ids if ids else Subreddit._byID(sr_ids,
                                                      data=True,
                                                      return_dict=False,
                                                      stale=True)
        else:
            return cls.default_subreddits(ids=ids)


    # Used to pull all of the SRs a given user moderates or is a contributor
    # to (which one is controlled by query_param)
    @classmethod
    def special_reddits(cls, user, query_param):
        lookup = getattr(cls, 'reverse_%s_ids' % query_param)
        return lookup(user)

    @classmethod
    def subscribe_defaults(cls, user):
        if not user.has_subscribed:
            user.has_subscribed = True
            user._commit()
            srs = cls.user_subreddits(user=None, ids=False, limit=None)
            cls.subscribe_multiple(user, srs)

    def keep_item(self, wrapped):
        if c.user_is_admin:
            return True

        user = c.user if c.user_is_loggedin else None
        return self.can_view(user)

    def __eq__(self, other):
        if type(self) != type(other):
            return False

        if isinstance(self, FakeSubreddit):
            return self is other

        return self._id == other._id

    def __ne__(self, other):
        return not self.__eq__(other)

    @staticmethod
    def get_all_mod_ids(srs):
        from r2.lib.db.thing import Merge
        srs = tup(srs)
        queries = [
            SRMember._simple_query(
                ["_thing2_id"],
                SRMember.c._thing1_id == sr._id,
                SRMember.c._name == 'moderator',
            ) for sr in srs
        ]

        merged = Merge(queries)
        return [rel._thing2_id for rel in list(merged)]

    def update_moderator_permissions(self, user, **kwargs):
        """Grants or denies permissions to this moderator.

        Does nothing if the given user is not a moderator. Args are named
        parameters with bool or None values (use None to all back to the default
        for a permission).
        """
        rel = self.get_moderator(user)
        if rel:
            rel.update_permissions(**kwargs)
            rel._commit()

    def add_rel_note(self, type, user, note):
        rel = getattr(self, "get_%s" % type)(user)
        if not rel:
            raise ValueError("User is not %s." % type)
        rel.note = note
        rel._commit()

    def get_live_promos(self):
        from r2.lib import promote
        return promote.get_live_promotions([self.name])

    def schedule_unban(self, kind, victim, banner, duration):
        return SubredditTempBan.schedule(
            self,
            kind,
            victim,
            banner,
            datetime.timedelta(days=duration),
        )

    def unschedule_unban(self, victim, type):
        SubredditTempBan.unschedule(self.name, victim.name, type)

    def get_tempbans(self, type=None, names=None):
        return SubredditTempBan.search(self.name, type, names)

    def get_muted_items(self, names=None):
        return MutedAccountsBySubreddit.search(self, names)

    def add_gilding_seconds(self):
        from r2.models.gold import get_current_value_of_month
        seconds = get_current_value_of_month()
        self._incr("gilding_server_seconds", int(seconds))

    @property
    def allow_gilding(self):
        return not self.quarantine

    @classmethod
    def get_promote_srid(cls):
        try:
            return cls._by_name(g.promo_sr_name, stale=True)._id
        except NotFound:
            return None

    def is_subscriber(self, user):
        try:
            return bool(SubscribedSubredditsByAccount.fast_query(user, self))
        except tdb_cassandra.NotFound:
            return False

    def add_subscriber(self, user):
        SubscribedSubredditsByAccount.create(user, self)
        SubscriptionsByDay.create(self, user)
        add_legacy_subscriber(self, user)
        self._incr('_ups', 1)

    @classmethod
    def subscribe_multiple(cls, user, srs):
        SubscribedSubredditsByAccount.create(user, srs)
        SubscriptionsByDay.create(srs, user)
        add_legacy_subscriber(srs, user)
        for sr in srs:
            sr._incr('_ups', 1)

    def remove_subscriber(self, user):
        SubscribedSubredditsByAccount.destroy(user, self)
        remove_legacy_subscriber(self, user)
        self._incr('_ups', -1)

    @classmethod
    def subscribed_ids_by_user(cls, user):
        return SubscribedSubredditsByAccount.get_all_sr_ids(user)

    @classmethod
    def reverse_subscriber_ids(cls, user):
        # This is just for consistency with all the other UserRel types
        return cls.subscribed_ids_by_user(user)

    def get_rgb(self, fade=0.8):
        r = int(256 - (hash(str(self._id)) % 256)*(1-fade))
        g = int(256 - (hash(str(self._id) + ' ') % 256)*(1-fade))
        b = int(256 - (hash(str(self._id) + '  ') % 256)*(1-fade))
        return (r, g, b)

    def set_sticky(self, link, log_user=None, num=None):
        unstickied_fullnames = []

        if not self.sticky_fullnames:
            self.sticky_fullnames = [link._fullname]
        else:
            # don't re-sticky something that's already stickied
            if link._fullname in self.sticky_fullnames:
                return

            # XXX: have to work with a copy of the list instead of modifying
            #   it directly, because it doesn't get marked as "dirty" and
            #   saved properly unless we assign a new list to the attr
            sticky_fullnames = self.sticky_fullnames[:]

            # if a particular slot was specified and is in use, replace it
            if num and num <= len(sticky_fullnames):
                unstickied_fullnames.append(sticky_fullnames[num-1])
                sticky_fullnames[num-1] = link._fullname
            else:
                # either didn't specify a slot or it's empty, just append

                # if we're already at the max number of stickies, remove
                # the bottom-most to make room for this new one
                if self.has_max_stickies:
                    unstickied_fullnames.extend(
                        sticky_fullnames[self.MAX_STICKIES-1:])
                    sticky_fullnames = sticky_fullnames[:self.MAX_STICKIES-1]

                sticky_fullnames.append(link._fullname)

            self.sticky_fullnames = sticky_fullnames

        self._commit()

        if log_user:
            from r2.models import Link, ModAction
            for fullname in unstickied_fullnames:
                unstickied = Link._by_fullname(fullname)
                ModAction.create(self, log_user, "unsticky",
                    target=unstickied, details="replaced")
            ModAction.create(self, log_user, "sticky", target=link)

    def remove_sticky(self, link, log_user=None):
        # XXX: have to work with a copy of the list instead of modifying
        #   it directly, because it doesn't get marked as "dirty" and
        #   saved properly unless we assign a new list to the attr
        sticky_fullnames = self.sticky_fullnames[:]
        try:
            sticky_fullnames.remove(link._fullname)
        except ValueError:
            return

        self.sticky_fullnames = sticky_fullnames
        self._commit()

        if log_user:
            from r2.models import ModAction
            ModAction.create(self, log_user, "unsticky", target=link)

    @property
    def has_max_stickies(self):
        if not self.sticky_fullnames:
            return False
        return len(self.sticky_fullnames) >= self.MAX_STICKIES


class SubscribedSubredditsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _connection_pool = 'main'
    _views = []
    _extra_schema_creation_args = {
        "default_validation_class": DATE_TYPE,
    }

    @classmethod
    def value_for(cls, user, sr):
        return datetime.datetime.now(g.tz)

    @classmethod
    def get_all_sr_ids(cls, user):
        key = cls.__name__ + user._id36
        sr_ids = g.cassandra_local_cache.get(key)
        if sr_ids is None:
            r = cls._cf.xget(user._id36)
            sr_ids = [int(sr_id36, 36) for sr_id36, val in r]
            g.cassandra_local_cache.set(key, sr_ids)

        return sr_ids


class SubscriptionsByDay(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = types.CompositeType(types.AsciiType(), types.AsciiType())
    _extra_schema_creation_args = {
        "key_validation_class": DATE_TYPE,
    }

    @classmethod
    def create(cls, srs, user):
        rowkey = datetime.datetime.now(g.tz).replace(
            hour=0,
            minute=0,
            second=0,
            microsecond=0,
        )
        srs = tup(srs)
        columns = {(sr._id36, user._id36): "" for sr in srs}
        cls._cf.insert(rowkey, columns)

    @classmethod
    def get_all_counts(cls, date):
        date = date.replace(
            hour=0,
            minute=0,
            second=0,
            microsecond=0,
            tzinfo=g.tz,
        )

        gen = cls._cf.xget(date)
        (prev_sr_id36, user_id36), val = next(gen)

        count = 1
        for (sr_id36, user_id36), val in gen:
            if sr_id36 == prev_sr_id36:
                count += 1
            else:
                yield (prev_sr_id36, count)
                prev_sr_id36 = sr_id36
                count = 1
        yield (prev_sr_id36, count)

    @classmethod
    def write_counts(cls, days_ago=1):
        from sqlalchemy.orm import scoped_session, sessionmaker
        from r2.models.traffic import SubscriptionsBySubreddit, engine

        Session = scoped_session(sessionmaker(bind=engine))

        date = datetime.datetime.now(g.tz) - datetime.timedelta(days=days_ago)
        pg_date = date.replace(
            hour=0,
            minute=0,
            second=0,
            microsecond=0,
            tzinfo=None,
        )
        print "writing subscribers for %s" % date

        num_srs = 0
        num_subscribers = 0
        for sr_id36, count in cls.get_all_counts(date):
            sr = Subreddit._byID36(sr_id36, data=True)
            row = SubscriptionsBySubreddit(
                subreddit=sr.name,
                date=pg_date,
                subscriber_count=count,
            )
            Session.merge(row)
            Session.commit()
            num_srs += 1
            num_subscribers += count
        print "%s subscribers in %s subreddits" % (num_subscribers, num_srs)
        Session.remove()


class FakeSubreddit(BaseSite):
    _defaults = dict(Subreddit._defaults,
        link_flair_position='right',
        flair_enabled=False,
    )

    def __init__(self):
        BaseSite.__init__(self)

    def keep_for_rising(self, sr_id):
        return False

    @property
    def _should_wiki(self):
        return False

    @property
    def allow_gilding(self):
        return True

    @property
    def allow_ads(self):
        return True

    def is_moderator(self, user):
        if c.user_is_loggedin and c.user_is_admin:
            return FakeSRMember(ModeratorPermissionSet)

    def can_view(self, user):
        return True

    def can_comment(self, user):
        return False

    def can_submit(self, user, promotion=False):
        return False

    def can_change_stylesheet(self, user):
        return False

    def is_banned(self, user):
        return False

    def is_muted(self, user):
        return False

    def get_all_comments(self):
        from r2.lib.db import queries
        return queries.get_all_comments()

    def get_gilded(self):
        raise NotImplementedError()

    def spammy(self):
        return False

class FriendsSR(FakeSubreddit):
    name = 'friends'
    title = 'friends'
    _defaults = dict(
        FakeSubreddit._defaults,
        login_required=True,
    )

    def get_links(self, sort, time):
        from r2.lib.db import queries

        friends = c.user.get_recently_submitted_friend_ids()
        if not friends:
            return []

        # with the precomputer enabled, this Subreddit only supports
        # being sorted by 'new'. it would be nice to have a
        # cleaner UI than just blatantly ignoring their sort,
        # though
        sort = 'new'
        time = 'all'

        friends = Account._byID(friends, return_dict=False)

        crs = [queries.get_submitted(friend, sort, time)
               for friend in friends]
        return queries.MergedCachedResults(crs)

    def get_all_comments(self):
        from r2.lib.db import queries

        friends = c.user.get_recently_commented_friend_ids()
        if not friends:
            return []

        # with the precomputer enabled, this Subreddit only supports
        # being sorted by 'new'. it would be nice to have a
        # cleaner UI than just blatantly ignoring their sort,
        # though
        sort = 'new'
        time = 'all'

        friends = Account._byID(friends,
                                return_dict=False)

        crs = [queries.get_comments(friend, sort, time)
               for friend in friends]
        return queries.MergedCachedResults(crs)

    def get_gilded(self):
        from r2.lib.db.queries import get_gilded_users

        friends = c.user.friend_ids()

        if not friends:
            return []

        return get_gilded_users(friends)


class AllSR(FakeSubreddit):
    name = 'all'
    title = 'all subreddits'
    path = '/r/all'

    def keep_for_rising(self, sr_id):
        return True

    def get_links(self, sort, time):
        from r2.models import Link
        from r2.lib.db import queries
        q = Link._query(
            sort=queries.db_sort(sort),
            read_cache=True,
            write_cache=True,
            cache_time=60,
            data=True,
            filter_primary_sort_only=True,
        )
        if time != 'all':
            q._filter(queries.db_times[time])
        return q

    def get_all_comments(self):
        from r2.lib.db import queries
        return queries.get_all_comments()

    def get_gilded(self):
        from r2.lib.db import queries
        return queries.get_all_gilded()

    def get_reported(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        from r2.lib.db.thing import Merge
        qs = []

        if include_links:
            qs.append(queries.get_reported_links(None))

        if include_comments:
            qs.append(queries.get_reported_comments(None))

        return MergedCachedQuery(qs)

class AllMinus(AllSR):
    analytics_name = "all"
    name = _("%s (filtered)") % "all"

    def __init__(self, srs):
        AllSR.__init__(self)
        self.exclude_srs = srs
        self.exclude_sr_ids = [sr._id for sr in srs]

    def keep_for_rising(self, sr_id):
        return sr_id not in self.exclude_sr_ids

    @property
    def title(self):
        sr_names = ', '.join(sr.name for sr in self.exclude_srs)
        return 'all subreddits except ' + sr_names

    @property
    def path(self):
        return '/r/all-' + '-'.join(sr.name for sr in self.exclude_srs)

    def get_links(self, sort, time):
        from r2.models import Link
        from r2.lib.db.operators import not_
        q = AllSR.get_links(self, sort, time)
        if c.user.gold and self.exclude_sr_ids:
            q._filter(not_(Link.c.sr_id.in_(self.exclude_sr_ids)))
        return q


class Filtered(object):
    unfiltered_path = None

    @property
    def path(self):
        return '/me/f/%s' % self.filtername

    @property
    def title(self):
        return self.name

    @property
    def name(self):
        return _("%s (filtered)") % self.filtername

    @property
    def multi_path(self):
        return ('/user/%s/f/%s' % (c.user.name, self.filtername)).lower()

    def _get_filtered_subreddits(self):
        try:
            multi = LabeledMulti._byID(self.multi_path)
        except tdb_cassandra.NotFound:
            multi = None
        filtered_srs = multi.srs if multi else []
        return sorted(filtered_srs, key=lambda sr: sr.name)


class AllFiltered(Filtered, AllMinus):
    unfiltered_path = '/r/all'
    filtername = 'all'

    def __init__(self):
        filters = self._get_filtered_subreddits() if c.user.gold else []
        AllMinus.__init__(self, filters)


class _DefaultSR(FakeSubreddit):
    analytics_name = 'frontpage'
    #notice the space before reddit.com
    name = ' reddit.com'
    path = '/'
    header = g.default_header_url

    def _get_sr_ids(self):
        if not c.defaultsr_cached_sr_ids:
            user = c.user if c.user_is_loggedin else None
            c.defaultsr_cached_sr_ids = Subreddit.user_subreddits(user)
        return c.defaultsr_cached_sr_ids

    def keep_for_rising(self, sr_id):
        return sr_id in self._get_sr_ids()

    def is_moderator(self, user):
        return False

    def get_links(self, sort, time):
        sr_ids = self._get_sr_ids()
        return get_links_sr_ids(sr_ids, sort, time)

    @property
    def title(self):
        return _(g.short_description)

# This is the base class for the instantiated front page reddit
class DefaultSR(_DefaultSR):
    @property
    def _base(self):
        try:
            return Subreddit._by_name(g.default_sr, stale=True)
        except NotFound:
            return None

    def wiki_can_submit(self, user):
        return True

    @property
    def wiki_use_subreddit_karma(self):
        return False

    @property
    def _should_wiki(self):
        return True

    @property
    def wikimode(self):
        return self._base.wikimode if self._base else "disabled"

    @property
    def wiki_edit_karma(self):
        return self._base.wiki_edit_karma

    @property
    def wiki_edit_age(self):
        return self._base.wiki_edit_age

    def is_wikicontributor(self, user):
        return self._base.is_wikicontributor(user)

    def is_wikibanned(self, user):
        return self._base.is_wikibanned(user)

    def is_wikicreate(self, user):
        return self._base.is_wikicreate(user)

    @property
    def _fullname(self):
        return "t5_6"

    @property
    def _id36(self):
        return self._base._id36

    @property
    def type(self):
        return self._base.type if self._base else "public"

    @property
    def header(self):
        return (self._base and self._base.header) or _DefaultSR.header

    @property
    def header_title(self):
        return (self._base and self._base.header_title) or ""

    @property
    def header_size(self):
        return (self._base and self._base.header_size) or None

    @property
    def stylesheet_url(self):
        return self._base.stylesheet_url if self._base else ""

    @property
    def stylesheet_url_http(self):
        return self._base.stylesheet_url_http if self._base else ""

    @property
    def stylesheet_url_https(self):
        return self._base.stylesheet_url_https if self._base else ""

    def get_all_comments(self):
        from r2.lib.db.queries import _get_sr_comments, merge_results
        sr_ids = Subreddit.user_subreddits(c.user)
        results = [_get_sr_comments(sr_id) for sr_id in sr_ids]
        return merge_results(*results)

    def get_gilded(self):
        from r2.lib.db.queries import get_gilded
        return get_gilded(Subreddit.user_subreddits(c.user))

    def get_live_promos(self):
        from r2.lib import promote
        srs = Subreddit.user_subreddits(c.user, ids=False)
        # '' is for promos targeted to the frontpage
        sr_names = [self.name] + [sr.name for sr in srs]
        return promote.get_live_promotions(sr_names)


class MultiReddit(FakeSubreddit):
    name = 'multi'
    header = ""
    _defaults = dict(
        FakeSubreddit._defaults,
        weighting_scheme="classic",
    )

    # See comment in normalized_hot before adding new values here.
    AGEWEIGHTS = {
        "classic": 0.0,
        "fresh": 0.15,
    }

    def __init__(self, path=None, srs=None):
        FakeSubreddit.__init__(self)
        if path is not None:
            self._path = path
        self._srs = srs or []

    @property
    def srs(self):
        return self._srs

    @property
    def sr_ids(self):
        return [sr._id for sr in self.srs]

    @property
    def kept_sr_ids(self):
        return [sr._id for sr in self.srs if not sr._spam]

    @property
    def banned_sr_ids(self):
        return [sr._id for sr in self.srs if sr._spam]

    @property
    def allows_referrers(self):
        return all(sr.allows_referrers for sr in self.srs)

    def keep_for_rising(self, sr_id):
        return sr_id in self.kept_sr_ids

    def is_moderator(self, user):
        if not user:
            return False

        # Get moderator SRMember relations for all in srs
        # if a relation doesn't exist there will be a None entry in the
        # returned dict
        mod_rels = SRMember._fast_query(self.srs, user, 'moderator', data=True)
        if None in mod_rels.values():
            return False
        else:
            return FakeSRMember(ModeratorPermissionSet)

    def srs_with_perms(self, user, *perms):
        return [sr for sr in self.srs
                if sr.is_moderator_with_perms(user, *perms) and not sr._spam]

    @property
    def title(self):
        return _('posts from %s') % ', '.join(sr.name for sr in self.srs)

    @property
    def path(self):
        return self._path

    @property
    def over_18(self):
        return any(sr.over_18 for sr in self.srs)

    @property
    def ageweight(self):
        return self.AGEWEIGHTS.get(self.weighting_scheme, 0.0)

    def get_links(self, sort, time):
        return get_links_sr_ids(self.kept_sr_ids, sort, time)

    def get_all_comments(self):
        from r2.lib.db.queries import _get_sr_comments, merge_results
        results = [_get_sr_comments(sr_id) for sr_id in self.kept_sr_ids]
        return merge_results(*results)

    def get_gilded(self):
        from r2.lib.db.queries import get_gilded
        return get_gilded(self.kept_sr_ids)

    def get_live_promos(self):
        from r2.lib import promote
        srs = Subreddit._byID(self.kept_sr_ids, return_dict=False)
        sr_names = [sr.name for sr in srs]
        return promote.get_live_promotions(sr_names)


class TooManySubredditsError(Exception):
    pass


class BaseLocalizedSubreddits(tdb_cassandra.View):
    """Mapping of location to subreddit ids"""
    _use_db = False
    _compare_with = ASCII_TYPE
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "default_validation_class": ASCII_TYPE,
    }
    GLOBAL = "GLOBAL"

    @classmethod
    def _rowkey(cls, location):
        return str(location)

    @classmethod
    def lookup(cls, keys, update=False):
        def _lookup(keys):
            rows = cls._cf.multiget(keys)
            ret = {}
            for key in keys:
                columns = rows[key] if key in rows else {}
                id36s = columns.keys()
                ret[key] = id36s
            return ret

        id36s_by_location = sgm(
            cache=g.gencache,
            keys=keys,
            miss_fn=_lookup,
            prefix=cls.CACHE_PREFIX,
            stale=True,
            _update=update,
            ignore_set_errors=True,
        )
        ids_by_location = {location: [int(id36, 36) for id36 in id36s]
                           for location, id36s in id36s_by_location.iteritems()}
        return ids_by_location

    @classmethod
    def set_srs(cls, location, srs):
        rowkey = cls._rowkey(location)
        columns = {sr._id36: '' for sr in srs}

        # update cassandra
        try:
            existing = cls._cf.get(rowkey)
        except tdb_cassandra.NotFoundException:
            existing = {}

        cls._set_values(rowkey, columns)
        removed_srid36s = set(existing.keys()) - set(columns.keys())
        cls._remove(rowkey, removed_srid36s)

        # update cache
        id36s = columns.keys()
        g.gencache.set_multi({rowkey: id36s}, prefix=cls.CACHE_PREFIX)

    @classmethod
    def set_global_srs(cls, srs):
        location = cls.GLOBAL
        cls.set_srs(location, srs)

    @classmethod
    def get_srids(cls, location):
        if not location:
            return []

        rowkey = cls._rowkey(location)
        ids_by_location = cls.lookup([rowkey])
        srids = ids_by_location[rowkey]
        return srids

    @classmethod
    def get_global_defaults(cls):
        return cls.get_srids(cls.GLOBAL)

    @classmethod
    def get_localized_srs(cls, location):
        location_key = cls._rowkey(location) if location else None
        global_key = cls._rowkey(cls.GLOBAL)
        keys = filter(None, [location_key, global_key])

        ids_by_location = cls.lookup(keys)

        if location_key and ids_by_location[location_key]:
            c.used_localized_defaults = True
            return ids_by_location[location_key]
        else:
            return ids_by_location[global_key]


class LocalizedDefaultSubreddits(BaseLocalizedSubreddits):
    _use_db = True
    _type_prefix = "LocalizedDefaultSubreddits"
    CACHE_PREFIX = "defaultsrs:"

    @classmethod
    def get_defaults(cls, location):
        return cls.get_localized_srs(location)


class LocalizedFeaturedSubreddits(BaseLocalizedSubreddits):
    _use_db = True
    _type_prefix = "LocalizedFeaturedSubreddits"
    CACHE_PREFIX = "featuredsrs:"

    @classmethod
    def get_featured(cls, location):
        return cls.get_localized_srs(location)


class LabeledMulti(tdb_cassandra.Thing, MultiReddit):
    """Thing with special columns that hold Subreddit ids and properties."""
    _use_db = True
    _views = []
    _bool_props = ('is_symlink', )
    _defaults = dict(
        MultiReddit._defaults,
        visibility='private',
        is_symlink=False,
        description_md='',
        display_name='',
        copied_from=None,
        key_color="#cee3f8",  # A lovely shade of blue
        icon_id='',
        weighting_scheme="classic",
    )
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "column_name_class": UTF8_TYPE,
        "default_validation_class": UTF8_TYPE,
        "column_validation_classes": {
            "date": pycassa.system_manager.DATE_TYPE,
        },
    }
    _float_props = (
        "base_normalized_age_weight",
    )
    _compare_with = UTF8_TYPE
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    SR_PREFIX = 'SR_'
    MAX_SR_COUNT = 100

    def __init__(self, _id=None, *args, **kwargs):
        tdb_cassandra.Thing.__init__(self, _id, *args, **kwargs)
        MultiReddit.__init__(self)
        self._owner = None

    @classmethod
    def _byID(cls, ids, return_dict=True, properties=None, load_subreddits=True,
              load_linked_multis=True):
        ret = super(cls, cls)._byID(ids, return_dict=False,
                                    properties=properties)
        if not ret:
            # the falsy return object must be converted to the proper type
            # based on whether ids was an iterable and return_dict
            if ret == []:
                if return_dict:
                    return {}
                else:
                    return []
            else:
                return

        ret = cls._load(ret, load_subreddits=load_subreddits,
                        load_linked_multis=load_linked_multis)
        if isinstance(ret, cls):
            return ret
        elif return_dict:
            return {thing._id: thing for thing in ret}
        else:
            return ret

    @classmethod
    def _load(cls, things, load_subreddits=True, load_linked_multis=True):
        things, single = tup(things, ret_is_single=True)

        # some objects are being loaded for the first time and need basic setup
        never_loaded = [t for t in things if not t._owner]
        if never_loaded:
            owner_fullnames = set(t.owner_fullname for t in never_loaded)
            owners = Thing._by_fullname(
                owner_fullnames, data=True, return_dict=True)
            for t in things:
                if t in never_loaded:
                    t._owner = owners[t.owner_fullname]
                    t._srs_loaded = False
                    t._linked_multi = None

        if load_linked_multis:
            needs_linked_multis = [t.copied_from for t in things
                                   if t.is_symlink and not t._linked_multi]
            if needs_linked_multis:
                multis = LabeledMulti._byID(needs_linked_multis, return_dict=True)
                for t in things:
                    if t.copied_from in needs_linked_multis:
                        t._linked_multi = multis[t.copied_from]

        # some objects may have been retrieved from cache and need srs
        if load_subreddits:
            needs_srs = [t for t in things if not t._srs_loaded]
            if needs_srs:
                sr_ids = set(
                    itertools.chain.from_iterable(t.sr_ids for t in needs_srs))
                srs = Subreddit._byID(
                    sr_ids, data=True, return_dict=True, stale=True)
                for t in things:
                    if t in needs_srs:
                        t._srs = [srs[sr_id] for sr_id in t.sr_ids]
                        t._srs_loaded = True

        return things[0] if single else things

    @property
    def linked_multi(self):
        return self._linked_multi

    @property
    def sr_ids(self):
        return self.sr_props.keys()

    @property
    def srs(self):
        if self.is_symlink:
            if (not self.copied_from or self.copied_from == self._id
                    or not self.linked_multi):
                raise RedditError("Upstream symlinked multi can't be retrieved.")
            if not self.linked_multi.can_view(self.owner):
                raise RedditError("Upstream symlinked multi is not visible.")

            return self.linked_multi.srs

        if not self._srs_loaded:
            g.log.error("%s: accessed subreddits without loading", self)
            self._srs = Subreddit._byID(
                self.sr_ids, data=True, return_dict=False)
        return self._srs

    @property
    def owner(self):
        return self._owner

    @property
    def sr_columns(self):
        # limit to max subreddit count, allowing a little fudge room for
        # cassandra inconsistency
        if self.is_symlink:
            if not getattr(self, '_linked_multi', None):
                self._linked_multi = LabeledMulti._byID(self.copied_from)
            return self.linked_multi.sr_columns

        remaining = self.MAX_SR_COUNT + 10
        sr_columns = {}
        for k, v in self._t.iteritems():
            if not k.startswith(self.SR_PREFIX):
                continue

            sr_columns[k] = v

            remaining -= 1
            if remaining <= 0:
                break
        return sr_columns

    @property
    def kind(self):
        return self._id.split('/')[3]

    @property
    def sr_props(self):
        return self.columns_to_sr_props(self.sr_columns)

    @property
    def path(self):
        if isinstance(self.owner, Account):
            return '/user/%(username)s/%(kind)s/%(multiname)s' % {
                'username': self.owner.name,
                'kind': self.kind,
                'multiname': self.name,
            }
        if isinstance(self.owner, Subreddit):
            return '/r/%(srname)s/%(kind)s/%(multiname)s' % {
                'srname': self.owner.name,
                'kind': self.kind,
                'multiname': self.name,
            }

    @property
    def user_path(self):
        if self.owner == c.user:
            return '/me/%s/%s' % (self.kind, self.name)
        else:
            return self.path

    @property
    def name(self):
        return self._id.split('/')[-1]

    @property
    def analytics_name(self):
        # classify as "multi" (as for unnamed multis) until our traffic system
        # is smarter
        return 'multi'

    @property
    def allows_referrers(self):
        if not self.is_public():
            return False
        return super(LabeledMulti, self).allows_referrers

    @property
    def title(self):
        if isinstance(self.owner, Account):
            return _('%s subreddits curated by /u/%s') % (self.name, self.owner.name)
        return _('%s subreddits') % self.name

    def is_public(self):
        return self.visibility == "public"

    def is_hidden(self):
        return self.visibility == "hidden"

    def can_view(self, user):
        if c.user_is_admin:
            return True

        if self.is_public():
            return True

        if isinstance(user, FakeAccount):
            return False

        # subreddit multireddit (mod can view)
        if isinstance(self.owner, Subreddit):
            return self.owner.is_moderator_with_perms(user, 'config')

        return user == self.owner

    def can_edit(self, user):
        if isinstance(user, FakeAccount):
            return False

        # subreddit multireddit (admin can edit)
        if isinstance(self.owner, Subreddit):
            return (c.user_is_admin or
                    self.owner.is_moderator_with_perms(user, 'config'))

        if c.user_is_admin and self.owner == Account.system_user():
            return True

        return user == self.owner

    @property
    def icon_url(self):
        from r2.lib.template_helpers import static
        if self.icon_id:
            path = "multi_icons/{}.png".format(self.icon_id.replace(" ", "_"))
            return static(path)
        else:
            return None

    def set_icon_by_name(self, name):
        """Set this multi's icon information by icon name

        Note: tdb_cassandra.Thing doesn't support property.setter properly;
        it appears to write through directly to self._t['icon_name'].

        """
        if not name:
            self.icon_id = ''
        elif name in g.multi_icons:
            self.icon_id = name
        else:
            raise ValueError("invalid multi icon name")

    @classmethod
    def by_owner(cls, owner, kinds=None, load_subreddits=True):
        try:
            multi_ids = LabeledMultiByOwner._byID(owner._fullname)._t.keys()
        except tdb_cassandra.NotFound:
            return []

        kinds = ('m',) if not kinds else kinds
        multis = cls._byID(
            multi_ids, return_dict=False, load_subreddits=load_subreddits)
        return [multi for multi in multis if multi.kind in kinds]

    @classmethod
    def create(cls, path, owner):
        obj = cls(_id=path, owner_fullname=owner._fullname)
        obj._commit()
        obj._owner = owner
        obj._srs_loaded = False
        return obj

    @classmethod
    def copy(cls, path, multi, owner, symlink=False):
        if symlink:
            # remove all the sr_ids from the properties
            props = {k: v for k, v in multi._t.iteritems()
                     if k not in multi.sr_columns.keys()}
            props["is_symlink"] = True
        else:
            props = multi._t

        obj = cls(_id=path, **props)
        obj._srs = multi._srs
        obj._srs_loaded = multi._srs_loaded
        obj.owner_fullname = owner._fullname
        obj.copied_from = multi.path.lower()
        obj._commit()
        obj._linked_multi = multi if symlink else None
        obj._owner = owner

        return obj

    @classmethod
    def slugify(cls, owner, display_name, type_="m"):
        """Generate user multi path from display name."""
        slug = unicode_title_to_ascii(display_name)
        if isinstance(owner, Subreddit):
            prefix = "/r/" + owner.name + "/" + type_ + "/"
        else:
            prefix = "/user/" + owner.name + "/" + type_ + "/"
        new_path = prefix + slug
        try:
            existing = LabeledMultiByOwner._byID(owner._fullname)._t.keys()
        except tdb_cassandra.NotFound:
            existing = []
        count = 0
        while new_path in existing:
            count += 1
            new_path = prefix + slug + str(count)
        return new_path

    @classmethod
    def sr_props_to_columns(cls, sr_props):
        columns = {}
        sr_ids = []
        for sr_id, props in sr_props.iteritems():
            if isinstance(sr_id, BaseSite):
                sr_id = sr_id._id
            sr_ids.append(sr_id)
            columns[cls.SR_PREFIX + str(sr_id)] = json.dumps(props)
        return sr_ids, columns

    @classmethod
    def columns_to_sr_props(cls, columns):
        ret = {}
        for s, sr_prop_dump in columns.iteritems():
            sr_id = long(s.strip(cls.SR_PREFIX))
            sr_props = json.loads(sr_prop_dump)
            ret[sr_id] = sr_props
        return ret

    def _on_create(self):
        for view in self._views:
            view.add_object(self)

    def unlink(self):
        if not self.is_symlink:
            return

        self._srs = self.srs
        sr_props = dict.fromkeys(self.srs, {})
        sr_ids, sr_columns = self.sr_props_to_columns(sr_props)
        for attr, val in sr_columns.iteritems():
            self.__setattr__(attr, val)

        self.is_symlink = False

    def add_srs(self, sr_props):
        """Add/overwrite subreddit(s)."""
        if self.is_symlink:
            self.unlink()
        sr_ids, sr_columns = self.sr_props_to_columns(sr_props)

        if len(set(sr_columns) | set(self.sr_columns)) > self.MAX_SR_COUNT:
            raise TooManySubredditsError

        new_sr_ids = set(sr_ids) - set(self.sr_ids)
        new_srs = Subreddit._byID(
            new_sr_ids, data=True, return_dict=False, stale=True)
        self._srs.extend(new_srs)

        for attr, val in sr_columns.iteritems():
            self.__setattr__(attr, val)

    def del_srs(self, sr_ids):
        """Delete subreddit(s)."""
        if self.is_symlink:
            self.unlink()

        sr_props = dict.fromkeys(tup(sr_ids), {})
        sr_ids, sr_columns = self.sr_props_to_columns(sr_props)

        for key in sr_columns.iterkeys():
            self.__delitem__(key)

        self._srs = [sr for sr in self._srs if sr._id not in sr_ids]

    def clear_srs(self):
        self.del_srs(self.sr_ids)

    def delete(self):
        # Do we want to actually delete objects?
        self._destroy()
        for view in self._views:
            rowkey = view._rowkey(self)
            column = view._obj_to_column(self)
            view._remove(rowkey, column)


@tdb_cassandra.view_of(LabeledMulti)
class LabeledMultiByOwner(tdb_cassandra.View):
    _use_db = True

    @classmethod
    def _rowkey(cls, lm):
        return lm.owner_fullname


class RandomReddit(FakeSubreddit):
    name = 'random'
    header = ""

class RandomNSFWReddit(FakeSubreddit):
    name = 'randnsfw'
    header = ""

class RandomSubscriptionReddit(FakeSubreddit):
    name = 'myrandom'
    header = ""

class ModContribSR(MultiReddit):
    name  = None
    title = None
    query_param = None
    _defaults = dict(
        MultiReddit._defaults,
        login_required=True,
    )

    def __init__(self):
        # Can't lookup srs right now, c.user not set
        MultiReddit.__init__(self)

    @property
    def sr_ids(self):
        if c.user_is_loggedin:
            return Subreddit.special_reddits(c.user, self.query_param)
        else:
            return []

    @property
    def srs(self):
        return Subreddit._byID(self.sr_ids, data=True, return_dict=False)

    @property
    def allows_referrers(self):
        return False


class ModSR(ModContribSR):
    name  = "subreddits you moderate"
    title = "subreddits you moderate"
    query_param = "moderator"
    path = "/r/mod"

    def is_moderator(self, user):
        return FakeSRMember(ModeratorPermissionSet)


class ModMinus(ModSR):
    analytics_name = "mod"

    def __init__(self, exclude_srs):
        ModSR.__init__(self)
        self.exclude_srs = exclude_srs
        self.exclude_sr_ids = [sr._id for sr in exclude_srs]

    @property
    def sr_ids(self):
        sr_ids = super(ModMinus, self).sr_ids
        return [sr_id for sr_id in sr_ids if not sr_id in self.exclude_sr_ids]

    @property
    def name(self):
        exclude_text = ', '.join(sr.name for sr in self.exclude_srs)
        return 'subreddits you moderate except ' + exclude_text

    @property
    def title(self):
        return self.name

    @property
    def path(self):
        return '/r/mod-' + '-'.join(sr.name for sr in self.exclude_srs)


class ModFiltered(Filtered, ModMinus):
    unfiltered_path = '/r/mod'
    filtername = 'mod'

    def __init__(self):
        ModMinus.__init__(self, self._get_filtered_subreddits())


class ContribSR(ModContribSR):
    name  = "contrib"
    title = "communities you're approved on"
    query_param = "contributor"
    path = "/r/contrib"


class DomainSR(FakeSubreddit):
    @property
    def path(self):
        return '/domain/' + self.domain

    def __init__(self, domain):
        FakeSubreddit.__init__(self)
        domain = domain.lower()
        self.domain = domain
        self.name = domain
        self.title = _("%(domain)s on %(reddit.com)s") % {
            "domain": domain, "reddit.com": g.domain}
        try:
            idn = domain.decode('idna')
            if idn != domain:
                self.idn = idn
        except UnicodeError:
            # If we were given a bad domain name (e.g. xn--.com) we'll get an
            # error here. These domains are invalid to register so it should
            # be fine to ignore the error.
            pass

    def get_links(self, sort, time):
        from r2.lib.db import queries
        return queries.get_domain_links(self.domain, sort, time)

    @property
    def allow_gilding(self):
        return False


class SearchResultSubreddit(Subreddit):
    _nodb = True

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.controllers.reddit_base import UnloggedUser
        Subreddit.add_props(user, wrapped)
        for item in wrapped:
            url = UrlParser(item.path)
            url.update_query(ref="search_subreddits")
            item.search_path = url.unparse()
            can_view = item.can_view(user)
            if isinstance(user, UnloggedUser):
                can_comment = item.type == "public"
            else:
                can_comment = item.can_comment(user)
            if not can_view:
                item.display_type = "private"
            elif item.type == "archived":
                item.display_type = "archived"
            elif not can_comment:
                item.display_type = "restricted"
            else:
                item.display_type = "public"
        Printable.add_props(user, wrapped)

Frontpage = DefaultSR()
Friends = FriendsSR()
Mod = ModSR()
Contrib = ContribSR()
All = AllSR()
Random = RandomReddit()
RandomNSFW = RandomNSFWReddit()
RandomSubscription = RandomSubscriptionReddit()

# add to _specials so they can be retrieved with Subreddit._by_name, e.g.
# Subreddit._by_name("all")
Subreddit._specials.update({
    sr.name: sr for sr in (
        Friends,
        RandomNSFW,
        RandomSubscription,
        Random,
        Contrib,
        All,
        Frontpage,
    )
})

# some subreddits have unfortunate names
Subreddit._specials['mod'] = Mod


SubredditUserRelations = collections.namedtuple(
    "SubredditUserRelations",
    ["subscriber", "moderator", "contributor", "banned", "muted"],
)


class SRMember(Relation(Subreddit, Account)):
    _defaults = dict(encoded_permissions=None)
    _permission_class = None
    _cache = g.srmembercache
    _rel_cache = g.srmembercache

    @classmethod
    def _cache_prefix(cls):
        return "srmember:"

    @classmethod
    def _rel_cache_prefix(cls):
        return "srmemberrel:"

    def has_permission(self, perm):
        """Returns whether this member has explicitly been granted a permission.
        """
        return self.get_permissions().get(perm, False)

    def get_permissions(self):
        """Returns permission set for this member (or None if N/A)."""
        if not self._permission_class:
            raise NotImplementedError
        return self._permission_class.loads(self.encoded_permissions)

    def update_permissions(self, **kwargs):
        """Grants or denies permissions to this member.

        Args are named parameters with bool or None values (use None to disable
        granting or denying the permission). After calling this method,
        the relation will be _dirty until _commit is called.
        """
        if not self._permission_class:
            raise NotImplementedError
        perm_set = self._permission_class.loads(self.encoded_permissions)
        if perm_set is None:
            perm_set = self._permission_class()
        for k, v in kwargs.iteritems():
            if v is None:
                if k in perm_set:
                    del perm_set[k]
            else:
                perm_set[k] = v
        self.encoded_permissions = perm_set.dumps()

    def set_permissions(self, perm_set):
        """Assigns a permission set to this relation."""
        self.encoded_permissions = perm_set.dumps()

    def is_superuser(self):
        return self.get_permissions().is_superuser()


class FakeSRMember:
    """All-permission granting stub for SRMember, used by FakeSubreddits."""
    def __init__(self, permission_class):
        self.permission_class = permission_class

    def has_permission(self, perm):
        return True

    def get_permissions(self):
        return self.permission_class(all=True)

    def is_superuser(self):
        return True


Subreddit.__bases__ += (
    UserRel('moderator', SRMember,
            permission_class=ModeratorPermissionSet),
    UserRel('moderator_invite', SRMember,
            permission_class=ModeratorPermissionSet),
    UserRel('contributor', SRMember, disable_ids_fn=True),
    UserRel('banned', SRMember, disable_ids_fn=True),
    UserRel('muted', SRMember, disable_ids_fn=True),
    UserRel('wikibanned', SRMember),
    UserRel('wikicontributor', SRMember),
)


def add_legacy_subscriber(srs, user):
    srs = tup(srs)
    for sr in srs:
        rel = SRMember(sr, user, "subscriber")
        try:
            rel._commit()
        except CreationError:
            break


def remove_legacy_subscriber(sr, user):
    rels = SRMember._fast_query([sr], [user], "subscriber")
    rel = rels.get((sr, user, "subscriber"))
    if rel:
        rel._delete()


class SubredditTempBan(object):
    def __init__(self, sr, kind, victim, banner, duration):
        self.sr = sr._id36
        self._srname = sr.name
        self.who = victim._id36
        self._whoname = victim.name
        self.type = kind
        self.banner = banner._id36
        self.duration = duration

    @classmethod
    def schedule(cls, sr, kind, victim, banner, duration):
        info = {
            'sr': sr._id36,
            'who': victim._id36,
            'type': kind,
            'banner': banner._id36,
        }
        result = TryLaterBySubject.schedule(
            cls.cancel_rowkey(sr.name, kind),
            cls.cancel_colkey(victim.name),
            json.dumps(info),
            duration,
            trylater_rowkey=cls.schedule_rowkey(),
        )
        return {victim.name: result.keys()[0]}

    @classmethod
    def cancel_colkey(cls, name):
        return name

    @classmethod
    def cancel_rowkey(cls, name, type):
        return "srunban:%s:%s" % (name, type)

    @classmethod
    def schedule_rowkey(cls):
        return "srunban"

    @classmethod
    def search(cls, srname, bantype, subjects):
        results = TryLaterBySubject.search(cls.cancel_rowkey(srname, bantype),
                                           subjects)

        def convert_uuid_to_datetime(uu):
            return datetime.datetime.fromtimestamp(convert_uuid_to_time(uu),
                                                   g.tz)
        return {
            name: convert_uuid_to_datetime(uu)
                for name, uu in results.iteritems()
        }

    @classmethod
    def unschedule(cls, srname, victim_name, bantype):
        TryLaterBySubject.unschedule(
            cls.cancel_rowkey(srname, bantype),
            cls.cancel_colkey(victim_name),
            cls.schedule_rowkey(),
        )


@trylater_hooks.on('trylater.srunban')
def on_subreddit_unban(data):
    from r2.models.modaction import ModAction
    for blob in data.itervalues():
        baninfo = json.loads(blob)
        container = Subreddit._byID36(baninfo['sr'], data=True)
        victim = Account._byID36(baninfo['who'], data=True)
        banner = Account._byID36(baninfo['banner'], data=True)
        kind = baninfo['type']
        remove_function = getattr(container, 'remove_' + kind)
        new = remove_function(victim)
        g.log.info("Unbanned %s from %s", victim.name, container.name)

        if new:
            action = dict(
                banned='unbanuser',
                wikibanned='wikiunbanned',
            ).get(kind, None)
            ModAction.create(container, banner, action, target=victim,
                             description="was temporary")


class MutedAccountsBySubreddit(object):
    @classmethod
    def mute(cls, sr, user, muter, parent_message=None):
        NUM_HOURS = 72

        from r2.lib.db import queries
        from r2.models import Message, ModAction
        info = {
            'sr': sr._id36,
            'who': user._id36,
            'muter': muter._id36,
        }

        result = TryLaterBySubject.schedule(
            cls.cancel_rowkey(sr),
            cls.cancel_colkey(user),
            json.dumps(info),
            datetime.timedelta(hours=NUM_HOURS),
            trylater_rowkey=cls.schedule_rowkey(),
        )

        #if the user has interacted with the subreddit before, message them
        if user.has_interacted_with(sr):
            subject = "You have been muted from r/%(subredditname)s"
            subject %= dict(subredditname=sr.name)
            message = ("You have been [temporarily muted](%(muting_link)s) "
                "from r/%(subredditname)s. You will not be able to message "
                "the moderators of r/%(subredditname)s for %(num_hours)s hours.")
            message %= dict(
                muting_link="https://reddit.zendesk.com/hc/en-us/articles/205269739",
                subredditname=sr.name,
                num_hours=NUM_HOURS,
            )
            if parent_message:
                subject = parent_message.subject
                re = "re: "
                if not subject.startswith(re):
                    subject = re + subject

            item, inbox_rel = Message._new(muter, user, subject, message,
                request.ip, parent=parent_message, sr=sr, from_sr=True)
            queries.new_message(item, inbox_rel, update_modmail=True)

        return {user.name: result.keys()[0]}

    @classmethod
    def cancel_colkey(cls, user):
        return user.name

    @classmethod
    def cancel_rowkey(cls, subreddit):
        return "srmute:%s" % subreddit.name

    @classmethod
    def schedule_rowkey(cls):
        return "srmute"

    @classmethod
    def search(cls, subreddit, subjects):
        results = TryLaterBySubject.search(cls.cancel_rowkey(subreddit),
                                           subjects)

        return {
            name: datetime.datetime.fromtimestamp(convert_uuid_to_time(uu),
                    g.tz)
                for name, uu in results.iteritems()
        }

    @classmethod
    def unmute(cls, sr, user, automatic=False):
        from r2.models import ModAction

        TryLaterBySubject.unschedule(
            cls.cancel_rowkey(sr),
            cls.cancel_colkey(user),
            cls.schedule_rowkey(),
        )

        if automatic:
            unmuter = Account.system_user()
            ModAction.create(sr, unmuter, 'unmuteuser', target=user)


@trylater_hooks.on('trylater.srmute')
def unmute_hook(data):
    for blob in data.itervalues():
        muteinfo = json.loads(blob)
        subreddit = Subreddit._byID36(muteinfo['sr'], data=True)
        user = Account._byID36(muteinfo['who'], data=True)

        subreddit.remove_muted(user)
        MutedAccountsBySubreddit.unmute(subreddit, user, automatic=True)


class SubredditsActiveForFrontPage(tdb_cassandra.View):
    """Tracks which subreddits currently have valid frontpage posts.

    The front page's "hot" page only includes posts that are newer than
    g.HOT_PAGE_AGE, so there's no point including subreddits in it if they
    haven't had a post inside that period. Since we pick random subsets of
    users' subscriptions when they subscribe to more subreddits than we
    build the page from, this means that inactive subreddits can effectively
    "waste" some of these slots, since they may not have any posts that can
    possibly be added to the page.

    This CF will get an entry inserted for each subreddit whenever a new
    post is made in that subreddit, with a TTL equal to g.HOT_PAGE_AGE. We
    will then be able to query it to determine which subreddits don't have
    any posts recent enough to contribute to the front page, and exclude
    them from consideration for a user's front page set.
    """

    _use_db = True
    _connection_pool = "main"
    _ttl = datetime.timedelta(days=g.HOT_PAGE_AGE)
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
    }
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    ROWKEY = "1"

    @classmethod
    def mark_new_post(cls, subreddit):
        cls._set_values(cls.ROWKEY, {subreddit._id36: ""})

    @classmethod
    def filter_inactive_ids(cls, subreddit_ids):
        sr_id36s = [to36(sr_id) for sr_id in subreddit_ids]
        try:
            results = cls._cf.get(cls.ROWKEY, columns=sr_id36s)
        except tdb_cassandra.NotFoundException:
            results = {}

        num_filtered = len(subreddit_ids) - len(results)
        g.stats.simple_event("frontpage.filter_inactive", delta=num_filtered)

        return [int(sr_id36, 36) for sr_id36 in results.keys()]
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import OrderedDict
from datetime import datetime
from uuid import uuid1

from pycassa.system_manager import INT_TYPE, TIME_UUID_TYPE, UTF8_TYPE
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, N_

from r2.config import feature
from r2.lib.unicode import _force_unicode
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import Thing
from r2.lib.utils import Enum, to_datetime
from r2.models.subreddit import Subreddit, Frontpage


PROMOTE_STATUS = Enum("unpaid", "unseen", "accepted", "rejected",
                      "pending", "promoted", "finished", "edited_live")

PROMOTE_COST_BASIS = Enum('fixed_cpm', 'cpm', 'cpc',)


class PriorityLevel(object):
    name = ''
    _text = N_('')
    _description = N_('')
    default = False
    inventory_override = False

    def __repr__(self):
        return "<PriorityLevel %s>" % self.name

    @property
    def text(self):
        return _(self._text) if self._text else ''

    @property
    def description(self):
        return _(self._description) if self._description else ''


class HighPriority(PriorityLevel):
    name = 'high'
    _text = N_('highest')


class MediumPriority(PriorityLevel):
    name = 'standard'
    _text = N_('standard')
    default = True


class RemnantPriority(PriorityLevel):
    name = 'remnant'
    _text = N_('remnant')
    _description = N_('lower priority, impressions are not guaranteed')
    inventory_override = True


class HousePriority(PriorityLevel):
    name = 'house'
    _text = N_('house')
    _description = N_('non-CPM, displays in all unsold impressions')
    inventory_override = True


class AuctionPriority(PriorityLevel):
    name = 'auction'
    _text = N_('auction')
    _description = N_('auction priority; all self-serve are auction priority')
    inventory_override = True


HIGH, MEDIUM, REMNANT, HOUSE, AUCTION = (HighPriority(), MediumPriority(),
                                         RemnantPriority(), HousePriority(),
                                         AuctionPriority(),)
PROMOTE_PRIORITIES = OrderedDict((p.name, p) for p in (HIGH, MEDIUM, REMNANT,
                                                       HOUSE, AUCTION,))


def PROMOTE_DEFAULT_PRIORITY(context=None):
    if (context and (not feature.is_enabled('ads_auction') or
                     context.user_is_sponsor)):
        return MEDIUM
    else:
        return AUCTION

class Location(object):
    DELIMITER = '-'
    def __init__(self, country, region=None, metro=None):
        self.country = country or None
        self.region = region or None
        self.metro = metro or None

    def __repr__(self):
        return '<%s (%s/%s/%s)>' % (self.__class__.__name__, self.country,
                                    self.region, self.metro)

    def to_code(self):
        fields = [self.country, self.region, self.metro]
        return self.DELIMITER.join(i or '' for i in fields)

    @classmethod
    def from_code(cls, code):
        country, region, metro = [i or None for i in code.split(cls.DELIMITER)]
        return cls(country, region, metro)

    def contains(self, other):
        if not self.country:
            # self is set of all countries, it includes all possible
            # values of other.country
            return True
        elif not other or not other.country:
            # self is more specific than other
            return False
        else:
            # both self and other specify a country
            if self.country != other.country:
                # countries don't match
                return False
            else:
                # countries match
                if not self.metro:
                    # self.metro is set of all metros within country, it
                    # includes all possible values of other.metro
                    return True
                elif not other.metro:
                    # self is more specific than other
                    return False
                else:
                    return self.metro == other.metro

    def __eq__(self, other):
        if not isinstance(other, Location):
            return False

        return (self.country == other.country and
                self.region == other.region and
                self.metro == other.metro)

    def __ne__(self, other):
        return not self.__eq__(other)


def calc_impressions(total_budget_pennies, cpm_pennies):
    return int(total_budget_pennies / cpm_pennies * 1000)


NO_TRANSACTION = 0


class Collection(object):
    def __init__(self, name, sr_names, over_18=False, description=None,
            is_spotlight=False):
        self.name = name
        self.over_18 = over_18
        self.sr_names = sr_names
        self.description = description
        self.is_spotlight = is_spotlight

    @classmethod
    def by_name(cls, name):
        return CollectionStorage.get_collection(name)

    @classmethod
    def get_all(cls):
        """
        Return collections in this order:
        1. SFW/NSFW
        2. Spotlighted
        3. Alphabetical
        """
        all_collections = CollectionStorage.get_all()
        sorted_collections = sorted(all_collections, key=lambda collection:
            (collection.over_18, -collection.is_spotlight,
            collection.name.lower()))
        return sorted_collections

    def __repr__(self):
        return "<%s: %s>" % (self.__class__.__name__, self.name)


class CollectionStorage(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "column_name_class": UTF8_TYPE,
        "default_validation_class": UTF8_TYPE,
    }
    _compare_with = UTF8_TYPE
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    SR_NAMES_DELIM = '|'

    @classmethod
    def _from_columns(cls, name, columns):
        description = columns['description']
        sr_names = columns['sr_names'].split(cls.SR_NAMES_DELIM)
        over_18 = columns.get("over_18") == "True"
        is_spotlight = columns.get("is_spotlight") == "True"
        return Collection(name, sr_names, over_18=over_18,
            description=description, is_spotlight=is_spotlight)

    @classmethod
    def _to_columns(cls, description, srs, over_18, is_spotlight):
        columns = {
            'description': description,
            'sr_names': cls.SR_NAMES_DELIM.join(sr.name for sr in srs),
            'over_18': str(over_18),
            'is_spotlight': str(is_spotlight),
        }
        return columns

    @classmethod
    def set(cls, name, description, srs, over_18=False, is_spotlight=False):
        rowkey = name
        columns = cls._to_columns(description, srs, over_18, is_spotlight)
        cls._set_values(rowkey, columns)

    @classmethod
    def _set_attributes(cls, name, attributes):
        rowkey = name
        for key in attributes:
            if not hasattr(Collection.by_name(name), key):
                raise AttributeError('No attribute on %s called %s'
                    % (name, key))

        columns = attributes
        cls._set_values(rowkey, columns)

    @classmethod
    def set_over_18(cls, name, over_18):
        cls._set_attributes(name, {'over_18': str(over_18)})

    @classmethod
    def set_is_spotlight(cls, name, is_spotlight):
        cls._set_attributes(name, {'is_spotlight': str(is_spotlight)})

    @classmethod
    def get_collection(cls, name):
        if not name:
            return None

        rowkey = name
        try:
            columns = cls._cf.get(rowkey)
        except tdb_cassandra.NotFoundException:
            return None

        return cls._from_columns(name, columns)

    @classmethod
    def get_all(cls):
        ret = []
        for name, columns in cls._cf.get_range():
            ret.append(cls._from_columns(name, columns))
        return ret

    @classmethod
    def delete(cls, name):
        rowkey = name
        cls._cf.remove(rowkey)


class Target(object):
    """Wrapper around either a Collection or a Subreddit name"""
    def __init__(self, target):
        if isinstance(target, Collection):
            self.collection = target
            self.is_collection = True
        elif isinstance(target, basestring):
            self.subreddit_name = target
            self.is_collection = False
        else:
            raise ValueError("target must be a Collection or Subreddit name")

        # defer looking up subreddits, we might only need their names
        self._subreddits = None

    @property
    def over_18(self):
        if self.is_collection:
            return self.collection.over_18
        else:
            subreddits = self.subreddits_slow
            return subreddits and subreddits[0].over_18

    @property
    def subreddit_names(self):
        if self.is_collection:
            return self.collection.sr_names
        else:
            return [self.subreddit_name]

    @property
    def subreddits_slow(self):
        if self._subreddits is not None:
            return self._subreddits

        sr_names = self.subreddit_names
        srs = Subreddit._by_name(sr_names).values()
        self._subreddits = srs
        return srs

    def __eq__(self, other):
        if self.is_collection != other.is_collection:
            return False

        return set(self.subreddit_names) == set(other.subreddit_names)

    def __ne__(self, other):
        return not self.__eq__(other)

    @property
    def pretty_name(self):
        if self.is_collection:
            return _("collection: %(name)s") % {'name': self.collection.name}
        elif self.subreddit_name == Frontpage.name:
            return _("frontpage")
        else:
            return "/r/%s" % self.subreddit_name

    def __repr__(self):
        return "<%s: %s>" % (self.__class__.__name__, self.pretty_name)


class PromoCampaign(Thing):
    _cache = g.thingcache
    _defaults = dict(
        priority_name=PROMOTE_DEFAULT_PRIORITY().name,
        trans_id=NO_TRANSACTION,
        trans_ip=None,
        trans_ip_country=None,
        trans_billing_country=None,
        trans_country_match=None,
        location_code=None,
        platform='desktop',
        mobile_os_names=None,
        ios_device_names=None,
        ios_version_names=None,
        android_device_names=None,
        android_version_names=None,
        frequency_cap=None,
        has_served=False,
        paused=False,
        total_budget_pennies=0,
        cost_basis=PROMOTE_COST_BASIS.fixed_cpm,
        bid_pennies=g.default_bid_pennies,
        adserver_spent_pennies=0,
    )

    # special attributes that shouldn't set Thing data attributes because they
    # have special setters that set other data attributes
    _derived_attrs = (
        "location",
        "priority",
        "target",
        "mobile_os",
        "ios_devices",
        "ios_version_range",
        "android_devices",
        "android_version_range",
        "is_auction",
    )

    SR_NAMES_DELIM = '|'
    SUBREDDIT_TARGET = "subreddit"
    MOBILE_TARGET_DELIM = ','

    @classmethod
    def _cache_prefix(cls):
        return "campaign:"

    def __getattr__(self, attr):
        val = super(PromoCampaign, self).__getattr__(attr)

        if (attr == 'total_budget_pennies' and hasattr(self, 'bid') and
                not getattr(self, 'bid_migrated', False)):
            old_bid = int(super(PromoCampaign, self).__getattr__('bid') * 100)
            self.total_budget_pennies = old_bid
            self.bid_migrated = True
            return self.total_budget_pennies

        if (attr == 'bid_pennies' and hasattr(self, 'cpm') and
                not getattr(self, 'cpm_migrated', False)):
            old_cpm = super(PromoCampaign, self).__getattr__('cpm')
            self.bid_pennies = old_cpm
            self.cpm_migrated = True
            return self.bid_pennies

        if attr in ('start_date', 'end_date'):
            val = to_datetime(val)
            if not val.tzinfo:
                val = val.replace(tzinfo=g.tz)
        return val

    def __setattr__(self, attr, val, make_dirty=True):
        if attr in self._derived_attrs:
            object.__setattr__(self, attr, val)
        else:
            Thing.__setattr__(self, attr, val, make_dirty=make_dirty)

    def __getstate__(self):
        """
        Remove _target before returning object state for pickling.

        Thing objects are pickled for caching. The state of the object is
        obtained by calling the __getstate__ method. Remove the _target
        attribute because it may contain Subreddits or other non-trivial objects
        that shouldn't be included.

        """

        state = self.__dict__
        if "_target" in state:
            state = {k: v for k, v in state.iteritems() if k != "_target"}
        return state

    @property
    def is_auction(self):
        if (self.cost_basis is not PROMOTE_COST_BASIS.fixed_cpm):
            return True

        return False

    def priority_name_from_priority(self, priority):
        if not priority in PROMOTE_PRIORITIES.values():
            raise ValueError("%s is not a valid priority" % priority.name)
        return priority.name

    @classmethod
    def location_code_from_location(cls, location):
        return location.to_code() if location else None

    @classmethod
    def unpack_target(cls, target):
        """Convert a Target into attributes suitable for storage."""
        sr_names = target.subreddit_names
        target_sr_names = cls.SR_NAMES_DELIM.join(sr_names)
        target_name = (target.collection.name if target.is_collection
                                              else cls.SUBREDDIT_TARGET)
        return target_sr_names, target_name

    @classmethod
    def create(cls, link, target, start_date, end_date,
               frequency_cap, priority, location,
               platform, mobile_os, ios_devices, ios_version_range,
               android_devices, android_version_range, total_budget_pennies,
               cost_basis, bid_pennies):
        pc = PromoCampaign(
            link_id=link._id,
            start_date=start_date,
            end_date=end_date,
            trans_id=NO_TRANSACTION,
            owner_id=link.author_id,
            total_budget_pennies=total_budget_pennies,
            cost_basis=cost_basis,
            bid_pennies=bid_pennies,
        )
        pc.frequency_cap = frequency_cap
        pc.priority = priority
        pc.location = location
        pc.target = target
        pc.platform = platform
        pc.mobile_os = mobile_os
        pc.ios_devices = ios_devices
        pc.ios_version_range = ios_version_range
        pc.android_devices = android_devices
        pc.android_version_range = android_version_range
        pc._commit()
        return pc

    @classmethod
    def _by_link(cls, link_id):
        '''
        Returns an iterable of campaigns associated with link_id or an empty
        list if there are none.
        '''
        return cls._query(PromoCampaign.c.link_id == link_id, data=True)

    @classmethod
    def _by_user(cls, account_id):
        '''
        Returns an iterable of all campaigns owned by account_id or an empty
        list if there are none.
        '''
        return cls._query(PromoCampaign.c.owner_id == account_id, data=True)

    @property
    def ndays(self):
        return (self.end_date - self.start_date).days

    @property
    def impressions(self):
        if self.cost_basis == PROMOTE_COST_BASIS.fixed_cpm:
            return calc_impressions(self.total_budget_pennies, self.bid_pennies)

        return 0

    @property
    def priority(self):
        return PROMOTE_PRIORITIES[self.priority_name]

    @priority.setter
    def priority(self, priority):
        self.priority_name = self.priority_name_from_priority(priority)

    @property
    def location(self):
        if self.location_code is not None:
            return Location.from_code(self.location_code)
        else:
            return None

    @location.setter
    def location(self, location):
        self.location_code = self.location_code_from_location(location)

    @property
    def target(self):
        if hasattr(self, "_target"):
            return self._target

        sr_names = self.target_sr_names.split(self.SR_NAMES_DELIM)
        if self.target_name == self.SUBREDDIT_TARGET:
            sr_name = sr_names[0]
            target = Target(sr_name)
        else:
            collection = Collection(self.target_name, sr_names)
            target = Target(collection)

        self._target = target
        return target

    @target.setter
    def target(self, target):
        self.target_sr_names, self.target_name = self.unpack_target(target)

        # set _target so we don't need to lookup on subsequent access
        self._target = target

    def _mobile_target_getter(self, target):
        if not target:
            return None
        else:
            return target.split(self.MOBILE_TARGET_DELIM)

    def _mobile_target_setter(self, target_names):
        if not target_names:
            return None
        else:
            return self.MOBILE_TARGET_DELIM.join(target_names)

    @property
    def mobile_os(self):
        return self._mobile_target_getter(self.mobile_os_names)

    @mobile_os.setter
    def mobile_os(self, mobile_os_names):
        self.mobile_os_names = self._mobile_target_setter(mobile_os_names)

    @property
    def ios_devices(self):
        return self._mobile_target_getter(self.ios_device_names)

    @ios_devices.setter
    def ios_devices(self, ios_device_names):
        self.ios_device_names = self._mobile_target_setter(ios_device_names)

    @property
    def android_devices(self):
        return self._mobile_target_getter(self.android_device_names)

    @android_devices.setter
    def android_devices(self, android_device_names):
        self.android_device_names = self._mobile_target_setter(android_device_names)

    @property
    def ios_version_range(self):
        return self._mobile_target_getter(self.ios_version_names)

    @ios_version_range.setter
    def ios_version_range(self, ios_version_names):
        self.ios_version_names = self._mobile_target_setter(ios_version_names)

    @property
    def android_version_range(self):
        return self._mobile_target_getter(self.android_version_names)

    @android_version_range.setter
    def android_version_range(self, android_version_names):
        self.android_version_names = self._mobile_target_setter(android_version_names)

    @property
    def location_str(self):
        if not self.location:
            return ''
        elif self.location.region:
            country = self.location.country
            region = self.location.region
            if self.location.metro:
                metro_str = (g.locations[country]['regions'][region]
                             ['metros'][self.location.metro]['name'])
                return '/'.join([country, region, metro_str])
            else:
                region_name = g.locations[country]['regions'][region]['name']
                return ('%s, %s' % (region_name, country))
        else:
            return g.locations[self.location.country]['name']

    @property
    def is_paid(self):
        return self.trans_id != 0 or self.priority == HOUSE

    def is_freebie(self):
        return self.trans_id < 0

    def is_live_now(self):
        now = datetime.now(g.tz)
        return self.start_date < now and self.end_date > now

    @property
    def is_house(self):
       return self.priority == HOUSE

    @property
    def total_budget_dollars(self):
        return self.total_budget_pennies / 100.

    @property
    def bid_dollars(self):
        return self.bid_pennies / 100.

    def delete(self):
        self._deleted = True
        self._commit()


def backfill_campaign_targets():
    from r2.lib.db.operators import desc
    from r2.lib.utils import fetch_things2

    q = PromoCampaign._query(sort=desc("_date"), data=True)
    for campaign in fetch_things2(q):
        sr_name = campaign.sr_name or Frontpage.name
        campaign.target = Target(sr_name)
        campaign._commit()

class PromotionLog(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE

    @classmethod
    def _rowkey(cls, link):
        return link._fullname

    @classmethod
    def add(cls, link, text):
        name = c.user.name if c.user_is_loggedin else "<AUTOMATED>"
        now = datetime.now(g.tz).strftime("%Y-%m-%d %H:%M:%S")
        text = "[%s: %s] %s" % (name, now, text)
        rowkey = cls._rowkey(link)
        column = {uuid1(): _force_unicode(text)}
        cls._set_values(rowkey, column)
        return text

    @classmethod
    def get(cls, link):
        rowkey = cls._rowkey(link)
        try:
            row = cls._byID(rowkey)
        except tdb_cassandra.NotFound:
            return []
        tuples = sorted(row._values().items(), key=lambda t: t[0].time)
        return [t[1] for t in tuples]


class PromotionPrices(tdb_cassandra.View):
    """
    Check all the following potentially specially priced conditions:
    * metro level targeting
    * country level targeting (but not if the metro targeting is used)
    * collection targeting
    * frontpage targeting
    * subreddit targeting

    The price is the maximum price for all matching conditions. If no special
    conditions are met use the global price.

    """

    _use_db = True
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.ALL
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "column_name_class": UTF8_TYPE,
        "default_validation_class": INT_TYPE,
    }

    COLLECTION_DEFAULT = g.cpm_selfserve_collection.pennies
    SUBREDDIT_DEFAULT = g.cpm_selfserve.pennies
    COUNTRY_DEFAULT = g.cpm_selfserve_geotarget_country.pennies
    METRO_DEFAULT = g.cpm_selfserve_geotarget_metro.pennies

    @classmethod
    def _rowkey_and_column_from_target(cls, target):
        rowkey = column_name = None

        if isinstance(target, Target):
            if target.is_collection:
                rowkey = "COLLECTION"
                column_name = target.collection.name
            else:
                rowkey = "SUBREDDIT"
                column_name = target.subreddit_name

        if not rowkey or not column_name:
            raise ValueError("target must be Target")

        return rowkey, column_name

    @classmethod
    def _rowkey_and_column_from_location(cls, location):
        if not isinstance(location, Location):
            raise ValueError("location must be Location")

        if location.metro:
            rowkey = "METRO"
            # NOTE: the column_name will also be the key used in the frontend
            # to determine pricing
            column_name = ''.join(map(str, (location.country, location.metro)))
        else:
            rowkey = "COUNTRY"
            column_name = location.country
        return rowkey, column_name

    @classmethod
    def set_target_price(cls, target, cpm):
        rowkey, column_name = cls._rowkey_and_column_from_target(target)
        cls._cf.insert(rowkey, {column_name: cpm})

    @classmethod
    def set_location_price(cls, location, cpm):
        rowkey, column_name = cls._rowkey_and_column_from_location(location)
        cls._cf.insert(rowkey, {column_name: cpm})

    @classmethod
    def lookup_target_price(cls, target, default):
        rowkey, column_name = cls._rowkey_and_column_from_target(target)
        target_price = cls._lookup_price(rowkey, column_name)
        return target_price or default

    @classmethod
    def lookup_location_price(cls, location, default):
        rowkey, column_name = cls._rowkey_and_column_from_location(location)
        location_price = cls._lookup_price(rowkey, column_name)
        return location_price or default

    @classmethod
    def _lookup_price(cls, rowkey, column_name):
        try:
            columns = cls._cf.get(rowkey, columns=[column_name])
        except tdb_cassandra.NotFoundException:
            columns = {}

        return columns.get(column_name)

    @classmethod
    def get_price(cls, user, target, location):
        if user.selfserve_cpm_override_pennies:
            return user.selfserve_cpm_override_pennies

        prices = []

        # set location specific prices or use defaults
        if location and location.metro:
            metro_price = cls.lookup_location_price(location, cls.METRO_DEFAULT)
            prices.append(metro_price)
        elif location:
            country_price = cls.lookup_location_price(
                location, cls.COUNTRY_DEFAULT)
            prices.append(country_price)

        # set target specific prices or use default
        if (not target.is_collection and
                target.subreddit_name == Frontpage.name):
            # Frontpage is priced as a collection
            prices.append(cls.COLLECTION_DEFAULT)
        elif target.is_collection:
            collection_price = cls.lookup_target_price(
                target, cls.COLLECTION_DEFAULT)
            prices.append(collection_price)
        else:
            subreddit_price = cls.lookup_target_price(
                target, cls.SUBREDDIT_DEFAULT)
            prices.append(subreddit_price)

        return max(prices)

    @classmethod
    def get_price_dict(cls, user):
        if user.selfserve_cpm_override_pennies:
            r = {
                "COLLECTION": {},
                "SUBREDDIT": {},
                "COUNTRY": {},
                "METRO": {},
                "COLLECTION_DEFAULT": user.selfserve_cpm_override_pennies,
                "SUBREDDIT_DEFAULT": user.selfserve_cpm_override_pennies,
                "COUNTRY_DEFAULT": user.selfserve_cpm_override_pennies,
                "METRO_DEFAULT": user.selfserve_cpm_override_pennies,
            }
        else:
            r = {
                "COLLECTION": {},
                "SUBREDDIT": {},
                "COUNTRY": {},
                "METRO": {},
                "COLLECTION_DEFAULT": g.cpm_selfserve_collection.pennies,
                "SUBREDDIT_DEFAULT": g.cpm_selfserve.pennies,
                "COUNTRY_DEFAULT": g.cpm_selfserve_geotarget_country.pennies,
                "METRO_DEFAULT": g.cpm_selfserve_geotarget_metro.pennies,
            }

            try:
                collections = cls._cf.get("COLLECTION")
            except tdb_cassandra.NotFoundException:
                collections = {}

            try:
                subreddits = cls._cf.get("SUBREDDIT")
            except tdb_cassandra.NotFoundException:
                subreddits = {}

            try:
                countries = cls._cf.get("COUNTRY")
            except tdb_cassandra.NotFoundException:
                countries = {}

            try:
                metros = cls._cf.get("METRO")
            except tdb_cassandra.NotFoundException:
                metros = {}

            for name, cpm in collections.iteritems():
                r["COLLECTION"][name] = cpm

            for name, cpm in subreddits.iteritems():
                r["SUBREDDIT"][name] = cpm

            for name, cpm in countries.iteritems():
                r["COUNTRY"][name] = cpm

            for name, cpm in metros.iteritems():
                r["METRO"][name] = cpm

        return r
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime

from pylons import request
from pylons import app_globals as g
from sqlalchemy import (
    and_,
    Boolean,
    BigInteger,
    Column,
    DateTime,
    Date,
    distinct,
    Float,
    func as safunc,
    Integer,
    String,
)
from sqlalchemy.dialects.postgresql.base import PGInet as Inet
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.orm.exc import NoResultFound

from r2.lib.db.thing import Thing, NotFound
from r2.lib.memoize import memoize
from r2.lib.utils import Enum, to_date, tup
from r2.models.account import Account
from r2.models import Link, Frontpage


engine = g.dbm.get_engine('authorize')
# Allocate a session maker for communicating object changes with the back end  
Session = sessionmaker(autocommit = True, autoflush = True, bind = engine)
# allocate a SQLalchemy base class for auto-creation of tables based
# on class fields.  
# NB: any class that inherits from this class will result in a table
# being created, and subclassing doesn't work, hence the
# object-inheriting interface classes.
Base = declarative_base(bind = engine)

class Sessionized(object):
    """
    Interface class for wrapping up the "session" in the 0.5 ORM
    required for all database communication.  This allows subclasses
    to have a "query" and "commit" method that doesn't require
    managing of the session.
    """
    session = Session()

    def __init__(self, *a, **kw):
        """
        Common init used by all other classes in this file.  Allows
        for object-creation based on the __table__ field which is
        created by Base (further explained in _disambiguate_args).
        """
        for k, v in self._disambiguate_args(None, *a, **kw):
            setattr(self, k.name, v)
    
    @classmethod
    def _new(cls, *a, **kw):
        """
        Just like __init__, except the new object is committed to the
        db before being returned.
        """
        obj = cls(*a, **kw)
        obj._commit()
        return obj

    def _commit(self):
        """
        Commits current object to the db.
        """
        with self.session.begin():
            self.session.add(self)

    def _delete(self):
        """
        Deletes current object from the db. 
        """
        with self.session.begin():
            self.session.delete(self)

    @classmethod
    def query(cls, **kw):
        """
        Ubiquitous class-level query function. 
        """
        q = cls.session.query(cls)
        if kw:
            q = q.filter_by(**kw)
        return q

    @classmethod
    def _disambiguate_args(cls, filter_fn, *a, **kw):
        """
        Used in _lookup and __init__ to interpret *a as being a list
        of args to match columns in the same order as __table__.c

        For example, if a class Foo has fields a and b, this function
        allows the two to work identically:
        
        >>> foo = Foo(a = 'arg1', b = 'arg2')
        >>> foo = Foo('arg1', 'arg2')

        Additionally, this function invokes _make_storable on each of
        the values in the arg list (including *a as well as
        kw.values())

        """
        args = []
        if filter_fn is None:
            cols = cls.__table__.c
        else:
            cols = filter(filter_fn, cls.__table__.c)
        for k, v in zip(cols, a):
            if not kw.has_key(k.name):
                args.append((k, cls._make_storable(v)))
            else:
                raise TypeError,\
                      "got multiple arguments for '%s'" % k.name

        cols = dict((x.name, x) for x in cls.__table__.c)
        for k, v in kw.iteritems():
            if cols.has_key(k):
                args.append((cols[k], cls._make_storable(v)))
        return args

    @classmethod
    def _make_storable(self, val):
        if isinstance(val, Account):
            return val._id
        elif isinstance(val, Thing):
            return val._fullname
        else:
            return val

    @classmethod
    def _lookup(cls, multiple, *a, **kw):
        """
        Generates an executes a query where it matches *a to the
        primary keys of the current class's table.

        The primary key nature can be overridden by providing an
        explicit list of columns to search.

        This function is only a convenience function, and is called
        only by one() and lookup().
        """
        args = cls._disambiguate_args(lambda x: x.primary_key, *a, **kw)
        res = cls.query().filter(and_(*[k == v for k, v in args]))
        try:
            res = res.all() if multiple else res.one()
            # res.one() will raise NoResultFound, while all() will
            # return an empty list.  This will make the response
            # uniform
            if not res:
                raise NoResultFound
        except NoResultFound: 
            raise NotFound, "%s with %s" % \
                (cls.__name__,
                 ",".join("%s=%s" % x for x in args))
        return res

    @classmethod
    def lookup(cls, *a, **kw):
        """
        Returns all objects which match the kw list, or primary keys
        that match the *a.
        """
        return cls._lookup(True, *a, **kw)

    @classmethod
    def one(cls, *a, **kw):
        """
        Same as lookup, but returns only one argument. 
        """
        return cls._lookup(False, *a, **kw)

    @classmethod
    def add(cls, key, *a):
        try:
            cls.one(key, *a)
        except NotFound:
            cls(key, *a)._commit()
    
    @classmethod
    def delete(cls, key, *a):
        try:
            cls.one(key, *a)._delete()
        except NotFound:
            pass
    
    @classmethod
    def get(cls, key):
        try:
            return cls.lookup(key)
        except NotFound:
            return []

class CustomerID(Sessionized, Base):
    __tablename__  = "authorize_account_id"

    account_id    = Column(BigInteger, primary_key = True,
                           autoincrement = False)
    authorize_id  = Column(BigInteger)

    def __repr__(self):
        return "<AuthNetID(%s)>" % self.authorize_id

    @classmethod
    def set(cls, user, _id):
        try:
            existing = cls.one(user)
            existing.authorize_id = _id
            existing._commit()
        except NotFound:
            cls(user, _id)._commit()
    
    @classmethod
    def get_id(cls, user):
        try:
            return cls.one(user).authorize_id
        except NotFound:
            return

class PayID(Sessionized, Base):
    __tablename__ = "authorize_pay_id"

    account_id    = Column(BigInteger, primary_key = True,
                           autoincrement = False)
    pay_id        = Column(BigInteger, primary_key = True,
                           autoincrement = False)

    def __repr__(self):
        return "<%s(%d)>" % (self.__class__.__name__, self.authorize_id)

    @classmethod
    def get_ids(cls, key):
        return [int(x.pay_id) for x in cls.get(key)]

class Bid(Sessionized, Base):
    __tablename__ = "bids"

    STATUS        = Enum("AUTH", "CHARGE", "REFUND", "VOID")

    # will be unique from authorize
    transaction   = Column(BigInteger, primary_key = True,
                           autoincrement = False)

    # identifying characteristics
    account_id    = Column(BigInteger, index = True, nullable = False)
    pay_id        = Column(BigInteger, index = True, nullable = False)
    thing_id      = Column(BigInteger, index = True, nullable = False)

    # breadcrumbs
    ip            = Column(Inet)
    date          = Column(DateTime(timezone = True), default = safunc.now(),
                           nullable = False)

    # bid information:
    bid           = Column(Float, nullable = False)
    charge        = Column(Float)

    status        = Column(Integer, nullable = False,
                           default = STATUS.AUTH)

    # make this a primary key as well so that we can have more than
    # one freebie per campaign
    campaign      = Column(Integer, default = 0, primary_key = True)

    @classmethod
    def _new(cls, trans_id, user, pay_id, thing_id, bid, campaign = 0):
        bid = Bid(trans_id, user, pay_id, 
                  thing_id, getattr(request, 'ip', '0.0.0.0'), bid = bid,
                  campaign = campaign)
        bid._commit()
        return bid

#    @classmethod
#    def for_transactions(cls, transids):
#        transids = filter(lambda x: x != 0, transids)
#        if transids:
#            q = cls.query()
#            q = q.filter(or_(*[cls.transaction == i for i in transids]))
#            return dict((p.transaction, p) for p in q)
#        return {}

    def set_status(self, status):
        if self.status != status:
            self.status = status
            self._commit()

    def auth(self):
        self.set_status(self.STATUS.AUTH)

    def is_auth(self):
        return (self.status == self.STATUS.AUTH)

    def void(self):
        self.set_status(self.STATUS.VOID)

    def is_void(self):
        return (self.status == self.STATUS.VOID)

    def charged(self):
        self.charge = self.bid
        self.set_status(self.STATUS.CHARGE)
        self._commit()

    def is_charged(self):
        """
        Returns True if transaction has been charged with authorize.net or is
        a freebie with "charged" status.
        """
        return (self.status == self.STATUS.CHARGE)

    def refund(self, amount):
        current_charge = self.charge or self.bid    # needed if charged() not
                                                    # setting charge attr
        self.charge = current_charge - amount
        self.set_status(self.STATUS.REFUND)
        self._commit()

    def is_refund(self):
        return (self.status == self.STATUS.REFUND)

    @property
    def charge_amount(self):
        return self.charge or self.bid


class PromotionWeights(Sessionized, Base):
    __tablename__ = "promotion_weight"

    thing_name = Column(String, primary_key = True,
                        nullable = False, index = True)

    promo_idx    = Column(BigInteger, index = True, autoincrement = False,
                          primary_key = True)

    sr_name    = Column(String, primary_key = True,
                        nullable = True,  index = True)
    date       = Column(Date(), primary_key = True,
                        nullable = False, index = True)

    # because we might want to search by account
    account_id   = Column(BigInteger, index = True, autoincrement = False)

    # bid and weight should always be the same, but they don't have to be
    bid        = Column(Float, nullable = False)
    weight     = Column(Float, nullable = False)
    finished   = Column(Boolean)
    # NOTE: bid, weight, finished columns are not used

    @classmethod
    def filter_sr_name(cls, sr_name):
        # LEGACY: use empty string to indicate Frontpage
        return '' if sr_name == Frontpage.name else sr_name

    @classmethod
    def reschedule(cls, link, campaign):
        cls.delete(link, campaign)
        cls.add(link, campaign)

    @classmethod
    def add(cls, link, campaign):
        start_date = to_date(campaign.start_date)
        end_date = to_date(campaign.end_date)
        ndays = (end_date - start_date).days
        # note that end_date is not included
        dates = [start_date + datetime.timedelta(days=i) for i in xrange(ndays)]

        sr_names = campaign.target.subreddit_names
        sr_names = {cls.filter_sr_name(sr_name) for sr_name in sr_names}

        with cls.session.begin():
            for sr_name in sr_names:
                for date in dates:
                    obj = cls(
                        thing_name=link._fullname,
                        promo_idx=campaign._id,
                        sr_name=sr_name,
                        date=date,
                        account_id=link.author_id,
                        bid=0.,
                        weight=0.,
                        finished=False,
                    )
                    cls.session.add(obj)

    @classmethod
    def delete(cls, link, campaign):
        query = cls.query(thing_name=link._fullname, promo_idx=campaign._id)
        with cls.session.begin():
            for item in query:
                cls.session.delete(item)

    @classmethod
    def _filter_query(cls, query, start, end=None, link=None,
                      author_id=None, sr_names=None):
        start = to_date(start)

        if end:
            end = to_date(end)
            query = query.filter(and_(cls.date >= start, cls.date < end))
        else:
            query = query.filter(cls.date == start)

        if link:
            query = query.filter(cls.thing_name == link._fullname)

        if author_id:
            query = query.filter(cls.account_id == author_id)

        if sr_names:
            sr_names = [cls.filter_sr_name(sr_name) for sr_name in sr_names]
            query = query.filter(cls.sr_name.in_(sr_names))

        return query

    @classmethod
    def get_campaign_ids(cls, start, end=None, link=None, author_id=None,
                         sr_names=None):
        query = cls.session.query(distinct(cls.promo_idx))
        query = cls._filter_query(query, start, end, link, author_id, sr_names)
        return {i[0] for i in query}

    @classmethod
    def get_link_names(cls, start, end=None, link=None, author_id=None,
                       sr_names=None):
        query = cls.session.query(distinct(cls.thing_name))
        query = cls._filter_query(query, start, end, link, author_id, sr_names)
        return {i[0] for i in query}


# do all the leg work of creating/connecting to tables
if g.db_create_tables:
    Base.metadata.create_all()

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from account import *
from ip import *
from link import *
from listing import *
from vote import *
from report import *
from rules import *
from subreddit import *
from flair import *
from award import *
from bidding import *
from mail_queue import Email, has_opted_out, opt_count
from gold import *
from admintools import *
from token import *
from modaction import *
from promo import *

# r2.models.builder will import other models, so pulling its classes/vars into
# r2.models needs to be done last to ensure that the models it depends
# on are already loaded.
from builder import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from itertools import product

from pycassa.types import IntegerType

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup


class PromoMetrics(tdb_cassandra.View):
    '''
    Cassandra data store for promotion metrics. Used for inventory prediction.

    Usage:
      # set metric value for many subreddits at once
      > PromoMetrics.set('min_daily_pageviews.GET_listing',
                          {'funny': 63432, 'pics': 48829, 'books': 4})

      # get metric value for one subreddit
      > res = PromoMetrics.get('min_daily_pageviews.GET_listing', 'funny')
      {'funny': 1234}

      # get metric value for many subreddits
      > res = PromoMetrics.get('min_daily_pageviews.GET_listing',
                               ['funny', 'pics'])
      {'funny':1234, 'pics':4321}

      # get metric values for all subreddits
      > res = PromoMetrics.get('min_daily_pageviews.GET_listing')
    '''
    _use_db = True
    _value_type = 'int'
    _fetch_all_columns = True

    @classmethod
    def get(cls, metric_name, sr_names=None):
        sr_names = tup(sr_names)
        try:
            metric = cls._byID(metric_name, properties=sr_names)
            return metric._values()  # might have additional values
        except tdb_cassandra.NotFound:
            return {}

    @classmethod
    def set(cls, metric_name, values_by_sr):
        cls._set_values(metric_name, values_by_sr)


class LocationPromoMetrics(tdb_cassandra.View):
    _use_db = True
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _read_consistency_level = tdb_cassandra.CL.ONE
    _extra_schema_creation_args = {
        "default_validation_class": IntegerType(),
    }

    @classmethod
    def _rowkey(cls, location):
        fields = [location.country, location.region, location.metro]
        return '-'.join(map(lambda field: field or '', fields))

    @classmethod
    def _column_name(cls, sr):
        return sr.name

    @classmethod
    def get(cls, srs, locations):
        srs, srs_is_single = tup(srs, ret_is_single=True)
        locations, locations_is_single = tup(locations, ret_is_single=True)
        is_single = srs_is_single and locations_is_single

        rowkeys = {location: cls._rowkey(location) for location in locations}
        columns = {sr: cls._column_name(sr) for sr in srs}
        rcl = cls._read_consistency_level
        metrics = cls._cf.multiget(rowkeys.values(), columns.values(),
                                   read_consistency_level=rcl)
        ret = {}

        for sr, location in product(srs, locations):
            rowkey = rowkeys[location]
            column = columns[sr]
            impressions = metrics.get(rowkey, {}).get(column, 0)
            ret[(sr, location)] = impressions

        if is_single:
            return ret.values()[0]
        else:
            return ret

    @classmethod
    def set(cls, metrics):
        wcl = cls._write_consistency_level
        with cls._cf.batch(write_consistency_level=wcl) as b:
            for location, sr, impressions in metrics:
                rowkey = cls._rowkey(location)
                column = {cls._column_name(sr): impressions}
                b.insert(rowkey, column)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict, namedtuple
from copy import deepcopy
import datetime
import heapq
from random import shuffle
import time

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.config import feature
from r2.config.extensions import API_TYPES, RSS_TYPES
from r2.lib import hooks
from r2.lib.comment_tree import (
    conversation,
    get_comment_scores,
    moderator_messages,
    sr_conversation,
    subreddit_messages,
    tree_sort_fn,
    user_messages,
)
from r2.lib.wrapped import Wrapped
from r2.lib.db import operators, tdb_cassandra
from r2.lib.filters import _force_unicode
from r2.lib.jsontemplates import get_trimmed_sr_dicts
from r2.lib.utils import (
    long_datetime,
    SimpleSillyStub,
    Storage,
    to36,
    tup,
)

from r2.models import (
    Account,
    Comment,
    CommentSavesByAccount,
    Link,
    LinkSavesByAccount,
    Message,
    MoreChildren,
    MoreMessages,
    MoreRecursion,
    Subreddit,
    Thing,
    wiki,
)
from r2.models.admintools import ip_span
from r2.models.comment_tree import CommentTree
from r2.models.flair import Flair
from r2.models.listing import Listing
from r2.models.vote import Vote


EXTRA_FACTOR = 1.5
MAX_RECURSION = 10


class InconsistentCommentTreeError(Exception):
  pass


class Builder(object):
    def __init__(self, wrap=Wrapped, prewrap_fn=None, keep_fn=None, stale=True,
                 spam_listing=False):
        self.wrap = wrap
        self.prewrap_fn = prewrap_fn
        self.keep_fn = keep_fn
        self.stale = stale
        self.spam_listing = spam_listing

    def keep_item(self, item):
        if self.keep_fn:
            return self.keep_fn(item)
        else:
            return item.keep_item(item)

    def wrap_items(self, items):
        from r2.lib.db import queries
        from r2.lib.template_helpers import (
            add_friend_distinguish,
            add_admin_distinguish,
            add_moderator_distinguish,
            add_cakeday_distinguish,
            add_special_distinguish,
        )

        user = c.user if c.user_is_loggedin else None
        aids = set(l.author_id for l in items if hasattr(l, 'author_id')
                   and l.author_id is not None)

        authors = Account._byID(aids, data=True, stale=self.stale)
        now = datetime.datetime.now(g.tz)
        cakes = {a._id for a in authors.itervalues()
                       if a.cake_expiration and a.cake_expiration >= now}
        friend_rels = user.friend_rels() if user and user.gold else {}

        subreddits = Subreddit.load_subreddits(items, stale=self.stale)
        can_ban_set = set()

        if user:
            for sr_id, sr in subreddits.iteritems():
                if sr.can_ban(user):
                    can_ban_set.add(sr_id)

        #get likes/dislikes
        try:
            likes = queries.get_likes(user, items)
        except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
            g.log.warning("Cassandra vote lookup failed: %r", e)
            likes = {}

        types = {}
        wrapped = []

        for item in items:
            w = self.wrap(item)
            wrapped.append(w)
            # add for caching (plus it should be bad form to use _
            # variables in templates)
            w.fullname = item._fullname
            types.setdefault(w.render_class, []).append(w)

            w.author = None
            w.friend = False
            w.enemy = False

            w.distinguished = None
            if hasattr(item, "distinguished"):
                if item.distinguished == 'yes':
                    w.distinguished = 'moderator'
                elif item.distinguished in ('admin', 'special',
                                            'gold', 'gold-auto'):
                    w.distinguished = item.distinguished

            if getattr(item, "author_id", None):
                w.author = authors.get(item.author_id)

            if hasattr(item, "sr_id") and item.sr_id is not None:
                w.subreddit = subreddits[item.sr_id]

            distinguish_attribs_list = []

            # if display_author exists, then w.author is unknown to the
            # receiver, so we can't check for friend or cakeday
            author_is_hidden = hasattr(item, 'display_author')

            if user and w.author:
                # the enemy flag will trigger keep_item to fail in Printable
                if w.author._id in user.enemies:
                    w.enemy = True

                elif not author_is_hidden and w.author._id in user.friends:
                    w.friend = True
                    if item.author_id in friend_rels:
                        note = getattr(friend_rels[w.author._id], "note", None)
                    else:
                        note = None
                    add_friend_distinguish(distinguish_attribs_list, note)

            if (w.distinguished == 'admin' and w.author):
                add_admin_distinguish(distinguish_attribs_list)

            if w.distinguished == 'moderator':
                add_moderator_distinguish(distinguish_attribs_list, w.subreddit)

            if w.distinguished == 'special':
                add_special_distinguish(distinguish_attribs_list, w.author)

            if (not author_is_hidden and
                    w.author and w.author._id in cakes and not c.profilepage):
                add_cakeday_distinguish(distinguish_attribs_list, w.author)

            w.attribs = distinguish_attribs_list

            user_vote_dir = likes.get((user, item))

            if user_vote_dir == Vote.DIRECTIONS.up:
                w.likes = True
            elif user_vote_dir == Vote.DIRECTIONS.down:
                w.likes = False
            else:
                w.likes = None

            w.upvotes = item._ups
            w.downvotes = item._downs

            total_votes = max(item.num_votes, 1)
            w.upvote_ratio = float(item._ups) / total_votes

            w.is_controversial = self._is_controversial(w)

            w.score = w.upvotes - w.downvotes

            if user_vote_dir == Vote.DIRECTIONS.up:
                base_score = w.score - 1
            elif user_vote_dir == Vote.DIRECTIONS.down:
                base_score = w.score + 1
            else:
                base_score = w.score

            # store the set of available scores based on the vote
            # for ease of i18n when there is a label
            w.voting_score = [(base_score + x - 1) for x in range(3)]

            w.deleted = item._deleted

            w.link_notes = []

            if c.user_is_admin:
                if item._deleted:
                    w.link_notes.append("deleted link")
                if getattr(item, "verdict", None):
                    if not item.verdict.endswith("-approved"):
                        w.link_notes.append(w.verdict)

            if c.user_is_admin and getattr(item, 'ip', None):
                w.ip_span = ip_span(item.ip)
            else:
                w.ip_span = ""

            # if the user can ban things on a given subreddit, or an
            # admin, then allow them to see that the item is spam, and
            # add the other spam-related display attributes
            w.show_reports = False
            w.show_spam    = False
            w.can_ban      = False
            w.use_big_modbuttons = self.spam_listing

            if (c.user_is_admin
                or (user
                    and hasattr(item,'sr_id')
                    and item.sr_id in can_ban_set)):
                if getattr(item, "promoted", None) is None:
                    w.can_ban = True

                ban_info = getattr(item, 'ban_info', {})
                w.unbanner = ban_info.get('unbanner')

                if item._spam:
                    w.show_spam = True
                    w.moderator_banned = ban_info.get('moderator_banned', False)
                    w.autobanned = ban_info.get('auto', False)
                    w.banner = ban_info.get('banner')
                    w.banned_at = ban_info.get("banned_at", None)
                    if ban_info.get('note', None) and w.banner:
                        w.banner += ' (%s)' % ban_info['note']
                    w.use_big_modbuttons = True
                    if getattr(w, "author", None) and w.author._spam:
                        w.show_spam = "author"

                    if c.user == w.author and c.user._spam:
                        w.show_spam = False
                        w._spam = False
                        w.use_big_modbuttons = False

                elif (getattr(item, 'reported', 0) > 0
                      and (not getattr(item, 'ignore_reports', False) or
                           c.user_is_admin)):
                    w.show_reports = True
                    w.use_big_modbuttons = True

                    # report_count isn't used in any template, but add it to
                    # the Wrapped so it's pulled into the render cache key in
                    # instances when reported will be used in the template
                    w.report_count = item.reported

            w.approval_checkmark = None
            if w.can_ban:
                verdict = getattr(w, "verdict", None)
                if verdict in ('admin-approved', 'mod-approved'):
                    approver = None
                    approval_time = None
                    baninfo = getattr(w, "ban_info", None)
                    if baninfo:
                        approver = baninfo.get("unbanner", None)
                        approval_time = baninfo.get("unbanned_at", None)

                    approver = approver or _("a moderator")

                    if approval_time:
                        text = _("approved by %(who)s at %(when)s") % {
                                    "who": approver,
                                    "when": long_datetime(approval_time)}
                    else:
                        text = _("approved by %s") % approver
                    w.approval_checkmark = text

            hooks.get_hook("builder.wrap_items").call(item=item, wrapped=w)

        # recache the user object: it may be None if user is not logged in,
        # whereas now we are happy to have the UnloggedUser object
        user = c.user
        for cls in types.keys():
            cls.add_props(user, types[cls])

        return wrapped

    def get_items(self):
        raise NotImplementedError

    def convert_items(self, items):
        """Convert a list of items to the desired output format"""
        if self.prewrap_fn:
            items = [self.prewrap_fn(i) for i in items]

        if self.wrap:
            items = self.wrap_items(items)
        else:
            # make a copy of items so the converted items can be mutated without
            # changing the original items
            items = items[:]
        return items

    def valid_after(self, after):
        """
        Return whether `after` could ever be shown to the user.

        Necessary because an attacker could use info about the presence
        and position of `after` within a listing to leak info about `after`s
        that the attacker could not normally access.
        """
        w = self.convert_items((after,))[0]
        return not self.must_skip(w)

    def item_iter(self, a):
        """Iterates over the items returned by get_items"""
        raise NotImplementedError

    def must_skip(self, item):
        """whether or not to skip any item regardless of whether the builder
        was contructed with skip=true"""
        user = c.user if c.user_is_loggedin else None

        if hasattr(item, "promoted") and item.promoted is not None:
            return False

        # can_view_slow only exists for Messages, but checking was_comment
        # is also necessary because items may also be comments that are being
        # viewed from the inbox page where their render class is overridden.
        # This check needs to be done before looking at whether they can view
        # the subreddit, or modmail to/from private subreddits that the user
        # doesn't have access to will be skipped.
        if hasattr(item, 'can_view_slow') and not item.was_comment:
            return not item.can_view_slow()

        if hasattr(item, 'subreddit') and not item.subreddit.can_view(user):
            return True

    def _is_controversial(self, wrapped):
        """Determine if an item meets all criteria to display as controversial."""

        # A sample-size threshold before posts can be considered controversial
        num_votes = wrapped.upvotes + wrapped.downvotes
        if num_votes < g.live_config['cflag_min_votes']:
            return False

        # If an item falls within a boundary of upvote ratios, it's controversial
        # e.g. 0.4 < x < 0.6
        lower = g.live_config['cflag_lower_bound']
        upper = g.live_config['cflag_upper_bound']
        if lower <= wrapped.upvote_ratio <= upper:
            return True

        return False


class QueryBuilder(Builder):
    def __init__(self, query, skip=False, num=None, sr_detail=None, count=0,
                 after=None, reverse=False, **kw):
        self.query = query
        self.skip = skip
        self.num = num
        self.sr_detail = sr_detail
        self.start_count = count or 0
        self.after = after
        self.reverse = reverse
        Builder.__init__(self, **kw)

    def __repr__(self):
        return "<%s(%r)>" % (self.__class__.__name__, self.query)

    def item_iter(self, a):
        """Iterates over the items returned by get_items"""
        for i in a[0]:
            yield i

    def init_query(self):
        q = self.query

        if self.reverse:
            q._reverse()

        q._data = True
        self.orig_rules = deepcopy(q._rules)
        if self.after:
            q._after(self.after)

    def fetch_more(self, last_item, num_have):
        done = False
        q = self.query
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                #will cause the loop below to break
                return True, None
            else:
                #q = self.query
                #check last_item if we have a num because we may need to iterate
                if last_item:
                    q._rules = deepcopy(self.orig_rules)
                    q._after(last_item)
                    last_item = None
                q._limit = max(int(num_need * EXTRA_FACTOR), self.num // 2, 1)
        else:
            done = True
        new_items = list(q)

        return done, new_items

    def get_items(self):
        self.init_query()

        num_have = 0
        done = False
        items = []
        count = self.start_count
        fetch_after = None
        loopcount = 0
        stopped_early = False

        while not done:
            done, fetched_items = self.fetch_more(fetch_after, num_have)

            #log loop
            loopcount += 1
            if loopcount == 20:
                done = True
                stopped_early = True

            #no results, we're done
            if not fetched_items:
                break

            #if fewer results than we wanted, we're done
            elif self.num and len(fetched_items) < self.num - num_have:
                done = True

            # Wrap the fetched items if necessary
            new_items = self.convert_items(fetched_items)

            #skip and count
            while new_items and (not self.num or num_have < self.num):
                i = new_items.pop(0)

                if not (self.must_skip(i) or
                        self.skip and not self.keep_item(i)):
                    items.append(i)
                    num_have += 1
                    count = count - 1 if self.reverse else count + 1
                    if self.wrap:
                        i.num = count

            fetch_after = fetched_items[-1]

        # Is there a next page or not?
        have_next = True
        if self.num and num_have < self.num and not stopped_early:
            have_next = False

        if getattr(self, "sr_detail", False) and c.render_style in API_TYPES:
            items_by_subreddit = defaultdict(list)
            for item in items:
                if isinstance(item.lookups[0], Link):
                    items_by_subreddit[item.subreddit].append(item)

            srs = items_by_subreddit.keys()
            sr_dicts = get_trimmed_sr_dicts(srs, c.user)

            for sr, sr_items in items_by_subreddit.iteritems():
                sr_detail = sr_dicts[sr._id]
                for item in sr_items:
                    item.sr_detail = sr_detail

        # Make sure first_item and last_item refer to things in items
        # NOTE: could retrieve incorrect item if there were items with
        # duplicate _id
        first_item = None
        last_item = None
        if items:
            if self.start_count > 0:
                first_item = items[0]
            last_item = items[-1]

        if self.reverse:
            items.reverse()
            last_item, first_item = first_item, have_next and last_item
            before_count = count
            after_count = self.start_count - 1
        else:
            last_item = have_next and last_item
            before_count = self.start_count + 1
            after_count = count

        #listing is expecting (things, prev, next, bcount, acount)
        return (items,
                first_item,
                last_item,
                before_count,
                after_count)

class IDBuilder(QueryBuilder):
    def thing_lookup(self, names):
        return Thing._by_fullname(names, data=True, return_dict=False,
                                  stale=self.stale)

    def init_query(self):
        names = list(tup(self.query))

        after = self.after._fullname if self.after else None

        self.names = self._get_after(names,
                                     after,
                                     self.reverse)

    @staticmethod
    def _get_after(l, after, reverse):
        names = list(l)

        if reverse:
            names.reverse()

        if after:
            try:
                i = names.index(after)
            except ValueError:
                names = ()
            else:
                names = names[i + 1:]

        return names

    def fetch_more(self, last_item, num_have):
        done = False
        names = self.names
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                return True, None
            else:
                if last_item:
                    last_item = None
                slice_size = max(int(num_need * EXTRA_FACTOR), self.num // 2, 1)
        else:
            slice_size = len(names)
            done = True

        self.names, new_names = names[slice_size:], names[:slice_size]
        new_items = self.thing_lookup(new_names)
        return done, new_items


class ActionBuilder(IDBuilder):
    def init_query(self):
        self.actions = {}
        ids = []
        for id, date, action in self.query:
            ids.append(id)
            self.actions[id] = action
        self.query = ids

        super(ActionBuilder, self).init_query()

    def thing_lookup(self, names):
        items = super(ActionBuilder, self).thing_lookup(names)

        for item in items:
            if item._fullname in self.actions:
                item.action_type = self.actions[item._fullname]
        return items


class CampaignBuilder(IDBuilder):
    """Build on a list of PromoTuples."""
    @staticmethod
    def _get_after(promo_tuples, after, reverse):
        promo_tuples = list(promo_tuples)

        if not after:
            return promo_tuples

        if reverse:
            promo_tuples.reverse()

        fullname_to_index = {pt.link: i for i, pt in enumerate(promo_tuples)}
        try:
            i = fullname_to_index[after]
        except KeyError:
            promo_tuples = ()
        else:
            promo_tuples = promo_tuples[i + 1:]

        return promo_tuples

    def thing_lookup(self, tuples):
        links = Link._by_fullname([t.link for t in tuples], data=True,
                                  return_dict=True, stale=self.stale)

        return [Storage({'thing': links[t.link],
                         '_id': links[t.link]._id,
                         '_fullname': links[t.link]._fullname,
                         'weight': t.weight,
                         'campaign': t.campaign}) for t in tuples]

    def wrap_items(self, items):
        links = [i.thing for i in items]
        wrapped = IDBuilder.wrap_items(self, links)
        by_link = defaultdict(list)
        for w in wrapped:
            by_link[w._fullname].append(w)

        ret = []
        for i in items:
            w = by_link[i.thing._fullname].pop()
            w.campaign = i.campaign
            w.weight = i.weight
            ret.append(w)

        return ret

    def valid_after(self, after):
        # CampaignBuilder has special wrapping logic to operate on
        # PromoTuples and PromoCampaigns. `after` is just a Link, so bypass
        # the special wrapping logic and use the base class.
        if self.prewrap_fn:
            after = self.prewrap_fn(after)
        if self.wrap:
            after = Builder.wrap_items(self, (after,))[0]
        return not self.must_skip(after)


class ModActionBuilder(QueryBuilder):
    def wrap_items(self, items):
        wrapped = []
        by_render_class = defaultdict(list)

        for item in items:
            w = self.wrap(item)
            wrapped.append(w)
            w.fullname = item._fullname
            by_render_class[w.render_class].append(w)

        for render_class, _items in by_render_class.iteritems():
            render_class.add_props(c.user, _items)

        return wrapped


class SimpleBuilder(IDBuilder):
    def thing_lookup(self, names):
        return names

    def init_query(self):
        items = list(tup(self.query))

        if self.reverse:
            items.reverse()

        if self.after:
            for i, item in enumerate(items):
                if item._id == self.after:
                    self.names = items[i + 1:]
                    break
            else:
                self.names = ()
        else:
            self.names = items

    def get_items(self):
        items, prev_item, next_item, bcount, acount = IDBuilder.get_items(self)
        prev_item_id = prev_item._id if prev_item else None
        next_item_id = next_item._id if next_item else None
        return (items, prev_item_id, next_item_id, bcount, acount)


class SearchBuilder(IDBuilder):
    def __init__(self, query, skip_deleted_authors=True, **kw):
        self.skip_deleted_authors = skip_deleted_authors
        IDBuilder.__init__(self, query, **kw)

    def init_query(self):
        self.skip = True

        self.start_time = time.time()

        self.results = self.query.run()
        names = list(self.results.docs)
        self.total_num = self.results.hits
        self.subreddit_facets = self.results.subreddit_facets

        after = self.after._fullname if self.after else None

        self.names = self._get_after(names,
                                     after,
                                     self.reverse)

    def keep_item(self, item):
        # doesn't use the default keep_item because we want to keep
        # things that were voted on, even if they've chosen to hide
        # them in normal listings
        user = c.user if c.user_is_loggedin else None

        if item._spam or item._deleted:
            return False
        # If checking (wrapped) links, filter out banned subreddits
        elif hasattr(item, 'subreddit') and item.subreddit.spammy():
            return False
        elif (hasattr(item, 'subreddit') and
              not c.user_is_admin and
              not item.subreddit.is_exposed(user)):
            return False
        elif (self.skip_deleted_authors and
              getattr(item, "author", None) and item.author._deleted):
            return False
        elif isinstance(item.lookups[0], Subreddit) and not item.is_exposed(user):
            return False

        # show NSFW to API and RSS users unless obey_over18=true
        is_api_or_rss = (c.render_style in API_TYPES
                         or c.render_style in RSS_TYPES)
        if is_api_or_rss:
            include_over18 = not c.obey_over18 or c.over18
        elif feature.is_enabled('safe_search'):
            include_over18 = c.over18
        else:
            include_over18 = True

        is_nsfw = (item.over_18 or
            (hasattr(item, 'subreddit') and item.subreddit.over_18))
        if is_nsfw and not include_over18:
            return False

        return True


class WikiRevisionBuilder(QueryBuilder):
    show_extended = True

    def __init__(self, revisions, user=None, sr=None, page=None, **kw):
        self.user = user
        self.sr = sr
        self.page = page
        QueryBuilder.__init__(self, revisions, **kw)

    def wrap_items(self, items):
        from r2.lib.validator.wiki import this_may_revise
        types = {}
        wrapped = []
        extended = self.show_extended and c.is_wiki_mod
        extended = extended and this_may_revise(self.page)
        for item in items:
            w = self.wrap(item)
            w.show_extended = extended
            w.show_compare = self.show_extended
            types.setdefault(w.render_class, []).append(w)
            wrapped.append(w)

        user = c.user
        for cls in types.keys():
            cls.add_props(user, types[cls])

        return wrapped

    def must_skip(self, item):
        return item.admin_deleted and not c.user_is_admin

    def keep_item(self, item):
        from r2.lib.validator.wiki import may_view
        return ((not item.is_hidden) and
                may_view(self.sr, self.user, item.wikipage))

class WikiRecentRevisionBuilder(WikiRevisionBuilder):
    show_extended = False

    def must_skip(self, item):
        if WikiRevisionBuilder.must_skip(self, item):
            return True
        item_age = datetime.datetime.now(g.tz) - item.date
        return item_age.days >= wiki.WIKI_RECENT_DAYS


CommentTuple = namedtuple("CommentTuple",
    ["comment_id", "depth", "parent_id", "num_children", "child_ids"])


MissingChildrenTuple = namedtuple("MissingChildrenTuple",
    ["num_children", "child_ids"])


class CommentOrdererBase(object):
    def __init__(self, link, sort, max_comments, max_depth, timer):
        self.link = link
        self.sort = sort
        self.rev_sort = isinstance(sort, operators.desc)
        self.max_comments = max_comments
        self.max_depth = max_depth
        self.timer = timer

    def get_comment_order(self):
        """Return a list of CommentTuples in tree insertion order.

        Also add a MissingChildrenTuple to the end of the list if there
        are missing root level comments.

        """

        with g.stats.get_timer('comment_tree.get.1') as comment_tree_timer:
            comment_tree = CommentTree.by_link(self.link, comment_tree_timer)
            sort_name = self.sort.col
            sorter = get_comment_scores(
                self.link, sort_name, comment_tree.cids, comment_tree_timer)
            comment_tree_timer.intermediate('get_scores')

        if isinstance(self.sort, operators.shuffled):
            # randomize the scores of top level comments
            top_level_ids = comment_tree.tree.get(None, [])
            top_level_scores = [
                sorter[comment_id] for comment_id in top_level_ids]
            shuffle(top_level_scores)
            for i, comment_id in enumerate(top_level_ids):
                sorter[comment_id] = top_level_scores[i]

        self.timer.intermediate("load_storage")

        comment_tree = self.modify_comment_tree(comment_tree)
        self.timer.intermediate("modify_comment_tree")

        initial_candidates, offset_depth = self.get_initial_candidates(comment_tree)

        comment_tuples = self.get_initial_comment_list(comment_tree)
        if comment_tuples:
            # some comments have bypassed the sorting/inserting process, remove
            # them from `initial_candidates` so they won't be inserted again
            comment_tuple_ids = {
                comment_tuple.comment_id for comment_tuple in comment_tuples}
            initial_candidates = [
                comment_id for comment_id in initial_candidates
                if comment_id not in comment_tuple_ids
            ]

        candidates = []
        self.update_candidates(candidates, sorter, initial_candidates)
        self.timer.intermediate("pick_candidates")

        # choose which comments to show
        while candidates and len(comment_tuples) < self.max_comments:
            sort_val, comment_id = heapq.heappop(candidates)
            if comment_id not in comment_tree.cids:
                continue

            comment_depth = comment_tree.depth[comment_id] - offset_depth
            if comment_depth >= self.max_depth:
                continue

            child_ids = comment_tree.tree.get(comment_id, [])

            comment_tuples.append(CommentTuple(
                comment_id=comment_id,
                depth=comment_depth,
                parent_id=comment_tree.parents[comment_id],
                num_children=comment_tree.num_children[comment_id],
                child_ids=child_ids,
            ))

            child_depth = comment_depth + 1
            if child_depth < self.max_depth:
                self.update_candidates(candidates, sorter, child_ids)

        self.timer.intermediate("pick_comments")

        # add all not-selected top level comments to the comment_tuples list
        # so we can make MoreChildren for them later
        top_level_not_visible = {
            comment_id for sort_val, comment_id in candidates
            if comment_tree.depth.get(comment_id, 0) - offset_depth == 0
        }

        if top_level_not_visible:
            num_children_not_visible = sum(
                1 + comment_tree.num_children[comment_id]
                for comment_id in top_level_not_visible
            )
            comment_tuples.append(MissingChildrenTuple(
                num_children=num_children_not_visible,
                child_ids=top_level_not_visible,
            ))

        self.timer.intermediate("handle_morechildren")
        return comment_tuples

    def modify_comment_tree(self, comment_tree):
        """Potentially rewrite parts of comment_tree."""
        return comment_tree

    def get_initial_candidates(self, comment_tree):
        """Return comments to start building the tree from and offset_depth."""
        raise NotImplementedError

    def get_initial_comment_list(self, comment_tree):
        """Return the starting list of CommentTuples, possibly inserting some
        and bypassing the regular sorting/inserting process."""
        return []

    def update_candidates(self, candidates, sorter, to_add=None):
        for comment in (comment for comment in tup(to_add)
                                if comment in sorter):
            sort_val = -sorter[comment] if self.rev_sort else sorter[comment]
            heapq.heappush(candidates, (sort_val, comment))


SORT_OPERATOR_BY_NAME = {
    "new": operators.desc('_date'),
    "old": operators.asc('_date'),
    "controversial": operators.desc('_controversy'),
    "confidence": operators.desc('_confidence'),
    "qa": operators.desc('_qa'),
    "hot": operators.desc('_hot'),
    "top": operators.desc('_score'),
    "random": operators.shuffled('_confidence'),
}


class CommentOrderer(CommentOrdererBase):
    def get_initial_candidates(self, comment_tree):
        """Build the tree starting from all root level comments."""
        initial_candidates = comment_tree.tree.get(None, [])
        if initial_candidates:
            offset_depth = min(comment_tree.depth[comment_id]
                for comment_id in initial_candidates)
        else:
            offset_depth = 0
        return initial_candidates, offset_depth

    def get_initial_comment_list(self, comment_tree):
        """Promote the sticky comment, if any."""
        comment_tuples = []

        if self.link.sticky_comment_id:
            root_level_comments = comment_tree.tree.get(None, [])
            sticky_comment_id = self.link.sticky_comment_id
            if sticky_comment_id in root_level_comments:
                comment_tuples.append(CommentTuple(
                    comment_id=sticky_comment_id,
                    depth=0,
                    parent_id=None,
                    num_children=comment_tree.num_children[sticky_comment_id],
                    child_ids=comment_tree.tree.get(sticky_comment_id, []),
                ))
            else:
                g.log.warning("Non-top-level sticky comment detected on "
                              "link %r.", self.link)
        return comment_tuples

    def cache_key(self):
        key = "order:{link}_{operator}{column}".format(
            link=self.link._id36,
            operator=self.sort.__class__.__name__,
            column=self.sort.col,
        )
        return key

    @classmethod
    def write_cache(cls, link, sort, timer):
        comment_orderer = cls(link, sort,
            max_comments=g.max_comments_gold,
            max_depth=MAX_RECURSION,
            timer=timer,
        )
        comment_tuples = comment_orderer._get_comment_order()

        key = comment_orderer.cache_key()
        existing_tuples = g.permacache.get(key) or []

        if comment_tuples != existing_tuples:
            # don't write cache if the order hasn't changed
            g.permacache.set(key, comment_tuples)

    def should_read_cache(self):
        if self.link.precomputed_sorts:
            precomputed_sorts = [
                SORT_OPERATOR_BY_NAME[sort_name]
                for sort_name in self.link.precomputed_sorts
                if sort_name in SORT_OPERATOR_BY_NAME
            ]
            return self.sort in precomputed_sorts
        else:
            return False

    def read_cache(self):
        key = self.cache_key()
        comment_tuples = g.permacache.get(key) or []
        self.timer.intermediate("read_precomputed")

        # precomputed order might have returned more than max_comments. before
        # dealing with that we need to preserve the MissingChildrenTuple for
        # missing root level comments
        if comment_tuples and isinstance(comment_tuples[-1], MissingChildrenTuple):
            mct = comment_tuples.pop(-1)
            top_level_not_visible = mct.child_ids
            num_children_not_visible = mct.num_children
        else:
            top_level_not_visible = set()
            num_children_not_visible = 0

        # precomputed order uses the default max_depth. filter the list
        # if we need a different max_depth. NOTE: we may end up with fewer
        # comments than were requested.
        if self.max_depth < MAX_RECURSION:
            comment_tuples = [
                comment_tuple for comment_tuple in comment_tuples
                if comment_tuple.depth < self.max_depth
            ]

        if len(comment_tuples) > self.max_comments:
            top_level_not_visible.update({
                comment_tuple.comment_id
                for comment_tuple in comment_tuples[self.max_comments:]
                if comment_tuple.depth == 0
            })
            num_children_not_visible += sum(
                1 + comment_tuple.num_children
                for comment_tuple in comment_tuples[self.max_comments:]
                if comment_tuple.depth == 0
            )
            comment_tuples = comment_tuples[:self.max_comments]

        if top_level_not_visible:
            comment_tuples.append(MissingChildrenTuple(
                num_children=num_children_not_visible,
                child_ids=top_level_not_visible,
            ))

        self.timer.intermediate("prune_precomputed")

        return comment_tuples

    def _get_comment_order(self):
        return CommentOrdererBase.get_comment_order(self)

    def get_comment_order(self):
        num_comments = self.link.num_comments
        if num_comments == 0:
            bucket = "0"
        elif num_comments >= 100:
            bucket = "100_plus"
        else:
            bucket_start = num_comments / 5 * 5
            bucket_end = bucket_start + 5
            bucket = "%s_%s" % (bucket_start, bucket_end)

        # record the number of comments on this link so we can get an idea of
        # what value to use for 'precomputed_comment_sort_min_comments'
        g.stats.simple_event("CommentOrderer.num_comments.%s" % bucket)

        if self.link.num_comments <= 0:
            return []

        if self.should_read_cache():
            with g.stats.get_timer("CommentOrderer.read_cache") as timer:
                return self.read_cache()
        else:
            if bucket == "100_plus":
                for sort_name, operator in SORT_OPERATOR_BY_NAME.iteritems():
                    if operator == self.sort:
                        break
                else:
                    sort_name = "None"
                g.stats.simple_event("CommentOrderer.100_plus_sort.%s" % sort_name)

            timer_name = "CommentOrderer.by_num_comments.%s" % bucket
            with g.stats.get_timer(timer_name) as timer:
                return self._get_comment_order()


class QACommentOrderer(CommentOrderer):
    def _get_comment_order(self):
        """Filter out the comments we don't want to show in QA sort.

        QA sort only displays comments that are:
        1. Top-level
        2. Responses from the OP(s)
        3. Within one level of an OP reply
        4. Distinguished

        All ancestors of comments meeting the above rules will also be shown.
        This ensures the question responded to by OP is shown.

        """

        comment_tuples = CommentOrdererBase.get_comment_order(self)
        if not comment_tuples:
            return comment_tuples
        elif isinstance(comment_tuples[-1], MissingChildrenTuple):
            missing_children_tuple = comment_tuples.pop()
        else:
            missing_children_tuple = None

        special_responder_ids = self.link.responder_ids

        # unfortunately we need to look up all the Comments for QA
        comment_ids = {ct.comment_id for ct in comment_tuples}
        comments_by_id = Comment._byID(comment_ids, data=True)

        # figure out which comments will be kept (all others are discarded)
        kept_comment_ids = set()
        for comment_tuple in comment_tuples:
            if comment_tuple.depth == 0:
                kept_comment_ids.add(comment_tuple.comment_id)
                continue

            comment = comments_by_id[comment_tuple.comment_id]
            parent = comments_by_id[comment.parent_id] if comment.parent_id else None

            if comment.author_id in special_responder_ids:
                kept_comment_ids.add(comment_tuple.comment_id)
                continue

            if parent and parent.author_id in special_responder_ids:
                kept_comment_ids.add(comment_tuple.comment_id)
                continue

            if hasattr(comment, "distinguished") and comment.distinguished != "no":
                kept_comment_ids.add(comment_tuple.comment_id)
                continue

        # add all ancestors to kept_comment_ids
        for comment_id in sorted(kept_comment_ids):
            # sort the comments so we start with the most root level comments
            comment = comments_by_id[comment_id]
            parent_id = comment.parent_id

            counter = 0
            while (parent_id and
                        parent_id not in kept_comment_ids and
                        counter < g.max_comment_parent_walk):
                kept_comment_ids.add(parent_id)
                counter += 1

                comment = comments_by_id[parent_id]
                parent_id = comment.parent_id

        # remove all comment tuples that aren't in kept_comment_ids
        comment_tuples = [comment_tuple for comment_tuple in comment_tuples
            if comment_tuple.comment_id in kept_comment_ids
        ]

        if missing_children_tuple:
            comment_tuples.append(missing_children_tuple)

        return comment_tuples


def get_active_sort_orders_for_link(link):
    # only activate precomputed sorts for links with enough comments.
    # (value of 0 means not active for any value of link.num_comments)
    min_comments = g.live_config['precomputed_comment_sort_min_comments']
    if min_comments <= 0 or link.num_comments < min_comments:
        return set()

    active_sorts = set(g.live_config['precomputed_comment_sorts'])
    if g.live_config['precomputed_comment_suggested_sort']:
        suggested_sort = link.sort_if_suggested()
        if suggested_sort:
            active_sorts.add(suggested_sort)

    return active_sorts


def write_comment_orders(link):
    # we don't really care about getting detailed timings here, the entire
    # process will be timed by the caller
    timer = SimpleSillyStub()

    precomputed_sorts = set()
    for sort_name in get_active_sort_orders_for_link(link):
        sort = SORT_OPERATOR_BY_NAME.get(sort_name)
        if not sort:
            continue

        if sort_name == "qa":
            QACommentOrderer.write_cache(link, sort, timer)
        else:
            CommentOrderer.write_cache(link, sort, timer)

        precomputed_sorts.add(sort_name)

    if precomputed_sorts:
        g.stats.simple_event("CommentOrderer.write_comment_orders.write")
    else:
        g.stats.simple_event("CommentOrderer.write_comment_orders.noop")

    # replace empty set with None to match the Link._defaults value
    link.precomputed_sorts = precomputed_sorts or None
    link._commit()


class PermalinkCommentOrderer(CommentOrdererBase):
    def __init__(self, link, sort, max_comments, max_depth, timer, comment,
                 context):
        CommentOrdererBase.__init__(
            self, link, sort, max_comments, max_depth, timer)
        self.comment = comment
        self.context = context

    @classmethod
    def get_path_to_comment(cls, comment, context, comment_tree):
        """Return the path back to top level from comment.

        Restrict the path to a maximum of `context` levels deep."""

        if comment._id not in comment_tree.cids:
            # the comment isn't in the tree
            raise InconsistentCommentTreeError

        comment_id = comment._id
        path = []
        while comment_id and len(path) <= context:
            path.append(comment_id)
            try:
                comment_id = comment_tree.parents[comment_id]
            except KeyError:
                # the comment's parent is missing from the tree. this might
                # just mean that the child was added to the tree first and
                # the tree will be correct when the parent is added.
                raise InconsistentCommentTreeError

        # reverse the list so the first element is the most root level comment
        path.reverse()
        return path

    def modify_comment_tree(self, comment_tree):
        path = self.get_path_to_comment(
            self.comment, self.context, comment_tree)

        # work through the path in reverse starting with the requested comment
        for comment_id in reversed(path):
            # rewrite parent's tree so it leads only to the requested comment
            parent_id = comment_tree.parents[comment_id]
            comment_tree.tree[parent_id] = [comment_id]

            # rewrite parent's num_children to count only this branch
            if parent_id is not None:
                branch_num_children = comment_tree.num_children[comment_id]
                comment_tree.num_children[parent_id] = branch_num_children + 1

        return comment_tree

    def get_initial_candidates(self, comment_tree):
        """Start the tree from the first ancestor of requested comment."""
        path = self.get_path_to_comment(
            self.comment, self.context, comment_tree)

        # get_path_to_comment returns path ordered from ancestor to
        # selected comment
        root_comment = path[0]
        initial_candidates = [root_comment]
        offset_depth = comment_tree.depth[root_comment]
        return initial_candidates, offset_depth


class ChildrenCommentOrderer(CommentOrdererBase):
    def __init__(self, link, sort, max_comments, max_depth, timer, children):
        CommentOrdererBase.__init__(
            self, link, sort, max_comments, max_depth, timer)
        self.children = children

    def get_initial_candidates(self, comment_tree):
        """Start the tree from the requested children."""

        children = [
            comment_id for comment_id in self.children
            if comment_id in comment_tree.depth
        ]

        if children:
            children_depth = min(
                comment_tree.depth[comment_id] for comment_id in children)

            children = [
                comment_id for comment_id in children
                if comment_tree.depth[comment_id] == children_depth
            ]

        initial_candidates = children

        # BUG: current viewing depth isn't considered, so requesting children
        # of a deep comment can return nothing. the fix is to send the current
        # offset_depth along with the MoreChildren request
        offset_depth = 0

        return initial_candidates, offset_depth


def make_child_listing():
    l = Listing(builder=None, nextprev=None)
    l.things = []
    child = Wrapped(l)
    return child


def add_to_child_listing(parent, child_thing):
    if not hasattr(parent, 'child'):
        child = make_child_listing()
        child.parent_name = "deleted" if parent.deleted else parent._fullname
        parent.child = child

    parent.child.things.append(child_thing)


class CommentBuilder(Builder):
    """Build (lookup and wrap) comments for display."""
    def __init__(self, link, sort, comment=None, children=None, context=None,
                 load_more=True, continue_this_thread=True,
                 max_depth=MAX_RECURSION, edits_visible=True, num=None,
                 show_deleted=False, **kw):
        self.link = link
        self.sort = sort

        # arguments for permalink mode
        self.comment = comment
        self.context = context or 0

        # argument for morechildren mode
        self.children = children

        # QA mode only activates for the full comments view
        self.in_qa_mode = sort.col == '_qa' and not (comment or children)

        self.load_more = load_more
        self.max_depth = max_depth
        self.show_deleted = show_deleted or c.user_is_admin

        # uncollapse everything in QA mode because the sorter will prune
        self.uncollapse_all = c.user_is_admin or self.in_qa_mode
        self.edits_visible = edits_visible
        self.num = num
        self.continue_this_thread = continue_this_thread

        self.comments = None
        Builder.__init__(self, **kw)

    def get_items(self):
        if self.comments is None:
            self._get_comments()
        return self._make_wrapped_tree()

    def _get_comments(self):
        self.load_comment_order()
        comment_ids = {
            comment_tuple.comment_id
            for comment_tuple in self.ordered_comment_tuples
        }
        self.comments = Comment._byID(
            comment_ids, data=True, return_dict=False, stale=self.stale)
        self.timer.intermediate("lookup_comments")

    def load_comment_order(self):
        self.timer = g.stats.get_timer("CommentBuilder.get_items")
        self.timer.start()

        if self.comment:
            orderer = PermalinkCommentOrderer(
                self.link, self.sort, self.num, self.max_depth, self.timer,
                self.comment, self.context)

            try:
                comment_tuples = orderer.get_comment_order()
            except InconsistentCommentTreeError:
                g.log.error("Hack - self.comment (%d) not in depth. Defocusing..."
                            % self.comment._id)
                self.comment = None
                orderer = CommentOrderer(
                    self.link, self.sort, self.num, self.max_depth, self.timer)
                comment_tuples = orderer.get_comment_order()

        elif self.children:
            orderer = ChildrenCommentOrderer(
                self.link, self.sort, self.num, self.max_depth, self.timer,
                self.children)
            comment_tuples = orderer.get_comment_order()
        elif self.in_qa_mode:
            orderer = QACommentOrderer(
                self.link, self.sort, self.num, self.max_depth, self.timer)
            comment_tuples = orderer.get_comment_order()
        else:
            orderer = CommentOrderer(
                self.link, self.sort, self.num, self.max_depth, self.timer)
            comment_tuples = orderer.get_comment_order()

        if (comment_tuples and
                isinstance(comment_tuples[-1], MissingChildrenTuple)):
            mct = comment_tuples.pop(-1)
            missing_root_comments = mct.child_ids
            missing_root_count = mct.num_children
        else:
            missing_root_comments = set()
            missing_root_count = 0

        self.ordered_comment_tuples = comment_tuples
        self.missing_root_comments = missing_root_comments
        self.missing_root_count = missing_root_count

    def keep_item(self, item):
        if not self.show_deleted:
            if item.deleted and not item.num_children:
                return False
        return item.keep_item(item)

    def _make_wrapped_tree(self):
        timer = self.timer
        ordered_comment_tuples = self.ordered_comment_tuples
        missing_root_comments = self.missing_root_comments
        missing_root_count = self.missing_root_count

        if not ordered_comment_tuples:
            self.timer.stop()
            return []

        comment_order = [
            comment_tuple.comment_id for comment_tuple in ordered_comment_tuples
        ]

        wrapped = self.make_wrapped_items(ordered_comment_tuples)
        timer.intermediate("wrap_comments")

        wrapped_by_id = {
            comment._id: comment for comment in wrapped}
        self.uncollapse_special_comments(wrapped_by_id)

        redacted_ids = set()

        visible_ids = {
            comment._id for comment in wrapped
            if not getattr(comment, 'hidden', False)
        }

        final = []
        for comment_id in comment_order:
            comment = wrapped_by_id[comment_id]

            if getattr(comment, 'hidden', False):
                continue

            if not self.keep_item(comment):
                redacted_ids.add(comment_id)
                continue

            # add the comment as a child of its parent or to the top level of
            # the tree if it has no parent or the parent isn't in the listing
            parent = wrapped_by_id.get(comment.parent_id)
            if parent:
                add_to_child_listing(parent, comment)
            else:
                final.append(comment)

        self.timer.intermediate("build_comments")

        if not self.load_more:
            timer.stop()
            return final

        # add MoreRecursion and MoreChildren last so they'll be the last item in
        # a comment's child listing
        for comment in wrapped:
            if (self.continue_this_thread and
                    comment.depth == self.max_depth - 1 and
                    comment.num_children > 0):
                # only comments with depth < max_depth are visible
                # if this comment is as deep as we can go and has children then
                # we need to insert a MoreRecursion child
                mr = MoreRecursion(self.link, depth=0, parent_id=comment._id)
                w = Wrapped(mr)
                add_to_child_listing(comment, w)
            elif comment.depth < self.max_depth - 1:
                missing_child_ids = (
                    set(comment.child_ids) - visible_ids - redacted_ids
                )
                if missing_child_ids:
                    missing_depth = comment.depth + 1
                    mc = MoreChildren(self.link, self.sort, depth=missing_depth,
                            parent_id=comment._id)
                    w = Wrapped(mc)
                    visible_count = sum(
                        1 + wrapped_by_id[child_id].num_children
                        for child_id in comment.child_ids
                        if child_id in visible_ids
                    )
                    w.count = comment.num_children - visible_count
                    w.children.extend(missing_child_ids)
                    add_to_child_listing(comment, w)

        if missing_root_comments:
            mc = MoreChildren(self.link, self.sort, depth=0, parent_id=None)
            w = Wrapped(mc)
            w.count = missing_root_count
            w.children.extend(missing_root_comments)
            final.append(w)

        self.timer.intermediate("build_morechildren")
        self.timer.stop()
        return final

    def make_wrapped_items(self, comment_tuples):
        wrapped = Builder.wrap_items(self, self.comments)
        wrapped_by_id = {comment._id: comment for comment in wrapped}

        for comment_tuple in comment_tuples:
            comment = wrapped_by_id[comment_tuple.comment_id]
            comment.num_children = comment_tuple.num_children
            comment.child_ids = comment_tuple.child_ids
            comment.depth = comment_tuple.depth
            comment.edits_visible = self.edits_visible

            if self.children:
                # rewrite the parent links to use anchor tags because the parent
                # is on the page but wasn't included in this batch
                if comment.parent_id:
                    comment.parent_permalink = '#' + to36(comment.parent_id)

        return wrapped

    def uncollapse_special_comments(self, wrapped_by_id):
        """Undo collapsing for special comments.

        The builder may have set `collapsed` and `hidden` attributes for
        comments that we want to ensure are shown.

        """

        if self.uncollapse_all:
            dont_collapse = set(wrapped_by_id.keys())
        elif self.comment:
            dont_collapse = set([self.comment._id])
            parent_id = self.comment.parent_id
            while parent_id:
                dont_collapse.add(parent_id)
                if parent_id in wrapped_by_id:
                    parent_id = wrapped_by_id[parent_id].parent_id
                else:
                    parent_id = None
        elif self.children:
            dont_collapse = set(self.children)
        else:
            dont_collapse = set()

        # we only care about preventing collapse of wrapped comments
        dont_collapse &= set(wrapped_by_id.keys())

        maybe_collapse = set(wrapped_by_id.keys()) - dont_collapse

        for comment_id in maybe_collapse:
            comment = wrapped_by_id[comment_id]
            if comment.distinguished and comment.distinguished != "no":
                dont_collapse.add(comment_id)

        maybe_collapse -= dont_collapse

        # ensure all ancestors of dont_collapse comments are not collapsed
        if maybe_collapse:
            for comment_id in sorted(dont_collapse):
                # sort comments so we start with the most root level comments
                comment = wrapped_by_id[comment_id]
                parent_id = comment.parent_id

                counter = 0
                while (parent_id and
                            parent_id not in dont_collapse and
                            parent_id in wrapped_by_id and
                            counter < g.max_comment_parent_walk):
                    dont_collapse.add(parent_id)
                    counter += 1

                    comment = wrapped_by_id[parent_id]
                    parent_id = comment.parent_id

        for comment_id in dont_collapse:
            comment = wrapped_by_id[comment_id]
            if comment.collapsed:
                comment.collapsed = False
                comment.hidden = False

    def item_iter(self, a):
        for i in a:
            yield i
            if hasattr(i, 'child'):
                for j in self.item_iter(i.child.things):
                    yield j


class MessageBuilder(Builder):
    def __init__(self, skip=True, num=None, parent=None, after=None,
                 reverse=False, threaded=False, **kw):
        self.skip = skip
        self.num = num
        self.parent = parent
        self.after = after
        self.reverse = reverse
        self.threaded = threaded
        Builder.__init__(self, **kw)

    def get_tree(self):
        raise NotImplementedError, "get_tree"

    def valid_after(self, after):
        w = self.convert_items((after,))[0]
        return self._viewable_message(w)

    def _viewable_message(self, m):
        if (c.user_is_admin or
                getattr(m, "author_id", 0) == c.user._id or
                getattr(m, "to_id", 0) == c.user._id):
            return True

        # m is wrapped at this time, so it should have an SR
        subreddit = getattr(m, "subreddit", None)
        if subreddit and subreddit.is_moderator_with_perms(c.user, 'mail'):
            return True

        return False

    def _apply_pagination(self, tree):
        if self.parent or self.num is None:
            return tree, None, None

        prev_item = None
        next_item = None

        if self.after:
            # truncate the tree to only show before/after requested message
            if self.reverse:
                next_item = self.after._id
                tree = [
                    (parent_id, child_ids) for parent_id, child_ids in tree
                    if tree_sort_fn((parent_id, child_ids)) >= next_item
                ]

                # special handling for after+reverse (before link): truncate
                # the tree so it has num messages before the requested one
                if len(tree) > self.num:
                    first_id, first_children = tree[-(self.num + 1)]
                    prev_item = tree_sort_fn((first_id, first_children))
                    tree = tree[-self.num:]
            else:
                prev_item = self.after._id
                tree = [
                    (parent_id, child_ids) for parent_id, child_ids in tree
                    if tree_sort_fn((parent_id, child_ids)) < prev_item
                ]

        if len(tree) > self.num:
            # truncate the tree to show only num conversations
            tree = tree[:self.num]
            last_id, last_children = tree[-1]
            next_item = tree_sort_fn((last_id, last_children))
        return tree, prev_item, next_item

    @classmethod
    def should_collapse(cls, message):
        # don't collapse this message if it has a new direct child
        if hasattr(message, "child"):
            has_new_child = any(child.new for child in message.child.things)
        else:
            has_new_child = False

        return (message.is_collapsed and
            not message.new and
            not has_new_child)

    def get_items(self):
        tree = self.get_tree()
        tree, prev_item, next_item = self._apply_pagination(tree)

        message_ids = []
        for parent_id, child_ids in tree:
            message_ids.append(parent_id)
            message_ids.extend(child_ids)

        if prev_item:
            message_ids.append(prev_item)

        messages = Message._byID(message_ids, data=True, return_dict=False)
        wrapped = {m._id: m for m in self.wrap_items(messages)}

        if prev_item:
            prev_item = wrapped[prev_item]
        if next_item:
            next_item = wrapped[next_item]

        final = []
        for parent_id, child_ids in tree:
            if parent_id not in wrapped:
                continue

            parent = wrapped[parent_id]

            if not self._viewable_message(parent):
                continue

            children = [
                wrapped[child_id] for child_id in child_ids
                if child_id in wrapped
            ]

            depth = {parent_id: 0}
            substitute_parents = {}

            if (children and self.skip and not self.threaded and
                    not self.parent and not parent.new and parent.is_collapsed):
                for i, child in enumerate(children):
                    if child.new or not child.is_collapsed:
                        break
                else:
                    i = -1
                # in flat view replace collapsed chain with MoreMessages
                child = make_child_listing()
                child.parent_name = "deleted" if parent.deleted else parent._fullname
                parent = Wrapped(MoreMessages(parent, child))
                children = children[i:]

            for child in sorted(children, key=lambda child: child._id):
                # iterate from the root outwards so we can check the depth
                if self.threaded:
                    try:
                        child_parent = wrapped[child.parent_id]
                    except KeyError:
                        # the stored comment tree was missing this message's
                        # parent, treat it as a top level reply
                        child_parent = parent
                else:
                    # for flat view all messages are decendants of the
                    # parent message
                    child_parent = parent
                parent_depth = depth[child_parent._id]
                child_depth = parent_depth + 1
                depth[child._id] = child_depth

                if child_depth == MAX_RECURSION:
                    # current message is at maximum depth level, all its
                    # children will be displayed as children of its parent
                    substitute_parents[child._id] = child_parent._id

                if child_depth > MAX_RECURSION:
                    child_parent_id = substitute_parents[child.parent_id]
                    substitute_parents[child._id] = child_parent_id
                    child_parent = wrapped[child_parent_id]

                child.is_child = True
                add_to_child_listing(child_parent, child)

            for child in children:
                # look over the children again to decide whether they can be
                # collapsed
                child.threaded = self.threaded
                child.collapsed = self.should_collapse(child)

            if self.threaded and children:
                most_recent_child_id = max(child._id for child in children)
                most_recent_child = wrapped[most_recent_child_id]
                most_recent_child.most_recent = True

            parent.is_parent = True
            parent.threaded = self.threaded
            parent.collapsed = self.should_collapse(parent)
            final.append(parent)

        return (final, prev_item, next_item, len(final), len(final))

    def item_iter(self, builder_items):
        items = builder_items[0]

        def _item_iter(_items):
            for i in _items:
                yield i
                if hasattr(i, "child"):
                    for j in _item_iter(i.child.things):
                        yield j

        return _item_iter(items)


class ModeratorMessageBuilder(MessageBuilder):
    def __init__(self, user, **kw):
        self.user = user
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            sr = Subreddit._byID(self.parent.sr_id)
            return sr_conversation(sr, self.parent)
        sr_ids = Subreddit.reverse_moderator_ids(self.user)
        return moderator_messages(sr_ids)


class MultiredditMessageBuilder(MessageBuilder):
    def __init__(self, sr, **kw):
        self.sr = sr
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            sr = Subreddit._byID(self.parent.sr_id)
            return sr_conversation(sr, self.parent)
        return moderator_messages(self.sr.sr_ids)


class TopCommentBuilder(CommentBuilder):
    """A comment builder to fetch only the top-level, non-spam,
       non-deleted comments"""
    def __init__(self, link, sort, num=None, wrap=Wrapped):
        CommentBuilder.__init__(self, link, sort, load_more=False,
            continue_this_thread=False, max_depth=1, wrap=wrap, num=num)

    def get_items(self):
        final = CommentBuilder.get_items(self)
        return [ cm for cm in final if not cm.deleted ]


class SrMessageBuilder(MessageBuilder):
    def __init__(self, sr, **kw):
        self.sr = sr
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            return sr_conversation(self.sr, self.parent)
        return subreddit_messages(self.sr)


class UserMessageBuilder(MessageBuilder):
    def __init__(self, user, **kw):
        self.user = user
        MessageBuilder.__init__(self, **kw)

    def _viewable_message(self, message):
        is_author = message.author_id == c.user._id
        if not c.user_is_admin:
            if not is_author and message._spam:
                return False

            if message.author_id in self.user.enemies:
                return False

            # do not show messages which were deleted on recipient
            if (hasattr(message, "del_on_recipient") and
                    message.to_id == c.user._id and message.del_on_recipient):
                return False

        return super(UserMessageBuilder, self)._viewable_message(message)

    def get_tree(self):
        if self.parent:
            return conversation(self.user, self.parent)
        return user_messages(self.user)

    def valid_after(self, after):
        # Messages that have been spammed are still valid afters
        w = self.convert_items((after,))[0]
        return MessageBuilder._viewable_message(self, w)


class UserListBuilder(QueryBuilder):
    def thing_lookup(self, rels):
        accounts = Account._byID([rel._thing2_id for rel in rels], data=True)
        for rel in rels:
            rel._thing2 = accounts.get(rel._thing2_id)
        return rels

    def must_skip(self, item):
        return item.user._deleted

    def valid_after(self, after):
        # Users that have been deleted are still valid afters
        return True

    def wrap_items(self, rels):
        return [self.wrap(rel) for rel in rels]

class SavedBuilder(IDBuilder):
    def wrap_items(self, items):
        categories = LinkSavesByAccount.fast_query(c.user, items).items()
        categories += CommentSavesByAccount.fast_query(c.user, items).items()
        categories = {item[1]._id: category for item, category in categories if category}
        wrapped = QueryBuilder.wrap_items(self, items)
        for w in wrapped:
            category = categories.get(w._id, '')
            w.savedcategory = category
        return wrapped


class FlairListBuilder(UserListBuilder):
    def init_query(self):
        q = self.query

        if self.reverse:
            q._reverse()

        q._data = True
        self.orig_rules = deepcopy(q._rules)
        # FlairLists use Accounts for afters
        if self.after:
            if self.reverse:
                q._filter(Flair.c._thing2_id < self.after._id)
            else:
                q._filter(Flair.c._thing2_id > self.after._id)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
This module provides a Cassandra-backed lockless query cache.  Rather than
doing complicated queries on the fly to populate listings, a list of items that
would be in that listing are maintained in Cassandra for fast lookup.  The
result can then be fed to IDBuilder to generate a final result.

Whenever an operation occurs that would modify the contents of the listing, the
listing should be updated somehow.  In some cases, this can be done by directly
mutating the listing and in others it must be done offline in batch processing
jobs.

"""

import json
import random
import datetime
import collections

from pylons import app_globals as g
from pycassa.system_manager import ASCII_TYPE, UTF8_TYPE
from pycassa.batch import Mutator

from r2.models import Thing
from r2.lib.db import tdb_cassandra
from r2.lib.db.operators import asc, desc, BooleanOp
from r2.lib.db.sorts import epoch_seconds
from r2.lib.utils import flatten, to36


CONNECTION_POOL = g.cassandra_pools['main']
PRUNE_CHANCE = g.querycache_prune_chance
MAX_CACHED_ITEMS = 1000
LOG = g.log


class ThingTupleComparator(object):
    """A callable usable for comparing sort-data in a cached query.

    The query cache stores minimal sort data on each thing to be able to order
    the items in a cached query.  This class provides the ordering for those
    thing tuples.

    """

    def __init__(self, sorts):
        self.sorts = sorts

    def __call__(self, t1, t2):
        for i, s in enumerate(self.sorts):
            # t1 and t2 are tuples of (fullname, *sort_cols), so we
            # can get the value to compare right out of the tuple
            v1, v2 = t1[i + 1], t2[i + 1]
            if v1 != v2:
                return cmp(v1, v2) if isinstance(s, asc) else cmp(v2, v1)
        #they're equal
        return 0


class _CachedQueryBase(object):
    def __init__(self, sort):
        self.sort = sort
        self.sort_cols = [s.col for s in self.sort]
        self.data = []
        self._fetched = False

    def fetch(self, force=False):
        """Fill the cached query's sorted item list from Cassandra.

        If the query has already been fetched, this method is a no-op unless
        force=True.

        """
        if not force and self._fetched:
            return

        self._fetch()
        self._sort_data()
        self._fetched = True

    def _fetch(self):
        raise NotImplementedError()

    def _sort_data(self):
        comparator = ThingTupleComparator(self.sort_cols)
        self.data.sort(cmp=comparator)

    def __iter__(self):
        self.fetch()

        for x in self.data[:MAX_CACHED_ITEMS]:
            yield x[0]


class CachedQuery(_CachedQueryBase):
    """A materialized view of a complex query.

    Complicated queries can take way too long to sort in the databases.  This
    class provides a fast-access view of a given listing's items.  The cache
    stores each item's ID and a minimal subset of its data as required for
    sorting.

    Each time the listing is fetched, it is sorted. Because of this, we need to
    ensure the listing does not grow too large.  On each insert, a "pruning"
    can occur (with a configurable probability) which will remove excess items
    from the end of the listing.

    Use CachedQueryMutator to make changes to the cached query's item list.

    """

    def __init__(self, model, key, sort, filter_fn, is_precomputed):
        self.model = model
        self.key = key
        self.filter = filter_fn
        self.timestamps = None  # column timestamps, for safe pruning
        self.is_precomputed = is_precomputed
        super(CachedQuery, self).__init__(sort)

    def _make_item_tuple(self, item):
        """Return an item tuple from the result of a query.

        The item tuple is used to sort the items in a query without having to
        look them up.

        """
        filtered_item = self.filter(item)
        lst = [filtered_item._fullname]
        for col in self.sort_cols:
            # take the property of the original
            attr = getattr(item, col)
            # convert dates to epochs to take less space
            if isinstance(attr, datetime.datetime):
                attr = epoch_seconds(attr)
            lst.append(attr)
        return tuple(lst)

    def _fetch(self):
        self._fetch_multi([self])

    @classmethod
    def _fetch_multi(self, queries):
        """Fetch the unsorted query results for multiple queries at once.

        In the case of precomputed queries, do an extra lookup first to
        determine which row key to find the latest precomputed values for the
        query in.

        """

        by_model = collections.defaultdict(list)
        for q in queries:
            by_model[q.model].append(q)

        cached_queries = {}
        for model, queries in by_model.iteritems():
            pure, need_mangling = [], []
            for q in queries:
                if not q.is_precomputed:
                    pure.append(q.key)
                else:
                    need_mangling.append(q.key)

            mangled = model.index_mangle_keys(need_mangling)
            fetched = model.get(pure + mangled.keys())
            for key, values in fetched.iteritems():
                key = mangled.get(key, key)
                cached_queries[key] = values

        for q in queries:
            cached_query = cached_queries.get(q.key)
            if cached_query:
                q.data, q.timestamps = cached_query

    def _cols_from_things(self, things):
        cols = {}
        for thing in things:
            t = self._make_item_tuple(thing)
            cols[t[0]] = tuple(t[1:])
        return cols

    def _insert(self, mutator, things):
        if not things:
            return

        cols = self._cols_from_things(things)
        self.model.insert(mutator, self.key, cols)

    def _replace(self, mutator, things, ttl):
        cols = self._cols_from_things(things)
        self.model.replace(mutator, self.key, cols, ttl)

    def _delete(self, mutator, things):
        if not things:
            return

        fullnames = [self.filter(x)._fullname for x in things]
        self.model.remove(mutator, self.key, fullnames)

    def _prune(self, mutator):
        to_keep = [t[0] for t in self.data[:MAX_CACHED_ITEMS]]
        to_prune = [t[0] for t in self.data[MAX_CACHED_ITEMS:]]

        if to_prune:
            oldest_keep = min(self.timestamps[_id] for _id in to_keep)
            fast_prunable = [_id for _id in to_prune
                if self.timestamps[_id] < oldest_keep]

            num_to_prune = len(to_prune)
            num_fast_prunable = len(fast_prunable)
            num_unpruned_if_fast = num_to_prune - num_fast_prunable
            if (num_fast_prunable > num_to_prune * 0.5 and
                    num_unpruned_if_fast < MAX_CACHED_ITEMS * 0.5):
                # do a fast prune if we can remove a good number of items but
                # don't let the cached query grow too large
                newest_prune = max(self.timestamps[_id] for _id in fast_prunable)
                self.model.remove_older_than(mutator, self.key, newest_prune)
                event_name = 'fast_pruned'
                num_pruned = num_fast_prunable
            else:
                # if something has gone wrong with previous prunings, there may
                # be a lot of items to prune.
                #
                # On each attempt we have PRUNE_CHANCE likelihood that we will
                # get to prune. Assume that each prune attempt occurs as the
                # result of adding one item to the `CachedQuery`. So, to prevent
                # unbounded growth we need to remove on average at least one
                # item per prune attempt.
                # so:
                # N_avg = 1 = PRUNE_CHANCE * PRUNE_SIZE
                # PRUNE_SIZE = 1 / PRUNE_CHANCE
                # We'll multiply this value by 1.5 to ensure that we return
                # quickly to the maximum allowed size.
                prune_size = int(1.5 * 1 / PRUNE_CHANCE)
                to_prune = to_prune[-prune_size:]

                self.model.remove_if_unchanged(mutator, self.key,
                                               to_prune, self.timestamps)
                event_name = 'pruned'
                num_pruned = len(to_prune)

            cf_name = self.model.__name__
            query_name = self.key.split('.')[0]
            counter_key = "cache.%s.%s" % (cf_name, query_name)
            counter = g.stats.get_counter(counter_key)
            if counter:
                counter.increment(event_name, delta=num_pruned)

    @classmethod
    def _prune_multi(cls, queries):
        cls._fetch_multi(queries)

        with Mutator(CONNECTION_POOL) as m:
            for q in queries:
                q._sort_data()
                q._prune(m)

    def __hash__(self):
        return hash(self.key)

    def __eq__(self, other):
        return self.key == other.key

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self):
        return "%s(%s, %r)" % (self.__class__.__name__,
                               self.model.__name__, self.key)


class MergedCachedQuery(_CachedQueryBase):
    """A cached query built by merging multiple sub-queries.

    Merged queries can be read, but cannot be modified as it is not easy to
    determine from a given item which sub-query should get modified.

    """

    def __init__(self, queries):
        self.queries = queries

        if queries:
            sort = queries[0].sort
            assert all(sort == q.sort for q in queries)
        else:
            sort = []
        super(MergedCachedQuery, self).__init__(sort)

    def _fetch(self):
        CachedQuery._fetch_multi(self.queries)
        self.data = flatten([q.data for q in self.queries])


class CachedQueryMutator(object):
    """Utility to manipulate cached queries with batching.

    This implements the context manager protocol so it can be used with the
    with statement for clean batches.

    """

    def __init__(self):
        self.mutator = Mutator(CONNECTION_POOL)
        self.to_prune = set()

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        self.send()

    def insert(self, query, things):
        """Insert items into the given cached query.

        If the items are already in the query, they will have their sorts
        updated.

        This will sometimes trigger pruning with a configurable probability
        (see g.querycache_prune_chance).

        """
        if not things:
            return

        LOG.debug("Inserting %r into query %r", things, query)

        assert not query.is_precomputed
        query._insert(self.mutator, things)

        if (random.random() / len(things)) < PRUNE_CHANCE:
            self.to_prune.add(query)

    def replace(self, query, things, ttl=None):
        """Replace a precomputed query with a new set of things.

        The query index will be updated. If a TTL is specified, it will be
        applied to all columns generated by this action allowing old
        precomputed queries to fall away after they're no longer useful.

        """
        assert query.is_precomputed

        if isinstance(ttl, datetime.timedelta):
            ttl = ttl.total_seconds()

        query._replace(self.mutator, things, ttl)

    def delete(self, query, things):
        """Remove things from the query."""
        if not things:
            return

        LOG.debug("Deleting %r from query %r", things, query)

        query._delete(self.mutator, things)

    def send(self):
        """Commit the mutations batched up so far and potentially do pruning.

        This is automatically called by __exit__ when used as a context
        manager.

        """
        self.mutator.send()

        if self.to_prune:
            LOG.debug("Pruning queries %r", self.to_prune)
            CachedQuery._prune_multi(self.to_prune)


def filter_identity(x):
    """Return the same thing given.

    Use this as the filter_fn of simple Thing-based cached queries so that
    the enumerated things will be returned for rendering.

    """
    return x


def filter_thing2(x):
    """Return the thing2 of a given relationship.

    Use this as the filter_fn of a cached Relation query so that the related
    things will be returned for rendering.

    """
    return x._thing2


def filter_thing(x):
    """Return "thing" from a proxy object.

    Use this as the filter_fn when some object that's not a Thing or Relation
    is used as the basis of a cached query.

    """
    return x.thing


def _is_query_precomputed(query):
    """Return if this query must be updated offline in a batch job.

    Simple queries can be modified in place in the query cache, but ones
    with more complicated eligibility criteria, such as a time limit ("top
    this month") cannot be modified this way and must instead be
    recalculated periodically.  Rather than replacing a single row
    repeatedly, the precomputer stores in a new row every time it runs and
    updates an index of the latest run.

    """

    # visit all the nodes in the rule tree to see if there are time limitations
    # if we find one, this query is one that must be precomputed
    rules = list(query._rules)
    while rules:
        rule = rules.pop()

        if isinstance(rule, BooleanOp):
            rules.extend(rule.ops)
            continue

        if rule.lval.name == "_date":
            return True
    return False


class FakeQuery(object):
    """A somewhat query-like object for conveying sort information."""

    def __init__(self, sort, precomputed=False):
        self._sort = sort
        self.precomputed = precomputed


def cached_query(model, filter_fn=filter_identity):
    """Decorate a function describing a cached query.

    The decorated function is expected to follow the naming convention common
    in queries.py -- "get_something".  The cached query's key will be generated
    from the combination of the function name and its arguments separated by
    periods.

    The decorated function should return a raw thingdb query object
    representing the query that is being cached. If there is no valid
    underlying query to build off of, a FakeQuery specifying the correct
    sorting criteria for the enumerated objects can be returned.

    """
    def cached_query_decorator(fn):
        def cached_query_wrapper(*args):
            # build the row key from the function name and arguments
            assert fn.__name__.startswith("get_")
            row_key_components = [fn.__name__[len('get_'):]]

            if len(args) > 0:
                # we want to accept either a Thing or a thing's ID at this
                # layer, but the query itself should always get just an ID
                if isinstance(args[0], Thing):
                    args = list(args)
                    args[0] = args[0]._id

                if isinstance(args[0], (int, long)):
                    serialized = to36(args[0])
                else:
                    serialized = str(args[0])
                row_key_components.append(serialized)

            row_key_components.extend(str(x) for x in args[1:])
            row_key = '.'.join(row_key_components)

            query = fn(*args)

            query_sort = query._sort
            try:
                is_precomputed = query.precomputed
            except AttributeError:
                is_precomputed = _is_query_precomputed(query)

            return CachedQuery(model, row_key, query_sort, filter_fn,
                               is_precomputed)
        return cached_query_wrapper
    return cached_query_decorator


def merged_cached_query(fn):
    """Decorate a function describing a cached query made up of others.

    The decorated function should return a sequence of cached queries whose
    results will be merged together into a final listing.

    """
    def merge_wrapper(*args, **kwargs):
        queries = fn(*args, **kwargs)
        return MergedCachedQuery(queries)
    return merge_wrapper


class _BaseQueryCache(object):
    """The model through which cached queries to interact with Cassandra.

    Each cached query is stored as a distinct row in Cassandra.  The row key is
    given by higher level code (see the cached_query decorator above).  Each
    item in the materialized result of the query is stored as a separate
    column.  Each column name is the fullname of the item, while each value is
    the stuff CachedQuery needs to be able to sort the items (see
    CachedQuery._make_item_tuple).

    """

    __metaclass__ = tdb_cassandra.ThingMeta
    _connection_pool = 'main'
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)
    _compare_with = ASCII_TYPE
    _use_db = False
    _type_prefix = None
    _cf_name = None

    @classmethod
    def get(cls, keys):
        """Retrieve the items in a set of cached queries.

        For each cached query, this returns the thing tuples and the column
        timestamps for them.  The latter is useful for conditional removal
        during pruning.

        """
        rows = cls._cf.multiget(keys, include_timestamp=True,
                                column_count=tdb_cassandra.max_column_count)

        res = {}
        for row, columns in rows.iteritems():
            data = []
            timestamps = []

            for (key, (value, timestamp)) in columns.iteritems():
                value = json.loads(value)
                data.append((key,) + tuple(value))
                timestamps.append((key, timestamp))

            res[row] = (data, dict(timestamps))

        return res

    @classmethod
    def index_mangle_keys(cls, keys):
        if not keys:
            return {}

        index_keys = ["/".join((key, "index")) for key in keys]
        rows = cls._cf.multiget(index_keys,
                                column_reversed=True,
                                column_count=1)

        res = {}
        for key, columns in rows.iteritems():
            root_key = key.rsplit("/")[0]
            index_component = columns.keys()[0]
            mangled = "/".join((root_key, index_component))
            res[mangled] = root_key
        return res

    @classmethod
    @tdb_cassandra.will_write
    def insert(cls, mutator, key, columns, ttl=None):
        """Insert things into the cached query.

        This works as an upsert; if the thing already exists, it is updated. If
        not, it is actually inserted.

        """
        updates = dict((key, json.dumps(value))
                       for key, value in columns.iteritems())
        mutator.insert(cls._cf, key, updates, ttl=ttl)

    @classmethod
    @tdb_cassandra.will_write
    def replace(cls, mutator, key, columns, ttl):
        # XXX: this assumes that precomputed queries aren't updated at a
        # frequency / simultaneously in a way that could collide.
        job_key = datetime.datetime.now(g.tz).isoformat()
        cls.insert(mutator, key + "/" + job_key, columns, ttl=ttl)
        mutator.insert(cls._cf, key + "/index", {job_key: ""}, ttl=ttl)

    @classmethod
    @tdb_cassandra.will_write
    def remove(cls, mutator, key, columns):
        """Unconditionally remove things from the cached query."""
        mutator.remove(cls._cf, key, columns=columns)

    @classmethod
    @tdb_cassandra.will_write
    def remove_if_unchanged(cls, mutator, key, columns, timestamps):
        """Remove things from the cached query if unchanged.

        If the things have been changed since the specified timestamps, they
        will not be removed.  This is useful for avoiding race conditions while
        pruning.

        """
        for col in columns:
            mutator.remove(cls._cf, key, columns=[col],
                           timestamp=timestamps.get(col))

    @classmethod
    @tdb_cassandra.will_write
    def remove_older_than(cls, mutator, key, removal_timestamp):
        """Remove things older than the specified timestamp.

        Removing specific columns can cause tombstones to build up. When a row
        has tons of tombstones fetching that row gets slow because Cassandra
        must retrieve all the tombstones as well. Issuing a row remove with
        the timestamp specified clears out all the columns modified before
        that timestamp and somehow doesn't result in tombstones being left
        behind. This behavior was verified via request tracing.

        """

        mutator.remove(cls._cf, key, timestamp=removal_timestamp)


class UserQueryCache(_BaseQueryCache):
    """A query cache column family for user-keyed queries."""
    _use_db = True


class SubredditQueryCache(_BaseQueryCache):
    """A query cache column family for subreddit-keyed queries."""
    _use_db = True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
from datetime import datetime, timedelta
import json
from uuid import uuid1

from pycassa.types import CompositeType, AsciiType
from pycassa.system_manager import TIME_UUID_TYPE
from pylons import app_globals as g
import pytz

from r2.lib import hooks
from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import (
    ASCII_TYPE,
    UTF8_TYPE,
)
from r2.lib.utils import Enum, epoch_timestamp

from r2.models import Account


class Vote(object):
    DIRECTIONS = Enum("up", "down", "unvote")
    SERIALIZED_DIRECTIONS = {
        DIRECTIONS.up: 1,
        DIRECTIONS.down: -1,
        DIRECTIONS.unvote: 0,
    }
    DESERIALIZED_DIRECTIONS = {
        v: k for k, v in SERIALIZED_DIRECTIONS.iteritems()}

    def __init__(self, user, thing, direction, date, data=None, effects=None,
            get_previous_vote=True, event_data=None):
        if not thing.is_votable:
            raise TypeError("Can't create vote on unvotable thing %s" % thing)

        if direction not in self.DIRECTIONS:
            raise ValueError("Invalid vote direction: %s" % direction)

        self.user = user
        self.thing = thing
        self.direction = direction
        self.date = date.replace(tzinfo=g.tz)
        self.data = data
        self.event_data = event_data

        # see if the user has voted on this thing before
        if get_previous_vote:
            self.previous_vote = VoteDetailsByThing.get_vote(user, thing)
            if self.previous_vote:
                # XXX: why do we keep the old date?
                self.date = self.previous_vote.date.replace(tzinfo=g.tz)
        else:
            self.previous_vote = None

        self.effects = VoteEffects(self, effects)

    def __eq__(self, other):
        return (self.user == other.user and
            self.thing == other.thing and
            self.direction == other.direction)

    def __ne__(self, other):
        return not self == other

    @classmethod
    def serialize_direction(cls, direction):
        """Convert the DIRECTIONS enum to values used when storing."""
        if direction not in cls.DIRECTIONS:
            raise ValueError("Invalid vote direction: %s" % direction)

        return cls.SERIALIZED_DIRECTIONS[direction]

    @classmethod
    def deserialize_direction(cls, direction):
        """Convert stored vote direction value back to DIRECTIONS enum."""
        direction = int(direction)

        if direction not in cls.DESERIALIZED_DIRECTIONS:
            raise ValueError("Invalid vote direction: %s" % direction)

        return cls.DESERIALIZED_DIRECTIONS[direction]

    @property
    def _id(self):
        return "%s_%s" % (self.user._id36, self.thing._id36)

    @property
    def affected_thing_attr(self):
        """The attr on the thing this vote will increment."""
        if not self.effects.affects_score:
            return None

        if self.is_upvote:
            return "_ups"
        elif self.is_downvote:
            return "_downs"

    @property
    def is_upvote(self):
        return self.direction == self.DIRECTIONS.up

    @property
    def is_downvote(self):
        return self.direction == self.DIRECTIONS.down

    @property
    def is_self_vote(self):
        """Whether the voter is also the author of the thing voted on."""
        return self.user._id == self.thing.author_id

    @property
    def is_automatic_initial_vote(self):
        """Whether this is the automatic vote cast on things when posted."""
        return self.is_self_vote and not self.previous_vote

    @property
    def delay(self):
        """How long after the thing was posted that the vote was cast."""
        if self.is_automatic_initial_vote:
            return timedelta(0)
        
        return self.date - self.thing._date

    def apply_effects(self):
        """Apply the effects of the vote to the thing that was voted on."""
        # remove the old vote
        if self.previous_vote and self.previous_vote.affected_thing_attr:
            self.thing._incr(self.previous_vote.affected_thing_attr, -1)

        # add the new vote
        if self.affected_thing_attr:
            self.thing._incr(self.affected_thing_attr, 1)

        if self.effects.affects_karma:
            change = self.effects.karma_change
            if self.previous_vote:
                change -= self.previous_vote.effects.karma_change

            if change:
                self.thing.author_slow.incr_karma(
                    kind=self.thing.affects_karma_type,
                    sr=self.thing.subreddit_slow,
                    amt=change,
                )

        hooks.get_hook("vote.apply_effects").call(vote=self)

    def commit(self):
        """Apply the vote's effects and persist it."""
        if self.previous_vote and self == self.previous_vote:
            return

        self.apply_effects()
        VotesByAccount.write_vote(self)

        # Always update the search index if the thing has fewer than 20 votes.
        # When the thing has more votes queue an update less often.
        if self.thing.num_votes < 20 or self.thing.num_votes % 10 == 0:
            self.thing.update_search_index(boost_only=True)

        if self.event_data:
            g.events.vote_event(self)

        g.stats.simple_event('vote.total')


class VoteEffects(object):
    """Contains details about how a vote affects the thing voted on."""
    def __init__(self, vote, effects=None):
        """Initialize a new set of vote effects.

        If a dict of previously-determined effects are passed in as `effects`,
        those will be used instead of calculating the effects.
        """
        self.note_codes = {}
        self.validator = None

        if effects:
            self.affects_score = effects.pop("affects_score")
            self.affects_karma = effects.pop("affects_karma")
            self.other_effects = effects
        else:
            hook = hooks.get_hook("vote.get_validator")
            self.validator = hook.call_until_return(vote=vote, effects=self)

            self.affects_score = self.determine_affects_score(vote)
            self.affects_karma = self.determine_affects_karma(vote)
            self.other_effects = self.determine_other_effects(vote)

        self.karma_change = 0
        if self.affects_karma:
            if vote.is_upvote:
                self.karma_change = 1
            elif vote.is_downvote:
                self.karma_change = -1

    def add_note(self, code, message=None):
        self.note_codes[code] = message

    @property
    def notes(self):
        notes = []

        for code, message in self.note_codes.iteritems():
            note = code
            if message:
                note += " (%s)" % message
            notes.append(note)

        return notes

    def determine_affects_score(self, vote):
        """Determine whether the vote should affect the thing's score."""
        # If it's the automatic upvote on the user's own post, it won't affect
        # the score because we create it with a score of 1 already.
        if vote.is_automatic_initial_vote:
            self.add_note("AUTOMATIC_INITIAL_VOTE")
            return False

        if vote.previous_vote:
            if not vote.previous_vote.effects.affects_score:
                self.add_note("PREVIOUS_VOTE_NO_EFFECT")
                return False

        if self.validator:
            affects_score = self.validator.determine_affects_score()
            if affects_score is not None:
                return affects_score

        return True

    def determine_affects_karma(self, vote):
        """Determine whether the vote should affect the author's karma."""
        from r2.models import Comment

        if not self.affects_score:
            return False

        if vote.previous_vote:
            if not vote.previous_vote.effects.affects_karma:
                self.add_note("PREVIOUS_VOTE_NO_KARMA")
                return False

        if not bool(vote.thing.affects_karma_type):
            self.add_note("KARMALESS_THING")
            return False

        # never give karma on stickied comments. Only check distinguished
        # comments to avoid fetching the link on most votes, for performance.
        if isinstance(vote.thing, Comment) and vote.thing.is_distinguished:
            link = vote.thing.link_slow
            if vote.thing._id == link.sticky_comment_id:
                self.add_note("COMMENT_STICKIED")
                return False

        if self.validator:
            affects_karma = self.validator.determine_affects_karma()
            if affects_karma is not None:
                return affects_karma

        return True

    def determine_other_effects(self, vote):
        """Determine any other effects of the vote."""
        other_effects = {}

        if self.validator:
            other_effects.update(self.validator.other_effects)

        return other_effects

    @property
    def serializable_data(self):
        """Return the effects data in a format suitable for storing."""
        data = {
            "affects_score": self.affects_score,
            "affects_karma": self.affects_karma,
        }

        for key, value in self.other_effects.iteritems():
            data[key] = value

        if self.notes:
            data["notes"] = ", ".join(self.notes)

        return data


class VotesByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = False
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def rel(cls, thing_cls):
        from r2.models import Comment, Link
        if thing_cls == Link:
            return LinkVotesByAccount
        elif thing_cls == Comment:
            return CommentVotesByAccount

        raise TypeError("Can't find %r class for %r" % (cls, thing_cls))

    @classmethod
    def write_vote(cls, vote):
        rel = cls.rel(vote.thing.__class__)
        rel.create(vote.user, vote.thing, vote=vote)

    @classmethod
    def value_for(cls, thing1, thing2, vote):
        return str(Vote.serialize_direction(vote.direction))


class LinkVotesByAccount(VotesByAccount):
    _use_db = True
    _views = []
    _last_modified_name = "LinkVote"
    # this is taken care of in r2.lib.voting:cast_vote
    _write_last_modified = False


class CommentVotesByAccount(VotesByAccount):
    _use_db = True
    _views = []
    _last_modified_name = "CommentVote"
    # this is taken care of in r2.lib.voting:cast_vote
    _write_last_modified = False


class VoteDetailsByThing(tdb_cassandra.View):
    _use_db = False
    _fetch_all_columns = True
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)

    @classmethod
    def create(cls, user, thing, vote):
        # we don't use the user or thing args, but they need to be there for
        # calling this automatically when updating views of a DenormalizedRel
        vote_data = vote.data.copy()

        # pull the IP out of the data to store it separately with a TTL
        ip = vote_data.pop("ip")

        effects_data = vote.effects.serializable_data
        # split the notes out to store separately
        notes = effects_data.pop("notes", None)

        data = json.dumps({
            "direction": Vote.serialize_direction(vote.direction),
            "date": int(epoch_timestamp(vote.date)),
            "data": vote_data,
            "effects": effects_data,
        })

        cls._set_values(vote.thing._id36, {vote.user._id36: data})

        # write the IP data and notes separately so they can be TTLed
        if ip:
            VoterIPByThing.create(vote, ip)

        if notes:
            VoteNote.set(vote, notes)

    @classmethod
    def get_vote(cls, user, thing):
        details = cls.get_details(thing, [user])
        if details:
            return details[0]

        return None

    @staticmethod
    def convert_old_details(old_data):
        if "valid_thing" not in old_data:
            return old_data

        converted_data = {}
        converted_data["direction"] = int(old_data.pop("direction"))
        converted_data["date"] = int(old_data.pop("date"))

        valid_thing = old_data.pop("valid_thing", True)
        valid_user = old_data.pop("valid_user", True)
        converted_data["effects"] = {
            "affects_score": valid_thing,
            "affects_karma": valid_user,
        }

        if old_data:
            converted_data["data"] = old_data

        return converted_data

    @classmethod
    def get_details(cls, thing, voters=None):
        from r2.models import Comment, Link
        if isinstance(thing, Link):
            details_cls = VoteDetailsByLink
        elif isinstance(thing, Comment):
            details_cls = VoteDetailsByComment
        else:
            raise ValueError

        voter_id36s = None
        if voters:
            voter_id36s = [voter._id36 for voter in voters]

        try:
            row = details_cls._byID(thing._id36, properties=voter_id36s)
            raw_details = row._values()
        except tdb_cassandra.NotFound:
            return []

        try:
            row = VoterIPByThing._byID(thing._fullname, properties=voter_id36s)
            ips = row._values()
        except tdb_cassandra.NotFound:
            ips = {}

        details = []
        for voter_id36, json_data in raw_details.iteritems():
            data = json.loads(json_data)
            data = cls.convert_old_details(data)

            user = Account._byID36(voter_id36, data=True)
            direction = Vote.deserialize_direction(data.pop("direction"))
            date = datetime.utcfromtimestamp(data.pop("date"))
            effects = data.pop("effects")
            data["ip"] = ips.get(voter_id36)

            vote = Vote(user, thing, direction, date, data, effects,
                get_previous_vote=False)
            details.append(vote)
        details.sort(key=lambda d: d.date)

        return details


@tdb_cassandra.view_of(LinkVotesByAccount)
class VoteDetailsByLink(VoteDetailsByThing):
    _use_db = True


@tdb_cassandra.view_of(CommentVotesByAccount)
class VoteDetailsByComment(VoteDetailsByThing):
    _use_db = True


class VoterIPByThing(tdb_cassandra.View):
    _use_db = True
    _ttl = timedelta(days=100)
    _fetch_all_columns = True
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)

    @classmethod
    def create(cls, vote, ip):
        cls._set_values(vote.thing._fullname, {vote.user._id36: ip})


class VoteNote(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _ttl = timedelta(days=100)

    @classmethod
    def _rowkey(cls, vote):
        return '%s_%s' % (vote.user._fullname, vote.thing._fullname)

    @classmethod
    def set(cls, vote, note):
        rowkey = cls._rowkey(vote)
        column = {uuid1(): note}
        cls._set_values(rowkey, column)

    @classmethod
    def get(cls, vote):
        rowkey = cls._rowkey(vote)
        try:
            all_notes = cls._byID(rowkey)
        except tdb_cassandra.NotFound:
            return None

        return ", ".join(all_notes._values().values())
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import bcrypt
from collections import Counter, OrderedDict
from datetime import datetime, timedelta
import hashlib
import hmac
import time

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pycassa.system_manager import ASCII_TYPE, DATE_TYPE, UTF8_TYPE

from r2.config import feature
from r2.lib import amqp, filters, hooks
from r2.lib.db.thing import Thing, Relation, NotFound
from r2.lib.db.operators import lower
from r2.lib.db.userrel import UserRel
from r2.lib.db import tdb_cassandra
from r2.lib.memoize import memoize
from r2.lib.utils import (
    randstr,
    UrlParser,
    constant_time_compare,
    canonicalize_email,
    tup,
)
from r2.models.bans import TempTimeout
from r2.models.last_modified import LastModified
from r2.models.modaction import ModAction
from r2.models.trylater import TryLater


trylater_hooks = hooks.HookRegistrar()
COOKIE_TIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'


class AccountExists(Exception):
    pass


class Account(Thing):
    _cache = g.thingcache
    _data_int_props = Thing._data_int_props + ('link_karma', 'comment_karma',
                                               'report_made', 'report_correct',
                                               'report_ignored', 'spammer',
                                               'reported', 'gold_creddits',
                                               'inbox_count',
                                               'num_payment_methods',
                                               'num_failed_payments',
                                               'num_gildings',
                                               'admin_takedown_strikes',
                                              )
    _int_prop_suffix = '_karma'
    _essentials = ('name', )
    _defaults = dict(pref_numsites = 25,
                     pref_newwindow = False,
                     pref_clickgadget = 5,
                     pref_store_visits = False,
                     pref_public_votes = False,
                     pref_hide_from_robots = False,
                     pref_research = False,
                     pref_hide_ups = False,
                     pref_hide_downs = False,
                     pref_min_link_score = -4,
                     pref_min_comment_score = -4,
                     pref_num_comments = g.num_comments,
                     pref_highlight_controversial=False,
                     pref_default_comment_sort = 'confidence',
                     pref_lang = g.lang,
                     pref_content_langs = (g.lang,),
                     pref_over_18 = False,
                     pref_compress = False,
                     pref_domain_details = False,
                     pref_organic = True,
                     pref_no_profanity = True,
                     pref_label_nsfw = True,
                     pref_show_stylesheets = True,
                     pref_enable_default_themes=False,
                     pref_default_theme_sr=None,
                     pref_show_flair = True,
                     pref_show_link_flair = True,
                     pref_mark_messages_read = True,
                     pref_threaded_messages = True,
                     pref_collapse_read_messages = False,
                     pref_email_messages = False,
                     pref_private_feeds = True,
                     pref_force_https = False,
                     pref_hide_ads = False,
                     pref_show_trending=True,
                     pref_highlight_new_comments = True,
                     pref_monitor_mentions=True,
                     pref_collapse_left_bar=False,
                     pref_public_server_seconds=False,
                     pref_ignore_suggested_sort=False,
                     pref_beta=False,
                     pref_legacy_search=False,
                     mobile_compress = False,
                     mobile_thumbnail = True,
                     reported = 0,
                     report_made = 0,
                     report_correct = 0,
                     report_ignored = 0,
                     spammer = 0,
                     sort_options = {},
                     has_subscribed = False,
                     pref_media = 'subreddit',
                     pref_media_preview = 'subreddit',
                     wiki_override = None,
                     email = "",
                     email_verified = False,
                     nsfw_media_acknowledged = False,
                     ignorereports = False,
                     pref_show_promote = None,
                     gold = False,
                     gold_charter = False,
                     gold_creddits = 0,
                     num_gildings=0,
                     cake_expiration=None,
                     otp_secret=None,
                     state=0,
                     modmsgtime=None,
                     inbox_count=0,
                     banned_profile_visible=False,
                     pref_use_global_defaults=False,
                     pref_hide_locationbar=False,
                     pref_creddit_autorenew=False,
                     update_sent_messages=True,
                     num_payment_methods=0,
                     num_failed_payments=0,
                     pref_show_snoovatar=False,
                     gild_reveal_username=False,
                     selfserve_cpm_override_pennies=None,
                     pref_show_gold_expiration=False,
                     admin_takedown_strikes=0,
                     pref_threaded_modmail=False,
                     in_timeout=False,
                     has_used_mobile_app=False,
                     disable_karma=False,
                     )
    _preference_attrs = tuple(k for k in _defaults.keys()
                              if k.startswith("pref_"))

    @classmethod
    def _cache_prefix(cls):
        return "account:"

    def preferences(self):
        return {pref: getattr(self, pref) for pref in self._preference_attrs}

    def __eq__(self, other):
        if type(self) != type(other):
            return False

        return self._id == other._id

    def __ne__(self, other):
        return not self.__eq__(other)

    def has_interacted_with(self, sr):
        try:
            r = SubredditParticipationByAccount.fast_query(self, [sr])
        except tdb_cassandra.NotFound:
            return False

        return (self, sr) in r

    def karma(self, kind, sr = None):
        suffix = '_' + kind + '_karma'

        #if no sr, return the sum
        if sr is None:
            total = 0
            for k, v in self._t.iteritems():
                if k.endswith(suffix):
                    total += v

            # link karma includes both "link" and "self" values
            if kind == "link":
                total += self.karma("self")

            return total

        # if positive karma overall, default to MIN_UP_KARMA instead of 0
        if self.karma(kind) > 0:
            default_karma = g.MIN_UP_KARMA
        else:
            default_karma = 0

        if kind == "link":
            # link karma includes both "link" and "self", so it's a bit trickier
            link_karma = getattr(self, sr.name + suffix, None)
            self_karma = getattr(self, "%s_self_karma" % sr.name, None)

            # return default value only if they have *neither* link nor self
            if all(karma is None for karma in (link_karma, self_karma)):
                return default_karma

            return sum(karma for karma in (link_karma, self_karma) if karma)
        else:
            return getattr(self, sr.name + suffix, default_karma)

    def incr_karma(self, kind, sr, amt):
        # accounts can (manually) have their ability to gain/lose karma
        # disabled, to prevent special accounts like AutoModerator from
        # having a massive number of subreddit-karma attributes
        if self.disable_karma:
            return

        if sr.name.startswith('_'):
            g.log.info("Ignoring karma increase for subreddit %r" % (sr.name,))
            return

        prop = '%s_%s_karma' % (sr.name, kind)
        if hasattr(self, prop):
            return self._incr(prop, amt)
        else:
            default_val = self.karma(kind, sr)
            setattr(self, prop, default_val + amt)
            self._commit()

    @property
    def link_karma(self):
        return self.karma('link')

    @property
    def comment_karma(self):
        return self.karma('comment')

    def all_karmas(self, include_old=True):
        """Get all of the user's subreddit-specific karma totals.

        Returns an OrderedDict keyed on subreddit name and containing
        (link_karma, comment_karma) tuples, ordered by the combined total
        descending.
        """
        link_suffix = '_link_karma'
        self_suffix = '_self_karma'
        comment_suffix = '_comment_karma'

        comment_karmas = Counter()
        link_karmas = Counter()
        combined_karmas = Counter()

        for key, value in self._t.iteritems():
            if key.endswith(link_suffix):
                sr_name = key[:-len(link_suffix)]
                link_karmas[sr_name] += value
            elif key.endswith(self_suffix):
                # self karma gets added to link karma too
                sr_name = key[:-len(self_suffix)]
                link_karmas[sr_name] += value
            elif key.endswith(comment_suffix):
                sr_name = key[:-len(comment_suffix)]
                comment_karmas[sr_name] = value
            else:
                continue

            combined_karmas[sr_name] += value

        all_karmas = OrderedDict()
        for sr_name, total in combined_karmas.most_common():
            all_karmas[sr_name] = (link_karmas[sr_name],
                                   comment_karmas[sr_name])

        if include_old:
            old_link_karma = self._t.get('link_karma', 0)
            old_comment_karma = self._t.get('comment_karma', 0)
            if old_link_karma or old_comment_karma:
                all_karmas['ancient history'] = (old_link_karma,
                                                 old_comment_karma)

        return all_karmas

    def update_last_visit(self, current_time):
        from admintools import apply_updates

        timer = g.stats.get_timer("account.update_last_visit")
        timer.start()

        apply_updates(self, timer)

        prev_visit = LastModified.get(self._fullname, "Visit")
        timer.intermediate("get_last_modified")

        if prev_visit and current_time - prev_visit < timedelta(days=1):
            timer.intermediate("set_last_modified.noop")
            timer.stop()
            return

        LastModified.touch(self._fullname, "Visit")
        timer.intermediate("set_last_modified.done")
        timer.stop()

    def make_cookie(self, timestr=None):
        timestr = timestr or time.strftime(COOKIE_TIMESTAMP_FORMAT)
        id_time = str(self._id) + ',' + timestr
        to_hash = ','.join((id_time, self.password, g.secrets["SECRET"]))
        return id_time + ',' + hashlib.sha1(to_hash).hexdigest()

    def make_admin_cookie(self, first_login=None, last_request=None):
        first_login = first_login or datetime.utcnow().strftime(COOKIE_TIMESTAMP_FORMAT)
        last_request = last_request or datetime.utcnow().strftime(COOKIE_TIMESTAMP_FORMAT)
        hashable = ','.join((first_login, last_request, request.ip, request.user_agent, self.password))
        mac = hmac.new(g.secrets["SECRET"], hashable, hashlib.sha1).hexdigest()
        return ','.join((first_login, last_request, mac))

    def make_otp_cookie(self, timestamp=None):
        timestamp = timestamp or datetime.utcnow().strftime(COOKIE_TIMESTAMP_FORMAT)
        secrets = [request.user_agent, self.otp_secret, self.password]
        signature = hmac.new(g.secrets["SECRET"], ','.join([timestamp] + secrets), hashlib.sha1).hexdigest()

        return ",".join((timestamp, signature))

    def needs_captcha(self):
        if g.disable_captcha:
            return False

        hook = hooks.get_hook("account.is_captcha_exempt")
        captcha_exempt = hook.call_until_return(account=self)
        if captcha_exempt:
            return False

        if self.link_karma >= g.live_config["captcha_exempt_link_karma"]:
            return False

        if self.comment_karma >= g.live_config["captcha_exempt_comment_karma"]:
            return False

        return True

    @property
    def can_create_subreddit(self):
        hook = hooks.get_hook("account.can_create_subreddit")
        can_create = hook.call_until_return(account=self)
        if can_create is not None:
            return can_create

        min_age = timedelta(days=g.live_config["create_sr_account_age_days"])
        if self._age < min_age:
            return False

        if (self.link_karma < g.live_config["create_sr_link_karma"] and
                self.comment_karma < g.live_config["create_sr_comment_karma"]):
            return False

        return True

    @classmethod
    @memoize('account._by_name')
    def _by_name_cache(cls, name, allow_deleted=False):
        #relower name here, just in case
        deleted = (True, False) if allow_deleted else False
        q = cls._query(
            lower(cls.c.name) == name.lower(),
            cls.c._spam == (True, False),
            cls.c._deleted == deleted,
            data=True,
        )

        q._limit = 1
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_name(cls, name, allow_deleted = False, _update = False):
        #lower name here so there is only one cache
        uid = cls._by_name_cache(name.lower(), allow_deleted, _update = _update)
        if uid:
            return cls._byID(uid, data=True)
        else:
            raise NotFound, 'Account %s' % name

    @classmethod
    def _names_to_ids(cls, names, ignore_missing=False, allow_deleted=False,
                      _update=False):
        for name in names:
            uid = cls._by_name_cache(name.lower(), allow_deleted, _update=_update)
            if not uid:
                if ignore_missing:
                    continue
                raise NotFound('Account %s' % name)
            yield uid

    # Admins only, since it's not memoized
    @classmethod
    def _by_name_multiple(cls, name):
        q = cls._query(lower(Account.c.name) == name.lower(),
                       Account.c._spam == (True, False),
                       Account.c._deleted == (True, False))
        return list(q)

    @property
    def friends(self):
        return self.friend_ids()

    @property
    def enemies(self):
        return self.enemy_ids()

    @property
    def is_moderator_somewhere(self):
        # modmsgtime can be:
        #   - a date: the user is a mod somewhere and has unread modmail
        #   - False: the user is a mod somewhere and has no unread modmail
        #   - None: (the default) the user is not a mod anywhere
        return self.modmsgtime is not None

    def is_mutable(self, subreddit):
        # Don't allow muting of other mods in the subreddit
        if subreddit.is_moderator(self):
            return False

        # Don't allow muting of u/reddit or u/AutoModerator
        return not (self == self.system_user() or
            self == self.automoderator_user())

    # Used on the goldmember version of /prefs/friends
    @memoize('account.friend_rels')
    def friend_rels_cache(self):
        q = Friend._query(
            Friend.c._thing1_id == self._id,
            Friend.c._name == 'friend',
            thing_data=True,
        )
        return list(f._id for f in q)

    def friend_rels(self, _update = False):
        rel_ids = self.friend_rels_cache(_update=_update)
        try:
            rels = Friend._byID_rel(rel_ids, return_dict=False,
                                    eager_load = True, data = True,
                                    thing_data = True)
            rels = list(rels)
        except NotFound:
            if _update:
                raise
            else:
                return self.friend_rels(_update=True)

        if not _update:
            sorted_1 = sorted([r._thing2_id for r in rels])
            sorted_2 = sorted(list(self.friends))
            if sorted_1 != sorted_2:
                self.friend_ids(_update=True)
                return self.friend_rels(_update=True)
        return dict((r._thing2_id, r) for r in rels)

    def add_friend_note(self, friend, note):
        rels = self.friend_rels()
        rel = rels[friend._id]
        rel.note = note
        rel._commit()

    def _get_friend_ids_by(self, data_value_name, limit):
        friend_ids = self.friend_ids()
        if len(friend_ids) <= limit:
            return friend_ids

        with g.stats.get_timer("friends_query.%s" % data_value_name):
            result = self.sort_ids_by_data_value(
                friend_ids, data_value_name, limit=limit, desc=True)

        return result.fetchall()

    @memoize("get_recently_submitted_friend_ids", time=10*60)
    def get_recently_submitted_friend_ids(self, limit=100):
        return self._get_friend_ids_by("last_submit_time", limit)

    @memoize("get_recently_commented_friend_ids", time=10*60)
    def get_recently_commented_friend_ids(self, limit=100):
        return self._get_friend_ids_by("last_comment_time", limit)

    def delete(self, delete_message=None):
        self.delete_message = delete_message
        self.delete_time = datetime.now(g.tz)
        self._deleted = True
        self._commit()

        #update caches
        Account._by_name(self.name, allow_deleted = True, _update = True)
        #we need to catch an exception here since it will have been
        #recently deleted
        try:
            Account._by_name(self.name, _update = True)
        except NotFound:
            pass

        # Mark this account for immediate cleanup tasks
        amqp.add_item('account_deleted', self._fullname)

        # schedule further cleanup after a possible recovery period
        TryLater.schedule("account_deletion", self._id36,
                          delay=timedelta(days=90))

    # 'State' bitfield properties
    @property
    def _banned(self):
        return self.state & 1

    @_banned.setter
    def _banned(self, value):
        if value and not self._banned:
            self.state |= 1
            # Invalidate all cookies by changing the password
            # First back up the password so we can reverse this
            self.backup_password = self.password
            # New PW doesn't matter, they can't log in with it anyway.
            # Even if their PW /was/ 'banned' for some reason, this
            # will change the salt and thus invalidate the cookies
            change_password(self, 'banned')

            # deauthorize all access tokens
            from r2.models.token import OAuth2AccessToken
            from r2.models.token import OAuth2RefreshToken

            OAuth2AccessToken.revoke_all_by_user(self)
            OAuth2RefreshToken.revoke_all_by_user(self)
        elif not value and self._banned:
            self.state &= ~1

            # Undo the password thing so they can log in
            self.password = self.backup_password

            # They're on their own for OAuth tokens, though.

        self._commit()

    @property
    def subreddits(self):
        from subreddit import Subreddit
        return Subreddit.user_subreddits(self)

    def special_distinguish(self):
        if self._t.get("special_distinguish_name"):
            return dict((k, self._t.get("special_distinguish_"+k, None))
                        for k in ("name", "kind", "symbol", "cssclass", "label", "link"))
        else:
            return None

    def set_email(self, email):
        old_email = self.email
        self.email = email
        self._commit()
        AccountsByCanonicalEmail.update_email(self, old_email, email)

    def canonical_email(self):
        return canonicalize_email(self.email)

    @classmethod
    def system_user(cls):
        try:
            return cls._by_name(g.system_user)
        except (NotFound, AttributeError):
            return None

    @classmethod
    def automoderator_user(cls):
        try:
            return cls._by_name(g.automoderator_account)
        except (NotFound, AttributeError):
            return None

    def use_subreddit_style(self, sr):
        """Return whether to show subreddit stylesheet depending on
        individual selection if available, else use pref_show_stylesheets"""
        # if FakeSubreddit, there is no stylesheet
        if not hasattr(sr, '_id'):
            return False
        if not feature.is_enabled('stylesheets_everywhere'):
            return self.pref_show_stylesheets
        # if stylesheet isn't individually enabled/disabled, use global pref
        return bool(getattr(self, "sr_style_%s_enabled" % sr._id,
            self.pref_show_stylesheets))

    def set_subreddit_style(self, sr, use_style):
        if hasattr(sr, '_id'):
            setattr(self, "sr_style_%s_enabled" % sr._id, use_style)
            self._commit()

    def flair_enabled_in_sr(self, sr_id):
        return getattr(self, 'flair_%s_enabled' % sr_id, True)

    def flair_text(self, sr_id, obey_disabled=False):
        if obey_disabled and not self.flair_enabled_in_sr(sr_id):
            return None
        return getattr(self, 'flair_%s_text' % sr_id, None)

    def flair_css_class(self, sr_id, obey_disabled=False):
        if obey_disabled and not self.flair_enabled_in_sr(sr_id):
            return None
        return getattr(self, 'flair_%s_css_class' % sr_id, None)

    def can_flair_in_sr(self, user, sr):
        """Return whether a user can set this one's flair in a subreddit."""
        can_assign_own = self._id == user._id and sr.flair_self_assign_enabled

        return can_assign_own or sr.is_moderator_with_perms(user, "flair")

    def set_flair(self, subreddit, text=None, css_class=None, set_by=None,
            log_details="edit"):
        log_details = "flair_%s" % log_details
        if not text and not css_class:
            # set to None instead of potentially empty strings
            text = css_class = None
            subreddit.remove_flair(self)
            log_details = "flair_delete"
        elif not subreddit.is_flair(self):
            subreddit.add_flair(self)

        setattr(self, 'flair_%s_text' % subreddit._id, text)
        setattr(self, 'flair_%s_css_class' % subreddit._id, css_class)
        self._commit()

        if set_by and set_by != self:
            ModAction.create(subreddit, set_by, action='editflair',
                target=self, details=log_details)

    def get_trophy_id(self, uid):
        '''Return the ID of the Trophy associated with the given "uid"

        `uid` - The unique identifier for the Trophy to look up

        '''
        return getattr(self, 'received_trophy_%s' % uid, None)

    def set_trophy_id(self, uid, trophy_id):
        '''Recored that a user has received a Trophy with "uid"

        `uid` - The trophy "type" that the user should only have one of
        `trophy_id` - The ID of the corresponding Trophy object

        '''
        return setattr(self, 'received_trophy_%s' % uid, trophy_id)

    @property
    def employee(self):
        """Return if the user is an employee.

        Being an employee grants them various special privileges.

        """
        return (hasattr(self, 'name') and
                (self.name in g.admins or
                 self.name in g.sponsors or
                 self.name in g.employees))

    @property
    def has_gold_subscription(self):
        return bool(getattr(self, 'gold_subscr_id', None))

    @property
    def has_paypal_subscription(self):
        return (self.has_gold_subscription and
                not self.gold_subscr_id.startswith('cus_'))

    @property
    def has_stripe_subscription(self):
        return (self.has_gold_subscription and
                self.gold_subscr_id.startswith('cus_'))

    @property
    def gold_will_autorenew(self):
        return (self.has_gold_subscription or
                (self.pref_creddit_autorenew and self.gold_creddits > 0))

    @property
    def timeout_expiration(self):
        """Find the expiration date of the user's temp-timeout as a datetime
        object.

        Returns None if no temp-timeout found.
        """
        if not self.in_timeout:
            return None

        return TempTimeout.search(self.name).get(self.name)

    @property
    def days_remaining_in_timeout(self):
        if not self.in_timeout:
            return 0

        expires = self.timeout_expiration

        if not expires:
            return 0

        # TryLater runs periodically, so if the suspension expires
        # within that time, then the remaining number of days is 0
        # which is the same as a permanent suspension. Return 1 day
        # remaining if it already expired but the TryLater queue hasn't
        # cleared it yet.
        days_left = (expires - datetime.now(g.tz)).days + 1
        return max(days_left, 1)

    def incr_admin_takedown_strikes(self, amt=1):
        return self._incr('admin_takedown_strikes', amt)

    def get_style_override(self):
        """Return the subreddit selected for reddit theme.

        If the user has a theme selected and enabled and also has
        the feature flag enabled, return the subreddit name.
        Otherwise, return None.
        """
        # Experiment to change the default style to determine if
        # engagement metrics change
        if (feature.is_enabled("default_design") and
                feature.variant("default_design") == "nautclassic"):
            return "nautclassic"

        if (feature.is_enabled("default_design") and
                feature.variant("default_design") == "serene"):
            return "serene"

        # Reddit themes is not enabled for this user
        if not feature.is_enabled('stylesheets_everywhere'):
            return None

        # Make sure they have the theme enabled
        if not self.pref_enable_default_themes:
            return None

        return self.pref_default_theme_sr

    def has_been_atoed(self):
        """Return true if this account has ever been required to reset their password
        """
        return 'force_password_reset' in self._t


class FakeAccount(Account):
    _nodb = True
    pref_no_profanity = True

    def __eq__(self, other):
        return self is other

def valid_admin_cookie(cookie):
    if g.read_only_mode:
        return (False, None)

    # parse the cookie
    try:
        first_login, last_request, hash = cookie.split(',')
    except ValueError:
        return (False, None)

    # make sure it's a recent cookie
    try:
        first_login_time = datetime.strptime(first_login, COOKIE_TIMESTAMP_FORMAT)
        last_request_time = datetime.strptime(last_request, COOKIE_TIMESTAMP_FORMAT)
    except ValueError:
        return (False, None)

    cookie_age = datetime.utcnow() - first_login_time
    if cookie_age.total_seconds() > g.ADMIN_COOKIE_TTL:
        return (False, None)

    idle_time = datetime.utcnow() - last_request_time
    if idle_time.total_seconds() > g.ADMIN_COOKIE_MAX_IDLE:
        return (False, None)

    # validate
    expected_cookie = c.user.make_admin_cookie(first_login, last_request)
    return (constant_time_compare(cookie, expected_cookie),
            first_login)


def valid_otp_cookie(cookie):
    if g.read_only_mode:
        return False

    # parse the cookie
    try:
        remembered_at, signature = cookie.split(",")
    except ValueError:
        return False

    # make sure it hasn't expired
    try:
        remembered_at_time = datetime.strptime(remembered_at, COOKIE_TIMESTAMP_FORMAT)
    except ValueError:
        return False

    age = datetime.utcnow() - remembered_at_time
    if age.total_seconds() > g.OTP_COOKIE_TTL:
        return False

    # validate
    expected_cookie = c.user.make_otp_cookie(remembered_at)
    return constant_time_compare(cookie, expected_cookie)


def valid_feed(name, feedhash, path):
    if name and feedhash and path:
        from r2.lib.template_helpers import add_sr
        path = add_sr(path)
        try:
            user = Account._by_name(name)
            if (user.pref_private_feeds and
                constant_time_compare(feedhash, make_feedhash(user, path))):
                return user
        except NotFound:
            pass

def make_feedhash(user, path):
    return hashlib.sha1("".join([user.name, user.password,
                                 g.secrets["FEEDSECRET"]])
                   ).hexdigest()

def make_feedurl(user, path, ext = "rss"):
    u = UrlParser(path)
    u.update_query(user = user.name,
                   feed = make_feedhash(user, path))
    u.set_extension(ext)
    return u.unparse()

def valid_password(a, password, compare_password=None):
    # bail out early if the account or password's invalid
    if not hasattr(a, 'name') or not hasattr(a, 'password') or not password:
        return False

    convert_password = False
    if compare_password is None:
        convert_password = True
        compare_password = a.password

    # standardize on utf-8 encoding
    password = filters._force_utf8(password)

    if compare_password.startswith('$2a$'):
        # it's bcrypt.

        try:
            expected_hash = bcrypt.hashpw(password, compare_password)
        except ValueError:
            # password is invalid because it contains null characters
            return False

        if not constant_time_compare(compare_password, expected_hash):
            return False

        # if it's using the current work factor, we're done, but if it's not
        # we'll have to rehash.
        # the format is $2a$workfactor$salt+hash
        work_factor = int(compare_password.split("$")[2])
        if work_factor == g.bcrypt_work_factor:
            return a
    else:
        # alright, so it's not bcrypt. how old is it?
        # if the length of the stored hash is 43 bytes, the sha-1 hash has a salt
        # otherwise it's sha-1 with no salt.
        salt = ''
        if len(compare_password) == 43:
            salt = compare_password[:3]
        expected_hash = passhash(a.name, password, salt)

        if not constant_time_compare(compare_password, expected_hash):
            return False

    # since we got this far, it's a valid password but in an old format
    # let's upgrade it
    if convert_password:
        a.password = bcrypt_password(password)
        a._commit()
    return a

def bcrypt_password(password):
    salt = bcrypt.gensalt(log_rounds=g.bcrypt_work_factor)
    return bcrypt.hashpw(password, salt)

def passhash(username, password, salt = ''):
    if salt is True:
        salt = randstr(3)
    tohash = '%s%s %s' % (salt, username, password)
    return salt + hashlib.sha1(tohash).hexdigest()

def change_password(user, newpassword):
    user.password = bcrypt_password(newpassword)
    user._commit()
    LastModified.touch(user._fullname, 'Password')
    return True


def register(name, password, registration_ip):
    # get a lock for registering an Account with this name to prevent
    # simultaneous operations from creating multiple Accounts with the same name
    with g.make_lock("account_register", "register_%s" % name.lower()):
        try:
            account = Account._by_name(name)
            raise AccountExists
        except NotFound:
            account = Account(
                name=name,
                password=bcrypt_password(password),
                # new accounts keep the profanity filter settings until opting out
                pref_no_profanity=True,
                registration_ip=registration_ip,
            )
            account._commit()

            # update Account._by_name to pick up this new name->Account
            Account._by_name(name, _update=True)
            Account._by_name(name, allow_deleted=True, _update=True)

            return account


class Friend(Relation(Account, Account)):
    _cache = g.thingcache

    @classmethod
    def _cache_prefix(cls):
        return "friend:"


Account.__bases__ += (UserRel('friend', Friend, disable_reverse_ids_fn=True),
                      UserRel('enemy', Friend, disable_reverse_ids_fn=False))

class DeletedUser(FakeAccount):
    @property
    def name(self):
        return '[deleted]'

    @property
    def _deleted(self):
        return True

    def _fullname(self):
        raise NotImplementedError

    def _id(self):
        raise NotImplementedError

    def __setattr__(self, attr, val):
        if attr == '_deleted':
            pass
        else:
            object.__setattr__(self, attr, val)


class BlockedSubredditsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _last_modified_name = 'block_subreddit'
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _connection_pool = 'main'
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def block(cls, user, sr):
        cls.create(user, sr)

    @classmethod
    def unblock(cls, user, sr):
        cls.destroy(user, sr)

    @classmethod
    def is_blocked(cls, user, sr):
        try:
            r = cls.fast_query(user, [sr])
        except tdb_cassandra.NotFound:
            return False
        return (user, sr) in r


@trylater_hooks.on("trylater.account_deletion")
def deleted_account_cleanup(data):
    from r2.models import Subreddit
    from r2.models.admin_notes import AdminNotesBySystem
    from r2.models.flair import Flair
    from r2.models.token import OAuth2Client

    for account_id36 in data.itervalues():
        account = Account._byID36(account_id36, data=True)

        if not account._deleted:
            continue

        # wipe the account's password and email address
        account.password = ""
        account.email = ""
        account.email_verified = False

        notes = ""

        # "noisy" rel removals, we'll record all of these in the account's
        # usernotes in case we need the information later
        rel_removal_descriptions = {
            "moderator": "Unmodded",
            "moderator_invite": "Cancelled mod invite",
            "contributor": "Removed as contributor",
            "banned": "Unbanned",
            "wikibanned": "Un-wikibanned",
            "wikicontributor": "Removed as wiki contributor",
        }
        if account.has_subscribed:
            rel_removal_descriptions["subscriber"] = "Unsubscribed"

        for rel_type, description in rel_removal_descriptions.iteritems():
            try:
                ids_fn = getattr(Subreddit, "reverse_%s_ids" % rel_type)
                sr_ids = ids_fn(account)

                sr_names = []
                srs = Subreddit._byID(sr_ids, data=True, return_dict=False)
                for subreddit in srs:
                    remove_fn = getattr(subreddit, "remove_" + rel_type)
                    remove_fn(account)
                    sr_names.append(subreddit.name)

                if description and sr_names:
                    sr_list = ", ".join(sr_names)
                    notes += "* %s from %s\n" % (description, sr_list)
            except Exception as e:
                notes += "* Error cleaning up %s rels: %s\n" % (rel_type, e)

        # silent rel removals, no record left in the usernotes
        rel_classes = {
            "flair": Flair,
            "friend": Friend,
            "enemy": Friend,
        }

        for rel_name, rel_cls in rel_classes.iteritems():
            try:
                rels = rel_cls._query(
                    rel_cls.c._thing2_id == account._id,
                    rel_cls.c._name == rel_name,
                    eager_load=True,
                )
                for rel in rels:
                    remove_fn = getattr(rel._thing1, "remove_" + rel_name)
                    remove_fn(account)
            except Exception as e:
                notes += "* Error cleaning up %s rels: %s\n" % (rel_name, e)

        # add the note with info about the major changes to the account
        if notes:
            AdminNotesBySystem.add(
                system_name="user",
                subject=account.name,
                note="Account deletion cleanup summary:\n\n%s" % notes,
                author="<automated>",
                when=datetime.now(g.tz),
            )

        account._commit()


class AccountsByCanonicalEmail(tdb_cassandra.View):
    __metaclass__ = tdb_cassandra.ThingMeta

    _use_db = True
    _compare_with = UTF8_TYPE
    _extra_schema_creation_args = dict(
        key_validation_class=UTF8_TYPE,
    )

    @classmethod
    def update_email(cls, account, old, new):
        old, new = map(canonicalize_email, (old, new))

        if old == new:
            return

        with cls._cf.batch() as b:
            if old:
                b.remove(old, {account._id36: ""})
            if new:
                b.insert(new, {account._id36: ""})

    @classmethod
    def get_accounts(cls, email_address):
        canonical = canonicalize_email(email_address)
        if not canonical:
            return []
        account_id36s = cls.get_time_sorted_columns(canonical).keys()
        return Account._byID36(account_id36s, data=True, return_dict=False)


class SubredditParticipationByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "default_validation_class": DATE_TYPE,
    }

    @classmethod
    def value_for(cls, thing1, thing2):
        return datetime.now(g.tz)

    @classmethod
    def mark_participated(cls, account, subreddit):
        cls.create(account, [subreddit])


class QuarantinedSubredditOptInsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _last_modified_name = 'QuarantineSubredditOptin'
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "default_validation_class": DATE_TYPE,
    }
    _connection_pool = 'main'
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return datetime.now(g.tz)

    @classmethod
    def opt_in(cls, account, subreddit):
        if subreddit.quarantine:
            cls.create(account, subreddit)

    @classmethod
    def opt_out(cls, account, subreddit):
        if subreddit.is_subscriber(account):
            subreddit.remove_subscriber(account)
        cls.destroy(account, subreddit)

    @classmethod
    def is_opted_in(cls, user, subreddit):
        try:
            r = cls.fast_query(user, [subreddit])
        except tdb_cassandra.NotFound:
            return False
        return (user, subreddit) in r
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import app_globals as g

from r2.lib.db.operators import asc, desc, lower
from r2.lib.db.thing import Thing, Relation, NotFound
from r2.lib.memoize import memoize
from r2.models import Account


class Award(Thing):
    _cache = g.thingcache
    _defaults = dict(
        awardtype='regular',
        api_ok=False,
    )

    @classmethod
    def _cache_prefix(cls):
        return "award:"

    @classmethod
    @memoize('award.all_awards')
    def _all_awards_cache(cls):
        return [ a._id for a in Award._query(sort=asc('_date'), limit=100) ]

    @classmethod
    def _all_awards(cls, _update=False):
        all = Award._all_awards_cache(_update=_update)
        # Can't just return Award._byID() results because
        # the ordering will be lost
        d = Award._byID(all, data=True)
        return [ d[id] for id in all ]

    @classmethod
    def _new(cls, codename, title, awardtype, imgurl, api_ok):
        a = Award(codename=codename, title=title, awardtype=awardtype,
                  imgurl=imgurl, api_ok=api_ok)
        a._commit()
        Award._all_awards_cache(_update=True)

    @classmethod
    def _by_codename(cls, codename):
        q = cls._query(lower(Award.c.codename) == codename.lower())
        q._limit = 1
        award = list(q)

        if award:
            return cls._byID(award[0]._id, True)
        else:
            raise NotFound, 'Award %s' % codename

    @classmethod
    def give_if_needed(cls, codename, user,
                       description=None, url=None):
        """Give an award to a user, unless they already have it.
           Returns the trophy. Does nothing and prints nothing
           (except for g.log.debug) if the award doesn't exist."""

        try:
            award = Award._by_codename(codename)
        except NotFound:
            g.log.debug("No award named '%s'" % codename)
            return None

        trophies = Trophy.by_account(user)

        for trophy in trophies:
            if trophy._thing2.codename == codename:
                g.log.debug("%s already has %s" % (user, codename))
                return trophy

        g.log.debug("Gave %s to %s" % (codename, user))
        return Trophy._new(user, award, description=description,
                        url=url)

    @classmethod
    def take_away(cls, codename, user):
        """Takes an award out of a user's trophy case.  Returns silently
           (except for g.log.debug) if there's no such award."""

        found = False

        try:
            award = Award._by_codename(codename)
        except NotFound:
            g.log.debug("No award named '%s'" % codename)
            return

        trophies = Trophy.by_account(user)

        for trophy in trophies:
            if trophy._thing2.codename == codename:
                if found:
                    g.log.debug("%s had multiple %s awards!" % (user, codename))
                trophy._delete()
                Trophy.by_account(user, _update=True)
                Trophy.by_award(award, _update=True)
                found = True

        if found:
            g.log.debug("Took %s from %s" % (codename, user))
        else:
            g.log.debug("%s didn't have %s" % (user, codename))


class FakeTrophy(object):
    def __init__(self, recipient, award, description=None, url=None):
        self._thing2 = award
        self._thing1 = recipient
        self.description = description
        self.url = url
        self.trophy_url = getattr(self, "url",
                                  getattr(self._thing2, "url", None))
        self._id = self._id36 = None


class Trophy(Relation(Account, Award)):
    _cache = g.thingcache
    _enable_fast_query = False

    @classmethod
    def _cache_prefix(cls):
        return "trophy:"

    @classmethod
    def _new(cls, recipient, award, description=None, url=None):
        # The "name" column of the Relation can't be a constant or else a
        # given account would not be allowed to win a given award more than
        # once.
        t = Trophy(recipient, award, name="trophy")
        t._name = str(t._date)

        if description:
            t.description = description

        if url:
            t.url = url

        t._commit()
        t.update_caches()
        return t
    
    def update_caches(self):
        self.by_account(self._thing1, _update=True)
        self.by_award(self._thing2, _update=True)

    @classmethod
    @memoize('trophy.by_account2')
    def by_account_cache(cls, account_id):
        q = Trophy._query(Trophy.c._thing1_id == account_id,
                          sort = desc('_date'))
        q._limit = 500
        return [ t._id for t in q ]

    @classmethod
    def by_account(cls, account, _update=False):
        rel_ids = cls.by_account_cache(account._id, _update=_update)
        trophies = Trophy._byID_rel(rel_ids, data=True, eager_load=True,
            thing_data=True, return_dict=False, ignore_missing=True)
        return trophies

    @classmethod
    @memoize('trophy.by_award2')
    def by_award_cache(cls, award_id):
        q = Trophy._query(Trophy.c._thing2_id == award_id,
                          sort = desc('_date'))
        q._limit = 50
        return [ t._id for t in q ]

    @classmethod
    def by_award(cls, award, _update=False):
        rel_ids = cls.by_award_cache(award._id, _update=_update)
        trophies = Trophy._byID_rel(rel_ids, data=True, eager_load=True,
                                    thing_data=True, return_dict = False)
        return trophies

    @classmethod
    def claim(cls, user, uid, award, description, url):
        with g.make_lock("claim_award", str("%s_%s" % (user.name, uid))):
            existing_trophy_id = user.get_trophy_id(uid)
            if existing_trophy_id:
                trophy = cls._byID(existing_trophy_id)
                preexisting = True
            else:
                preexisting = False
                trophy = cls._new(user, award, description=description,
                                  url=url)
                user.set_trophy_id(uid, trophy._id)
                user._commit()
        return trophy, preexisting

    @property
    def trophy_url(self):
        return getattr(self, "url", getattr(self._thing2, "url", None))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import Counter

from r2.lib.db.thing import Relation, MultiRelation
from r2.lib.utils import tup
from r2.models import Link, Comment, Message, Subreddit, Account

from pylons import tmpl_context as c
from pylons import app_globals as g


_LinkReport = Relation(Account, Link)
_CommentReport = Relation(Account, Comment)
_SubredditReport = Relation(Account, Subreddit)
_MessageReport = Relation(Account, Message)
REPORT_RELS = (_LinkReport, _CommentReport, _SubredditReport, _MessageReport)

for report_cls in REPORT_RELS:
    report_cls._cache = g.thingcache

_LinkReport._cache_prefix = classmethod(lambda cls: "reportlink:")
_CommentReport._cache_prefix = classmethod(lambda cls: "reportcomment:")
_SubredditReport._cache_prefix = classmethod(lambda cls: "reportsr:")
_MessageReport._cache_prefix = classmethod(lambda cls: "reportmessage:")


class Report(MultiRelation('report', *REPORT_RELS)):
    _field = 'reported'

    @classmethod
    def new(cls, user, thing, reason=None):
        from r2.lib.db import queries

        # check if this report exists already!
        rel = cls.rel(user, thing)
        q = rel._fast_query(user, thing, ['-1', '0', '1'])
        q = [ report for (tupl, report) in q.iteritems() if report ]
        if q:
            # stop if we've seen this before, so that we never get the
            # same report from the same user twice
            oldreport = q[0]
            g.log.debug("Ignoring duplicate report %s" % oldreport)
            return oldreport

        kw = {}
        if reason:
            kw['reason'] = reason

        r = Report(user, thing, '0', **kw)

        # mark item as reported
        try:
            thing._incr(cls._field)
        except (ValueError, TypeError):
            g.log.error("%r has bad field %r = %r" % (thing, cls._field,
                         getattr(thing, cls._field, "(nonexistent)")))
            raise

        r._commit()

        if hasattr(thing, 'author_id'):
            author = Account._byID(thing.author_id, data=True)
            author._incr('reported')

        if not getattr(thing, "ignore_reports", False):
            # update the reports queue if it exists
            queries.new_report(thing, r)

            # if the thing is already marked as spam, accept the report
            if thing._spam:
                cls.accept(thing)

        return r

    @classmethod
    def for_thing(cls, thing):
        rel = cls.rel(Account, thing.__class__)
        rels = rel._query(rel.c._thing2_id == thing._id, data=True)

        return list(rels)

    @classmethod
    def accept(cls, things, correct = True):
        from r2.lib.db import queries

        things = tup(things)

        things_by_cls = {}
        for thing in things:
            things_by_cls.setdefault(thing.__class__, []).append(thing)

        for thing_cls, cls_things in things_by_cls.iteritems():
            to_clear = []
            # look up all of the reports for each thing
            rel_cls = cls.rel(Account, thing_cls)
            thing_ids = [t._id for t in cls_things]
            rels = rel_cls._query(rel_cls.c._thing2_id == thing_ids)
            for r in rels:
                if r._name == '0':
                    r._name = '1' if correct else '-1'
                    r._commit()

            for thing in cls_things:
                if thing.reported > 0:
                    thing.reported = 0
                    thing._commit()
                    to_clear.append(thing)

            queries.clear_reports(to_clear, rels)

    @classmethod
    def get_reports(cls, wrapped, max_user_reasons=20):
        """Get two lists of mod and user reports on the item."""
        if (wrapped.reported > 0 and
                (wrapped.can_ban or
                 getattr(wrapped, "promoted", None) and c.user_is_sponsor)):
            from r2.models import SRMember

            reports = cls.for_thing(wrapped.lookups[0])

            query = SRMember._query(SRMember.c._thing1_id == wrapped.sr_id,
                                    SRMember.c._name == "moderator")
            mod_dates = {rel._thing2_id: rel._date for rel in query}

            if g.automoderator_account:
                automoderator = Account._by_name(g.automoderator_account)
            else:
                automoderator = None

            mod_reports = []
            user_reports = []

            for report in reports:
                # always include AutoModerator reports
                if automoderator and report._thing1_id == automoderator._id:
                    mod_reports.append(report)
                # include in mod reports if made after the user became a mod
                elif (report._thing1_id in mod_dates and
                        report._date >= mod_dates[report._thing1_id]):
                    mod_reports.append(report)
                else:
                    user_reports.append(report)

            # mod reports return as tuples with (reason, name)
            mods = Account._byID([report._thing1_id
                                  for report in mod_reports],
                                 data=True, return_dict=True)
            mod_reports = [(getattr(report, "reason", None),
                            mods[report._thing1_id].name)
                            for report in mod_reports]

            # user reports return as tuples with (reason, count)
            user_reports = Counter([getattr(report, "reason", None)
                                    for report in user_reports])
            user_reports = user_reports.most_common(max_user_reasons)

            return mod_reports, user_reports
        else:
            return [], []

    @classmethod
    def get_reasons(cls, wrapped):
        """Transition method in case API clients were already using this."""
        if wrapped.can_ban and wrapped.reported > 0:
            return [("This attribute is deprecated. Please use mod_reports "
                     "and user_reports instead.")]
        else:
            return []
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pycassa.system_manager import ASCII_TYPE, UTF8_TYPE
from pycassa.types import LongType

from r2.config import feature
from r2.lib.db.thing import (
    Thing, Relation, NotFound, MultiRelation, CreationError)
from r2.lib.db.operators import desc
from r2.lib.errors import RedditError
from r2.lib.tracking import (
    get_site,
)
from r2.lib.utils import (
    base_url,
    domain,
    epoch_timestamp,
    feature_utils,
    strip_www,
    timesince,
    title_to_url,
    tup,
    UrlParser,
)
from account import (
    Account,
    BlockedSubredditsByAccount,
    DeletedUser,
    SubredditParticipationByAccount,
)
from subreddit import (
    DefaultSR,
    DomainSR,
    FakeSubreddit,
    Subreddit,
    SubredditsActiveForFrontPage,
)
from printable import Printable
from r2.config import extensions
from r2.lib.memoize import memoize
from r2.lib.wrapped import Wrapped
from r2.lib.filters import _force_utf8, _force_unicode
from r2.lib import hooks, utils
from mako.filters import url_escape
from r2.lib.strings import strings, Score
from r2.lib.db import tdb_cassandra, sorts
from r2.lib.db.tdb_cassandra import view_of
from r2.lib.utils import sanitize_url
from r2.models.gold import (
    GildedCommentsByAccount,
    GildedLinksByAccount,
    make_gold_message,
)
from r2.models.modaction import ModAction
from r2.models.subreddit import MultiReddit
from r2.models.trylater import TryLater
from r2.models.query_cache import CachedQueryMutator
from r2.models.promo import PROMOTE_STATUS
from r2.models.vote import Vote

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _
from datetime import datetime, timedelta
from hashlib import md5
import simplejson as json

import random, re
import pycassa
from collections import defaultdict
from itertools import cycle
from pycassa.cassandra.ttypes import NotFoundException
from pycassa.system_manager import (
    ASCII_TYPE,
    DOUBLE_TYPE,
)
import pytz

NOTIFICATION_EMAIL_DELAY = timedelta(hours=1)

class LinkExists(Exception): pass


class Link(Thing, Printable):
    _cache = g.thingcache
    _data_int_props = Thing._data_int_props + (
        'num_comments', 'reported', 'gildings')
    _defaults = dict(is_self=False,
                     suggested_sort=None,
                     over_18=False,
                     over_18_override=False,
                     reported=0, num_comments=0,
                     moderator_banned=False,
                     banned_before_moderator=False,
                     media_object=None,
                     secure_media_object=None,
                     preview_object=None,
                     media_url=None,
                     gifts_embed_url=None,
                     media_autoplay=False,
                     domain_override=None,
                     third_party_tracking=None,
                     third_party_tracking_2=None,
                     promoted=None,
                     payment_flagged_reason="",
                     fraud=None,
                     managed_promo=False,
                     pending=False,
                     disable_comments=False,
                     locked=False,
                     selftext='',
                     sendreplies=True,
                     ip='0.0.0.0',
                     flair_text=None,
                     flair_css_class=None,
                     contest_mode=False,
                     sticky_comment_id=None,
                     ignore_reports=False,
                     gildings=0,
                     mobile_ad_url="",
                     admin_takedown=False,
                     removed_link_child=None,
                     precomputed_sorts=None,
                     )
    _essentials = ('sr_id', 'author_id')
    _nsfw = re.compile(r"\bnsf[wl]\b", re.I)

    SELFTEXT_MAX_LENGTH = 40000

    is_votable = True

    @classmethod
    def _cache_prefix(cls):
        return "link:"

    def __init__(self, *a, **kw):
        Thing.__init__(self, *a, **kw)

    @property
    def affects_karma_type(self):
        if self.is_self:
            return "self"

        return "link"

    @property
    def body(self):
        if self.is_self:
            return self.selftext
        else:
            raise AttributeError

    @property
    def has_thumbnail(self):
        return self._t.get('has_thumbnail', hasattr(self, 'thumbnail_url'))

    @property
    def is_nsfw(self):
        return self.over_18

    @property
    def is_embeddable(self):
        return not self.is_nsfw and self.subreddit_slow.is_embeddable

    @classmethod
    def _by_url(cls, url, sr):
        if isinstance(sr, FakeSubreddit):
            sr = None

        link_ids = LinksByUrlAndSubreddit.get_link_ids(url, sr)
        links = Link._byID(link_ids, data=True, return_dict=False)
        links = [l for l in links if not l._deleted]

        if not links:
            raise NotFound('Link "%s"' % url)

        return links

    def already_submitted_link(self, url, title):
        permalink = self.make_permalink_slow()
        p = UrlParser(permalink)
        p.update_query(already_submitted="true", submit_url=url,
                       submit_title=title)
        return p.unparse()

    @classmethod
    def resubmit_link(cls, url, title):
        p = UrlParser("/submit")
        p.update_query(resubmit="true", url=url, title=title)
        return p.unparse()

    @classmethod
    def _submit(cls, is_self, title, content, author, sr, ip,
                sendreplies=True):
        from r2.lib.voting import cast_vote
        from r2.models import admintools
        from r2.models.comment_tree import CommentTree

        if is_self:
            url = "self"
            selftext = content
        else:
            url = content
            selftext = cls._defaults["selftext"]

        over_18 = False
        if cls._nsfw.search(title):
            over_18 = True

        # determine whether the post should go straight into spam
        spam = author._spam
        if is_self:
            spam_filter_level = sr.spam_selfposts
        else:
            spam_filter_level = sr.spam_links
        if spam_filter_level == "all" and not sr.is_special(author):
            spam = True

        l = cls(
            _ups=1,
            title=title,
            url=url,
            selftext=selftext,
            _spam=spam,
            author_id=author._id,
            sendreplies=sendreplies,
            sr_id=sr._id,
            lang=sr.lang,
            ip=ip,
            is_self=is_self,
            over_18=over_18,
        )

        l._commit()
        # Note: this does not provide atomicity, so for self posts when
        # a request dies after the previous line but before the next line
        # you will have a link whose URL is literally 'self'
        if is_self:
            l.url = l.make_permalink_slow()
            l._commit()
        else:
            LinksByUrlAndSubreddit.add_link(l)

        LinksByAccount.add_link(author, l)
        SubredditParticipationByAccount.mark_participated(author, sr)
        SubredditsActiveForFrontPage.mark_new_post(sr)
        author.last_submit_time = int(epoch_timestamp(datetime.now(g.tz)))
        author._commit()

        # if the link is coming in removed, we still need to run it through
        # admintools.spam() to set data properly and update queries
        if l._spam:
            if author._spam:
                g.stats.simple_event('spam.autoremove.link')
                reason = "banned user"
            elif spam_filter_level == "all":
                reason = "subreddit setting"

            admintools.spam(l, banner=reason)

        hooks.get_hook('link.new').call(link=l)

        CommentTree.on_new_link(l)

        cast_vote(author, l, Vote.DIRECTIONS.up)

        return l

    def set_content(self, is_self, content):
        if not self.promoted:
            raise ValueError("set_content is only supported for promoted links")

        was_self = self.is_self
        self.is_self = is_self

        if is_self:
            if not was_self:
                LinksByUrlAndSubreddit.remove_link(self)

            self.url = self.make_permalink_slow()
            self.selftext = content
        else:
            if not was_self:
                LinksByUrlAndSubreddit.remove_link(self)

            self.url = content
            self.selftext = self._defaults.get("selftext", "")
            LinksByUrlAndSubreddit.add_link(self)

        self._commit()

    def _save(self, user, category=None):
        LinkSavesByAccount._save(user, self, category)

    def _unsave(self, user):
        LinkSavesByAccount._unsave(user, self)

    def _hide(self, user):
        LinkHidesByAccount._hide(user, self)

    def _unhide(self, user):
        LinkHidesByAccount._unhide(user, self)

    def _commit(self):
        # If we've updated the (denormalized) preview object, we also need to
        # update the metadata that keeps track of the denormalizations.
        if 'preview_object' in self._dirties:
            (old_val, val) = self._dirties['preview_object']
            if old_val:
                LinksByImage.remove_link(old_val['uid'], self)
            if val:
                LinksByImage.add_link(val['uid'], self)
        Thing._commit(self)

    def link_domain(self):
        if self.is_self:
            return 'self'
        else:
            return domain(self.url)

    @property
    def num_comments(self):
        # Paper over obviously broken comment counts (those that are negative).
        return max(self.__getattr__('num_comments'), 0)

    def keep_item(self, wrapped):
        user = c.user if c.user_is_loggedin else None
        if not c.user_is_admin and self._deleted:
            return False

        is_mod = wrapped.subreddit.is_moderator(user)

        if not (c.user_is_admin or (isinstance(c.site, DomainSR) and is_mod)):
            if self._spam and (not user or
                               (user and self.author_id != user._id)):
                return False

        if not (c.user_is_admin or is_mod) and wrapped.enemy:
            return False

        if user and not c.ignore_hide_rules:
            if wrapped.hidden:
                return False

            # determine if the post can be auto-hidden due to voting/score
            if self.author_id == user._id:
                # not if it's the user's own post
                allow_auto_hide = False
            elif wrapped.stickied and not wrapped.different_sr:
                # not if it's stickied and we're inside the subreddit
                allow_auto_hide = False
            else:
                allow_auto_hide = True

            if (allow_auto_hide and
                    ((user.pref_hide_ups and wrapped.likes == True) or
                     (user.pref_hide_downs and wrapped.likes == False) or
                     wrapped.score < user.pref_min_link_score)):
                return False

        # show NSFW to API and RSS users unless obey_over18=true
        is_api = c.render_style in extensions.API_TYPES
        is_rss = c.render_style in extensions.RSS_TYPES
        if (is_api or is_rss) and not c.obey_over18:
            return True

        is_nsfw = wrapped.over_18 or wrapped.subreddit.over_18
        return c.over18 or not is_nsfw

    cache_ignore = {
        'subreddit',
        'num_comments',
        'link_child',
        'fresh',
        'media_object',
        'secure_media_object',
    }.union(Printable.cache_ignore)

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        if wrapped.promoted is not None:
            s.extend([
                getattr(wrapped, "promote_status", -1),
                getattr(wrapped, "disable_comments", False),
                getattr(wrapped, "media_override", False),
                c.user_is_sponsor,
                wrapped.url,
                repr(wrapped.title),
            ])

        if style == "htmllite":
             s.extend([
                 request.GET.has_key('twocolumn'),
                 c.link_target,
            ])
        elif style == "xml":
            s.append(request.GET.has_key("nothumbs"))
        elif style == "compact":
            s.append(c.permalink_page)

        # add link flair to the key if the user and site have enabled it and it
        # exists
        if (c.user.pref_show_link_flair and
                c.site.link_flair_position and
                (wrapped.flair_text or wrapped.flair_css_class)):
            s.append(wrapped.flair_text)
            s.append(wrapped.flair_css_class)
            s.append(c.site.link_flair_position)

        if wrapped.locked:
            s.append('locked')

        return s

    def make_permalink(self, sr, force_domain=False):
        from r2.lib.template_helpers import get_domain
        p = "comments/%s/%s/" % (self._id36, title_to_url(self.title))
        # promoted links belong to a separate subreddit and shouldn't
        # include that in the path
        if self.promoted is not None:
            if force_domain:
                permalink_domain = get_domain(subreddit=False)
                res = "%s://%s/%s" % (g.default_scheme, permalink_domain, p)
            else:
                res = "/%s" % p
        elif not force_domain:
            res = "/r/%s/%s" % (sr.name, p)
        elif sr != c.site or force_domain:
            permalink_domain = get_domain(subreddit=False)
            res = "%s://%s/r/%s/%s" % (g.default_scheme, permalink_domain,
                                       sr.name, p)
        else:
            res = "/%s" % p

        # WARNING: If we ever decide to add any ?foo=bar&blah parameters
        # here, Comment.make_permalink will need to be updated or else
        # it will fail.

        return res

    def make_canonical_link(self, sr, subdomain='www'):
        domain = '%s.%s' % (subdomain, g.domain)
        path = 'comments/%s/%s/' % (self._id36, title_to_url(self.title))
        return '%s://%s/r/%s/%s' % (g.default_scheme, domain, sr.name, path)

    def make_permalink_slow(self, force_domain=False):
        return self.make_permalink(self.subreddit_slow,
                                   force_domain=force_domain)

    def markdown_link_slow(self):
        title = _force_unicode(self.title)
        title = title.replace("[", r"\[")
        title = title.replace("]", r"\]")
        return "[%s](%s)" % (title, self.make_permalink_slow())

    @classmethod
    def tracking_link(cls,
                      link,
                      wrapped_thing=None,
                      element_name=None,
                      context=None,
                      site_name=None):
        """Add utm query parameters to reddit.com links to track navigation.

        context => ?utm_medium (listing page, post listing on hybrid page)
        site_name => ?utm_name (subreddit that user is currently browsing)
        element_name => ?utm_content (what element leads to this link)
        """

        if (c.user_is_admin or
                not feature.is_enabled('utm_comment_links')):
            return link

        urlparser = UrlParser(link)
        if not urlparser.path:
            # `href="#some_anchor"`
            return link
        if urlparser.scheme == 'javascript':
            return link
        if not urlparser.is_reddit_url():
            return link

        query_params = {}

        query_params["utm_source"] = "reddit"

        if context is None:
            if (hasattr(wrapped_thing, 'context') and
                    wrapped_thing.context != cls.get_default_context()):
                context = wrapped_thing.context
            else:
                context = request.route_dict["controller"]
        if context:
            query_params["utm_medium"] = context

        if element_name:
            query_params["utm_content"] = element_name

        if site_name is None:
            site_name = get_site()
        if site_name:
            query_params["utm_name"] = site_name

        query_params = {k: v for (k, v) in query_params.iteritems() if (
                        v is not None)}

        if query_params:
            urlparser.update_query(**query_params)
            return urlparser.unparse()
        return link

    def _gild(self, user):
        now = datetime.now(g.tz)

        self._incr("gildings")
        self.subreddit_slow.add_gilding_seconds()

        GildedLinksByAccount.gild(user, self)

        from r2.lib.db import queries
        with CachedQueryMutator() as m:
            gilding = utils.Storage(thing=self, date=now)
            m.insert(queries.get_all_gilded_links(), [gilding])
            m.insert(queries.get_gilded_links(self.sr_id), [gilding])
            m.insert(queries.get_gilded_user_links(self.author_id),
                     [gilding])
            m.insert(queries.get_user_gildings(user), [gilding])

        hooks.get_hook('link.gild').call(link=self, gilder=user)

    @staticmethod
    def _should_expunge_selftext(link):
        if not link._spam:
            return False
        if not c.user_is_loggedin:
            return True
        if c.user_is_admin:
            return False
        if c.user == link.author:
            return False
        if link.can_ban:
            return False
        return True

    @classmethod
    def update_nofollow(cls, user, wrapped):
        user_is_loggedin = c.user_is_loggedin
        for item in wrapped:
            if user_is_loggedin and item.author_id == user._id:
                item.nofollow = False
            elif item._spam or item.author._spam:
                item.nofollow = True
            else:
                item.nofollow = False

        hooks.get_hook('link.update_nofollow').call(
            user=user,
            wrapped=wrapped,
        )

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.pages import make_link_child
        from r2.lib.count import incr_counts
        from r2.lib import media
        from r2.lib.utils import timeago
        from r2.lib.template_helpers import get_domain, unsafe, format_html
        from r2.models.report import Report
        from r2.lib.wrapped import CachedVariable

        # referencing c's getattr is cheap, but not as cheap when it
        # is in a loop that calls it 30 times on 25-200 things.
        user_is_admin = c.user_is_admin
        user_is_loggedin = c.user_is_loggedin
        pref_media = user.pref_media
        pref_media_preview = user.pref_media_preview
        site = c.site

        saved = hidden = visited = {}

        if user_is_admin:
            # Checking if a domain's banned isn't even cheap
            urls = [item.url for item in wrapped if hasattr(item, 'url')]
            # bans_for_domain_parts is just a generator; convert to a set for
            # easy use of 'intersection'
            from r2.models.admintools import bans_for_domain_parts
            banned_domains = {ban.domain
                              for ban in bans_for_domain_parts(urls)}

        if user_is_loggedin:
            gilded = [thing for thing in wrapped if thing.gildings > 0]
            try:
                user_gildings = GildedLinksByAccount.fast_query(user, gilded)
            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                g.log.warning("Cassandra gilding lookup failed: %r", e)
                user_gildings = {}

            try:
                saved = LinkSavesByAccount.fast_query(user, wrapped)
                hidden = LinkHidesByAccount.fast_query(user, wrapped)

                if user.gold and user.pref_store_visits:
                    visited = LinkVisitsByAccount.fast_query(user, wrapped)

            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                # saved or hidden or may have been done properly, so go ahead
                # with what we do have
                g.log.warning("Cassandra save/hide/visited lookup failed: %r", e)

        # determine which subreddits the user could assign link flair in
        if user_is_loggedin:
            srs = {item.subreddit for item in wrapped
                                  if item.subreddit.link_flair_position}
            mod_flair_srids = {sr._id for sr in srs
                               if (user_is_admin or
                                   sr.is_moderator_with_perms(c.user, 'flair'))}
            author_flair_srids = {sr._id for sr in srs
                                  if sr.link_flair_self_assign_enabled}

        if user_is_loggedin:
            srs = {item.subreddit for item in wrapped}
            is_moderator_srids = {sr._id for sr in srs if sr.is_moderator(user)}
        else:
            is_moderator_srids = set()

        # set the nofollow state where needed
        cls.update_nofollow(user, wrapped)

        for item in wrapped:
            show_media = False
            if not hasattr(item, "score_fmt"):
                item.score_fmt = Score.number_only
            if c.render_style in ('compact', extensions.api_type("compact")):
                item.score_fmt = Score.safepoints
            item.pref_compress = user.pref_compress
            if user.pref_compress:
                item.extra_css_class = "compressed"
                item.score_fmt = Score.safepoints
            elif pref_media == 'on' and not user.pref_compress:
                show_media = True
            elif pref_media == 'subreddit' and item.subreddit.show_media:
                show_media = True
            elif item.promoted and item.has_thumbnail:
                if user_is_loggedin and item.author_id == user._id:
                    show_media = True
                elif pref_media != 'off' and not user.pref_compress:
                    show_media = True

            show_media_preview = False
            if feature.is_enabled('autoexpand_media_previews'):
                if pref_media_preview == "on":
                    show_media_preview = True
                elif pref_media_preview == "subreddit" and item.subreddit.show_media_preview:
                    show_media_preview = True

            item.over_18 = item.over_18 or item.subreddit.over_18
            item.nsfw = item.over_18 and user.pref_label_nsfw

            item.quarantine = item.subreddit.quarantine

            item.is_author = (user == item.author)

            item.thumbnail_sprited = False

            if item.quarantine:
                item.thumbnail = ""
                item.preview_image = None
            # always show a promo author their own thumbnail
            elif item.promoted and (user_is_admin or item.is_author) and item.has_thumbnail:
                item.thumbnail = media.thumbnail_url(item)
                item.preview_image = getattr(item, 'preview_object', None)
            elif user.pref_no_profanity and item.over_18 and not c.site.over_18:
                if show_media:
                    item.thumbnail = "nsfw"
                    item.thumbnail_sprited = True
                else:
                    item.thumbnail = ""
                item.preview_image = None
            elif not show_media:
                item.thumbnail = ""
                item.preview_image = None
            elif (item._deleted or
                  item._spam and item._date < timeago("6 hours")):
                item.thumbnail = "default"
                item.thumbnail_sprited = True
                item.preview_image = None
            elif item.has_thumbnail:
                item.thumbnail = media.thumbnail_url(item)
                item.preview_image = getattr(item, 'preview_object', None)
            elif item.is_self:
                item.thumbnail = "self"
                item.thumbnail_sprited = True
                item.preview_image = getattr(item, 'preview_object', None)
            else:
                item.thumbnail = "default"
                item.thumbnail_sprited = True
                item.preview_image = getattr(item, 'preview_object', None)

            item.show_media_preview = show_media_preview

            item.score = max(0, item.score)

            if item.domain_override:
                item.domain = item.domain_override
            else:
                item.domain = (domain(item.url) if not item.is_self
                               else 'self.' + item.subreddit.name)

            if user_is_loggedin:
                item.user_gilded = (user, item) in user_gildings
                item.saved = (user, item) in saved
                item.hidden = (user, item) in hidden
                item.visited = (user, item) in visited

            else:
                item.user_gilded = False
                item.saved = item.hidden = item.visited = False

            if c.permalink_page or c.profilepage:
                item.gilded_message = make_gold_message(item, item.user_gilded)
            else:
                item.gilded_message = ''

            item.can_gild = (
                c.user_is_loggedin and
                # you can't gild your own submission
                not (item.author and
                     item.author._id == user._id) and
                # no point in showing the button for things you've already gilded
                not item.user_gilded and
                # ick, if the author deleted their account we shouldn't waste gold
                not item.author._deleted and
                # some subreddits can have gilding disabled
                item.subreddit.allow_gilding and
                not item._deleted
            )

            # set an attribute on the Wrapped item so that it will be
            # added to the render cache key
            if item.can_ban:
                item.ignore_reports_key = item.ignore_reports

            if c.user_is_loggedin and c.user.in_timeout:
                item.mod_reports, item.user_reports = [], []
            else:
                item.mod_reports, item.user_reports = Report.get_reports(item)

            item.num = None
            item.permalink = item.make_permalink(item.subreddit)
            if item.is_self:
                item.url = item.make_permalink(item.subreddit,
                                               force_domain=True)

            if g.shortdomain:
                item.shortlink = g.shortdomain + '/' + item._id36

            item.domain_str = None
            if c.user.pref_domain_details:
                urlparser = UrlParser(item.url)
                if (not item.is_self and urlparser.is_reddit_url() and
                        urlparser.is_web_safe_url()):
                    url_subreddit = urlparser.get_subreddit()
                    if (url_subreddit and
                            not isinstance(url_subreddit, DefaultSR)):
                        item.domain_str = ('{0}/r/{1}'
                                           .format(item.domain,
                                                   url_subreddit.name))
                elif isinstance(item.media_object, dict):
                    try:
                        author_url = item.media_object['oembed']['author_url']
                        if domain(author_url) == item.domain:
                            urlparser = UrlParser(author_url)
                            item.domain_str = strip_www(urlparser.hostname)
                            item.domain_str += urlparser.path
                    except KeyError:
                        pass

                    if not item.domain_str:
                        try:
                            author = item.media_object['oembed']['author_name']
                            author = _force_unicode(author)
                            item.domain_str = (_force_unicode('{0}: {1}')
                                               .format(item.domain, author))
                        except KeyError:
                            pass

            if not item.domain_str:
                item.domain_str = item.domain

            item.user_is_moderator = item.sr_id in is_moderator_srids

            # do we hide the score?
            if user_is_admin:
                item.hide_score = False
            elif user_is_loggedin and item.user_is_moderator:
                item.hide_score = False
            elif item.promoted and item.score <= 0:
                item.hide_score = True
            elif user == item.author:
                item.hide_score = False
            elif item._date > timeago("2 hours"):
                item.hide_score = True
            else:
                item.hide_score = False

            # is this link a member of a different (non-c.site) subreddit?
            item.different_sr = (isinstance(site, FakeSubreddit) or
                                 site.name != item.subreddit.name)

            item.stickied = item.is_stickied(item.subreddit)

            # we only want to style a sticky specially if we're inside the
            # subreddit that it's stickied in (not in places like front page)
            item.use_sticky_style = item.stickied and not item.different_sr

            item.subreddit_path = item.subreddit.path
            item.domain_path = "/domain/%s/" % item.domain
            if item.is_self:
                item.domain_path = item.subreddit_path

            # attach video or selftext as needed
            item.link_child, item.editable = make_link_child(item, show_media_preview)
            item.feature_media_previews = feature.is_enabled("media_previews")

            if item.is_self and not item.promoted:
                item.href_url = item.permalink
            else:
                item.href_url = item.url

            item.fresh = not any((item.likes != None,
                                  item.saved,
                                  item.visited,
                                  item.hidden,
                                  item._deleted,
                                  item._spam))

            # bits that we will render stubs (to make the cached
            # version more flexible)
            item.num_text = CachedVariable("num")
            item.commentcls = CachedVariable("commentcls")
            item.comment_label = CachedVariable("numcomments")
            item.lastedited = CachedVariable("lastedited")

            item.as_deleted = False
            if item.deleted and not c.user_is_admin:
                item.author = DeletedUser()
                item.as_deleted = True
                item.selftext = '[deleted]'

            item.archived = item.is_archived(item.subreddit)
            item.votable = not item.archived

            item.expunged = False
            if item.is_self:
                item.expunged = Link._should_expunge_selftext(item)

            item.editted = getattr(item, "editted", False)

            if user_is_loggedin:
                can_mod_flair = item.subreddit._id in mod_flair_srids
                can_author_flair = (item.is_author and
                                    item.subreddit._id in author_flair_srids)
                item.can_flair = can_mod_flair or can_author_flair
            else:
                item.can_flair = False

            taglinetext = ''
            if item.different_sr:
                author_text = format_html(" <span>%s</span>",
                                          _("by %(author)s to %(reddit)s"))
            else:
                author_text = format_html(" <span>%s</span>",
                                          _("by %(author)s"))
            if item.editted:
                if item.score_fmt in (Score.points, Score.safepoints):
                    taglinetext = format_html("<span>%s</span>",
                                              _("%(score)s submitted %(when)s "
                                                "%(lastedited)s"))
                    taglinetext = unsafe(taglinetext + author_text)
                elif item.different_sr:
                    taglinetext = _("submitted %(when)s %(lastedited)s "
                                    "by %(author)s to %(reddit)s")
                else:
                    taglinetext = _("submitted %(when)s %(lastedited)s "
                                    "by %(author)s")
            else:
                if item.score_fmt in (Score.points, Score.safepoints):
                    taglinetext = format_html("<span>%s</span>",
                                              _("%(score)s submitted %(when)s"))
                    taglinetext = unsafe(taglinetext + author_text)
                elif item.different_sr:
                    taglinetext = _("submitted %(when)s by %(author)s "
                                    "to %(reddit)s")
                else:
                    taglinetext = _("submitted %(when)s by %(author)s")
            item.taglinetext = taglinetext

            if item.is_author:
                item.should_incr_counts = False

            if user_is_admin:
                # Link notes
                url = getattr(item, 'url')
                # Pull just the relevant portions out of the url
                urlf = sanitize_url(_force_unicode(url))
                if urlf:
                    urlp = UrlParser(urlf)
                    hostname = urlp.hostname
                    if hostname:
                        parts = (hostname.encode("utf-8").rstrip(".").
                            split("."))
                        subparts = {".".join(parts[y:])
                                    for y in xrange(len(parts))}
                        if subparts.intersection(banned_domains):
                            item.link_notes.append('banned domain')

            # This is passed in promotedlink.html
            item.ads_auction_enabled = feature.is_enabled('ads_auction')

            if feature_utils.is_tracking_link_enabled(item):
                # Split cache for template rendered with tracking link
                item.use_tracking_link = True

        if user_is_loggedin:
            incr_counts(wrapped)


        # Run this last
        Printable.add_props(user, wrapped)

    @classmethod
    def get_default_context(cls):
        return request.route_dict["action_name"]

    @property
    def post_hint(self):
        """Returns a string that suggests the content of this link.

        As a hint, this is lossy and may be inaccurate in some cases.

        Currently one of:
            * self
            * video (a video file, like an mp4)
            * image (an image file, like a gif or png)
            * rich:video (a video embedded in HTML - like youtube or vimeo)
            * link (catch-all)
        """
        if self.is_self:
            return 'self'

        try:
            oembed_type = self.media_object['oembed']['type']
        except (KeyError, TypeError):
            oembed_type = None

        if oembed_type == 'photo':
            return 'image'

        if oembed_type == 'video':
            return 'rich:video'

        if oembed_type in {'link', 'rich'}:
            return 'link'

        p = UrlParser(self.url)
        if p.has_image_extension():
            return 'image'

        if p.path_extension().lower() in {'mp4', 'webm'}:
            return 'video'

        return 'link'

    @property
    def subreddit_slow(self):
        # The subreddit is often already on the wrapped link as .subreddit
        # If available, that should be used instead of calling this
        return Subreddit._byID(self.sr_id, stale=True)

    @property
    def author_slow(self):
        """Returns the link's author."""
        # The author is often already on the wrapped link as .author
        # If available, that should be used instead of calling this
        return Account._byID(self.author_id, data=True, return_dict=False)

    @property
    def responder_ids(self):
        """Returns an iterable of the OP and other official responders in a
        thread.

        Designed for Q&A-type threads (eg /r/iama).
        """
        return (self.author_id,)

    @property
    def archived_slow(self):
        sr = self.subreddit_slow
        return self.is_archived(sr)

    def is_archived(self, sr):
        return self._age >= sr.archive_age

    def can_view_promo(self, user):
        if self.promoted:
            # promos are visible only if the user is either the author
            # or the link is live/previously live.
            is_author = c.user_is_loggedin and user._id == self.author_id
            return (c.user_is_sponsor or
                    is_author or
                    self.promote_status >= PROMOTE_STATUS.promoted)

        # not a promo, therefore it is visible
        # this preserves the original behavior of `visible_promo()`
        return True

    def can_comment_slow(self, user):
        sr = self.subreddit_slow

        if self.is_archived(sr):
            return False

        if self.locked and not sr.can_distinguish(user):
            return False

        return sr.can_comment(user) and self.can_view_promo(user)

    def sort_if_suggested(self, sr=None):
        """Returns a sort, if the link or its subreddit has suggested one."""
        if self.suggested_sort == "blank":
            # A suggested sort of "blank" means explicitly empty: Do not obey
            # the subreddit's suggested sort, either.
            return None

        if self.suggested_sort:
            return self.suggested_sort

        sr = sr or self.subreddit_slow
        if sr.suggested_comment_sort:
            return sr.suggested_comment_sort

        return None

    def can_flair_slow(self, user):
        """Returns whether the specified user can flair this link"""
        site = self.subreddit_slow
        can_assign_own = (self.author_id == user._id and
                          site.link_flair_self_assign_enabled)

        return site.is_moderator_with_perms(user, 'flair') or can_assign_own

    def set_flair(self, text=None, css_class=None, set_by=None):
        self.flair_text = text
        self.flair_css_class = css_class
        self._commit()
        self.update_search_index()

        if set_by and set_by._id != self.author_id:
            ModAction.create(self.subreddit_slow, set_by, action='editflair',
                target=self, details='flair_edit')

    def set_sticky_comment(self, comment, set_by=None):
        """Given a comment, set it as the sticky (top) comment for this link.

        Only one comment may be stickied at a time, and stickied comments must
        be top level. `set_by` is an optional Account, which if set will add
        a ModAction event to the mod log.

        Raises `RedditError` on an attempt to sticky non-top-level comments.
        """
        if not comment.is_stickyable:
            raise RedditError('COMMENT_NOT_STICKYABLE', code=400)

        if self.sticky_comment_id == comment._id:
            return

        # Remove the current sticky if it exists before setting a new one
        self.remove_sticky_comment(set_by=set_by)

        self.sticky_comment_id = comment._id
        self._commit()

        if set_by:
            ModAction.create(
                self.subreddit_slow,
                set_by,
                action='sticky',
                target=comment,
            )

    def remove_sticky_comment(self, comment=None, set_by=None):
        """Remove the sticky (top) comment for this link, if it exists.

        `comment` is an optional argument that, if set, will ensure the
        sticky comment matches. If it does not match, it will not remove. If
        `comment` is unset, it will remove regardless of ID.

        `set_by` is an optional Account, which if set will add a ModAction
        event to the mod log.
        """
        if self.sticky_comment_id is None:
            return  # nothing to do

        if comment and self.sticky_comment_id != comment._id:
            return

        prev_sticky_comment = Comment._byID(self.sticky_comment_id)

        self.sticky_comment_id = None
        self._commit()

        if set_by:
            ModAction.create(
                self.subreddit_slow,
                set_by,
                action='unsticky',
                target=prev_sticky_comment,
            )

    @classmethod
    def _utf8_encode(cls, value):
        """
        Returns a deep copy of the parameter, UTF-8-encoding any strings
        encountered.
        """
        if isinstance(value, dict):
            return {cls._utf8_encode(key): cls._utf8_encode(value)
                    for key, value in value.iteritems()}
        elif isinstance(value, list):
            return [cls._utf8_encode(item)
                    for item in value]
        elif isinstance(value, unicode):
            return value.encode('utf-8')
        else:
            return value

    # There's an issue where pickling fails for collections with string values
    # that have unicode codepoints between 128 and 256.  Encoding the strings
    # as UTF-8 before storing them works around this.
    def set_media_object(self, value):
        self.media_object = Link._utf8_encode(value)

    def set_secure_media_object(self, value):
        self.secure_media_object = Link._utf8_encode(value)

    def set_preview_object(self, value):
        self.preview_object = Link._utf8_encode(value)

    def is_stickyable(self):
        if self._deleted or self._spam:
            return False

        return True

    @property
    def is_stickied_slow(self):
        return self.is_stickied(self.subreddit_slow)

    def is_stickied(self, subreddit):
        if not subreddit.sticky_fullnames:
            return False

        if self._fullname in subreddit.sticky_fullnames:
            return True

        return False


class LinksByUrlAndSubreddit(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE
    _compare_with = LongType()
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
    }

    @classmethod
    def make_canonical_url(cls, url):
        if not utils.domain(url) in g.case_sensitive_domains:
            keyurl = _force_utf8(UrlParser.base_url(url.lower()))
        else:
            # Convert only hostname to lowercase
            up = UrlParser(url)
            up.hostname = up.hostname.lower()
            keyurl = _force_utf8(UrlParser.base_url(up.unparse()))

        # Cassandra max key length is 65535, truncate url if it's near that
        # (leaving some space for the prefix)
        keyurl = keyurl[:65000]

        return keyurl

    @classmethod
    def make_sr_rowkey(cls, canonical_url, sr_id):
        return "{sr}:{url}".format(sr=sr_id, url=canonical_url)

    @classmethod
    def make_all_rowkey(cls, canonical_url):
        return "all:{url}".format(url=canonical_url)

    @classmethod
    def add_link(cls, link):
        canonical_url = cls.make_canonical_url(link.url)
        sr_rowkey = cls.make_sr_rowkey(canonical_url, link.sr_id)
        all_rowkey = cls.make_all_rowkey(canonical_url)
        column = {link._id: ""}
        cls._set_values(sr_rowkey, column)
        cls._set_values(all_rowkey, column)

    @classmethod
    def remove_link(cls, link):
        canonical_url = cls.make_canonical_url(link.url)
        sr_rowkey = cls.make_sr_rowkey(canonical_url, link.sr_id)
        all_rowkey = cls.make_all_rowkey(canonical_url)
        column = {link._id: ""}
        cls._remove(sr_rowkey, column)
        cls._remove(all_rowkey, column)

    @classmethod
    def get_link_ids(cls, url, sr=None, limit=1000):
        canonical_url = cls.make_canonical_url(url)
        if sr:
            rowkey = cls.make_sr_rowkey(canonical_url, sr._id)
        else:
            rowkey = cls.make_all_rowkey(canonical_url)
        try:
            columns = cls._cf.get(
                rowkey, column_reversed=True, column_count=limit)
        except tdb_cassandra.NotFoundException:
            return []

        link_ids = columns.keys()
        return link_ids


# Note that there are no instances of PromotedLink or LinkCompressed,
# so overriding their methods here will not change their behaviour
# (except for add_props). These classes are used to override the
# render_class on a Wrapped to change the template used for rendering

class PromotedLink(Link):
    _nodb = True

    # embeds are editable by users (advertisers) so they can change and should
    # be considered in the render cache key
    cache_ignore = Link.cache_ignore - {"media_object", "secure_media_object"}

    @classmethod
    def add_props(cls, user, wrapped):
        Link.add_props(user, wrapped)

        for item in wrapped:
            item.nofollow = True
            item.user_is_sponsor = c.user_is_sponsor

            status = "promoted"

            # change the status class if the viewer is the author or a sponsor
            if item.is_author or item.user_is_sponsor:
                try:
                    status = PROMOTE_STATUS.name[item.promote_status]
                except (AttributeError, IndexError):
                    pass

            item.rowstyle_cls = "link %s" % status

        # Run this last
        Printable.add_props(user, wrapped)


class ReadNextLink(Link):
    _nodb = True


class SearchResultLink(Link):
    _nodb = True

    @classmethod
    def add_props(cls, user, wrapped):
        Link.add_props(user, wrapped)
        for item in wrapped:
            url = UrlParser(item.permalink)
            url.update_query(ref="search_posts")
            item.permalink = url.unparse()
        Printable.add_props(user, wrapped)


class LegacySearchResultLink(Link):
    _nodb = True

    @classmethod
    def add_props(cls, user, wrapped):
        Link.add_props(user, wrapped)
        for item in wrapped:
            url = UrlParser(item.permalink)
            url.update_query(ref="search_posts")
            item.permalink = url.unparse()
            item.render_css_class = 'link'
        Printable.add_props(user, wrapped)


class Comment(Thing, Printable):
    _cache = g.thingcache
    _data_int_props = Thing._data_int_props + ('reported', 'gildings')
    _defaults = dict(
        reported=0,
        parent_id=None,
        moderator_banned=False,
        new=False,
        gildings=0,
        banned_before_moderator=False,
        ignore_reports=False,
        sendreplies=True,
        admin_takedown=False,
    )
    _essentials = ('link_id', 'author_id')

    is_votable = True

    @classmethod
    def _cache_prefix(cls):
        return "comment:"

    def _markdown(self):
        pass

    @property
    def affects_karma_type(self):
        return "comment"

    @classmethod
    def _new(cls, author, link, parent, body, ip):
        from r2.lib.emailer import message_notification_email
        from r2.lib.voting import cast_vote

        subreddit = link.subreddit_slow

        # determine whether the comment should go straight into spam
        spam = False
        if link.promoted and link.author_id == author._id:
            # don't spam promoted link authors commenting on their own promos
            spam = False
        elif author._spam:
            spam = True
            g.stats.simple_event('spam.autoremove.comment')
        elif (subreddit.spam_comments == "all" and
                not subreddit.is_special(author)):
            spam = True

        comment = Comment(
            _ups=1,
            body=body,
            link_id=link._id,
            sr_id=link.sr_id,
            author_id=author._id,
            ip=ip,
            _spam=spam,
        )

        # these props aren't relations
        if parent:
            comment.parent_id = parent._id

        comment._commit()

        link._incr('num_comments', 1)

        to = None
        name = 'inbox'
        if parent and parent.sendreplies:
            to = Account._byID(parent.author_id, True)
        if not parent and not link._deleted and link.sendreplies:
            to = Account._byID(link.author_id, True)
            name = 'selfreply'

        g.events.comment_event(comment, request=request, context=c)

        cast_vote(author, comment, Vote.DIRECTIONS.up)

        if link.num_comments < 20 or link.num_comments % 10 == 0:
            # link's number of comments changed so re-index it, but don't bother
            # re-indexing so often when it gets many comments
            link.update_search_index(boost_only=True)

        CommentsByAccount.add_comment(author, comment)
        SubredditParticipationByAccount.mark_participated(author, subreddit)
        author.last_comment_time = int(epoch_timestamp(datetime.now(g.tz)))
        author._commit()

        def should_send():
            # don't send the message to author if replying to own comment
            if author._id == to._id:
                return False
            # only global admins can be message spammed
            if to.name in g.admins:
                return True
            # don't send the message if spam
            # don't send the message if the recipient has blocked the author
            if comment._spam or author._id in to.enemies:
                return False
            return True

        inbox_rel = None
        if to and should_send():
            # Record the inbox relation and give the user an orangered
            inbox_rel = Inbox._add(to, comment, name, orangered=True)

            if to.pref_email_messages:
                data = {
                    'to': to._id36,
                    'from': '/u/%s' % author.name,
                    'comment': comment._fullname,
                    'permalink': comment.make_permalink_slow(force_domain=True),
                }
                data = json.dumps(data)
                TryLater.schedule('message_notification_email', data,
                                  NOTIFICATION_EMAIL_DELAY)

        hooks.get_hook('comment.new').call(comment=comment)

        return (comment, inbox_rel)

    def _save(self, user, category=None):
        CommentSavesByAccount._save(user, self, category)

    def _unsave(self, user):
        CommentSavesByAccount._unsave(user, self)

    @property
    def link_slow(self):
        """Fetch a comment's Link and return it.

        In most cases the Link is already on the wrapped comment (as .link),
        and that should be used when possible.
        """
        return Link._byID(self.link_id, data=True, return_dict=False)

    @property
    def subreddit_slow(self):
        # When the Comment is Wrapped the subreddit is available as .subreddit
        # and that should be used
        return Subreddit._byID(self.sr_id, stale=True)

    @property
    def author_slow(self):
        """Returns the comment's author."""
        # The author is often already on the wrapped comment as .author
        # If available, that should be used instead of calling this
        return Account._byID(self.author_id, data=True, return_dict=False)

    @property
    def archived_slow(self):
        sr = self.subreddit_slow
        return self.is_archived(sr)

    def is_archived(self, sr):
        return self._age >= sr.archive_age

    @property
    def is_stickyable(self):
        if self.parent_id is not None:
            return False

        return True

    def keep_item(self, wrapped):
        if c.user_is_admin:
            return True

        if c.user_is_loggedin:
            if wrapped.subreddit.is_moderator(c.user):
                return True
            if wrapped.author_id == c.user._id:
                return True
            if wrapped.author_id in c.user.enemies:
                return False

        return True

    cache_ignore = set((
        "subreddit",
        "link",
        "to",
        "num_children",
        "depth",
        "child_ids",
    )).union(Printable.cache_ignore)

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        s.extend([wrapped.body])
        s.extend([hasattr(wrapped, "link") and wrapped.link.contest_mode])
        if hasattr(wrapped, "link") and wrapped.link.locked:
            s.append('locked')
        return s

    def make_permalink(self, link, sr=None, context=None, anchor=False,
                       force_domain=False):
        url = link.make_permalink(sr, force_domain=force_domain) + self._id36
        if context:
            url += "?context=%d" % context
        if anchor:
            url += "#%s" % self._id36
        return url

    def make_permalink_slow(self, context=None, anchor=False,
                            force_domain=False):
        l = Link._byID(self.link_id, data=True)
        return self.make_permalink(l, l.subreddit_slow,
                                   context=context, anchor=anchor,
                                   force_domain=force_domain)

    def _gild(self, user):
        now = datetime.now(g.tz)

        self._incr("gildings")
        self.subreddit_slow.add_gilding_seconds()

        GildedCommentsByAccount.gild(user, self)

        from r2.lib.db import queries
        with CachedQueryMutator() as m:
            gilding = utils.Storage(thing=self, date=now)
            m.insert(queries.get_all_gilded_comments(), [gilding])
            m.insert(queries.get_gilded_comments(self.sr_id), [gilding])
            m.insert(queries.get_gilded_user_comments(self.author_id),
                     [gilding])
            m.insert(queries.get_user_gildings(user), [gilding])

        hooks.get_hook('comment.gild').call(comment=self, gilder=user)

    def _qa(self, children, responder_ids):
        """Sort a comment according to the Q&A-type sort.

        Arguments:

        * children -- a list of the children of this comment.
        * responder_ids -- a set of ids of users categorized as "answerers" for
          this thread.
        """
        # This sort type only makes sense for comments, unlike the other sorts
        # that can be applied to any Things, which is why it's defined here
        # instead of in Thing.

        op_children = [c for c in children if c.author_id in responder_ids]
        score = sorts.qa(self._ups, self._downs, len(self.body), op_children)

        # When replies to a question, we want to rank OP replies higher than
        # non-OP replies (generally).  This is a rough way to do so.
        # Don't add extra scoring when we've already added it due to replies,
        # though (because an OP responds to themselves).
        if self.author_id in responder_ids and not op_children:
            score *= 2

        return score

    @classmethod
    def update_nofollow(cls, user, wrapped):
        user_is_loggedin = c.user_is_loggedin
        for item in wrapped:
            if user_is_loggedin and item.author_id == user._id:
                item.nofollow = False
            elif item._spam or item.link._spam or item.author._spam:
                item.nofollow = True
            else:
                item.nofollow = False

        hooks.get_hook("comment.update_nofollow").call(
            user=user,
            wrapped=wrapped,
        )

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.template_helpers import add_submitter_distinguish, get_domain
        from r2.lib.utils import timeago
        from r2.lib.wrapped import CachedVariable
        from r2.lib.pages import WrappedUser
        from r2.models.report import Report

        #fetch parent links
        links = Link._byID(set(l.link_id for l in wrapped), data=True,
                           return_dict=True, stale=True)

        # fetch authors
        authors = Account._byID(set(l.author_id for l in links.values()), data=True,
                                return_dict=True, stale=True)

        #get srs for comments that don't have them (old comments)
        for cm in wrapped:
            if not hasattr(cm, 'sr_id'):
                cm.sr_id = links[cm.link_id].sr_id

        subreddits = {item.subreddit for item in wrapped}

        if c.user_is_loggedin:
            is_moderator_subreddits = {
                sr._id for sr in subreddits if sr.is_moderator(user)}
            can_reply_srs = set(
                sr._id for sr in subreddits if sr.can_comment(user))
            can_distinguish_srs = set(
                sr._id for sr in subreddits if sr.can_distinguish(user))
            promo_sr_id = Subreddit.get_promote_srid()
            if promo_sr_id:
                can_reply_srs.add(promo_sr_id)
        else:
            is_moderator_subreddits = set()
            can_reply_srs = set()
            can_distinguish_srs = set()

        cids = dict((w._id, w) for w in wrapped)
        parent_ids = set(cm.parent_id for cm in wrapped
                         if getattr(cm, 'parent_id', None)
                         and cm.parent_id not in cids)
        parents = Comment._byID(
            parent_ids, data=True, stale=True, ignore_missing=True)

        profilepage = c.profilepage
        user_is_admin = c.user_is_admin
        user_is_loggedin = c.user_is_loggedin
        focal_comment = c.focal_comment
        site = c.site

        if user_is_loggedin:
            gilded = [thing for thing in wrapped if thing.gildings > 0]
            try:
                user_gildings = GildedCommentsByAccount.fast_query(user, gilded)
            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                g.log.warning("Cassandra gilding lookup failed: %r", e)
                user_gildings = {}

            try:
                saved = CommentSavesByAccount.fast_query(user, wrapped)
            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                g.log.warning("Cassandra comment save lookup failed: %r", e)
                saved = {}
        else:
            user_gildings = {}
            saved = {}

        for item in wrapped:
            # for caching:
            item.profilepage = c.profilepage
            item.link = Wrapped(links.get(item.link_id))
            item.link.author = authors.get(item.link.author_id)
            item.show_admin_context = user_is_admin

            if not hasattr(item, 'subreddit'):
                item.subreddit = item.subreddit_slow

        cls.update_nofollow(user, wrapped)

        for item in wrapped:
            if item.author_id == item.link.author_id and not item.link._deleted:
                add_submitter_distinguish(item.attribs, item.link, item.subreddit)

            if not hasattr(item, 'target'):
                item.target = None

            parent = None
            if item.parent_id:
                if item.parent_id in parents:
                    parent = parents[item.parent_id]
                elif item.parent_id in cids:
                    parent = cids[item.parent_id]

            if parent and not parent.deleted:
                if item.parent_id in cids:
                    # parent is displayed on the page, use an anchor tag
                    item.parent_permalink = '#' + utils.to36(item.parent_id)
                else:
                    item.parent_permalink = parent.make_permalink(item.link, item.subreddit)
            else:
                item.parent_permalink = None

            item.archived = item.is_archived(item.subreddit)

            link_is_archived = item.link.is_archived(item.subreddit)
            link_is_locked = item.link.locked
            sr_can_distinguish = item.sr_id in can_distinguish_srs
            sr_can_reply = item.sr_id in can_reply_srs

            if user_is_loggedin:
                item.can_reply = (
                    not link_is_archived and
                    (not link_is_locked or sr_can_distinguish) and
                    sr_can_reply
                )
            else:
                item.can_reply = not link_is_archived and not link_is_locked

            item.can_embed = c.can_embed or False

            if user_is_loggedin:
                item.user_gilded = (user, item) in user_gildings
                item.saved = (user, item) in saved
            else:
                item.user_gilded = False
                item.saved = False
            item.gilded_message = make_gold_message(item, item.user_gilded)

            item.can_gild = (
                # you can't gild your own comment
                not (c.user_is_loggedin and
                     item.author and
                     item.author._id == user._id) and
                # no point in showing the button for things you've already gilded
                not item.user_gilded and
                # ick, if the author deleted their account we shouldn't waste gold
                not item.author._deleted and
                # some subreddits can have gilding disabled
                item.subreddit.allow_gilding
            )

            if c.user_is_loggedin and c.user.in_timeout:
                item.mod_reports, item.user_reports = [], []
            else:
                item.mod_reports, item.user_reports = Report.get_reports(item)

            # not deleted on profile pages,
            # deleted if spam and not author or admin
            item.deleted = (not profilepage and
                           (item._deleted or
                            (item._spam and
                             item.author != user and
                             not item.show_spam)))

            item.have_form = not item.deleted

            extra_css = ''
            if item.deleted:
                extra_css += "grayed"
                if not user_is_admin:
                    item.author = DeletedUser()
                    item.gildings = 0
                    item.distinguished = None
                    # If removed by an admin or moderator, distinguish that
                    # from being deleted by the user.
                    if item._spam:
                        item.body = '[removed]'
                    else:
                        item.body = '[deleted]'

            if focal_comment == item._id36:
                extra_css += " border"

            if profilepage:
                item.nsfw = user.pref_label_nsfw and (item.link.is_nsfw or item.subreddit.over_18)

                link_author = item.link.author
                if ((item.link._deleted or link_author._deleted) and
                        not user_is_admin):
                    link_author = DeletedUser()
                item.link_author = WrappedUser(link_author)
                item.full_comment_path = item.link.make_permalink(item.subreddit)
                item.full_comment_count = item.link.num_comments

                if item.sr_id == Subreddit.get_promote_srid():
                    item.taglinetext = _("%(link)s by %(author)s [sponsored link]")
                else:
                    item.taglinetext = _("%(link)s by %(author)s in %(subreddit)s")

            else:
                # these aren't used so set them to constant values to avoid
                # invalidating items in render cache
                item.full_comment_path = ''
                item.full_comment_count = 0

            item.subreddit_path = item.subreddit.path

            # always use the default collapse threshold in contest mode threads
            # if the user has a custom collapse threshold
            if (item.link.contest_mode and
                    user.pref_min_comment_score is not None):
                min_score = Account._defaults['pref_min_comment_score']
            else:
                min_score = user.pref_min_comment_score

            item.collapsed = False
            distinguished = item.distinguished and item.distinguished != "no"
            prevent_collapse = profilepage or user_is_admin or distinguished

            if (item.deleted and item.subreddit.collapse_deleted_comments and
                    not prevent_collapse):
                item.collapsed = True
            elif item.score < min_score and not prevent_collapse:
                item.collapsed = True
                item.collapsed_reason = _("comment score below threshold")
            elif user_is_loggedin and item.author_id in c.user.enemies:
                if "grayed" not in extra_css:
                    extra_css += " grayed"
                item.collapsed = True
                item.collapsed_reason = _("blocked user")

            item.editted = getattr(item, "editted", False)

            item.is_sticky = (item.link.sticky_comment_id == item._id)

            item.render_css_class = "comment"

            #will get updated in builder
            item.num_children = 0
            item.numchildren_text = CachedVariable("numchildren_text")
            item.score_fmt = Score.points
            item.permalink = item.make_permalink(item.link, item.subreddit)

            item.is_author = (user == item.author)
            item.is_focal = (focal_comment == item._id36)

            item.votable = item._age < item.subreddit.archive_age

            hide_period = ('{0} minutes'
                          .format(item.subreddit.comment_score_hide_mins))

            if item.is_sticky or item.link.contest_mode:
                item.score_hidden = True
            elif item._date > timeago(hide_period):
                item.score_hidden = not item.is_author
            else:
                item.score_hidden = False

            item.user_is_moderator = item.sr_id in is_moderator_subreddits

            if item.score_hidden and c.user_is_loggedin:
                if c.user_is_admin or item.user_is_moderator:
                    item.score_hidden = False

            if item.score_hidden:
                item.upvotes = 1
                item.downvotes = 0
                item.score = 1
                item.voting_score = [1, 1, 1]
                item.render_css_class += " score-hidden"

            # in contest mode, use only upvotes for the score if the subreddit
            # has been (manually) set to do so
            if (item.link.contest_mode and
                    item.subreddit.contest_mode_upvotes_only and
                    not item.score_hidden):
                item.score = item._ups
                item.voting_score = [
                    item.score - 1, item.score, item.score + 1]
                item.collapsed = False

            if item.is_author:
                item.inbox_replies_enabled = item.sendreplies

            #will seem less horrible when add_props is in pages.py
            from r2.lib.pages import UserText
            item.usertext = UserText(item, item.body,
                                     editable=item.is_author,
                                     nofollow=item.nofollow,
                                     target=item.target,
                                     extra_css=extra_css,
                                     have_form=item.have_form)

            item.lastedited = CachedVariable("lastedited")

        # Run this last
        Printable.add_props(user, wrapped)

    def update_search_index(self, boost_only=False):
        # no-op because Comments are not indexed
        return


class CommentScoresByLink(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE
    _fetch_all_columns = True

    _extra_schema_creation_args = {
        "column_name_class": ASCII_TYPE,
        "default_validation_class": DOUBLE_TYPE,
        "key_validation_class": ASCII_TYPE,
    }
    _value_type = "bytes"
    _compare_with = ASCII_TYPE

    @classmethod
    def _rowkey(cls, link, sort):
        assert sort.startswith('_')
        return '%s%s' % (link._id36, sort)

    @classmethod
    def set_scores(cls, link, sort, scores_by_comment):
        rowkey = cls._rowkey(link, sort)
        cls._set_values(rowkey, scores_by_comment)

    @classmethod
    def get_scores(cls, link, sort):
        rowkey = cls._rowkey(link, sort)
        try:
            return CommentScoresByLink._byID(rowkey)._values()
        except tdb_cassandra.NotFound:
            return {}


class MoreMessages(Printable):
    cachable = False
    display = ""
    new = False
    was_comment = False
    is_collapsed = True

    def __init__(self, parent, child):
        self.parent = parent
        self.child = child

    @staticmethod
    def wrapped_cache_key(item, style):
        return False

    @property
    def _fullname(self):
        return self.parent._fullname

    @property
    def _id36(self):
        return self.parent._id36

    @property
    def _id(self):
        return self.parent._id

    @property
    def subject(self):
        return self.parent.subject

    @property
    def childlisting(self):
        return self.child

    @property
    def to(self):
        return self.parent.to

    @property
    def author(self):
        return self.parent.author

    @property
    def user_is_recipient(self):
        return self.parent.user_is_recipient

    @property
    def sr_id(self):
        return self.parent.sr_id

    @property
    def subreddit(self):
        return self.parent.subreddit

    @property
    def accent_color(self):
        return getattr(self.parent, "accent_color", None)


class MoreComments(Printable):
    cachable = False
    display = ""

    @staticmethod
    def wrapped_cache_key(item, style):
        return False

    def __init__(self, link, depth, parent_id=None):
        if parent_id is not None:
            id36 = utils.to36(parent_id)
            self.parent_id = parent_id
            self.parent_name = "t%s_%s" % (utils.to36(Comment._type_id), id36)
            self.parent_permalink = link.make_permalink_slow() + id36
        self.link_name = link._fullname
        self.link_id = link._id
        self.depth = depth
        self.children = []
        self.count = 0

    @property
    def _fullname(self):
        return "t%s_%s" % (utils.to36(Comment._type_id), self._id36)

    @property
    def _id36(self):
        return utils.to36(self.children[0]) if self.children else '_'


class MoreRecursion(MoreComments):
    pass


class MoreChildren(MoreComments):
    def __init__(self, link, sort_operator, depth, parent_id=None):
        from r2.lib.menus import CommentSortMenu
        self.sort = CommentSortMenu.sort(sort_operator)
        MoreComments.__init__(self, link, depth, parent_id)


class Message(Thing, Printable):
    _cache = g.thingcache
    _defaults = dict(reported=0,
                     was_comment=False,
                     parent_id=None,
                     new=False,
                     first_message=None,
                     to_id=None,
                     sr_id=None,
                     to_collapse=None,
                     author_collapse=None,
                     from_sr=False,
                     display_author=None,
                     display_to=None,
                     email_id=None,
                     sent_via_email=False,
                     del_on_recipient=False,
                     )
    _data_int_props = Thing._data_int_props + ('reported',)
    _essentials = ('author_id',)
    cache_ignore = set(["to", "subreddit"]).union(Printable.cache_ignore)

    @classmethod
    def _cache_prefix(cls):
        return "message:"

    @classmethod
    def _new(cls, author, to, subject, body, ip, parent=None, sr=None,
             from_sr=False, can_send_email=True, sent_via_email=False,
             email_id=None):
        from r2.lib.emailer import message_notification_email
        from r2.lib.message_to_email import queue_modmail_email

        m = Message(subject=subject, body=body, author_id=author._id, new=True,
                    ip=ip, from_sr=from_sr, sent_via_email=sent_via_email,
                    email_id=email_id)
        m._spam = author._spam

        if author._spam:
            g.stats.simple_event('spam.autoremove.message')

        sr_id = None
        # check to see if the recipient is a subreddit and swap args accordingly
        if to and isinstance(to, Subreddit):
            if from_sr:
                raise CreationError("Cannot send from SR to SR")
            to_subreddit = True
            to, sr = None, to
        else:
            to_subreddit = False

        if sr:
            sr_id = sr._id

        if parent:
            m.parent_id = parent._id
            if parent.first_message:
                m.first_message = parent.first_message
            else:
                m.first_message = parent._id

            if parent.sr_id:
                sr_id = parent.sr_id

            if parent.display_author and not getattr(parent, "signed", False):
                m.display_to = parent.display_author

        if not to and not sr_id:
            raise CreationError("Message created with neither to nor sr_id")
        if from_sr and not sr_id:
            raise CreationError("Message sent from_sr without setting sr")

        m.to_id = to._id if to else None
        if sr_id is not None:
            m.sr_id = sr_id

        m._commit()

        hooks.get_hook('message.new').call(message=m)

        MessagesByAccount.add_message(author, m)

        if sr_id and not sr:
            sr = Subreddit._byID(sr_id)

        if to_subreddit:
            SubredditParticipationByAccount.mark_participated(author, sr)

        if sr_id:
            g.stats.simple_event("modmail.received_message")

        inbox_rel = []

        inbox_hook = hooks.get_hook("message.skip_inbox")
        skip_inbox = inbox_hook.call_until_return(message=m)
        if skip_inbox:
            m._spam = True
            m._commit()

        if not skip_inbox and sr_id:
            if parent or to_subreddit or from_sr:
                inbox_rel.append(ModeratorInbox._add(sr, m))

            if sr.is_moderator(author):
                m.distinguished = 'yes'
                m._commit()

            if (can_send_email and
                    sr.name in g.live_config['modmail_forwarding_email']):
                queue_modmail_email(m)

        if author.name in g.admins:
            m.distinguished = 'admin'
            m._commit()

        # if there is a "to" we may have to create an inbox relation as well
        # also, only global admins can be message spammed.
        if not skip_inbox and to and (not m._spam or to.name in g.admins):
            # if "to" is not a sr moderator they need to be notified
            if not sr_id or not sr.is_moderator(to):
                inbox_rel.append(Inbox._add(to, m, 'inbox'))

                if (
                    to.pref_email_messages and
                    m.author_id not in to.enemies and
                    to._id != m.author_id
                ):
                    from r2.lib.template_helpers import get_domain
                    if from_sr:
                        sender_name = '/r/%s' % sr.name
                    else:
                        sender_name = '/u/%s' % author.name
                    permalink = 'http://%(domain)s%(path)s' % {
                        'domain': get_domain(),
                        'path': m.permalink,
                    }
                    data = {
                        'to': to._id36,
                        'from': sender_name,
                        'comment': m._fullname,
                        'permalink': permalink,
                    }
                    data = json.dumps(data)
                    TryLater.schedule('message_notification_email', data,
                                      NOTIFICATION_EMAIL_DELAY)

        # update user inboxes for non-mods involved in a modmail conversation
        if not skip_inbox and sr_id and m.first_message:
            first_message = Message._byID(m.first_message, data=True)
            first_sender = Account._byID(first_message.author_id, data=True)
            first_sender_modmail = sr.is_moderator_with_perms(
                first_sender, 'mail')

            if (first_sender != author and
                    first_sender != to and
                    not first_sender_modmail):
                inbox_rel.append(Inbox._add(first_sender, m, 'inbox'))

            if first_message.to_id:
                first_recipient = Account._byID(first_message.to_id, data=True)
                first_recipient_modmail = sr.is_moderator_with_perms(
                    first_recipient, 'mail')
                if (first_recipient != author and
                        first_recipient != to and
                        not first_recipient_modmail):
                    inbox_rel.append(Inbox._add(first_recipient, m, 'inbox'))

        if sr_id:
            g.events.modmail_event(m, request=request, context=c)
        else:
            g.events.message_event(m, request=request, context=c)

        return (m, inbox_rel)

    @property
    def permalink(self):
        return "/message/messages/%s" % self._id36

    def make_permalink(self, force_domain=False):
        from r2.lib.template_helpers import get_domain
        p = self.permalink
        if force_domain:
            permalink_domain = get_domain(subreddit=False)
            res = "%s://%s%s" % (g.default_scheme, permalink_domain, p)
        else:
            res = p
        return res

    def make_permalink_slow(self, context=None, anchor=False, force_domain=False):
        return self.make_permalink(force_domain)

    def can_view_slow(self):
        if c.user_is_loggedin:
            if (c.user_is_admin or
                    c.user._id in (self.author_id, self.to_id)):
                return True
            elif self.sr_id:
                sr = Subreddit._byID(self.sr_id, data=True, stale=True)

                if sr.is_moderator_with_perms(c.user, 'mail'):
                    return True
                elif self.first_message:
                    first = Message._byID(self.first_message, data=True)
                    return c.user._id in (first.author_id, first.to_id)

    def get_muted_user_in_conversation(self):
        """Return the muted user involved in a modmail conversation (if any)."""
        if not self.sr_id:
            return None

        sr = self.subreddit_slow

        if self.first_message:
            first = Message._byID(self.first_message, data=True)
        else:
            first = self

        first_author = first.author_slow
        first_recipient = first.recipient_slow if first.to_id else None

        if sr.is_muted(first_author):
            return first_author
        elif first_recipient and sr.is_muted(first_recipient):
            return first_recipient
        else:
            return None

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.db import queries

        # make sure there is a sr_id set:
        for w in wrapped:
            if not hasattr(w, "sr_id"):
                w.sr_id = None

        to_ids = {w.to_id for w in wrapped if w.to_id}
        other_account_ids = {w.display_author or w.display_to for w in wrapped
            if not (w.was_comment or w.sr_id) and
                (w.display_author or w.display_to)}
        account_ids = to_ids | other_account_ids
        accounts = Account._byID(account_ids, data=True)

        link_ids = {w.link_id for w in wrapped if w.was_comment}
        links = Link._byID(link_ids, data=True)

        srs = {w.subreddit._id: w.subreddit for w in wrapped if w.sr_id}

        parent_ids = {w.parent_id for w in wrapped
            if w.parent_id and w.was_comment}
        parents = Comment._byID(parent_ids, data=True)

        # load full modlist for all subreddit messages
        mods_by_srid = {sr._id: sr.moderator_ids() for sr in srs.itervalues()}
        user_mod_sr_ids = {sr_id for sr_id, mod_ids in mods_by_srid.iteritems()
            if user._id in mod_ids}

        # special handling for mod replies to mod PMs
        mod_message_authors = {}
        mod_messages = [
            item for item in wrapped
            if (item.to_id is None and
                    item.sr_id and
                    item.parent_id and
                    (c.user_is_admin or item.sr_id in user_mod_sr_ids))
        ]
        if mod_messages:
            parent_ids = [item.parent_id for item in mod_messages]
            parents = Message._byID(parent_ids, data=True, return_dict=True)
            author_ids = {item.author_id for item in parents.itervalues()}
            authors = Account._byID(author_ids, data=True, return_dict=True)

            for item in mod_messages:
                parent = parents[item.parent_id]
                author = authors[parent.author_id]
                mod_message_authors[item._id] = author

        # load the unread list to determine message newness
        unread = set(queries.get_unread_inbox(user))

        # load the unread mod list for the same reason
        mod_msg_srs = {srs[w.sr_id] for w in wrapped
            if w.sr_id and not w.was_comment and w.sr_id in user_mod_sr_ids}
        mod_unread = set(
            queries.get_unread_subreddit_messages_multi(mod_msg_srs))

        # load blocked subreddits
        sr_blocks = BlockedSubredditsByAccount.fast_query(user, srs.values())
        blocked_srids = {sr._id for _user, sr in sr_blocks.iterkeys()}

        can_set_unread = (user.pref_mark_messages_read and
                            c.extension not in ("rss", "xml", "api", "json"))
        to_set_unread = []

        # accent colors for color coded modmail
        sr_colors = None
        if isinstance(c.site, FakeSubreddit):
            mod_sr_ids = Subreddit.reverse_moderator_ids(user)
            if len(mod_sr_ids) > 1:
                sr_colors = dict(zip(mod_sr_ids, cycle(Subreddit.ACCENT_COLORS)))

        for item in wrapped:
            user_is_recipient = item.to_id == user._id
            user_is_sender = (item.author_id == user._id and
                not getattr(item, "display_author", None))
            sent_by_sr = item.sr_id and getattr(item, 'from_sr', None)
            sent_to_sr = item.sr_id and not item.to_id

            item.to = accounts[item.to_id] if item.to_id else None
            item.is_mention = False
            item.is_collapsed = None
            item.score_fmt = Score.none
            item.hide_author = False

            if item.was_comment:
                item.user_is_recipient = user_is_recipient
                link = links[item.link_id]
                sr = srs[link.sr_id]
                item.to_collapse = False
                item.author_collapse = False
                item.link_title = link.title
                item.permalink = item.lookups[0].make_permalink(link, sr=sr)
                item.link_permalink = link.make_permalink(sr)
                item.full_comment_count = link.num_comments
                parent = parents[item.parent_id] if item.parent_id else None

                if parent:
                    item.parent = parent._fullname
                    item.parent_permalink = parent.make_permalink(link, sr)

                if parent and parent.author_id == user._id:
                    item.subject = _('comment reply')
                elif not parent and link.author_id == user._id:
                    item.subject = _('post reply')
                else:
                    item.subject = _('username mention')
                    item.is_mention = True

                item.taglinetext = _(
                    "from %(author)s via %(subreddit)s sent %(when)s")
            elif item.sr_id:
                item.user_is_recipient = not user_is_sender
                item.user_is_moderator = item.sr_id in user_mod_sr_ids

                if sr_colors and item.user_is_moderator:
                    item.accent_color = sr_colors.get(item.sr_id)

                if item.subreddit.is_muted(item.author):
                    item.sr_muted = True

                if sent_by_sr:
                    if item.sr_id in blocked_srids:
                        item.subject = _('[message from blocked subreddit]')
                        item.sr_blocked = True
                        item.is_collapsed = True

                    # use special handling of admin distinguish because ALL
                    # messages from admin accounts are marked for admin
                    # distinguish, but we only want to use the admin distinguish
                    # on messages sent from /r/reddit.com
                    if (item.distinguished == "admin" and
                            "/r/%s" % item.subreddit.name == g.admin_message_acct):
                        subreddit_distinguish = "admin"
                    elif (item.distinguished == "moderator" or
                            item.distinguished == "admin"):
                        subreddit_distinguish = "moderator"
                    else:
                        subreddit_distinguish = None

                    if item.sent_via_email:
                        item.hide_author = True
                        item.distinguished = "yes"
                        item.taglinetext = _(
                            "subreddit message via %(subreddit)s sent %(when)s")
                    elif not item.user_is_moderator and not c.user_is_admin:
                        item.author = item.subreddit
                        item.hide_author = True
                        item.taglinetext = _(
                            "subreddit message via %(subreddit)s sent %(when)s")
                        item.subreddit_distinguish = subreddit_distinguish
                    elif user_is_sender:
                        item.taglinetext = _(
                            "to %(dest)s via %(subreddit)s sent %(when)s")
                        item.subreddit_distinguish = subreddit_distinguish
                    else:
                        item.taglinetext = _(
                            "from %(author)s via %(subreddit)s to %(dest)s sent"
                            " %(when)s")
                        # don't set item.subreddit_distinguish because any
                        # distinguish will be associated with the author
                else:
                    if item._id in mod_message_authors:
                        # let moderators see the original author when a regular
                        # user responds to a modmail message from subreddit.
                        # item.to_id is not set, but we found the original
                        # sender by inspecting the parent message
                        item.to = mod_message_authors[item._id]

                    if user_is_recipient:
                        item.taglinetext = _(
                            "from %(author)s via %(subreddit)s sent %(when)s")
                    elif user_is_sender and sent_to_sr:
                        item.taglinetext = _("to %(subreddit)s sent %(when)s")
                    elif user_is_sender:
                        item.taglinetext = _(
                            "to %(dest)s via %(subreddit)s sent %(when)s")
                    elif sent_to_sr:
                        item.taglinetext = _(
                            "from %(author)s to %(subreddit)s sent %(when)s")
                    else:
                        item.taglinetext = _(
                            "from %(author)s via %(subreddit)s to %(dest)s sent"
                            " %(when)s")
            else:
                item.user_is_recipient = user_is_recipient

                if item.display_author:
                    item.author = accounts[item.display_author]

                if item.display_to:
                    item.to = accounts[item.display_to]
                    if item.to_id == user._id:
                        item.body = (strings.anonymous_gilder_warning +
                            _force_unicode(item.body))

                if user_is_recipient:
                    item.taglinetext = _("from %(author)s sent %(when)s")
                elif user_is_sender:
                    item.taglinetext = _("to %(dest)s sent %(when)s")
                else:
                    item.taglinetext = _(
                        "to %(dest)s from %(author)s sent %(when)s")

            if user_is_sender:
                item.new = False
            elif item._fullname in unread:
                item.new = True

                if can_set_unread:
                    to_set_unread.append(item.lookups[0])
            else:
                item.new = item._fullname in mod_unread

            if not item.new:
                if item.user_is_recipient:
                    item.is_collapsed = item.to_collapse
                if item.author_id == user._id:
                    item.is_collapsed = item.author_collapse
                if user.pref_collapse_read_messages:
                    item.is_collapsed = (item.is_collapsed is not False)

            if item.author_id in user.enemies and not item.was_comment:
                item.is_collapsed = True
                if not c.user_is_admin:
                    item.subject = _('[message from blocked user]')
                    item.body = _('[unblock user to see this message]')

            if item.sr_id and item.to:
                item.to_is_moderator = item.to._id in mods_by_srid[item.sr_id]

        if to_set_unread:
            queries.set_unread(to_set_unread, user, unread=False)

        Printable.add_props(user, wrapped)

    @property
    def subreddit_slow(self):
        from subreddit import Subreddit
        if self.sr_id:
            return Subreddit._byID(self.sr_id, data=True)

    @property
    def author_slow(self):
        """Returns the message's author."""
        # The author is often already on the wrapped message as .author
        # If available, that should be used instead of calling this
        return Account._byID(self.author_id, data=True, return_dict=False)

    @property
    def recipient_slow(self):
        """Returns the message's recipient."""
        return Account._byID(self.to_id, data=True, return_dict=False)

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        s.extend([wrapped.new, wrapped.collapsed])
        return s

    def keep_item(self, wrapped):
        if c.user_is_admin:
            return True
        # do not keep message which were deleted on recipient
        if (isinstance(self, Message) and
                self.to_id == c.user._id and self.del_on_recipient):
            return False
        return not wrapped.enemy


class _SaveHideByAccount(tdb_cassandra.DenormalizedRelation):
    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def _cached_queries(cls, user, thing):
        return []

    @classmethod
    def _savehide(cls, user, things, **kw):
        things = tup(things)
        now = datetime.now(g.tz)
        with CachedQueryMutator() as m:
            for thing in things:
                # action_date is only used by the cached queries as the sort
                # value, we don't want to write it. Report.new(link) needs to
                # incr link.reported but will fail if the link is dirty.
                thing.__setattr__('action_date', now, make_dirty=False)
                for q in cls._cached_queries(user, thing, **kw):
                    m.insert(q, [thing])
        cls.create(user, things, **kw)

    @classmethod
    def destroy(cls, user, things, **kw):
        things = tup(things)
        cls._cf.remove(user._id36, (things._id36 for things in things))

        for view in cls._views:
            view.destroy(user, things, **kw)

    @classmethod
    def _unsavehide(cls, user, things, **kw):
        things = tup(things)
        with CachedQueryMutator() as m:
            for thing in things:
                for q in cls._cached_queries(user, thing, **kw):
                    m.delete(q, [thing])
        cls.destroy(user, things, **kw)


class _ThingSavesByAccount(_SaveHideByAccount):
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    @classmethod
    def value_for(cls, thing1, thing2, category=None):
        return category or ''

    @classmethod
    def _remove_from_category_listings(cls, user, things, category):
        things = tup(things)
        oldcategories = cls.fast_query(user, things)
        changedthings = []
        for thing in things:
            oldcategory = oldcategories.get((user, thing)) or None
            if oldcategory != category:
                changedthings.append(thing)
        cls._unsavehide(user, changedthings, categories=oldcategories)

    @classmethod
    def _save(cls, user, things, category=None):
        category = category.lower() if category else None
        cls._remove_from_category_listings(user, things, category=category)
        cls._savehide(user, things, category=category)

    @classmethod
    def _unsave(cls, user, things):
        # Ensure we delete from existing category cached queries
        categories = cls.fast_query(user, tup(things))
        cls._unsavehide(user, things, categories=categories)

    @classmethod
    def _unsavehide(cls, user, things, categories=None):
        things = tup(things)
        with CachedQueryMutator() as m:
            for thing in things:
                category = categories.get((user, thing)) if categories else None
                for q in cls._cached_queries(user, thing, category=category):
                    m.delete(q, [thing])
        cls.destroy(user, things, categories=categories)

    @classmethod
    def _cached_queries_category(cls, user, thing,
                                 querycatfn, queryfn,
                                 category=None, only_category=False):
        from r2.lib.db import queries
        cached_queries = []
        if not only_category:
            cached_queries = [queryfn(user, 'none'), queryfn(user, thing.sr_id)]
        if category:
            cached_queries.append(querycatfn(user, 'none', category))
            cached_queries.append(querycatfn(user, thing.sr_id, category))
        return cached_queries

class LinkSavesByAccount(_ThingSavesByAccount):
    _use_db = True
    _last_modified_name = 'Save'
    _views = []

    @classmethod
    def _cached_queries(cls, user, thing, **kw):
        from r2.lib.db import queries
        return cls._cached_queries_category(
            user,
            thing,
            queries.get_categorized_saved_links,
            queries.get_saved_links,
            **kw)

class CommentSavesByAccount(_ThingSavesByAccount):
    _use_db = True
    _last_modified_name = 'CommentSave'
    _views = []

    @classmethod
    def _cached_queries(cls, user, thing, **kw):
        from r2.lib.db import queries
        return cls._cached_queries_category(
            user,
            thing,
            queries.get_categorized_saved_comments,
            queries.get_saved_comments,
            **kw)

class _ThingHidesByAccount(_SaveHideByAccount):
    @classmethod
    def _hide(cls, user, things):
        cls._savehide(user, things)

    @classmethod
    def _unhide(cls, user, things):
        cls._unsavehide(user, things)


class LinkHidesByAccount(_ThingHidesByAccount):
    _use_db = True
    _last_modified_name = 'Hide'
    _views = []

    @classmethod
    def _cached_queries(cls, user, thing):
        from r2.lib.db import queries
        return [queries.get_hidden_links(user)]

class LinkVisitsByAccount(_SaveHideByAccount):
    _use_db = True
    _last_modified_name = 'Visit'
    _views = []
    _ttl = timedelta(days=7)
    _write_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _visit(cls, user, things):
        cls._savehide(user, things)

    @classmethod
    def _unvisit(cls, user, things):
        cls._unsavehide(user, things)

class _ThingSavesBySubreddit(tdb_cassandra.View):
    @classmethod
    def _rowkey(cls, user, thing):
        return user._id36

    @classmethod
    def _column(cls, user, thing):
        return {utils.to36(thing.sr_id): ''}

    @classmethod
    def get_saved_values(cls, user):
        rowkey = cls._rowkey(user, None)
        try:
            columns = cls._cf.get(rowkey,
                                  column_count=tdb_cassandra.max_column_count)
        except NotFoundException:
            return []

        return columns.keys()

    @classmethod
    def get_saved_subreddits(cls, user):
        sr_id36s = cls.get_saved_values(user)
        srs = Subreddit._byID36(sr_id36s, return_dict=False, data=True)
        return sorted([sr.name for sr in srs])

    @classmethod
    def create(cls, user, things, **kw):
        for thing in things:
            rowkey = cls._rowkey(user, thing)
            column = cls._column(user, thing)
            cls._set_values(rowkey, column)

    @classmethod
    def _check_empty(cls, user, sr_id):
        return False

    @classmethod
    def destroy(cls, user, things, **kw):
        # See if thing's sr is present anymore
        sr_ids = set([thing.sr_id for thing in things])
        for sr_id in set(sr_ids):
            if cls._check_empty(user, sr_id):
                cls._cf.remove(user._id36, [utils.to36(sr_id)])

class _ThingSavesByCategory(_ThingSavesBySubreddit):
    @classmethod
    def create(cls, user, things, category=None):
        if not category:
            return
        for thing in things:
            rowkey = cls._rowkey(user, thing)
            column = {category: None}
            cls._set_values(rowkey, column)

    @classmethod
    def _get_query_fn():
        raise NotImplementedError

    @classmethod
    def _check_empty(cls, user, category):
        from r2.lib.db import queries
        q = cls._get_query_fn()(user, 'none', category)
        q.fetch()
        return not q.data

    @classmethod
    def get_saved_categories(cls, user):
        return cls.get_saved_values(user)

    @classmethod
    def destroy(cls, user, things, categories=None):
        if not categories:
            return
        for category in set(categories.values()):
            if not category or not cls._check_empty(user, category):
                continue
            cls._cf.remove(user._id36, [category])

@view_of(LinkSavesByAccount)
class LinkSavesByCategory(_ThingSavesByCategory):
    _use_db = True

    @classmethod
    def _get_query_fn(cls):
        from r2.lib.db import queries
        return queries.get_categorized_saved_links

@view_of(LinkSavesByAccount)
class LinkSavesBySubreddit(_ThingSavesBySubreddit):
    _use_db = True

    @classmethod
    def _check_empty(cls, user, sr_id):
        from r2.lib.db import queries
        q = queries.get_saved_links(user, sr_id)
        q.fetch()
        return not q.data


@view_of(CommentSavesByAccount)
class CommentSavesBySubreddit(_ThingSavesBySubreddit):
    _use_db = True

    @classmethod
    def _check_empty(cls, user, sr_id):
        from r2.lib.db import queries
        q = queries.get_saved_comments(user, sr_id)
        q.fetch()
        return not q.data

@view_of(CommentSavesByAccount)
class CommentSavesByCategory(_ThingSavesByCategory):
    _use_db = True

    @classmethod
    def _get_query_fn(cls):
        from r2.lib.db import queries
        return queries.get_categorized_saved_comments

class LinksByImage(tdb_cassandra.View):
    _use_db = True

    # If a popular site uses the same oembed image everywhere (*cough* reddit),
    # we may have a shitton of links pointing to the same image.
    _fetch_all_columns = True

    _extra_schema_creation_args = {
        'key_validation_class': ASCII_TYPE,
    }

    @classmethod
    def _rowkey(cls, image_uid):
        return image_uid

    @classmethod
    def add_link(cls, image_uid, link):
        rowkey = cls._rowkey(image_uid)
        column = {link._id36: ''}
        cls._set_values(rowkey, column)

    @classmethod
    def remove_link(cls, image_uid, link):
        """A weakly-guaranteed removal of the record tying a Link to an image."""
        rowkey = cls._rowkey(image_uid)
        columns = (link._id36,)
        cls._remove(rowkey, columns)

    @classmethod
    def get_link_id36s(cls, image_uid):
        rowkey = cls._rowkey(image_uid)
        try:
            columns = cls._byID(rowkey)._values()
        except NotFoundException:
            return []
        return columns.iterkeys()


_CommentInbox = Relation(Account, Comment)
_CommentInbox._defaults = {
    "new": True,
}
_CommentInbox._cache = g.thingcache
_CommentInbox._cache_prefix = classmethod(lambda cls: "inboxcomment:")


_MessageInbox = Relation(Account, Message)
_MessageInbox._defaults = {
    "new": True,
}
_MessageInbox._cache = g.thingcache
_MessageInbox._cache_prefix = classmethod(lambda cls: "inboxmessage:")


class Inbox(MultiRelation('inbox', _CommentInbox, _MessageInbox)):
    @classmethod
    def _add(cls, to, obj, name, orangered=True):
        if isinstance(obj, Comment):
            assert name in ("inbox", "selfreply", "mention")
        elif isinstance(obj, Message):
            assert name == "inbox"

        # don't orangered (ever!) for enemies
        if obj.author_id in to.enemies:
            orangered = False
        # don't get an orangered for messaging yourself.
        elif to._id == obj.author_id:
            orangered = False

        i = Inbox(to, obj, name)
        i._commit()

        if orangered:
            to._incr('inbox_count', 1)

        return i

    @classmethod
    def possible_recipients(cls, obj):
        """Determine all possible recipients of Inboxes for this object.
           `obj` may be one of (Comment, Message).
        """

        possible_recipients = []
        if isinstance(obj, Comment):
            # Item is a comment. Eligible types of inboxes: mentions,
            # selfreply (which can exist on all posts if sendreplies=True),
            # inbox (which is a comment reply)

            parent_id = getattr(obj, 'parent_id', None)
            if parent_id:
                # Comment reply
                parent_comment = Comment._byID(parent_id, data=True)
                possible_recipients.append(parent_comment.author_id)
            else:
                # Selfreply
                # Do not check sendreplies, as they may have flagged it off
                # between when the comment was created and when we are checking
                parent_link = Link._byID(obj.link_id, data=True)
                possible_recipients.append(parent_link.author_id)

            mentions = utils.extract_user_mentions(obj.body)
            if len(mentions) <= g.butler_max_mentions:
                possible_recipients.extend(Account._names_to_ids(
                    mentions,
                    ignore_missing=True,
                ))
        elif isinstance(obj, Message):
            if obj.to_id:
                possible_recipients.append(obj.to_id)
        else:
            g.log.warning("Unknown object type for recipients: %r", obj)

        return possible_recipients


    @classmethod
    def get_rels(cls, user, things):
        things = tup(things)
        messages = [t for t in things if isinstance(t, Message)]
        comments = [t for t in things if isinstance(t, Comment)]

        res = {}

        if messages:
            inbox_rel_cls = cls.rel(Account, Message)
            message_res = inbox_rel_cls._fast_query(
                thing1s=user,
                thing2s=messages,
                name="inbox",
            )
            res.update(message_res)

        if comments:
            inbox_rel_cls = cls.rel(Account, Comment)
            comment_res = inbox_rel_cls._fast_query(
                thing1s=user,
                thing2s=comments,
                name=("inbox", "selfreply", "mention"),
            )
            res.update(comment_res)

        # _fast_query returns a dict of {(t1, t2, name): rel}, with rel of None
        # if the relation doesn't exist
        inbox_rels = [inbox_rel for inbox_rel in res.itervalues() if inbox_rel]
        return inbox_rels

    @classmethod
    def set_unread(cls, inbox_rels, unread=True):
        inbox_rels = tup(inbox_rels)
        unread_count_by_user = defaultdict(int)
        for inbox_rel in inbox_rels:
            if inbox_rel.new != unread:
                user = inbox_rel._thing1
                unread_count_by_user[user] += 1 if unread else -1
                inbox_rel.new = unread
                inbox_rel._commit()

        for user, unread_count in unread_count_by_user.iteritems():
            if unread_count == 0:
                continue

            if user.inbox_count + unread_count < 0:
                g.log.info(
                    "Inbox count for %r would be negative: %d + %d. Zeroing.",
                    user.name,
                    user.inbox_count,
                    unread_count,
                )
                g.stats.simple_event("inbox_counts.negative_total_fix")
                unread_count = -user.inbox_count

            user._incr('inbox_count', unread_count)


class ModeratorInbox(Relation(Subreddit, Message)):
    _cache = g.thingcache
    _defaults = {
        "new": True,
    }

    @classmethod
    def _cache_prefix(cls):
        return "modinbox:"

    @classmethod
    def _add(cls, sr, obj):
        i = cls(sr, obj, name="inbox")
        i._commit()
        return i

    @classmethod
    def get_rels(cls, sr, messages):
        messages = tup(messages)
        res = ModeratorInbox._fast_query(
            thing1s=sr,
            thing2s=messages,
            name="inbox",
        )
        # _fast_query returns a dict of {(t1, t2, name): rel}, with rel of None
        # if the relation doesn't exist
        inbox_rels = [inbox_rel for inbox_rel in res.itervalues() if inbox_rel]
        return inbox_rels

    @classmethod
    def set_unread(cls, inbox_rels, unread=True):
        inbox_rels = tup(inbox_rels)
        for inbox_rel in inbox_rels:
            if inbox_rel.new != unread:
                inbox_rel.new = unread
                inbox_rel._commit()


class CommentsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def add_comment(cls, account, comment):
        cls.create(account, [comment])


class LinksByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def add_link(cls, account, link):
        cls.create(account, [link])


class MessagesByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def add_message(cls, account, message):
        cls.create(account, [message])


class CommentVisitsByUser(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.ONE
    _ttl = timedelta(days=2)
    _compare_with = tdb_cassandra.DateType()
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
    }
    MAX_VISITS = 10

    @classmethod
    def _rowkey(cls, user, link):
        return "%s-%s" % (user._id36, link._id36)

    @classmethod
    def get_previous_visits(cls, user, link):
        rowkey = cls._rowkey(user, link)
        try:
            columns = cls._cf.get(
                rowkey, column_count=cls.MAX_VISITS, column_reversed=True)
        except NotFoundException:
            return []
        # NOTE: dates return from pycassa are UTC but missing their timezone
        dates = [date.replace(tzinfo=pytz.UTC) for date in columns.keys()]
        return sorted(dates)

    @classmethod
    def add_visit(cls, user, link, visit_time):
        rowkey = cls._rowkey(user, link)
        column = {visit_time: ''}
        cls._set_values(rowkey, column)

    @classmethod
    def get_and_update(cls, user, link, visit_time):
        visits = cls.get_previous_visits(user, link)
        if visits:
            previous_visit = visits[-1]
            time_since_previous = visit_time - previous_visit

            if time_since_previous.total_seconds() <= g.comment_visits_period:
                visits.pop()
                return visits

        cls.add_visit(user, link, visit_time)
        return visits
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import hashlib
import time
import email.utils
from email.MIMEText import MIMEText
from email.errors import HeaderParseError

import sqlalchemy as sa
from sqlalchemy.dialects.postgresql.base import PGInet

from r2.lib.db.tdb_sql import make_metadata, index_str, create_table
from r2.lib.utils import Enum, tup
from r2.lib.memoize import memoize
from pylons import request
from pylons import app_globals as g
from pylons.i18n import _

def mail_queue(metadata):
    return sa.Table(g.db_app_name + '_mail_queue', metadata,
                    sa.Column("uid", sa.Integer,
                              sa.Sequence('queue_id_seq'), primary_key=True),

                    # unique hash of the message to carry around
                    sa.Column("msg_hash", sa.String),

                    # the id of the account who started it
                    sa.Column('account_id', sa.BigInteger),

                    # the name (not email) for the from
                    sa.Column('from_name', sa.String),

                    # the "To" address of the email
                    sa.Column('to_addr', sa.String),

                    # the "From" address of the email
                    sa.Column('fr_addr', sa.String),

                    # the "Reply-To" address of the email
                    sa.Column('reply_to', sa.String),

                    # fullname of the thing
                    sa.Column('fullname', sa.String),

                    # when added to the queue
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              nullable = False),

                    # IP of original request
                    sa.Column('ip', PGInet),

                    # enum of kind of event
                    sa.Column('kind', sa.Integer),

                    # any message that may have been included
                    sa.Column('body', sa.String),

                    )

def sent_mail_table(metadata, name = 'sent_mail'):
    return sa.Table(g.db_app_name + '_' + name, metadata,
                    # tracking hash of the email
                    sa.Column('msg_hash', sa.String, primary_key=True),

                    # the account who started it
                    sa.Column('account_id', sa.BigInteger),

                    # the "To" address of the email
                    sa.Column('to_addr', sa.String),

                    # the "From" address of the email
                    sa.Column('fr_addr', sa.String),

                    # the "reply-to" address of the email
                    sa.Column('reply_to', sa.String),

                    # IP of original request
                    sa.Column('ip', PGInet),

                    # fullname of the reference thing
                    sa.Column('fullname', sa.String),

                    # send date
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False),

                    # enum of kind of event
                    sa.Column('kind', sa.Integer),

                    )


def opt_out(metadata):
    return sa.Table(g.db_app_name + '_opt_out', metadata,
                    sa.Column('email', sa.String, primary_key = True),
                    # when added to the list
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False),
                    # why did they do it!?
                    sa.Column('msg_hash', sa.String),
                    )

class EmailHandler(object):
    def __init__(self, force = False):
        engine = g.dbm.get_engine('email')
        self.metadata = make_metadata(engine)
        self.queue_table = mail_queue(self.metadata)
        indices = [index_str(self.queue_table, "date", "date"),
                   index_str(self.queue_table, 'kind', 'kind')]
        create_table(self.queue_table, indices)

        self.opt_table = opt_out(self.metadata)
        indices = [index_str(self.opt_table, 'email', 'email')]
        create_table(self.opt_table, indices)

        self.track_table = sent_mail_table(self.metadata)
        self.reject_table = sent_mail_table(self.metadata, name = "reject_mail")

        def sent_indices(tab):
            indices = [index_str(tab, 'to_addr', 'to_addr'),
                       index_str(tab, 'date', 'date'),
                       index_str(tab, 'ip', 'ip'),
                       index_str(tab, 'kind', 'kind'),
                       index_str(tab, 'fullname', 'fullname'),
                       index_str(tab, 'account_id', 'account_id'),
                       index_str(tab, 'msg_hash', 'msg_hash'),
                       ]

        create_table(self.track_table, sent_indices(self.track_table))
        create_table(self.reject_table, sent_indices(self.reject_table))

    def __repr__(self):
        return "<email-handler>"

    def has_opted_out(self, email):
        o = self.opt_table
        s = sa.select([o.c.email], o.c.email == email, limit = 1)
        res = s.execute()
        return bool(res.fetchall())


    def opt_out(self, msg_hash):
        """Adds the recipient of the email to the opt-out list and returns
        that address."""
        email = self.get_recipient(msg_hash)
        if email:
            o = self.opt_table
            try:
                o.insert().values({o.c.email: email,
                                   o.c.msg_hash: msg_hash}).execute()
                g.stats.simple_event('share.opt_out')

                #clear caches
                has_opted_out(email, _update = True)
                opt_count(_update = True)
                return (email, True)
            except sa.exc.DBAPIError:
                return (email, False)
        return (None, False)

    def opt_in(self, msg_hash):
        """Removes recipient of the email from the opt-out list"""
        email = self.get_recipient(msg_hash)
        if email:
            o = self.opt_table
            if self.has_opted_out(email):
                sa.delete(o, o.c.email == email).execute()
                g.stats.simple_event('share.opt_in')

                #clear caches
                has_opted_out(email, _update = True)
                opt_count(_update = True)
                return (email, True)
            else:
                return (email, False)
        return (None, False)

    def get_recipient(self, msg_hash):
        t = self.track_table
        s = sa.select([t.c.to_addr], t.c.msg_hash == msg_hash).execute()
        res = s.fetchall()
        return res[0][0] if res and res[:1] else None


    def add_to_queue(self, user, emails, from_name, fr_addr, kind,
                     date = None, ip = None,
                     body = "", reply_to = "", thing = None):
        s = self.queue_table
        hashes = []
        if not date:
            date = datetime.datetime.now(g.tz)
        if not ip:
            ip = getattr(request, "ip", "127.0.0.1")
        for email in tup(emails):
            uid = user._id if user else 0
            tid = thing._fullname if thing else ""
            key = hashlib.sha1(str((email, from_name, uid, tid, ip, kind, body,
                               datetime.datetime.now(g.tz)))).hexdigest()
            s.insert().values({s.c.to_addr : email,
                               s.c.account_id : uid,
                               s.c.from_name : from_name,
                               s.c.fr_addr : fr_addr,
                               s.c.reply_to : reply_to,
                               s.c.fullname: tid,
                               s.c.ip : ip,
                               s.c.kind: kind,
                               s.c.body: body,
                               s.c.date : date,
                               s.c.msg_hash : key}).execute()
            hashes.append(key)
        return hashes


    def from_queue(self, max_date, batch_limit = 50, kind = None):
        from r2.models import Account, Thing
        keep_trying = True
        min_id = None
        s = self.queue_table
        while keep_trying:
            where = [s.c.date < max_date]
            if min_id:
                where.append(s.c.uid > min_id)
            if kind:
                where.append(s.c.kind == kind)

            res = sa.select([s.c.to_addr, s.c.account_id,
                             s.c.from_name, s.c.fullname, s.c.body,
                             s.c.kind, s.c.ip, s.c.date, s.c.uid,
                             s.c.msg_hash, s.c.fr_addr, s.c.reply_to],
                            sa.and_(*where),
                            order_by = s.c.uid, limit = batch_limit).execute()
            res = res.fetchall()

            if not res: break

            # batch load user accounts
            aids = [x[1] for x in res if x[1] > 0]
            accts = Account._byID(aids, data = True,
                                  return_dict = True) if aids else {}

            # batch load things
            tids = [x[3] for x in res if x[3]]
            things = Thing._by_fullname(tids, data = True,
                                        return_dict = True) if tids else {}

            # get the lower bound date for next iteration
            min_id = max(x[8] for x in res)

            # did we not fetch them all?
            keep_trying = (len(res) == batch_limit)

            for (addr, acct, fname, fulln, body, kind, ip, date, uid,
                 msg_hash, fr_addr, reply_to) in res:
                yield (accts.get(acct), things.get(fulln), addr,
                       fname, date, ip, kind, msg_hash, body,
                       fr_addr, reply_to)

    def clear_queue(self, max_date, kind = None):
        s = self.queue_table
        where = [s.c.date < max_date]
        if kind:
            where.append([s.c.kind == kind])
        sa.delete(s, sa.and_(*where)).execute()


class Email(object):
    handler = EmailHandler()

    # Do not modify in any way other than appending new items!
    # Database tables storing mail stuff use an int column as an index into 
    # this Enum, so anything other than appending new items breaks mail history.
    Kind = Enum("SHARE", "FEEDBACK", "ADVERTISE", "OPTOUT", "OPTIN",
                "VERIFY_EMAIL", "RESET_PASSWORD",
                "BID_PROMO",
                "ACCEPT_PROMO",
                "REJECT_PROMO",
                "QUEUED_PROMO",
                "LIVE_PROMO",
                "FINISHED_PROMO",
                "NEW_PROMO",
                "NERDMAIL",
                "GOLDMAIL",
                "PASSWORD_CHANGE",
                "EMAIL_CHANGE",
                "REFUNDED_PROMO",
                "VOID_PAYMENT",
                "GOLD_GIFT_CODE",
                "SUSPICIOUS_PAYMENT",
                "FRAUD_ALERT",
                "USER_FRAUD",
                "MESSAGE_NOTIFICATION",
                "ADS_ALERT",
                "EDITED_LIVE_PROMO",
                )

    # Do not remove anything from this dictionary!  See above comment.
    subjects = {
        Kind.SHARE : _("[reddit] %(user)s has shared a link with you"),
        Kind.FEEDBACK : _("[feedback] feedback from '%(user)s'"),
        Kind.ADVERTISE :  _("[advertising] feedback from '%(user)s'"),
        Kind.OPTOUT : _("[reddit] email removal notice"),
        Kind.OPTIN  : _("[reddit] email addition notice"),
        Kind.RESET_PASSWORD : _("[reddit] reset your password"),
        Kind.VERIFY_EMAIL : _("[reddit] verify your email address"),
        Kind.BID_PROMO : _("[reddit] your budget has been accepted"),
        Kind.ACCEPT_PROMO : _("[reddit] your promotion has been accepted"),
        Kind.REJECT_PROMO : _("[reddit] your promotion has been rejected"),
        Kind.QUEUED_PROMO : _("[reddit] your promotion has been charged"),
        Kind.LIVE_PROMO   : _("[reddit] your promotion is now live"),
        Kind.FINISHED_PROMO : _("[reddit] your promotion has finished"),
        Kind.NEW_PROMO : _("[reddit] your promotion has been created"),
        Kind.EDITED_LIVE_PROMO : _("[reddit] your promotion edit is being approved"),
        Kind.NERDMAIL : _("[reddit] hey, nerd!"),
        Kind.GOLDMAIL : _("[reddit] reddit gold activation link"),
        Kind.PASSWORD_CHANGE : _("[reddit] your password has been changed"),
        Kind.EMAIL_CHANGE : _("[reddit] your email address has been changed"),
        Kind.REFUNDED_PROMO: _("[reddit] your campaign didn't get enough impressions"),
        Kind.VOID_PAYMENT: _("[reddit] your payment has been voided"),
        Kind.GOLD_GIFT_CODE: _("[reddit] your reddit gold gift code"),
        Kind.SUSPICIOUS_PAYMENT: _("[selfserve] suspicious payment alert"),
        Kind.FRAUD_ALERT: _("[selfserve] fraud alert"),
        Kind.USER_FRAUD: _("[selfserve] a user has committed fraud"),
        Kind.MESSAGE_NOTIFICATION: _("[reddit] message notification"),
        Kind.ADS_ALERT: _("[reddit] Ads Alert"),
        }

    def __init__(self, user, thing, email, from_name, date, ip,
                 kind, msg_hash, body = '', from_addr = '',
                 reply_to = ''):
        self.user = user
        self.thing = thing
        self.to_addr = email
        self.fr_addr = from_addr
        self._from_name = from_name
        self.date = date
        self.ip = ip
        self.kind = kind
        self.sent = False
        self.body = body
        self.msg_hash = msg_hash
        self.reply_to = reply_to
        self.subject = self.subjects.get(kind, "")
        try:
            self.subject = self.subject % dict(user = self.from_name())
        except UnicodeDecodeError:
            self.subject = self.subject % dict(user = "a user")


    def from_name(self):
        if not self.user:
            name = "%(name)s"
        elif self._from_name != self.user.name:
            name = "%(name)s (%(uname)s)"
        else:
            name = "%(uname)s"
        return name % dict(name = self._from_name,
                           uname = self.user.name if self.user else '')

    @classmethod
    def get_unsent(cls, max_date, batch_limit = 50, kind = None):
        for e in cls.handler.from_queue(max_date, batch_limit = batch_limit,
                                        kind = kind):
            yield cls(*e)

    def should_queue(self):
        return (not self.user  or not self.user._spam) and \
               (not self.thing or not self.thing._spam) and \
               (self.kind == self.Kind.OPTOUT or
                not has_opted_out(self.to_addr))

    def set_sent(self, date = None, rejected = False):
        if not self.sent:
            self.date = date or datetime.datetime.now(g.tz)
            t = self.handler.reject_table if rejected else self.handler.track_table
            try:
                t.insert().values({t.c.account_id:
                                       self.user._id if self.user else 0,
                                   t.c.to_addr :   self.to_addr,
                                   t.c.fr_addr :   self.fr_addr,
                                   t.c.reply_to :  self.reply_to,
                                   t.c.ip :        self.ip,
                                   t.c.fullname:
                                       self.thing._fullname if self.thing else "",
                                   t.c.date:       self.date,
                                   t.c.kind :      self.kind,
                                   t.c.msg_hash :  self.msg_hash,
                                   }).execute()
            except:
                print "failed to send message"

            self.sent = True

    def to_MIMEText(self):
        def utf8(s, reject_newlines=True):
            if reject_newlines and '\n' in s:
                raise HeaderParseError(
                    'header value contains unexpected newline: {!r}'.format(s))
            return s.encode('utf8') if isinstance(s, unicode) else s

        fr = '"%s" <%s>' % (
            self.from_name().replace('"', ''),
            self.fr_addr.replace('>', ''),
        )

        # Addresses that start with a dash could confuse poorly-written
        # software's argument parsers, and thus are disallowed by default in
        # Postfix: http://www.postfix.org/postconf.5.html#allow_min_user
        if not fr.startswith('-') and not self.to_addr.startswith('-'):
            msg = MIMEText(utf8(self.body, reject_newlines=False))
            msg.set_charset('utf8')
            msg['To']      = utf8(self.to_addr)
            msg['From']    = utf8(fr)
            msg['Subject'] = utf8(self.subject)
            timestamp = time.mktime(self.date.timetuple())
            msg['Date'] = utf8(email.utils.formatdate(timestamp))
            if self.user:
                msg['X-Reddit-username'] = utf8(self.user.name)
            msg['X-Reddit-ID'] = self.msg_hash
            if self.reply_to:
                msg['Reply-To'] = utf8(self.reply_to)
            return msg
        return None

@memoize('r2.models.mail_queue.has_opted_out')
def has_opted_out(email):
    o = Email.handler.opt_table
    s = sa.select([o.c.email], o.c.email == email, limit = 1)
    res = s.execute()
    return bool(res.fetchall())


@memoize('r2.models.mail_queue.opt_count')
def opt_count():
    o = Email.handler.opt_table
    s = sa.select([sa.func.count(o.c.email)])
    res = s.execute().fetchone()
    return int(res[0])
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict

from pylons import app_globals as g

from r2.lib.utils import SimpleSillyStub
from r2.lib.utils.comment_tree_utils import get_tree_details, calc_num_children
from r2.models.link import Comment


"""Storage for comment trees

CommentTree is a class that provides an interface to the actual storage.
Whatever the underlying storage is, it must be able to generate the following
structures:
* tree: dict of comment id -> list of child comment ids. The `None` entry is
  top level comments
* cids: list of all comment ids in the comment tree
* depth: dict of comment id -> depth
* parents: dict of comment id -> parent comment id
* num_children: dict of comment id -> number of descendant comments, not just
  direct children

CommentTreePermacache uses permacache as the storage, and stores just the tree
structure. The cids, depth, parents and num_children are generated on the fly
from the tree.

Attempts were made to move to a different data model that would take advantage
of the column based storage of Cassandra and eliminate the need for locking when
adding a comment to the comment tree.

CommentTreeStorageV2: for each comment, write a column where the column name is
(parent_comment id, comment_id) and the column value is a counter giving the
size of the subtree rooted at the comment. This data model was abandoned because
counters ended up being unreliable and the shards put too much GC pressure on
the Cassandra JVM.

CommentTreeStorageV3: for each comment, write a column where the column name is
(depth, parent_comment_id, comment_id) and the column value is not used. This
data model was abandoned because of more unexpected GC problems after longer
time periods and generally insufficient regular-case performance.

"""


class CommentTreePermacache(object):
    @classmethod
    def _permacache_key(cls, link):
        return 'comments_' + str(link._id)

    @classmethod
    def _mutation_context(cls, link):
        """Return a lock for use during read-modify-write operations"""
        key = 'comment_lock_' + str(link._id)
        return g.make_lock("comment_tree", key)

    @classmethod
    def prepare_new_storage(cls, link):
        """Write an empty tree to permacache"""
        with cls._mutation_context(link) as lock:
            # read-modify-write, so get the lock
            existing_tree = cls._load_tree(link)
            if not existing_tree:
                # don't overwrite an existing non-empty tree
                tree = {}
                cls._write_tree(link, tree, lock)

    @classmethod
    def _load_tree(cls, link):
        key = cls._permacache_key(link)
        tree = g.permacache.get(key)
        return tree or {}   # assume empty tree on miss

    @classmethod
    def _write_tree(cls, link, tree, lock):
        assert lock.have_lock
        key = cls._permacache_key(link)
        g.permacache.set(key, tree)

    @classmethod
    def get_tree_pieces(cls, link, timer):
        tree = cls._load_tree(link)
        timer.intermediate('load')

        cids, depth, parents = get_tree_details(tree)
        num_children = calc_num_children(tree)
        num_children = defaultdict(int, num_children)
        timer.intermediate('calculate')

        return cids, tree, depth, parents, num_children

    @classmethod
    def add_comments(cls, link, comments):
        with cls._mutation_context(link) as lock:
            # adding comments requires read-modify-write, so get the lock
            tree = cls._load_tree(link)
            cids, _, _ = get_tree_details(tree)

            # skip any comments that are already in the stored tree and convert
            # to a set to remove any duplicate comments
            comments = {
                comment for comment in comments
                if comment._id not in cids
            }

            if not comments:
                return

            # warn on any comments whose parents are missing from the tree
            # because they will never be displayed unless their parent is
            # added. this can happen in normal operation if there are multiple
            # queue consumers and a child is processed before its parent.
            parent_ids = set(cids) | {comment._id for comment in comments}
            possible_orphan_comments = {
                comment for comment in comments
                if (comment.parent_id and comment.parent_id not in parent_ids)
            }
            if possible_orphan_comments:
                g.log.error("comment_tree_inconsistent: %s %s", link,
                    possible_orphan_comments)
                g.stats.simple_event('comment_tree_inconsistent')

            for comment in comments:
                tree.setdefault(comment.parent_id, []).append(comment._id)

            cls._write_tree(link, tree, lock)

    @classmethod
    def rebuild(cls, link, comments):
        """Generate a tree from comments and overwrite any existing tree."""
        with cls._mutation_context(link) as lock:
            # not reading, but we should block other read-modify-write
            # operations to avoid being clobbered by their write
            tree = {}
            for comment in comments:
                tree.setdefault(comment.parent_id, []).append(comment._id)

            cls._write_tree(link, tree, lock)


class CommentTree:
    def __init__(self, link, cids, tree, depth, parents, num_children):
        self.link = link
        self.cids = cids
        self.tree = tree
        self.depth = depth
        self.parents = parents
        self.num_children = num_children

    @classmethod
    def by_link(cls, link, timer=None):
        if timer is None:
            timer = SimpleSillyStub()

        pieces = CommentTreePermacache.get_tree_pieces(link, timer)
        cids, tree, depth, parents, num_children = pieces
        comment_tree = cls(link, cids, tree, depth, parents, num_children)
        return comment_tree

    @classmethod
    def on_new_link(cls, link):
        CommentTreePermacache.prepare_new_storage(link)

    @classmethod
    def add_comments(cls, link, comments):
        CommentTreePermacache.add_comments(link, comments)

    @classmethod
    def rebuild(cls, link):
        # retrieve all the comments for the link
        q = Comment._query(
            Comment.c.link_id == link._id,
            Comment.c._deleted == (True, False),
            Comment.c._spam == (True, False),
            optimize_rules=True,
        )
        comments = list(q)

        # remove any comments with missing parents
        comment_ids = {comment._id for comment in comments}
        comments = [
            comment for comment in comments
            if not comment.parent_id or comment.parent_id in comment_ids 
        ]

        CommentTreePermacache.rebuild(link, comments)

        link.num_comments = sum(1 for c in comments if not c._deleted)
        link._commit()
<EOF>
<BOF>
import datetime

from pycassa.batch import Mutator
from pycassa.system_manager import ASCII_TYPE
from pylons import app_globals as g
import pytz

from r2.lib.contrib.ipaddress import ip_address
from r2.lib.db import tdb_cassandra


__all__ = ["IPsByAccount", "AccountsByIP"]


CONNECTION_POOL = g.cassandra_pools['main']


CF_TTL = datetime.timedelta(days=100).total_seconds()


class IPsByAccount(tdb_cassandra.View):

    _use_db = True
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "default_validation_class": ASCII_TYPE,
    }
    _compare_with = tdb_cassandra.DateType()
    _ttl = CF_TTL

    @classmethod
    def set(cls, account_id, ip, date=None):
        if date is None:
            date = datetime.datetime.now(g.tz)
        cls._set_values(str(account_id), {date: ip})

    @classmethod
    def get(cls,
            account_id,
            column_start=None,
            column_finish=None,
            column_count=100,
            column_reversed=True):
        """Get the last accessed times of an account by IP address.

        Returns a list of dicts of the last accessed times of an account by
        IP address, most recent first.

        Example:

            >>> IPsByAccount.get(52)
            [
                {datetime.datetime(2016, 1, 24, 6, 23, 0, 326000, tzinfo=<UTC>): '127.0.0.3'},
                {datetime.datetime(2016, 1, 24, 6, 22, 58, 983000, tzinfo=<UTC>): '127.0.0.2'},
            ]

        Pagination is done based on the date of the entry.  For instance, to
        continue getting results from the previous set:

            >>> IPsByAccount.get(52, column_start=datetime.datetime(
                    2016, 1, 24, 6, 22, 58, 983000))
            [
                {datetime.datetime(2016, 1, 24, 6, 21, 50, 121000, tzinfo=<UTC>): '127.0.0.1'},
            ]
        """
        column_start = column_start or ""
        column_finish = column_finish or ""
        results = []
        query = tdb_cassandra.ColumnQuery(
            cls, (str(account_id),),
            column_start=column_start,
            column_finish=column_finish,
            column_count=column_count,
            column_reversed=column_reversed)
        for date_ip in query:
            for dt, ip in date_ip.iteritems():
                results.append({dt.replace(tzinfo=pytz.utc): ip})
        return results


class AccountsByIP(tdb_cassandra.View):

    _use_db = True
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "default_validation_class": ASCII_TYPE,
    }
    _compare_with = tdb_cassandra.DateType()
    _ttl = CF_TTL

    @classmethod
    def set(cls, ip, account_id, date=None):
        if date is None:
            date = datetime.datetime.now(g.tz)
        cls._set_values(ip, {date: str(account_id)})

    @classmethod
    def get(cls,
            ip,
            column_start=None,
            column_finish=None,
            column_count=100,
            column_reversed=True):
        """Get the times an IP address has accessed various account IDs.

        Returns a list of dicts of the times an IP address has accessed
        various account IDs, most recent first:

        Example:

            >>> AccountsByIP.get('127.0.0.1')
            [
                {datetime.datetime(2016, 1, 22, 23, 28, 21, 286000, tzinfo=<UTC>): 52},
                {datetime.datetime(2016, 1, 22, 23, 28, 24, 301000, tzinfo=<UTC>): 53},
            ]

        Pagination is also supported.  See the documentation for
        ``IPsByAccount.get``.
        """
        column_start = column_start or ""
        column_finish = column_finish or ""
        results = []
        query = tdb_cassandra.ColumnQuery(
            cls, (ip,),
            column_start=column_start,
            column_finish=column_finish,
            column_count=column_count,
            column_reversed=column_reversed)
        for date_account in query:
            for dt, account in date_account.iteritems():
                results.append({dt.replace(tzinfo=pytz.utc): int(account)})
        return results


def set_account_ip(account_id, ip, date=None):
    """Set an IP address as having accessed an account.

    Updates all underlying datastores.
    """
    # don't store private IPs, send event + string so we can investigate this
    if ip_address(ip).is_private:
        g.stats.simple_event('ip.private_ip_storage_prevented')
        g.stats.count_string('private_ip_storage_prevented', ip)
        return

    if date is None:
        date = datetime.datetime.now(g.tz)
    m = Mutator(CONNECTION_POOL)
    m.insert(IPsByAccount._cf, str(account_id), {date: ip}, ttl=CF_TTL)
    m.insert(AccountsByIP._cf, ip, {date: str(account_id)}, ttl=CF_TTL)
    m.send()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from ConfigParser import SafeConfigParser
from datetime import datetime, timedelta
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import NotFound
from r2.lib.merge import *
from r2.models.last_modified import LastModified
from pycassa.system_manager import TIME_UUID_TYPE
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.controllers.util import abort
from r2.lib.db.tdb_cassandra import NotFound
from r2.models.printable import Printable
from r2.models.account import Account
from collections import OrderedDict
from StringIO import StringIO

import pycassa.types

# Used for the key/id for pages,
PAGE_ID_SEP = '\t'

# Number of days to keep recent revisions for
WIKI_RECENT_DAYS = g.wiki_keep_recent_days

# Max length of a single page in bytes
MAX_PAGE_LENGTH_BYTES = g.wiki_max_page_length_bytes

# Page names which should never be
impossible_namespaces = ('edit/', 'revisions/', 'settings/', 'discussions/', 
                         'revisions/', 'pages/', 'create/')

# Namespaces in which access is denied to do anything but view
restricted_namespaces = ('reddit/', 'config/', 'special/')

# Pages which may only be edited by mods, must be within restricted namespaces
special_pages = {
    'config/automoderator',
    'config/description',
    'config/sidebar',
    'config/stylesheet',
    'config/submit_text',
}

special_page_view_permlevels = {
    "config/automoderator": 2,
}

# Pages that get created automatically from the subreddit settings page
automatically_created_pages = {
    'config/description',
    'config/sidebar',
    'config/stylesheet',
    'config/submit_text',
}

# Pages which have a special length restrictions (In bytes)
special_length_restrictions_bytes = {
    'config/stylesheet': 128*1024,
    'config/submit_text': 1024,
    'config/sidebar': 5120,
    'config/description': 500,
    'usernotes': 1024*1024,
}

modactions = {
    "config/automoderator": "Updated AutoModerator configuration",
    "config/description": "Updated subreddit description",
    "config/sidebar": "Updated subreddit sidebar",
    "config/submit_text": "Updated submission text",
}

# Page "index" in the subreddit "reddit.com" and a seperator of "\t" becomes:
#   "reddit.com\tindex"
def wiki_id(sr, page):
    return ('%s%s%s' % (sr, PAGE_ID_SEP, page)).lower()

class ContentLengthError(Exception):
    def __init__(self, max_length):
        Exception.__init__(self)
        self.max_length = max_length

class WikiPageExists(Exception):
    pass

class WikiBadRevision(Exception):
    pass

class WikiPageEditors(tdb_cassandra.View):
    _use_db = True
    _value_type = 'str'
    _connection_pool = 'main'

class WikiRevision(tdb_cassandra.UuidThing, Printable):
    """ Contains content (markdown), author of the edit, page the edit belongs to, and datetime of the edit """
    
    _use_db = True
    _connection_pool = 'main'
    
    _str_props = ('pageid', 'content', 'author', 'reason')
    _bool_props = ('hidden', 'admin_deleted')
    _defaults = {'admin_deleted': False}

    cache_ignore = set(list(_str_props)).union(Printable.cache_ignore).union(['wikipage'])
    
    def get_author(self):
        author = self._get('author')
        return Account._byID36(author, data=True) if author else None
    
    @classmethod
    def get_authors(cls, revisions):
        authors = [r._get('author') for r in revisions]
        authors = filter(None, authors)
        return Account._byID36(authors, data=True)
    
    @classmethod
    def get_printable_authors(cls, revisions):
        from r2.lib.pages import WrappedUser
        authors = cls.get_authors(revisions)
        return dict([(id36, WrappedUser(v))
                     for id36, v in authors.iteritems() if v])
    
    @classmethod
    def add_props(cls, user, wrapped):
        authors = cls.get_printable_authors(wrapped)
        pages = {r.page: None for r in wrapped}
        pages = WikiPage.get_multiple((c.site, page) for page in pages)
        for item in wrapped:
            item._hidden = item.is_hidden
            item._spam = False
            item.wikipage = pages[item.pageid]
            author = item._get('author')
            item.printable_author = authors.get(author, '[unknown]')
            item.reported = False
    
    @classmethod
    def get(cls, revid, pageid):
        wr = cls._byID(revid)
        if wr.pageid != pageid:
            raise WikiBadRevision('Revision is not for the expected page')
        return wr
    
    def toggle_hide(self):
        self.hidden = not self.is_hidden
        self._commit()
        return self.hidden

    @classmethod
    def create(cls, pageid, content, author=None, reason=None):
        kw = dict(pageid=pageid, content=content)
        if author:
            kw['author'] = author
        if reason:
            kw['reason'] = reason
        wr = cls(**kw)
        wr._commit()
        WikiRevisionHistoryByPage.add_object(wr)
        WikiRevisionsRecentBySR.add_object(wr)
        return wr

    def _on_commit(self):
        WikiRevisionHistoryByPage.add_object(self)
        WikiRevisionsRecentBySR.add_object(self)

    @classmethod
    def get_recent(cls, sr, count=100):
        return WikiRevisionsRecentBySR.query([sr._id36], count=count)
    
    @property
    def is_hidden(self):
        return bool(getattr(self, 'hidden', False))
    
    @property
    def info(self, sep=PAGE_ID_SEP):
        info = self.pageid.split(sep, 1)
        try:
            return {'sr': info[0], 'page': info[1]}
        except IndexError:
            g.log.error('Broken wiki page ID "%s" did PAGE_ID_SEP change?', self.pageid)
            return {'sr': 'broken', 'page': 'broken'}
    
    @property
    def page(self):
        return self.info['page']
    
    @property
    def sr(self):
        return self.info['sr']


class WikiPage(tdb_cassandra.Thing):
    """ Contains permissions, current content (markdown), subreddit, and current revision (ID)
        Key is subreddit-pagename """
    
    _use_db = True
    _connection_pool = 'main'
    
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    
    _date_props = ('last_edit_date')
    _str_props = ('revision', 'name', 'last_edit_by', 'content', 'sr')
    _int_props = ('permlevel')
    _bool_props = ('listed')
    _defaults = {'listed': True}

    def get_author(self):
        if self._get('last_edit_by'):
            return Account._byID36(self.last_edit_by, data=True)
        return None
    
    @classmethod
    def id_for(cls, sr, name):
        id = getattr(sr, '_id36', None)
        if not id:
            raise tdb_cassandra.NotFound
        return wiki_id(id, name)
    
    @classmethod
    def get_multiple(cls, pages):
        """Get multiple wiki pages.
        
        Arguments:
        pages -- list of tuples in the form of [(sr, names),..]
        """
        return cls._byID([cls.id_for(sr, name) for sr, name in pages])
    
    @classmethod
    def get(cls, sr, name):
        return cls._byID(cls.id_for(sr, name))

    @classmethod
    def create(cls, sr, name):
        if not name or not sr:
            raise ValueError

        name = name.lower()
        _id = wiki_id(sr._id36, name)
        lock_key = "wiki_create_%s:%s" % (sr._id36, name)
        with g.make_lock("wiki", lock_key):
            try:
                cls._byID(_id)
            except tdb_cassandra.NotFound:
                pass
            else:
                raise WikiPageExists

            page = cls(_id=_id, sr=sr._id36, name=name, permlevel=0, content='')
            page._commit()
            return page

    @property
    def restricted(self):
        return WikiPage.is_restricted(self.name)

    @classmethod
    def is_impossible(cls, page):
        return ("%s/" % page) in impossible_namespaces or page.startswith(impossible_namespaces)
    
    @classmethod
    def is_restricted(cls, page):
        return ("%s/" % page) in restricted_namespaces or page.startswith(restricted_namespaces)
    
    @classmethod
    def is_special(cls, page):
        return page in special_pages

    @classmethod
    def get_special_view_permlevel(cls, page):
        return special_page_view_permlevels.get(page, 0)

    @classmethod
    def is_automatically_created(cls, page):
        return page in automatically_created_pages
    
    @property
    def special(self):
        return WikiPage.is_special(self.name)
    
    def add_to_listing(self):
        WikiPagesBySR.add_object(self)
    
    def _on_create(self):
        self.add_to_listing()
    
    def _on_commit(self):
         self.add_to_listing()
    
    def remove_editor(self, user):
        WikiPageEditors._remove(self._id, [user])
    
    def add_editor(self, user):
        WikiPageEditors._set_values(self._id, {user: ''})
    
    @classmethod
    def get_pages(cls, sr, after=None, filter_check=None):
        NUM_AT_A_TIME = num = 1000
        pages = []
        while num >= NUM_AT_A_TIME:
            wikipages = WikiPagesBySR.query([sr._id36],
                                            after=after,
                                            count=NUM_AT_A_TIME)
            wikipages = list(wikipages)
            num = len(wikipages)
            pages += wikipages
            after = wikipages[-1] if num else None
        return filter(filter_check, pages)
    
    @classmethod
    def get_listing(cls, sr, filter_check=None):
        """
            Create a tree of pages from their path.
        """
        page_tree = OrderedDict()
        pages = cls.get_pages(sr, filter_check=filter_check)
        pages = sorted(pages, key=lambda page: page.name)
        for page in pages:
            p = page.name.split('/')
            cur_node = page_tree
            # Loop through all elements of the path except the page name portion
            for name in p[:-1]:
                next_node = cur_node.get(name)
                # If the element did not already exist in the tree, create it
                if not next_node:
                    new_node = OrderedDict()
                    cur_node[name] = [None, new_node]
                else:
                    # Otherwise, continue through
                    new_node = next_node[1]
                cur_node = new_node
            # Get the actual page name portion of the path
            pagename = p[-1]
            node = cur_node.get(pagename)
            # The node may already exist as a path name in the tree
            if node:
                node[0] = page
            else:
                cur_node[pagename] = [page, OrderedDict()]

        return page_tree, pages
    
    def get_editor_accounts(self):
        editors = self.get_editors()
        accounts = [Account._byID36(editor, data=True)
                    for editor in self.get_editors()]
        accounts = [account for account in accounts
                    if not account._deleted]
        return accounts
    
    def get_editors(self, properties=None):
        try:
            return WikiPageEditors._byID(self._id, properties=properties)._values().keys() or []
        except tdb_cassandra.NotFoundException:
            return []
    
    def has_editor(self, editor):
        return bool(self.get_editors(properties=[editor]))
    
    def revise(self, content, previous = None, author=None, force=False, reason=None):
        if content is None:
            content = ""
        if self.content == content:
            return
        force = True if previous is None else force
        max_length = special_length_restrictions_bytes.get(self.name, MAX_PAGE_LENGTH_BYTES)
        if len(content) > max_length:
            raise ContentLengthError(max_length)
        
        revision = getattr(self, 'revision', None)
        
        if not force and (revision and previous != revision):
            if previous:
                origcontent = WikiRevision.get(previous, pageid=self._id).content
            else:
                origcontent = ''
            try:
                content = threewaymerge(origcontent, content, self.content)
            except ConflictException as e:
                e.new_id = revision
                raise e
        
        wr = WikiRevision.create(self._id, content, author, reason)
        self.content = content
        self.last_edit_by = author
        self.last_edit_date = wr.date
        self.revision = str(wr._id)
        self._commit()

        LastModified.touch(self._fullname, "Edit")

        return wr
    
    def change_permlevel(self, permlevel, force=False):
        NUM_PERMLEVELS = 3
        if permlevel == self.permlevel:
            return
        if not force and int(permlevel) not in range(NUM_PERMLEVELS):
            raise ValueError('Permlevel not valid')
        self.permlevel = permlevel
        self._commit()

    def get_revisions(self, after=None, count=100):
        return WikiRevisionHistoryByPage.query(
            rowkeys=[self._id], after=after, count=count)


class WikiRevisionHistoryByPage(tdb_cassandra.View):
    """Create a time ordered index of revisions for a wiki page"""
    _use_db = True
    _connection_pool = 'main'
    _view_of = WikiRevision
    _compare_with = TIME_UUID_TYPE

    @classmethod
    def _rowkey(cls, wikirevision):
        return wikirevision.pageid

    @classmethod
    def _obj_to_column(cls, wikirevision):
        return {wikirevision._id: ''}


class WikiPagesBySR(tdb_cassandra.DenormalizedView):
    """ Associate revisions with subreddits, store only recent """
    _use_db = True
    _connection_pool = 'main'
    _view_of = WikiPage
    
    @classmethod
    def _rowkey(cls, wp):
        return wp.sr

class WikiRevisionsRecentBySR(tdb_cassandra.DenormalizedView):
    """ Associate revisions with subreddits, store only recent """
    _use_db = True
    _connection_pool = 'main'
    _view_of = WikiRevision
    _compare_with = TIME_UUID_TYPE
    _ttl = timedelta(days=WIKI_RECENT_DAYS)
    
    @classmethod
    def _rowkey(cls, wr):
        return wr.sr


class ImagesByWikiPage(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _extra_schema_creation_args = {
        "key_validation_class": pycassa.types.AsciiType(),
        "column_name_class": pycassa.types.UTF8Type(),
        "default_validation_class": pycassa.types.UTF8Type(),
    }

    @classmethod
    def add_image(cls, sr, page_name, image_name, url):
        rowkey = WikiPage.id_for(sr, page_name)
        cls._set_values(rowkey, {image_name: url})

    @classmethod
    def get_images(cls, sr, page_name):
        try:
            rowkey = WikiPage.id_for(sr, page_name)
            return cls._byID(rowkey)._values()
        except tdb_cassandra.NotFound:
            return {}

    @classmethod
    def get_image_count(cls, sr, page_name):
        rowkey = WikiPage.id_for(sr, page_name)
        return cls._cf.get_count(rowkey,
            read_consistency_level=cls._read_consistency_level)

    @classmethod
    def delete_image(cls, sr, page_name, image_name):
        rowkey = WikiPage.id_for(sr, page_name)
        cls._remove(rowkey, [image_name])


class WikiPageIniItem(object):
    _bool_values = ("is_enabled", "is_new")

    @classmethod
    def get_all(cls, return_dict=False):
        items = OrderedDict()
        try:
            wp = WikiPage.get(*cls._get_wiki_config())
        except NotFound:
            return items if return_dict else items.values()
        wp_content = StringIO(wp.content)
        cfg = SafeConfigParser(allow_no_value=True)
        cfg.readfp(wp_content)

        for section in cfg.sections():
            def_values = {'id': section}
            for name, value in cfg.items(section):
                # coerce boolean variables
                if name in cls._bool_values:
                    def_values[name] = cfg.getboolean(section, name)
                else:
                    def_values[name] = value

            try:
                item = cls(**def_values)
            except TypeError:
                # a required variable wasn't set for this item, skip
                continue

            if item.is_enabled:
                items[section] = item
        
        return items if return_dict else items.values()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import timedelta

from pycassa.cassandra.ttypes import NotFoundException
from pycassa.system_manager import ASCII_TYPE, UTF8_TYPE

from r2.lib.db import tdb_cassandra


class PerformedRulesByThing(tdb_cassandra.View):
    """Used to track which rules have previously matched a specific item."""
    _use_db = True
    _connection_pool = "main"
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _ttl = timedelta(days=3)
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "column_name_class": ASCII_TYPE,
        "default_validation_class": UTF8_TYPE,
    }

    @classmethod
    def _rowkey(cls, thing):
        return thing._fullname

    @classmethod
    def mark_performed(cls, thing, rule):
        rowkey = cls._rowkey(thing)
        cls._set_values(rowkey, {rule.unique_id: ''})

    @classmethod
    def get_already_performed(cls, thing):
        rowkey = cls._rowkey(thing)
        try:
            columns = cls._cf.get(rowkey)
        except NotFoundException:
            return []

        return columns.keys()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime

from pylons import app_globals as g
from pycassa.system_manager import ASCII_TYPE, DATE_TYPE

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup


class LastModified(tdb_cassandra.View):
    _use_db = True
    _value_type = "date"
    _connection_pool = "main"
    _read_consistency_level = tdb_cassandra.CL.ONE
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=DATE_TYPE)

    @classmethod
    def touch(cls, fullname, names):
        names = tup(names)
        now = datetime.datetime.now(g.tz)
        values = dict.fromkeys(names, now)
        cls._set_values(fullname, values)
        return now

    @classmethod
    def get(cls, fullname, name, touch_if_not_set=False):
        try:
            obj = cls._byID(fullname)
        except tdb_cassandra.NotFound:
            if touch_if_not_set:
                time = cls.touch(fullname, name)
                return time
            else:
                return None

        return getattr(obj, name, None)

    @classmethod
    def get_multi(cls, fullnames, name):
        res = cls._byID(fullnames, return_dict=True)

        return dict((k, getattr(v, name, None))
                    for k, v in res.iteritems())
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from account import *
from link import *
from vote import *
from report import *
from subreddit import DefaultSR, AllSR, Frontpage, Subreddit
from pylons import i18n, request
from pylons import app_globals as g
from pylons.i18n import _

from r2.config import feature
from r2.lib.wrapped import Wrapped, CachedVariable
from r2.lib import utils
from r2.lib.db import operators
from r2.models import rules

from collections import namedtuple
from copy import deepcopy, copy
import time


class Listing(object):
    # class used in Javascript to manage these objects
    _js_cls = "Listing"

    def __init__(self, builder, nextprev = True, next_link = True,
                 prev_link = True, params = None, **kw):
        self.builder = builder
        self.nextprev = nextprev
        self.next_link = True
        self.prev_link = True
        self.next = None
        self.prev = None
        self.params = params or request.GET.copy()
        self._max_num = 1

    @property
    def max_score(self):
        scores = [x.score for x in self.things if hasattr(x, 'score')]
        return max(scores) if scores else 0

    @property
    def max_num(self):
        return self._max_num

    def get_items(self, *a, **kw):
        """Wrapper around builder's get_items that caches the rendering."""
        from r2.lib.template_helpers import replace_render
        builder_items = self.builder.get_items(*a, **kw)
        for item in self.builder.item_iter(builder_items):
            # rewrite the render method
            if c.render_style != "api" and not hasattr(item, "render_replaced"):
                item.render = replace_render(self, item, item.render)
                item.render_replaced = True
        return builder_items

    def listing(self, next_suggestions=None):
        self.things, prev, next, bcount, acount = self.get_items()

        self.next_suggestions = next_suggestions
        self._max_num = max(acount, bcount)
        self.after = None
        self.before = None

        if self.nextprev and self.prev_link and prev and bcount > 1:
            p = self.params.copy()
            p.update({'after':None, 'before':prev._fullname, 'count':bcount})
            self.before = prev._fullname
            self.prev = (request.path + utils.query_string(p))
            p_first = self.params.copy()
            p_first.update({'after':None, 'before':None, 'count':None})
            self.first = (request.path + utils.query_string(p_first))
        if self.nextprev and self.next_link and next:
            p = self.params.copy()
            p.update({'after':next._fullname, 'before':None, 'count':acount})
            self.after = next._fullname
            self.next = (request.path + utils.query_string(p))

        for count, thing in enumerate(self.things):
            thing.rowstyle_cls = getattr(thing, 'rowstyle_cls', "")
            thing.rowstyle_cls += ' ' + ('even' if (count % 2) else 'odd')
            thing.rowstyle = CachedVariable("rowstyle")

        #TODO: need name for template -- must be better way
        return Wrapped(self)

    def __iter__(self):
        return iter(self.things)

class TableListing(Listing): pass

class ModActionListing(TableListing): pass

class WikiRevisionListing(TableListing): pass

class UserListing(TableListing):
    type = ''
    _class = ''
    title = ''
    form_title = ''
    destination = 'friend'
    has_add_form = True
    headers = None
    permissions_form = None

    def __init__(self,
                 builder,
                 show_jump_to=False,
                 show_not_found=False,
                 jump_to_value=None,
                 addable=True, **kw):
        self.addable = addable
        self.show_not_found = show_not_found
        self.show_jump_to = show_jump_to
        self.jump_to_value = jump_to_value
        TableListing.__init__(self, builder, **kw)

    @property
    def container_name(self):
        return c.site._fullname

class FriendListing(UserListing):
    type = 'friend'

    @property
    def _class(self):
        return '' if not c.user.gold else 'gold-accent rounded'

    @property
    def headers(self):
        if c.user.gold:
            return (_('user'), '', _('note'), _('friendship'), '')

    @property
    def form_title(self):
        return _('add a friend')

    @property
    def container_name(self):
        return c.user._fullname


class EnemyListing(UserListing):
    type = 'enemy'
    has_add_form = False

    @property
    def title(self):
        return _('blocked users')

    @property
    def container_name(self):
        return c.user._fullname

class BannedListing(UserListing):
    type = 'banned'

    def __init__(self, builder, show_jump_to=False, show_not_found=False,
            jump_to_value=None, addable=True, **kw):
        self.rules = rules.SubredditRules.get_rules(c.site)
        self.system_rules = rules.SITEWIDE_RULES
        UserListing.__init__(self, builder, show_jump_to, show_not_found,
            jump_to_value, addable, **kw)

    @classmethod
    def populate_from_tempbans(cls, item, tempbans=None):
        if not tempbans:
            return
        time = tempbans.get(item.user.name)
        if time:
            delay = time - datetime.now(g.tz)
            item.tempban = max(delay.days, 0)

    @property
    def form_title(self):
        return _("ban users")

    @property
    def title(self):
        return _("users banned from"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

    def get_items(self, *a, **kw):
        items = UserListing.get_items(self, *a, **kw)
        wrapped_items = items[0]
        names = [item.user.name for item in wrapped_items]
        tempbans = c.site.get_tempbans(self.type, names)
        for wrapped in wrapped_items:
            BannedListing.populate_from_tempbans(wrapped, tempbans)
        return items


class MutedListing(UserListing):
    type = 'muted'

    @classmethod
    def populate_from_muted(cls, item, muted=None):
        if not muted:
            return
        time = muted.get(item.user.name)
        if time:
            delay = time - datetime.now(g.tz)
            item.muted = max(int(delay.total_seconds()), 0)

    @property
    def form_title(self):
        return _("mute users")

    @property
    def title(self):
        return _("users muted from"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

    def get_items(self, *a, **kw):
        items = UserListing.get_items(self, *a, **kw)
        wrapped_items = items[0]
        names = [item.user.name for item in wrapped_items]
        muted = c.site.get_muted_items(names)
        for wrapped in wrapped_items:
            MutedListing.populate_from_muted(wrapped, muted)
        return items


class WikiBannedListing(BannedListing):
    type = 'wikibanned'

    @property
    def form_title(self):
        return _("ban wiki contibutors")

    @property
    def title(self):
        return _("wiki contibutors banned from"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

class ContributorListing(UserListing):
    type = 'contributor'

    @property
    def title(self):
        return _("approved submitters for"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

    @property
    def form_title(self):
        return _("add approved submitter")

class WikiMayContributeListing(ContributorListing):
    type = 'wikicontributor'

    @property
    def title(self):
        return _("approved wiki contributors"
                 " for /r/%(subreddit)s") % dict(subreddit=c.site.name)

    @property
    def form_title(self):
        return _("add approved wiki contributor")

class InvitedModListing(UserListing):
    type = 'moderator_invite'
    form_title = _('invite moderator')
    remove_self_title = _('you are a moderator of this subreddit. %(action)s')

    @property
    def permissions_form(self):
        from r2.lib.permissions import ModeratorPermissionSet
        from r2.lib.pages import ModeratorPermissions
        return ModeratorPermissions(
            user=None,
            permissions_type=self.type,
            permissions=ModeratorPermissionSet(all=True),
            editable=True,
            embedded=True,
        )

    @property
    def title(self):
        return _("invited moderators for"
                 " %(subreddit)s") % dict(subreddit=c.site.name)

class ModListing(InvitedModListing):
    type = 'moderator'
    form_title = _('force add moderator')

    @property
    def has_add_form(self):
        return c.user_is_admin

    @property
    def can_remove_self(self):
        return c.user_is_loggedin and c.site.is_moderator(c.user)

    @property
    def has_invite(self):
        return c.user_is_loggedin and c.site.is_moderator_invite(c.user)

    @property
    def title(self):
        return _("moderators of /r/%(subreddit)s") % dict(subreddit=c.site.name)

class LinkListing(Listing):
    def __init__(self, *a, **kw):
        Listing.__init__(self, *a, **kw)

        self.show_nums = kw.get('show_nums', False)

    def listing(self, *args, **kwargs):
        wrapped = Listing.listing(self, *args, **kwargs)
        self.rank_width = len(str(self.max_num)) * 1.1
        self.midcol_width = max(len(str(self.max_score)), 2) + 1.1
        return wrapped


class SearchListing(LinkListing):
    def __init__(self, *a, **kw):
        LinkListing.__init__(self, *a, **kw)
        self.heading = kw.get('heading', None)
        self.nav_menus = kw.get('nav_menus', None)

    def listing(self, legacy_render_class=False, *args, **kwargs):
        wrapped = LinkListing.listing(self, *args, **kwargs)
        if hasattr(self.builder, 'subreddit_facets'):
            self.subreddit_facets = self.builder.subreddit_facets
        if hasattr(self.builder, 'start_time'):
            self.timing = time.time() - self.builder.start_time

        if legacy_render_class:
            wrapped.render_class = LinkListing

        return wrapped


class ReadNextListing(Listing):
    pass


class NestedListing(Listing):
    def __init__(self, *a, **kw):
        Listing.__init__(self, *a, **kw)

        self.num = kw.get('num', g.num_comments)
        self.parent_name = kw.get('parent_name')

    def listing(self):
        ##TODO use the local builder with the render cache. this may
        ##require separating the builder's get_items and tree-building
        ##functionality
        wrapped_items = self.get_items()

        self.things = wrapped_items

        #make into a tree thing
        return Wrapped(self)

SpotlightTuple = namedtuple('SpotlightTuple',
                            ['link', 'is_promo', 'campaign', 'weight'])

class SpotlightListing(Listing):
    # class used in Javascript to manage these objects
    _js_cls = "OrganicListing"

    def __init__(self, *a, **kw):
        self.nextprev   = False
        self.show_nums  = True
        self._parent_max_num   = kw.get('max_num', 0)
        self._parent_max_score = kw.get('max_score', 0)
        self.interestbar = kw.get('interestbar')
        self.interestbar_prob = kw.get('interestbar_prob', 0.)
        self.show_promo = kw.get('show_promo', False)
        keywords = kw.get('keywords', [])
        self.keywords = '+'.join([keyword if keyword else Frontpage.name
                                 for keyword in keywords])
        self.navigable = kw.get('navigable', True)
        self.things = kw.get('organic_links', [])
        self.show_placeholder = isinstance(c.site, (DefaultSR, AllSR))

    def get_items(self):
        from r2.lib.template_helpers import replace_render
        things = self.things
        for t in things:
            if not hasattr(t, "render_replaced"):
                t.render = replace_render(self, t, t.render)
                t.render_replaced = True
        return things, None, None, 0, 0

    def listing(self):
        res = Listing.listing(self)
        for t in res.things:
            t.num_text = ""
        return Wrapped(self)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import pycassa
import time

from collections import defaultdict
from datetime import datetime, timedelta
from itertools import chain
from pylons import app_globals as g

from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import max_column_count
from r2.lib.utils import utils, tup
from r2.models import Account, LabeledMulti, Subreddit
from r2.lib.pages import ExploreItem

VIEW = 'imp'
CLICK = 'clk'
DISMISS = 'dis'
FEEDBACK_ACTIONS = [VIEW, CLICK, DISMISS]

# how long to keep each type of feedback
FEEDBACK_TTL = {VIEW: timedelta(hours=6).total_seconds(),  # link lifetime
                CLICK: timedelta(minutes=30).total_seconds(),  # one session
                DISMISS: timedelta(days=60).total_seconds()}  # two months


class AccountSRPrefs(object):
    """Class for managing user recommendation preferences.

    Builds a user profile on-the-fly based on the user's subscriptions,
    multireddits, and recent interactions with the recommender UI.

    Likes are used to generate recommendations, dislikes to filter out
    unwanted results, and recent views to make sure the same subreddits aren't
    recommended too often.

    """

    def __init__(self):
        self.likes = set()
        self.dislikes = set()
        self.recent_views = set()

    @classmethod
    def for_user(cls, account):
        """Return a new AccountSRPrefs obj populated with user's data."""
        prefs = cls()
        multis = LabeledMulti.by_owner(account)
        multi_srs = set(chain.from_iterable(multi.srs for multi in multis))
        feedback = AccountSRFeedback.for_user(account)
        # subscriptions and srs in the user's multis become likes
        subscriptions = Subreddit.user_subreddits(account, limit=None)
        prefs.likes.update(utils.to36(sr_id) for sr_id in subscriptions)
        prefs.likes.update(sr._id36 for sr in multi_srs)
        # recent clicks on explore tab items are also treated as likes
        prefs.likes.update(feedback[CLICK])
        # dismissed recommendations become dislikes
        prefs.dislikes.update(feedback[DISMISS])
        # dislikes take precedence over likes
        prefs.likes = prefs.likes.difference(prefs.dislikes)
        # recently recommended items won't be shown again right away
        prefs.recent_views.update(feedback[VIEW])
        return prefs


class AccountSRFeedback(tdb_cassandra.DenormalizedRelation):
    """Column family for storing users' recommendation feedback."""

    _use_db = True
    _views = []
    _write_last_modified = False
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    @classmethod
    def for_user(cls, account):
        """Return dict mapping each feedback type to a set of sr id36s."""

        feedback = defaultdict(set)
        try:
            row = AccountSRFeedback._cf.get(account._id36,
                                            column_count=max_column_count)
        except pycassa.NotFoundException:
            return feedback
        for colkey, colval in row.iteritems():
            action, sr_id36 = colkey.split('.')
            feedback[action].add(sr_id36)
        return feedback

    @classmethod
    def record_feedback(cls, account, srs, action):
        if action not in FEEDBACK_ACTIONS:
            g.log.error('Unrecognized feedback: %s' % action)
            return
        srs = tup(srs)
        # update user feedback record, setting appropriate ttls
        fb_rowkey = account._id36
        fb_colkeys = ['%s.%s' % (action, sr._id36) for sr in srs]
        col_data = {col: '' for col in fb_colkeys}
        ttl = FEEDBACK_TTL.get(action, 0)
        if ttl > 0:
            AccountSRFeedback._cf.insert(fb_rowkey, col_data, ttl=ttl)
        else:
            AccountSRFeedback._cf.insert(fb_rowkey, col_data)

    @classmethod
    def record_views(cls, account, srs):
        cls.record_feedback(account, srs, VIEW)


class ExploreSettings(tdb_cassandra.Thing):
    """Column family for storing users' view prefs for the /explore page."""
    _use_db = True
    _bool_props = ('personalized', 'discovery', 'rising', 'nsfw')

    @classmethod
    def for_user(cls, account):
        """Return user's prefs or default prefs if user has none."""
        try:
            return cls._byID(account._id36)
        except tdb_cassandra.NotFound:
            return DefaultExploreSettings()

    @classmethod
    def record_settings(cls,
                        user,
                        personalized=False,
                        discovery=False,
                        rising=False,
                        nsfw=False):
        """Update or create settings for user."""
        try:
            settings = cls._byID(user._id36)
        except tdb_cassandra.NotFound:
            settings = ExploreSettings(
                _id=user._id36,
                personalized=personalized,
                discovery=discovery,
                rising=rising,
                nsfw=nsfw,
            )
        else:
            settings.personalized = personalized
            settings.discovery = discovery
            settings.rising = rising
            settings.nsfw = nsfw
        settings._commit()


class DefaultExploreSettings(object):
    """Default values to use when no settings have been saved for the user."""
    def __init__(self):
        self.personalized = True
        self.discovery = True
        self.rising = True
        self.nsfw = False
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.tdb_sql import make_metadata, index_str, create_table

import json
import pytz
import uuid

from pycassa import NotFoundException
from pycassa.system_manager import ASCII_TYPE, INT_TYPE, TIME_UUID_TYPE, UTF8_TYPE
from pycassa.util import convert_uuid_to_time
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, ungettext
from datetime import datetime
import sqlalchemy as sa
from sqlalchemy.exc import IntegrityError, OperationalError
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.sql.expression import select
from sqlalchemy.sql.functions import sum as sa_sum

from r2.lib.utils import GoldPrice, randstr, to_date
import re
from random import choice
from time import time

from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import NotFound, view_of
from r2.models import Account
from r2.models.subreddit import Frontpage
from r2.models.wiki import WikiPage, WikiPageIniItem
from r2.lib.memoize import memoize

import stripe

gold_bonus_cutoff = datetime(2010,7,27,0,0,0,0,g.tz)
gold_static_goal_cutoff = datetime(2013, 11, 7, tzinfo=g.display_tz)

NON_REVENUE_STATUSES = ("declined", "chargeback", "fudge", "invalid",
                        "refunded", "reversed")

ENGINE_NAME = 'authorize'

ENGINE = g.dbm.get_engine(ENGINE_NAME)
METADATA = make_metadata(ENGINE)
TIMEZONE = pytz.timezone("America/Los_Angeles")

Session = scoped_session(sessionmaker(bind=ENGINE))
Base = declarative_base(bind=ENGINE)

gold_table = sa.Table('reddit_gold', METADATA,
                      sa.Column('trans_id', sa.String, nullable = False,
                                primary_key = True),
                      # status can be: invalid, unclaimed, claimed
                      sa.Column('status', sa.String, nullable = False),
                      sa.Column('date', sa.DateTime(timezone=True),
                                nullable = False,
                                default = sa.func.now()),
                      sa.Column('payer_email', sa.String, nullable = False),
                      sa.Column('paying_id', sa.String, nullable = False),
                      sa.Column('pennies', sa.Integer, nullable = False),
                      sa.Column('secret', sa.String, nullable = True),
                      sa.Column('account_id', sa.String, nullable = True),
                      sa.Column('days', sa.Integer, nullable = True),
                      sa.Column('subscr_id', sa.String, nullable = True),
                      sa.Column('gilding_type', sa.String, nullable = True))

indices = [index_str(gold_table, 'status', 'status'),
           index_str(gold_table, 'date', 'date'),
           index_str(gold_table, 'account_id', 'account_id'),
           index_str(gold_table, 'secret', 'secret'),
           index_str(gold_table, 'payer_email', 'payer_email'),
           index_str(gold_table, 'subscr_id', 'subscr_id')]
create_table(gold_table, indices)


class GoldRevenueGoalByDate(object):
    __metaclass__ = tdb_cassandra.ThingMeta

    _use_db = True
    _cf_name = "GoldRevenueGoalByDate"
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.ALL
    _extra_schema_creation_args = {
        "column_name_class": UTF8_TYPE,
        "default_validation_class": INT_TYPE,
    }
    _compare_with = UTF8_TYPE
    _type_prefix = None

    ROWKEY = '1'

    @staticmethod
    def _colkey(date):
        return date.strftime("%Y-%m-%d")

    @classmethod
    def set(cls, date, goal):
        cls._cf.insert(cls.ROWKEY, {cls._colkey(date): int(goal)})

    @classmethod
    def get(cls, date):
        """Gets the goal for a date, or the nearest previous goal."""
        try:
            colkey = cls._colkey(date)
            col = cls._cf.get(
                cls.ROWKEY,
                column_reversed=True,
                column_start=colkey,
                column_count=1,
            )
            return col.values()[0]
        except NotFoundException:
            return None


class GildedCommentsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _last_modified_name = 'Gilding'
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def gild(cls, user, thing):
        cls.create(user, [thing])


class GildedLinksByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _last_modified_name = 'Gilding'
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def gild(cls, user, thing):
        cls.create(user, [thing])


@view_of(GildedCommentsByAccount)
@view_of(GildedLinksByAccount)
class GildingsByThing(tdb_cassandra.View):
    _use_db = True
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "column_name_class": UTF8_TYPE,
    }

    @classmethod
    def get_gilder_ids(cls, thing):
        columns = cls.get_time_sorted_columns(thing._fullname)
        return [int(account_id, 36) for account_id in columns.iterkeys()]

    @classmethod
    def create(cls, user, things):
        for thing in things:
            cls._set_values(thing._fullname, {user._id36: ""})

    @classmethod
    def delete(cls, user, things):
        # gildings cannot be undone
        raise NotImplementedError()


@view_of(GildedCommentsByAccount)
@view_of(GildedLinksByAccount)
class GildingsByDay(tdb_cassandra.View):
    _use_db = True
    _compare_with = TIME_UUID_TYPE
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "column_name_class": TIME_UUID_TYPE,
        "default_validation_class": UTF8_TYPE,
    }

    @staticmethod
    def _rowkey(date):
        return date.strftime("%Y-%m-%d")

    @classmethod
    def get_gildings(cls, date):
        key = cls._rowkey(date)
        columns = cls.get_time_sorted_columns(key)
        gildings = []
        for name, json_blob in columns.iteritems():
            timestamp = convert_uuid_to_time(name)
            date = datetime.utcfromtimestamp(timestamp).replace(tzinfo=g.tz)

            gilding = json.loads(json_blob)
            gilding["date"] = date
            gilding["user"] = int(gilding["user"], 36)
            gildings.append(gilding)
        return gildings

    @classmethod
    def create(cls, user, things):
        key = cls._rowkey(datetime.now(g.tz))

        columns = {}
        for thing in things:
            columns[uuid.uuid1()] = json.dumps({
                "user": user._id36,
                "thing": thing._fullname,
            })
        cls._set_values(key, columns)

    @classmethod
    def delete(cls, user, things):
        # gildings cannot be undone
        raise NotImplementedError()


def create_unclaimed_gold (trans_id, payer_email, paying_id,
                           pennies, days, secret, date,
                           subscr_id = None):

    try:
        gold_table.insert().execute(trans_id=str(trans_id),
                                    subscr_id=subscr_id,
                                    status="unclaimed",
                                    payer_email=payer_email,
                                    paying_id=paying_id,
                                    pennies=pennies,
                                    days=days,
                                    secret=str(secret),
                                    date=date
                                    )
    except IntegrityError:
        rp = gold_table.update(
            sa.and_(gold_table.c.status == 'uncharged',
                    gold_table.c.trans_id == str(trans_id)),
            values = {
                gold_table.c.status: "unclaimed",
                gold_table.c.payer_email: payer_email,
                gold_table.c.paying_id: paying_id,
                gold_table.c.pennies: pennies,
                gold_table.c.days: days,
                gold_table.c.secret:secret,
                gold_table.c.subscr_id : subscr_id
                },
            ).execute()


def create_claimed_gold (trans_id, payer_email, paying_id,
                         pennies, days, secret, account_id, date,
                         subscr_id = None, status="claimed"):
    gold_table.insert().execute(trans_id=trans_id,
                                subscr_id=subscr_id,
                                status=status,
                                payer_email=payer_email,
                                paying_id=paying_id,
                                pennies=pennies,
                                days=days,
                                secret=secret,
                                account_id=account_id,
                                date=date)


def create_gift_gold(giver_id, recipient_id, days, date,
            signed, note=None, gilding_type=None):
    trans_id = "X%d%s-%s" % (int(time()), randstr(2), 'S' if signed else 'A')
    gold_table.insert().execute(
        trans_id=trans_id,
        status="gift",
        paying_id=giver_id,
        payer_email='',
        pennies=0,
        days=days,
        account_id=recipient_id,
        date=date,
        secret=note,
        gilding_type=gilding_type,
    )


def create_gold_code(trans_id, payer_email, paying_id, pennies, days, date):
    if not trans_id:
        trans_id = "GC%d%s" % (int(time()), randstr(2))

    valid_chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    # keep picking new codes until we find an unused one
    while True:
        code = randstr(10, alphabet=valid_chars)

        s = sa.select([gold_table],
                      sa.and_(gold_table.c.secret == code.lower(),
                              gold_table.c.status == 'unclaimed'))
        res = s.execute().fetchall()
        if not res:
            gold_table.insert().execute(
                trans_id=trans_id,
                status='unclaimed',
                payer_email=payer_email,
                paying_id=paying_id,
                pennies=pennies,
                days=days,
                secret=code.lower(),
                date=date)
            return code


def account_by_payingid(paying_id):
    s = sa.select([sa.distinct(gold_table.c.account_id)],
                  gold_table.c.paying_id == paying_id)
    res = s.execute().fetchall()

    if len(res) != 1:
        return None

    return int(res[0][0])

# returns None if the ID was never valid
# returns "already claimed" if it's already been claimed
# Otherwise, it's valid and the function claims it, returning a tuple with:
#   * the number of days
#   * the subscr_id, if any
def claim_gold(secret, account_id):
    if not secret:
        return None

    # The donation email has the code at the end of the sentence,
    # so they might get sloppy and catch the period or some whitespace.
    secret = secret.strip(". ")
    secret = secret.replace("-", "").lower()

    rp = gold_table.update(sa.and_(gold_table.c.status == 'unclaimed',
                                   gold_table.c.secret == secret),
                           values = {
                                      gold_table.c.status: 'claimed',
                                      gold_table.c.account_id: account_id,
                                    },
                           ).execute()
    if rp.rowcount == 0:
        just_claimed = False
    elif rp.rowcount == 1:
        just_claimed = True
    else:
        raise ValueError("rowcount == %d?" % rp.rowcount)

    s = sa.select([gold_table.c.days, gold_table.c.subscr_id],
                  gold_table.c.secret == secret,
                  limit = 1)
    rows = s.execute().fetchall()

    if not rows:
        return None
    elif just_claimed:
        return (rows[0].days, rows[0].subscr_id)
    else:
        return "already claimed"

def check_by_email(email):
    s = sa.select([gold_table.c.status,
                           gold_table.c.secret,
                           gold_table.c.days,
                           gold_table.c.account_id],
                          gold_table.c.payer_email == email)
    return s.execute().fetchall()


def has_prev_subscr_payments(subscr_id):
    s = sa.select([gold_table], gold_table.c.subscr_id == subscr_id)
    return bool(s.execute().fetchall())


def retrieve_gold_transaction(transaction_id):
    s = sa.select([gold_table], gold_table.c.trans_id == transaction_id)
    res = s.execute().fetchall()
    if res:
        return res[0]   # single row per transaction_id


def update_gold_transaction(transaction_id, status):
    rp = gold_table.update(gold_table.c.trans_id == str(transaction_id),
                           values={gold_table.c.status: status}).execute()


def transactions_by_user(user):
    s = sa.select([gold_table], gold_table.c.account_id == str(user._id))
    res = s.execute().fetchall()
    return res


def gold_payments_by_user(user):
    transactions = transactions_by_user(user)

    # filter out received gifts
    transactions = [trans for trans in transactions
                          if not trans.trans_id.startswith(('X', 'M'))]

    return transactions


def gold_received_by_user(user):
    transactions = transactions_by_user(user)
    transactions = [trans for trans in transactions
                          if trans.trans_id.startswith('X')]
    return transactions


def days_to_pennies(days):
    if days < 366:
        months = days / 31
        return months * g.gold_month_price.pennies
    else:
        years = days / 366
        return years * g.gold_year_price.pennies


def append_random_bottlecap_phrase(message):
    """Appends a random "bottlecap" phrase from the wiki page.

    The wiki page should be an unordered list with each item a separate
    bottlecap.
    """

    bottlecap = None
    try:
        wp = WikiPage.get(Frontpage, g.wiki_page_gold_bottlecaps)

        split_list = re.split('^[*-] ', wp.content, flags=re.MULTILINE)
        choices = [item.strip() for item in split_list if item.strip()]
        if len(choices):
            bottlecap = choice(choices)
    except NotFound:
        pass

    if bottlecap:
        message += '\n\n> ' + bottlecap
    return message


def gold_revenue_multi(dates):
    date_expr = sa.func.date_trunc('day',
                    sa.func.timezone(TIMEZONE.zone, gold_table.c.date))
    query = (select([date_expr, sa_sum(gold_table.c.pennies)])
                .where(~ gold_table.c.status.in_(NON_REVENUE_STATUSES))
                .where(date_expr.in_(dates))
                .group_by(date_expr)
            )
    return {truncated_time.date(): pennies
                for truncated_time, pennies in ENGINE.execute(query)}


@memoize("gold-revenue-volatile", time=600, stale=True)
def gold_revenue_volatile(date):
    return gold_revenue_multi([date]).get(date, 0)


@memoize("gold-revenue-steady", stale=True)
def gold_revenue_steady(date):
    return gold_revenue_multi([date]).get(date, 0)


@memoize("gold-goal", stale=True)
def gold_goal_on(date):
    """Returns the gold revenue goal (in pennies) for a given date."""
    goal = GoldRevenueGoalByDate.get(date)

    if not goal:
        return 0

    return float(goal)


def account_from_stripe_customer_id(stripe_customer_id):
    q = Account._query(Account.c.gold_subscr_id == stripe_customer_id,
                       Account.c._spam == (True, False), data=True)
    return next(iter(q), None)


@memoize("subscription-details", time=60)
def _get_subscription_details(stripe_customer_id):
    stripe.api_key = g.secrets['stripe_secret_key']
    customer = stripe.Customer.retrieve(stripe_customer_id)

    if getattr(customer, 'deleted', False):
        return {}

    subscription = customer.subscription
    card = customer.active_card
    end = datetime.fromtimestamp(subscription.current_period_end).date()
    last4 = card.last4
    pennies = subscription.plan.amount

    return {
        'next_charge_date': end,
        'credit_card_last4': last4,
        'pennies': pennies,
    }


def get_subscription_details(user):
    if not getattr(user, 'gold_subscr_id', None):
        return

    return _get_subscription_details(user.gold_subscr_id)


def paypal_subscription_url():
    return "https://www.paypal.com/cgi-bin/webscr?cmd=_subscr-find&alias=%s" % g.goldpayment_email


def get_discounted_price(gold_price):
    discount = float(getattr(g, 'BTC_DISCOUNT', '0'))
    price = (gold_price.pennies * (1 - discount)) / 100.
    return GoldPrice("%.2f" % price)


def make_gold_message(thing, user_gilded):
    from r2.models import Comment

    if thing.gildings == 0 or thing._spam or thing._deleted:
        return None

    author = Account._byID(thing.author_id, data=True)
    if not author._deleted:
        author_name = author.name
    else:
        author_name = _("[deleted]")

    if c.user_is_loggedin and thing.author_id == c.user._id:
        if isinstance(thing, Comment):
            gilded_message = ungettext(
                "a redditor gifted you a month of reddit gold for this "
                "comment.",
                "redditors have gifted you %(months)d months of reddit gold "
                "for this comment.",
                thing.gildings
            )
        else:
            gilded_message = ungettext(
                "a redditor gifted you a month of reddit gold for this "
                "submission.",
                "redditors have gifted you %(months)d months of reddit gold "
                "for this submission.",
                thing.gildings
            )
    elif user_gilded:
        if isinstance(thing, Comment):
            gilded_message = ungettext(
                "you have gifted reddit gold to %(recipient)s for this "
                "comment.",
                "you and other redditors have gifted %(months)d months of "
                "reddit gold to %(recipient)s for this comment.",
                thing.gildings
            )
        else:
            gilded_message = ungettext(
                "you have gifted reddit gold to %(recipient)s for this "
                "submission.",
                "you and other redditors have gifted %(months)d months of "
                "reddit gold to %(recipient)s for this submission.",
                thing.gildings
            )
    else:
        if isinstance(thing, Comment):
            gilded_message = ungettext(
                "a redditor has gifted reddit gold to %(recipient)s for this "
                "comment.",
                "redditors have gifted %(months)d months of reddit gold to "
                "%(recipient)s for this comment.",
                thing.gildings
            )
        else:
            gilded_message = ungettext(
                "a redditor has gifted reddit gold to %(recipient)s for this "
                "submission.",
                "redditors have gifted %(months)d months of reddit gold to "
                "%(recipient)s for this submission.",
                thing.gildings
            )

    return gilded_message % dict(
        recipient=author_name,
        months=thing.gildings,
    )


def creddits_lock(user):
    return g.make_lock("gold_creddits", "creddits_%s" % user._id)


PENNIES_PER_SERVER_SECOND = {
    datetime.strptime(datestr, "%Y/%m/%d").date(): v
    for datestr, v in g.live_config['pennies_per_server_second'].iteritems()
}


def calculate_server_seconds(pennies, date):
    cutoff_dates = sorted(PENNIES_PER_SERVER_SECOND.keys())
    date = to_date(date)
    key = max(filter(lambda cutoff_date: date >= cutoff_date, cutoff_dates))
    rate = PENNIES_PER_SERVER_SECOND[key]

    # for simplicity all payment processor fees are $0.30 + 2.9%
    net_pennies = pennies * (1 - 0.029) - 30

    return net_pennies / rate


def get_current_value_of_month():
    price = g.gold_month_price.pennies
    now = datetime.now(g.display_tz)
    seconds = calculate_server_seconds(price, now)
    return seconds


class StylesheetsEverywhere(WikiPageIniItem):
    @classmethod
    def _get_wiki_config(cls):
        return Frontpage, g.wiki_page_stylesheets_everywhere

    def __init__(self, id, tagline, thumbnail_url, preview_url, is_enabled=True):
        self.id = id
        self.tagline = tagline
        self.thumbnail_url = thumbnail_url
        self.preview_url = preview_url
        self.is_enabled = is_enabled
        self.checked = False
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2013-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import json

from datetime import (
    datetime,
    timedelta,
)
from pycassa.system_manager import ASCII_TYPE, UTF8_TYPE
from r2.lib.db import tdb_cassandra


Media = collections.namedtuple('_Media', ("media_object",
                                          "secure_media_object",
                                          "preview_object",
                                          "thumbnail_url",
                                          "thumbnail_size"))

ERROR_MEDIA = Media(None, None, None, None, None)


class MediaByURL(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _ttl = timedelta(minutes=720)

    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _int_props = {"thumbnail_width", "thumbnail_height"}
    _date_props = {"last_modified"}
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "column_name_class": UTF8_TYPE,
    }

    _defaults = {
        "state": "enqueued",
        "error": "",
        "thumbnail_url": "",
        "thumbnail_width": 0,
        "thumbnail_height": 0,
        "media_object": "",
        "secure_media_object": "",
        "preview_object": "",
        "last_modified": datetime.utcfromtimestamp(0),
    }

    @classmethod
    def _rowkey(cls, url, **kwargs):
        return (
            url +
            # pipe is not allowed in URLs, so use it as a delimiter
            "|" +

            # append the extra cache keys in kwargs as a canonical JSON string
            json.dumps(
                kwargs,
                ensure_ascii=True,
                encoding="ascii",
                indent=None,
                separators=(",", ":"),
                sort_keys=True,
            )
        )

    @classmethod
    def add_placeholder(cls, url, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        cls._set_values(rowkey, {
            "state": "enqueued",
            "error": "",
            "last_modified": datetime.utcnow(),
        })

    @classmethod
    def add(cls, url, media, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        columns = cls._defaults.copy()

        columns.update({
            "state": "processed",
            "error": "",
            "last_modified": datetime.utcnow(),
        })

        if media.thumbnail_url and media.thumbnail_size:
            columns.update({
                "thumbnail_url": media.thumbnail_url,
                "thumbnail_width": media.thumbnail_size[0],
                "thumbnail_height": media.thumbnail_size[1],
            })

        if media.media_object:
            columns.update({
                "media_object": json.dumps(media.media_object),
            })

        if media.secure_media_object:
            columns.update({
                "secure_media_object": (json.
                                        dumps(media.secure_media_object)),
            })

        if media.preview_object:
            columns.update({
                "preview_object": json.dumps(media.preview_object),
            })

        cls._set_values(rowkey, columns)

    @classmethod
    def add_error(cls, url, error, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        columns = {
            "error": error,
            "state": "processed",
            "last_modified": datetime.utcnow(),
        }
        cls._set_values(rowkey, columns)

    @classmethod
    def get(cls, url, max_cache_age=None, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        try:
            temp = cls._byID(rowkey)

            # Return None if this cache entry is too old
            if (max_cache_age is not None and
                datetime.datetime.utcnow() - temp.last_modified >
                max_cache_age):
                return None
            else:
                return temp
        except tdb_cassandra.NotFound:
            return None

    @property
    def media(self):
        if self.state == "processed":
            if not self.error:
                media_object = secure_media_object = preview_object = None
                thumbnail_url = thumbnail_size = None

                if (self.thumbnail_width and self.thumbnail_height and
                    self.thumbnail_url):
                    thumbnail_url = self.thumbnail_url
                    thumbnail_size = (self.thumbnail_width,
                                      self.thumbnail_height)

                if self.media_object:
                    media_object = json.loads(self.media_object)

                if self.secure_media_object:
                    secure_media_object = json.loads(self.secure_media_object)

                if self.preview_object:
                    preview_object = json.loads(self.preview_object)

                return Media(media_object, secure_media_object, preview_object,
                             thumbnail_url, thumbnail_size)
            else:
                return ERROR_MEDIA
        else:
            return None
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
These models represent the traffic statistics stored for subreddits and
promoted links.  They are written to by Pig-based MapReduce jobs and read from
various places in the UI.

All traffic statistics are divided up into three "intervals" of granularity,
hourly, daily, and monthly.  Individual hits are tracked as pageviews /
impressions, and can be safely summed.  Unique hits are tracked as well, but
cannot be summed safely because there's no way to know overlap at this point in
the data pipeline.

"""

import datetime

from pylons import app_globals as g
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.orm.exc import NoResultFound
from sqlalchemy.schema import Column
from sqlalchemy.sql.expression import desc, distinct
from sqlalchemy.sql.functions import sum as sa_sum
from sqlalchemy.types import (
    BigInteger,
    DateTime,
    Integer,
    String,
    TypeDecorator,
)

from r2.lib.memoize import memoize
from r2.lib.utils import timedelta_by_name, tup
from r2.models.link import Link


engine = g.dbm.get_engine("traffic")
Session = scoped_session(sessionmaker(bind=engine, autocommit=True))
Base = declarative_base(bind=engine)


def memoize_traffic(**memoize_kwargs):
    """Wrap the memoize decorator and automatically determine memoize key.

    The memoize key is based off the full name (including class name) of the
    method being memoized.

    """
    def memoize_traffic_decorator(fn):
        def memoize_traffic_wrapper(cls, *args, **kwargs):
            method = ".".join((cls.__name__, fn.__name__))
            actual_memoize_decorator = memoize(method, **memoize_kwargs)
            actual_memoize_wrapper = actual_memoize_decorator(fn)
            return actual_memoize_wrapper(cls, *args, **kwargs)
        return memoize_traffic_wrapper
    return memoize_traffic_decorator


class PeekableIterator(object):
    """Iterator that supports peeking at the next item in the iterable."""

    def __init__(self, iterable):
        self.iterator = iter(iterable)
        self.item = None

    def peek(self):
        """Get the next item in the iterable without advancing our position."""
        if not self.item:
            try:
                self.item = self.iterator.next()
            except StopIteration:
                return None
        return self.item

    def next(self):
        """Get the next item in the iterable and advance our position."""
        item = self.peek()
        self.item = None
        return item


def zip_timeseries(*series, **kwargs):
    """Zip timeseries data while gracefully handling gaps in the data.

    Timeseries data is expected to be a sequence of two-tuples (date, values).
    Values is expected itself to be a tuple. The width of the values tuples
    should be the same across all elements in a timeseries sequence. The result
    will be a single sequence in timeseries format.

    Gaps in sequences are filled with an appropriate number of zeros based on
    the size of the first value-tuple of that sequence.

    """

    next_slice = (max if kwargs.get("order", "descending") == "descending"
                  else min)
    iterators = [PeekableIterator(s) for s in series]
    widths = []
    for w in iterators:
        r = w.peek()
        if r:
            date, values = r
            widths.append(len(values))
        else:
            widths.append(0)

    while True:
        items = [it.peek() for it in iterators]
        if not any(items):
            return

        current_slice = next_slice(item[0] for item in items if item)

        data = []
        for i, item in enumerate(items):
            # each item is (date, data)
            if item and item[0] == current_slice:
                data.extend(item[1])
                iterators[i].next()
            else:
                data.extend([0] * widths[i])

        yield current_slice, tuple(data)


def decrement_month(date):
    """Given a truncated datetime, return a new one one month in the past."""

    if date.day != 1:
        raise ValueError("Input must be truncated to the 1st of the month.")

    date -= datetime.timedelta(days=1)
    return date.replace(day=1)


def fill_gaps_generator(time_points, query, *columns):
    """Generate a timeseries sequence with a value for every sample expected.

    Iterate over specified time points and pull the columns listed out of
    query. If the query doesn't have data for a time point, fill the gap with
    an appropriate number of zeroes.

    """

    iterator = PeekableIterator(query)
    for t in time_points:
        row = iterator.peek()

        if row and row.date == t:
            yield t, tuple(getattr(row, c) for c in columns)
            iterator.next()
        else:
            yield t, tuple(0 for c in columns)


def fill_gaps(*args, **kwargs):
    """Listify the generator returned by fill_gaps_generator for `memoize`."""
    generator = fill_gaps_generator(*args, **kwargs)
    return list(generator)


time_range_by_interval = dict(hour=datetime.timedelta(days=4),
                              day=datetime.timedelta(weeks=8),
                              month=datetime.timedelta(weeks=52))


def get_time_points(interval, start_time=None, stop_time=None):
    """Return time points for given interval type.

    Time points are in reverse chronological order to match the sort of
    queries this will be used with. If start_time and stop_time are not
    specified they will be picked based on the interval.

    """

    def truncate_datetime(dt):
        dt = dt.replace(minute=0, second=0, microsecond=0)
        if interval in ("day", "month"):
            dt = dt.replace(hour=0)
        if interval == "month":
            dt = dt.replace(day=1)
        return dt

    if start_time and stop_time:
        start_time, stop_time = sorted([start_time, stop_time])
        # truncate stop_time to an actual traffic time point
        stop_time = truncate_datetime(stop_time)
    else:
        # the stop time is the most recent slice-time; get this by truncating
        # the appropriate amount from the current time
        stop_time = datetime.datetime.utcnow()
        stop_time = truncate_datetime(stop_time)

        # then the start time is easy to work out
        range = time_range_by_interval[interval]
        start_time = stop_time - range

    step = timedelta_by_name(interval)
    current_time = stop_time
    time_points = []

    while current_time >= start_time:
        time_points.append(current_time)
        if interval != 'month':
            current_time -= step
        else:
            current_time = decrement_month(current_time)
    return time_points


def points_for_interval(interval):
    """Calculate the number of data points to render for a given interval."""
    range = time_range_by_interval[interval]
    interval = timedelta_by_name(interval)
    return range.total_seconds() / interval.total_seconds()


def make_history_query(cls, interval):
    """Build a generic query showing the history of a given aggregate."""

    time_points = get_time_points(interval)
    q = (Session.query(cls)
                .filter(cls.date.in_(time_points)))

    # subscription stats doesn't have an interval (it's only daily)
    if hasattr(cls, "interval"):
        q = q.filter(cls.interval == interval)

    q = q.order_by(desc(cls.date))

    return time_points, q


def top_last_month(cls, key, ids=None, num=None):
    """Aggregate a listing of the top items (by pageviews) last month.

    We use the last month because it's guaranteed to be fully computed and
    therefore will be more meaningful.

    """

    cur_month = datetime.date.today().replace(day=1)
    last_month = decrement_month(cur_month)

    q = (Session.query(cls)
                .filter(cls.date == last_month)
                .filter(cls.interval == "month")
                .order_by(desc(cls.date), desc(cls.pageview_count)))

    if ids:
        q = q.filter(getattr(cls, key).in_(ids))
    else:
        num = num or 55
        q = q.limit(num)

    return [(getattr(r, key), (r.unique_count, r.pageview_count))
            for r in q.all()]


class CoerceToLong(TypeDecorator):
    # source:
    # https://groups.google.com/forum/?fromgroups=#!topic/sqlalchemy/3fipkThttQA

    impl = BigInteger

    def process_result_value(self, value, dialect):
        if value is not None:
            value = long(value)
        return value


def sum(column):
    """Wrapper around sqlalchemy.sql.functions.sum to handle BigInteger.

    sqlalchemy returns a Decimal for sum over BigInteger values. Detect the
    column type and coerce to long if it's a BigInteger.

    """

    if isinstance(column.property.columns[0].type, BigInteger):
        return sa_sum(column, type_=CoerceToLong)
    else:
        return sa_sum(column)


def totals(cls, interval):
    """Aggregate sitewide totals for self-serve promotion traffic.

    We only aggregate codenames that start with a link type prefix which
    effectively filters out all DART / 300x100 etc. traffic numbers.

    """

    time_points = get_time_points(interval)

    q = (Session.query(cls.date, sum(cls.pageview_count).label("sum"))
                .filter(cls.interval == interval)
                .filter(cls.date.in_(time_points))
                .filter(cls.codename.startswith(Link._type_prefix))
                .group_by(cls.date)
                .order_by(desc(cls.date)))
    return fill_gaps(time_points, q, "sum")


def total_by_codename(cls, codenames):
    """Return total lifetime pageviews (or clicks) for given codename(s)."""
    codenames = tup(codenames)
    # uses hour totals to get the most up-to-date count
    q = (Session.query(cls.codename, sum(cls.pageview_count))
                       .filter(cls.interval == "hour")
                       .filter(cls.codename.in_(codenames))
                       .group_by(cls.codename))
    return list(q)


def promotion_history(cls, count_column, codename, start, stop):
    """Get hourly traffic for a self-serve promotion.

    Traffic stats are summed over all targets for classes that include a target.

    count_column should be cls.pageview_count or cls.unique_count.

    NOTE: when retrieving uniques the counts for ALL targets are summed, which
    isn't strictly correct but is the best we can do for now.

    """

    time_points = get_time_points('hour', start, stop)
    q = (Session.query(cls.date, sum(count_column))
                .filter(cls.interval == "hour")
                .filter(cls.codename == codename)
                .filter(cls.date.in_(time_points))
                .group_by(cls.date)
                .order_by(cls.date))
    return [(r[0], (r[1],)) for r in q.all()]


def campaign_history(cls, codenames, start, stop):
    """Get hourly traffic for given campaigns."""
    time_points = get_time_points('hour', start, stop)
    q = (Session.query(cls)
                .filter(cls.interval == "hour")
                .filter(cls.codename.in_(codenames))
                .filter(cls.date.in_(time_points))
                .order_by(cls.date))
    return [(r.date, r.codename, r.subreddit, (r.unique_count,
                                               r.pageview_count))
            for r in q.all()]


@memoize("traffic_last_modified", time=60 * 10)
def get_traffic_last_modified():
    """Guess how far behind the traffic processing system is."""
    try:
        return (Session.query(SitewidePageviews.date)
                   .order_by(desc(SitewidePageviews.date))
                   .limit(1)
                   .one()).date
    except NoResultFound:
        return datetime.datetime.min


@memoize("missing_traffic", time=60 * 10)
def get_missing_traffic(start, end):
    """Check for missing hourly traffic between start and end."""

    # NOTE: start, end must be UTC time without tzinfo
    time_points = get_time_points('hour', start, end)
    q = (Session.query(SitewidePageviews.date)
                .filter(SitewidePageviews.interval == "hour")
                .filter(SitewidePageviews.date.in_(time_points)))
    found = [t for (t,) in q]
    return [t for t in time_points if t not in found]


class SitewidePageviews(Base):
    """Pageviews across all areas of the site."""

    __tablename__ = "traffic_aggregate"

    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", BigInteger())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval):
        time_points, q = make_history_query(cls, interval)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")


class PageviewsBySubreddit(Base):
    """Pageviews within a subreddit (i.e. /r/something/...)."""

    __tablename__ = "traffic_subreddits"

    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, subreddit):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.subreddit == subreddit)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600 * 6)
    def top_last_month(cls, num=None):
        return top_last_month(cls, "subreddit", num=num)

    @classmethod
    @memoize_traffic(time=3600 * 6)
    def last_month(cls, srs):
        ids = [sr.name for sr in srs]
        return top_last_month(cls, "subreddit", ids=ids)


class PageviewsBySubredditAndPath(Base):
    """Pageviews within a subreddit with action included.

    `srpath` is the subreddit name, a dash, then the controller method called
    to render the page the user viewed. e.g. reddit.com-GET_listing. This is
    useful to determine how many pageviews in a subreddit are on listing pages,
    comment pages, or elsewhere.

    """

    __tablename__ = "traffic_srpaths"

    srpath = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())


class PageviewsByLanguage(Base):
    """Sitewide pageviews correlated by user's interface language."""

    __tablename__ = "traffic_lang"

    lang = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", BigInteger())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, lang):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.lang == lang)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600 * 6)
    def top_last_month(cls):
        return top_last_month(cls, "lang")


class ClickthroughsByCodename(Base):
    """Clickthrough counts for ads."""

    __tablename__ = "traffic_click"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, codename):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.codename == codename)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, cls.unique_count, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def historical_totals(cls, interval):
        return totals(cls, interval)

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codenames):
        return total_by_codename(cls, codenames)


class TargetedClickthroughsByCodename(Base):
    """Clickthroughs for ads, correlated by ad campaign."""

    __tablename__ = "traffic_clicktarget"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, cls.unique_count, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codenames):
        return total_by_codename(cls, codenames)

    @classmethod
    def campaign_history(cls, codenames, start, stop):
        return campaign_history(cls, codenames, start, stop)


class AdImpressionsByCodename(Base):
    """Impressions for ads."""

    __tablename__ = "traffic_thing"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", BigInteger())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, codename):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.codename == codename)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, cls.pageview_count, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def historical_totals(cls, interval):
        return totals(cls, interval)

    @classmethod
    @memoize_traffic(time=3600)
    def top_last_month(cls):
        return top_last_month(cls, "codename")

    @classmethod
    @memoize_traffic(time=3600)
    def recent_codenames(cls, fullname):
        """Get a list of recent codenames used for 300x100 ads.

        The 300x100 ads get a codename that looks like "fullname_campaign".
        This function gets a list of recent campaigns.

        """
        time_points = get_time_points('day')
        query = (Session.query(distinct(cls.codename).label("codename"))
                        .filter(cls.date.in_(time_points))
                        .filter(cls.codename.startswith(fullname)))
        return [row.codename for row in query]

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codename):
        return total_by_codename(cls, codename)


class TargetedImpressionsByCodename(Base):
    """Impressions for ads, correlated by ad campaign."""

    __tablename__ = "traffic_thingtarget"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, cls.pageview_count, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codenames):
        return total_by_codename(cls, codenames)

    @classmethod
    def campaign_history(cls, codenames, start, stop):
        return campaign_history(cls, codenames, start, stop)


class SubscriptionsBySubreddit(Base):
    """Subscription statistics for subreddits.

    This table is different from the rest of the traffic ones.  It only
    contains data at a daily interval (hence no `interval` column) and is
    updated separately in the subscribers cron job (see
    reddit-job-subscribers).

    """

    __tablename__ = "traffic_subscriptions"

    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    subscriber_count = Column("unique", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, subreddit):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.subreddit == subreddit)
        return fill_gaps(time_points, q, "subscriber_count")


# create the tables if they don't exist
if g.db_create_tables:
    Base.metadata.create_all()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import tmpl_context as c

from r2.lib.strings import Score
from r2.lib import hooks


class Printable(object):
    show_spam = False
    show_reports = False
    is_special = False
    can_ban = False
    deleted = False
    rowstyle_cls = ''
    collapsed = False
    author = None
    margin = 0
    is_focal = False
    childlisting = None
    cache_ignore = set(['c', 'author', 'score_fmt', 'child',
                        # displayed score is cachable, so remove score
                        # related fields.
                        'voting_score', 'display_score',
                        'render_score', 'score', '_score', 
                        'upvotes', '_ups',
                        'downvotes', '_downs',
                        'subreddit_slow', '_deleted', '_spam',
                        'cachable', 'make_permalink', 'permalink',
                        'timesince',
                        'num',  # listings only, replaced by CachedVariable
                        'rowstyle_cls',  # listings only, replaced by CachedVariable
                        'upvote_ratio',
                        'should_incr_counts',
                        'keep_item',
                        ])

    @classmethod
    def update_nofollow(cls, user, wrapped):
        pass

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.wrapped import CachedVariable
        for item in wrapped:
            # insert replacement variable for timesince to allow for
            # caching of thing templates
            item.display = CachedVariable("display")
            item.timesince = CachedVariable("timesince")
            item.childlisting = CachedVariable("childlisting")

            score_fmt = getattr(item, "score_fmt", Score.number_only)
            item.display_score = map(score_fmt, item.voting_score)

            if item.cachable:
                item.render_score  = item.display_score
                item.display_score = map(CachedVariable,
                                         ["scoredislikes", "scoreunvoted",
                                          "scorelikes"])

        hooks.get_hook("add_props").call(items=wrapped)

    @property
    def permalink(self, *a, **kw):
        raise NotImplementedError

    def keep_item(self, wrapped):
        return True

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = [wrapped._fullname, wrapped._spam]

        # Printables can contain embedded WrappedUsers, which need to consider
        # the site and user's flair settings. Add something to the key
        # indicating there might be flair--we haven't built the WrappedUser yet
        # so we can't check to see if there's actually flair.
        if c.site.flair_enabled and c.user.pref_show_flair:
            s.append('user_flair_enabled')

        if style == 'htmllite':
            s.extend([c.bgcolor, c.bordercolor, 
                      request.GET.has_key('style'),
                      request.GET.get("expanded"),
                      getattr(wrapped, 'embed_voting_style', None)])
        return s
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import functools
from os import urandom
from base64 import urlsafe_b64encode

from pycassa.system_manager import ASCII_TYPE, DATE_TYPE, UTF8_TYPE

from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.lib import hooks
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import NotFound
from r2.models.account import Account

def generate_token(size):
    return urlsafe_b64encode(urandom(size)).rstrip("=")


class Token(tdb_cassandra.Thing):
    """A unique randomly-generated token used for authentication."""

    _extra_schema_creation_args = dict(
        key_validation_class=ASCII_TYPE,
        default_validation_class=UTF8_TYPE,
        column_validation_classes=dict(
            date=DATE_TYPE,
            used=ASCII_TYPE
        )
    )

    @classmethod
    def _new(cls, **kwargs):
        if "_id" not in kwargs:
            kwargs["_id"] = cls._generate_unique_token()

        token = cls(**kwargs)
        token._commit()
        return token

    @classmethod
    def _generate_unique_token(cls):
        for i in range(3):
            token = generate_token(cls.token_size)
            try:
                cls._byID(token)
            except tdb_cassandra.NotFound:
                return token
            else:
                continue
        raise ValueError

    @classmethod
    def get_token(cls, _id):
        if _id is None:
            return None
        try:
            return cls._byID(_id)
        except tdb_cassandra.NotFound:
            return None


class ConsumableToken(Token):
    _defaults = dict(used=False)
    _bool_props = ("used",)
    _warn_on_partial_ttl = False

    @classmethod
    def get_token(cls, _id):
        token = super(ConsumableToken, cls).get_token(_id)
        if token and not token.used:
            return token
        else:
            return None

    def consume(self):
        self.used = True
        self._commit()


class OAuth2Scope:
    scope_info = {
        None: {
            "id": None,
            "name": _("Any Scope"),
            "description": _("Endpoint is accessible with any combination "
                "of other OAuth 2 scopes."),
        },
        "account": {
            "id": "account",
            "name": _("Update account information"),
            "description": _("Update preferences and related account "
                "information. Will not have access to your email or "
                "password."),
        },
        "creddits": {
            "id": "creddits",
            "name": _("Spend reddit gold creddits"),
            "description": _("Spend my reddit gold creddits on giving "
                "gold to other users."),
        },
        "edit": {
            "id": "edit",
            "name": _("Edit Posts"),
            "description": _("Edit and delete my comments and submissions."),
        },
        "flair": {
            "id": "flair",
            "name": _("Manage My Flair"),
            "description": _("Select my subreddit flair. "
                             "Change link flair on my submissions."),
        },
        "history": {
            "id": "history",
            "name": _("History"),
            "description": _(
                "Access my voting history and comments or submissions I've"
                " saved or hidden."),
        },
        "identity": {
            "id": "identity",
            "name": _("My Identity"),
            "description": _("Access my reddit username and signup date."),
        },
        "modcontributors": {
            "id": "modcontributors",
            "name": _("Approve submitters and ban users"),
            "description": _(
                "Add/remove users to approved submitter lists and "
                "ban/unban or mute/unmute users from subreddits I moderate."
            ),
        },
        "modflair": {
            "id": "modflair",
            "name": _("Moderate Flair"),
            "description": _(
                "Manage and assign flair in subreddits I moderate."),
        },
        "modposts": {
            "id": "modposts",
            "name": _("Moderate Posts"),
            "description": _(
                "Approve, remove, mark nsfw, and distinguish content"
                " in subreddits I moderate."),
        },
        "modconfig": {
            "id": "modconfig",
            "name": _("Moderate Subreddit Configuration"),
            "description": _(
                "Manage the configuration, sidebar, and CSS"
                " of subreddits I moderate."),
        },
        "modlog": {
            "id": "modlog",
            "name": _("Moderation Log"),
            "description": _(
                "Access the moderation log in subreddits I moderate."),
        },
        "modothers": {
            "id": "modothers",
            "name": _("Invite or remove other moderators"),
            "description": _(
                "Invite or remove other moderators from subreddits I moderate."
            ),
        },
        "modself": {
            "id": "modself",
            "name": _("Make changes to your subreddit moderator "
                      "and contributor status"),
            "description": _(
                "Accept invitations to moderate a subreddit. Remove myself as "
                "a moderator or contributor of subreddits I moderate or "
                "contribute to."
            ),
        },
        "modtraffic": {
            "id": "modtraffic",
            "name": _("Subreddit Traffic"),
            "description": _("Access traffic stats in subreddits I moderate."),
        },
        "modwiki": {
            "id": "modwiki",
            "name": _("Moderate Wiki"),
            "description": _(
                "Change editors and visibility of wiki pages"
                " in subreddits I moderate."),
        },
        "mysubreddits": {
            "id": "mysubreddits",
            "name": _("My Subreddits"),
            "description": _(
                "Access the list of subreddits I moderate, contribute to,"
                " and subscribe to."),
        },
        "privatemessages": {
            "id": "privatemessages",
            "name": _("Private Messages"),
            "description": _(
                "Access my inbox and send private messages to other users."),
        },
        "read": {
            "id": "read",
            "name": _("Read Content"),
            "description": _("Access posts and comments through my account."),
        },
        "report": {
            "id": "report",
            "name": _("Report content"),
            "description": _("Report content for rules violations. "
                             "Hide & show individual submissions."),
        },
        "save": {
            "id": "save",
            "name": _("Save Content"),
            "description": _("Save and unsave comments and submissions."),
        },
        "submit": {
            "id": "submit",
            "name": _("Submit Content"),
            "description": _("Submit links and comments from my account."),
        },
        "subscribe": {
            "id": "subscribe",
            "name": _("Edit My Subscriptions"),
            "description": _('Manage my subreddit subscriptions. Manage '
                '"friends" - users whose content I follow.'),
        },
        "vote": {
            "id": "vote",
            "name": _("Vote"),
            "description":
                _("Submit and change my votes on comments and submissions."),
        },
        "wikiedit": {
            "id": "wiki",
            "name": _("Wiki Editing"),
            "description": _("Edit wiki pages on my behalf"),
        },
        "wikiread": {
            "id": "wikiread",
            "name": _("Read Wiki Pages"),
            "description": _("Read wiki pages through my account"),
        },
    }

    # Special scope, granted implicitly to clients with app_type == "script"
    FULL_ACCESS = "*"

    class InsufficientScopeError(StandardError):
        pass

    def __init__(self, scope_str=None, subreddits=None, scopes=None):
        if scope_str:
            self._parse_scope_str(scope_str)
        elif subreddits is not None or scopes is not None:
            self.subreddit_only = bool(subreddits)
            self.subreddits = subreddits
            self.scopes = scopes
        else:
            self.subreddit_only = False
            self.subreddits = set()
            self.scopes = set()

    def _parse_scope_str(self, scope_str):
        srs, sep, scopes = scope_str.rpartition(':')
        if sep:
            self.subreddit_only = True
            self.subreddits = set(srs.split('+'))
        else:
            self.subreddit_only = False
            self.subreddits = set()
        self.scopes = set(scopes.replace(',', ' ').split(' '))

    def __str__(self):
        if self.subreddit_only:
            sr_part = '+'.join(sorted(self.subreddits)) + ':'
        else:
            sr_part = ''
        return sr_part + ' '.join(sorted(self.scopes))

    def has_access(self, subreddit, required_scopes):
        if self.FULL_ACCESS in self.scopes:
            return True
        if self.subreddit_only and subreddit not in self.subreddits:
            return False
        return (self.scopes >= required_scopes)

    def has_any_scope(self, required_scopes):
        if self.FULL_ACCESS in self.scopes:
            return True

        return bool(self.scopes & required_scopes)

    def is_valid(self):
        return all(scope in self.scope_info for scope in self.scopes)

    def details(self):
        if self.FULL_ACCESS in self.scopes:
            scopes = self.scope_info.keys()
        else:
            scopes = self.scopes
        return [(scope, self.scope_info[scope]) for scope in scopes]

    @classmethod
    def merge_scopes(cls, scopes):
        """Return a by-subreddit dict representing merged OAuth2Scopes.

        Takes an iterable of OAuth2Scopes. For each of those,
        if it defines scopes on multiple subreddits, it is split
        into one OAuth2Scope per subreddit. If multiple passed in
        OAuth2Scopes reference the same scopes, they'll be combined.

        """
        merged = {}
        for scope in scopes:
            srs = scope.subreddits if scope.subreddit_only else (None,)
            for sr in srs:
                if sr in merged:
                    merged[sr].scopes.update(scope.scopes)
                else:
                    new_scope = cls()
                    new_scope.subreddits = {sr}
                    new_scope.scopes = scope.scopes
                    if sr is not None:
                        new_scope.subreddit_only = True
                    merged[sr] = new_scope
        return merged


def extra_oauth2_scope(*scopes):
    """Wrap a function so that it only returns data if user has all `scopes`

    When not in an OAuth2 context, function returns normally.
    In an OAuth2 context, the function will not be run unless the user
    has granted all scopes required of this function. Instead, the function
    will raise an OAuth2Scope.InsufficientScopeError.

    """
    def extra_oauth2_wrapper(fn):
        @functools.wraps(fn)
        def wrapper_fn(*a, **kw):
            if not c.oauth_user:
                # Not in an OAuth2 context, run function normally
                return fn(*a, **kw)
            elif c.oauth_scope.has_access(c.site.name, set(scopes)):
                # In an OAuth2 context, and have scope for this function
                return fn(*a, **kw)
            else:
                # In an OAuth2 context, but don't have scope
                raise OAuth2Scope.InsufficientScopeError(scopes)
        return wrapper_fn
    return extra_oauth2_wrapper


class OAuth2Client(Token):
    """A client registered for OAuth2 access"""
    max_developers = 20
    token_size = 10
    client_secret_size = 20
    _bool_props = (
        "deleted",
    )
    _float_props = (
        "max_reqs_sec",
    )
    _defaults = dict(name="",
                     deleted=False,
                     description="",
                     about_url="",
                     icon_url="",
                     secret="",
                     redirect_uri="",
                     app_type="web",
                     max_reqs_sec=g.RL_OAUTH_AVG_REQ_PER_SEC,
                    )
    _use_db = True
    _connection_pool = "main"

    _developer_colname_prefix = 'has_developer_'

    APP_TYPES = ("web", "installed", "script")
    PUBLIC_APP_TYPES = ("installed",)

    @classmethod
    def _new(cls, **kwargs):
        if "secret" not in kwargs:
            kwargs["secret"] = generate_token(cls.client_secret_size)
        return super(OAuth2Client, cls)._new(**kwargs)

    @property
    def _developer_ids(self):
        for k, v in self._t.iteritems():
            if k.startswith(self._developer_colname_prefix) and v:
                try:
                    yield int(k[len(self._developer_colname_prefix):], 36)
                except ValueError:
                    pass

    @property
    def _max_reqs(self):
        return self.max_reqs_sec * g.RL_OAUTH_RESET_SECONDS

    @property
    def _developers(self):
        """Returns a list of users who are developers of this client."""

        devs = Account._byID(list(self._developer_ids), return_dict=False)
        return [dev for dev in devs if not dev._deleted]

    def _developer_colname(self, account):
        """Developer access is granted by way of adding a column with the
        account's ID36 to the client object.  This function returns the
        column name for a given Account.
        """

        return ''.join((self._developer_colname_prefix, account._id36))

    def has_developer(self, account):
        """Returns a boolean indicating whether or not the supplied Account is a developer of this application."""

        if account._deleted:
            return False
        else:
            return getattr(self, self._developer_colname(account), False)

    def add_developer(self, account, force=False):
        """Grants developer access to the supplied Account."""

        dev_ids = set(self._developer_ids)
        if account._id not in dev_ids:
            if not force and len(dev_ids) >= self.max_developers:
                raise OverflowError('max developers reached')
            setattr(self, self._developer_colname(account), True)
            self._commit()

        # Also update index
        OAuth2ClientsByDeveloper._set_values(account._id36, {self._id: ''})

    def remove_developer(self, account):
        """Revokes the supplied Account's developer access."""

        if hasattr(self, self._developer_colname(account)):
            del self[self._developer_colname(account)]
            if not len(self._developers):
                # No developers remain, delete the client
                self.deleted = True
            self._commit()

        # Also update index
        try:
            cba = OAuth2ClientsByDeveloper._byID(account._id36)
            del cba[self._id]
        except (tdb_cassandra.NotFound, KeyError):
            pass
        else:
            cba._commit()

    @classmethod
    def _by_developer(cls, account):
        """Returns a (possibly empty) list of clients for which Account is a developer."""

        if account._deleted:
            return []

        try:
            cba = OAuth2ClientsByDeveloper._byID(account._id36)
        except tdb_cassandra.NotFound:
            return []

        clients = cls._byID(cba._values().keys())
        return [client for client in clients.itervalues()
                if not client.deleted and client.has_developer(account)]

    @classmethod
    def _by_user(cls, account):
        """Returns a (possibly empty) list of client-scope-expiration triples for which Account has outstanding access tokens."""

        refresh_tokens = {
            token._id: token for token in OAuth2RefreshToken._by_user(account)
            if token.check_valid()}
        access_tokens = [token for token in OAuth2AccessToken._by_user(account)
                         if token.check_valid()]

        tokens = refresh_tokens.values()
        tokens.extend(token for token in access_tokens
                      if token.refresh_token not in refresh_tokens)

        clients = cls._byID([token.client_id for token in tokens])
        return [(clients[token.client_id], OAuth2Scope(token.scope),
                 token.date + datetime.timedelta(seconds=token._ttl)
                     if token._ttl else None)
                for token in tokens]

    @classmethod
    def _by_user_grouped(cls, account):
        token_tuples = cls._by_user(account)
        clients = {}
        for client, scope, expiration in token_tuples:
            if client._id in clients:
                client_data = clients[client._id]
                client_data['scopes'].append(scope)
            else:
                client_data = {'scopes': [scope], 'access_tokens': 0,
                               'refresh_tokens': 0, 'client': client}
                clients[client._id] = client_data
            if expiration:
                client_data['access_tokens'] += 1
            else:
                client_data['refresh_tokens'] += 1

        for client_data in clients.itervalues():
            client_data['scopes'] = OAuth2Scope.merge_scopes(client_data['scopes'])

        return clients

    def revoke(self, account):
        """Revoke all of the outstanding OAuth2AccessTokens associated with this client and user Account."""

        for token in OAuth2RefreshToken._by_user(account):
            if token.client_id == self._id:
                token.revoke()
        for token in OAuth2AccessToken._by_user(account):
            if token.client_id == self._id:
                token.revoke()

    def is_confidential(self):
        return self.app_type not in self.PUBLIC_APP_TYPES

    def is_first_party(self):
        return self.has_developer(Account.system_user())


class OAuth2ClientsByDeveloper(tdb_cassandra.View):
    """Index providing access to the list of OAuth2Clients of which an Account is a developer."""

    _use_db = True
    _type_prefix = 'OAuth2ClientsByDeveloper'
    _view_of = OAuth2Client
    _connection_pool = 'main'


class OAuth2AuthorizationCode(ConsumableToken):
    """An OAuth2 authorization code for completing authorization flow"""
    token_size = 20
    _ttl = datetime.timedelta(minutes=10)
    _defaults = dict(ConsumableToken._defaults.items() + [
                         ("client_id", ""),
                         ("redirect_uri", ""),
                         ("scope", ""),
                         ("refreshable", False)])
    _bool_props = ConsumableToken._bool_props + ("refreshable",)
    _warn_on_partial_ttl = False
    _use_db = True
    _connection_pool = "main"

    @classmethod
    def _new(cls, client_id, redirect_uri, user_id, scope, refreshable):
        return super(OAuth2AuthorizationCode, cls)._new(
                client_id=client_id,
                redirect_uri=redirect_uri,
                user_id=user_id,
                scope=str(scope),
                refreshable=refreshable)

    @classmethod
    def use_token(cls, _id, client_id, redirect_uri):
        token = cls.get_token(_id)
        if token and (token.client_id == client_id and
                      token.redirect_uri == redirect_uri):
            token.consume()
            return token
        else:
            return None


class OAuth2AccessToken(Token):
    """An OAuth2 access token for accessing protected resources"""
    token_size = 20
    _ttl = datetime.timedelta(minutes=60)
    _defaults = dict(scope="",
                     token_type="bearer",
                     refresh_token="",
                     user_id="",
                    )
    _use_db = True
    _connection_pool = "main"

    @classmethod
    def _new(cls, client_id, user_id, scope, refresh_token=None, device_id=None):
        try:
            user_id_prefix = int(user_id, 36)
        except (ValueError, TypeError):
            user_id_prefix = ""
        _id = "%s-%s" % (user_id_prefix, cls._generate_unique_token())
        return super(OAuth2AccessToken, cls)._new(
                     _id=_id,
                     client_id=client_id,
                     user_id=user_id,
                     scope=str(scope),
                     refresh_token=refresh_token,
                     device_id=device_id,
        )

    @classmethod
    def _by_user_view(cls):
        return OAuth2AccessTokensByUser

    def _on_create(self):
        hooks.get_hook("oauth2.create_token").call(token=self)

        # update the by-user view
        if self.user_id:
            self._by_user_view()._set_values(str(self.user_id), {self._id: ''})

        return super(OAuth2AccessToken, self)._on_create()

    def check_valid(self):
        """Returns boolean indicating whether or not this access token is still valid."""

        # Has the token been revoked?
        if getattr(self, 'revoked', False):
            return False

        # Is the OAuth2Client still valid?
        try:
            client = OAuth2Client._byID(self.client_id)
            if client.deleted:
                raise NotFound
        except AttributeError:
            g.log.error("bad token %s: %s", self, self._t)
            raise
        except NotFound:
            return False

        # Is the user account still valid?
        if self.user_id:
            try:
                account = Account._byID36(self.user_id)
                if account._deleted:
                    raise NotFound
            except NotFound:
                return False

        return True

    def revoke(self):
        """Revokes (invalidates) this access token."""

        self.revoked = True
        self._commit()

        if self.user_id:
            try:
                tba = self._by_user_view()._byID(self.user_id)
                del tba[self._id]
            except (tdb_cassandra.NotFound, KeyError):
                # Not fatal, since self.check_valid() will still be False.
                pass
            else:
                tba._commit()

        hooks.get_hook("oauth2.revoke_token").call(token=self)

    @classmethod
    def revoke_all_by_user(cls, account):
        """Revokes all access tokens for a given user Account."""
        tokens = cls._by_user(account)
        for token in tokens:
            token.revoke()

    @classmethod
    def _by_user(cls, account):
        """Returns a (possibly empty) list of valid access tokens for a given user Account."""

        try:
            tba = cls._by_user_view()._byID(account._id36)
        except tdb_cassandra.NotFound:
            return []

        tokens = cls._byID(tba._values().keys())
        return [token for token in tokens.itervalues() if token.check_valid()]

class OAuth2AccessTokensByUser(tdb_cassandra.View):
    """Index listing the outstanding access tokens for an account."""

    _use_db = True
    _ttl = OAuth2AccessToken._ttl
    _type_prefix = 'OAuth2AccessTokensByUser'
    _view_of = OAuth2AccessToken
    _connection_pool = 'main'


class OAuth2RefreshToken(OAuth2AccessToken):
    """A refresh token for obtaining new access tokens for the same grant."""

    _type_prefix = None
    _ttl = None

    def _on_create(self):
        if self.user_id:
            self._by_user_view()._set_values(str(self.user_id), {self._id: ''})

        # skip OAuth2AccessToken._on_create to avoid "oauth2.create_token" hook
        return Token._on_create(self)

    @classmethod
    def _by_user_view(cls):
        return OAuth2RefreshTokensByUser

    def revoke(self):
        super(OAuth2RefreshToken, self).revoke()
        account = Account._byID36(self.user_id)
        access_tokens = OAuth2AccessToken._by_user(account)
        for token in access_tokens:
            if token.refresh_token == self._id:
                token.revoke()

class OAuth2RefreshTokensByUser(tdb_cassandra.View):
    """Index listing the outstanding refresh tokens for an account."""

    _use_db = True
    _ttl = OAuth2RefreshToken._ttl
    _type_prefix = 'OAuth2RefreshTokensByUser'
    _view_of = OAuth2RefreshToken
    _connection_pool = 'main'


class EmailVerificationToken(ConsumableToken):
    _use_db = True
    _connection_pool = "main"
    _ttl = datetime.timedelta(hours=12)
    token_size = 20

    @classmethod
    def _new(cls, user):
        return super(EmailVerificationToken, cls)._new(user_id=user._fullname,
                                                       email=user.email)

    def valid_for_user(self, user):
        return self.email == user.email


class PasswordResetToken(ConsumableToken):
    _use_db = True
    _connection_pool = "main"
    _ttl = datetime.timedelta(hours=12)
    token_size = 20

    @classmethod
    def _new(cls, user):
        return super(PasswordResetToken, cls)._new(user_id=user._fullname,
                                                   email_address=user.email,
                                                   password=user.password)

    def valid_for_user(self, user):
        return (self.email_address == user.email and
                self.password == user.password)


class AwardClaimToken(ConsumableToken):
    token_size = 20
    _ttl = datetime.timedelta(days=30)
    _defaults = dict(ConsumableToken._defaults.items() + [
                         ("awardfullname", ""),
                         ("description", ""),
                         ("url", ""),
                         ("uid", "")])
    _use_db = True
    _connection_pool = "main"

    @classmethod
    def _new(cls, uid, award, description, url):
        '''Create an AwardClaimToken with the given parameters

        `uid` - A string that uniquely identifies the kind of
                Trophy the user would be claiming.*
        `award_codename` - The codename of the Award the user will claim
        `description` - The description the Trophy will receive
        `url` - The URL the Trophy will receive

        *Note that this differs from Award codenames, because it may be
        desirable to allow users to have multiple copies of the same Award,
        but restrict another aspect of the Trophy. For example, users
        are allowed to have multiple Translator awards, but should only get
        one for each language, so the `unique_award_id`s for those would be
        of the form "i18n_%(language)s"

        '''
        return super(AwardClaimToken, cls)._new(
            awardfullname=award._fullname,
            description=description or "",
            url=url or "",
            uid=uid,
        )

    def post_url(self):
        # Relative URL; should be used on an on-site form
        return "/awards/claim/%s" % self._id

    def confirm_url(self):
        # Full URL; for emailing, PM'ing, etc.
        base = g.https_endpoint or g.origin
        return "%s/awards/confirm/%s" % (base, self._id)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import pytz
import time
from datetime import datetime
from pycassa.system_manager import UTF8_TYPE
from pylons.i18n import _

from r2.lib.db import tdb_cassandra

OLD_SITEWIDE_RULES = [
    _("spam"),
    _("vote manipulation"),
    _("personal information"),
    _("sexualizing minors"),
    _("breaking reddit"),
]

SITEWIDE_RULES = [
    _("Spam"),
    _("Personal and confidential information"),
    _("Threatening, harassing, or inciting violence"),
]
MAX_RULES_PER_SUBREDDIT = 10


class SubredditRules(tdb_cassandra.View):
    _use_db = True
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "column_name_class": UTF8_TYPE,
        "default_validation_class": UTF8_TYPE,
    }
    _compare_with = UTF8_TYPE
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.ONE
    _connection_pool = "main"

    @classmethod
    def get_rule_blob(self, short_name, description, priority, kind,
            created_utc=None):
        if not created_utc:
            created_utc = time.mktime(datetime.now(pytz.UTC).timetuple())

        rule_params = {
            "description": description,
            "priority": priority,
            "created_utc": created_utc,
        }
        if kind and kind != 'all':
            rule_params["kind"] = kind

        jsonpacked = json.dumps(rule_params)
        blob = {short_name: jsonpacked}
        return blob

    @classmethod
    def create(self, subreddit, short_name, description, kind=None,
            created_utc=None):
        """Create a rule and append to the end of the priority list."""
        try:
            priority = len(list(self._cf.get(subreddit._id36)))
        except tdb_cassandra.NotFoundException:
            priority = 0

        if priority >= MAX_RULES_PER_SUBREDDIT:
            return

        blob = self.get_rule_blob(short_name, description, priority,
            kind, created_utc)
        self._set_values(subreddit._id36, blob)

    @classmethod
    def remove_rule(self, subreddit, short_name):
        """Remove a rule and update priorities of remaining rules."""
        self._remove(subreddit._id36, [short_name])

        rules = self.get_rules(subreddit)
        blobs = {}
        for index, rule in enumerate(rules):
            if rule["priority"] != index:
                blobs.update(self.get_rule_blob(
                    short_name=rule["short_name"],
                    description=rule["description"],
                    priority=index,
                    kind=rule.get("kind"),
                    created_utc=rule["created_utc"],
                ))
        self._set_values(subreddit._id36, blobs)

    @classmethod
    def update(self, subreddit, old_short_name, short_name, description,
            kind=None):
        """Update the short_name or description of a rule."""
        rules = self._cf.get(subreddit._id36)
        if old_short_name != short_name:
            old_rule = rules.get(old_short_name, None)
            self._remove(subreddit._id36, [old_short_name])
        else:
            old_rule = rules.get(short_name, None)
        if not old_rule:
            return False

        old_rule = json.loads(old_rule)
        if not old_rule.get("created_utc"):
                old_rule["created_utc"] = time.mktime(
                    datetime.strptime(
                        old_rule.pop("when")[:-6], "%Y-%m-%d %H:%M:%S.%f"
                    ).timetuple())

        blob = self.get_rule_blob(
            short_name=short_name,
            description=description,
            priority=old_rule["priority"],
            kind=kind,
            created_utc=old_rule["created_utc"],
        )
        self._set_values(subreddit._id36, blob)

    @classmethod
    def reorder(self, subreddit, short_name, priority):
        """Update the priority spot of a rule

        Move an existing rule to the desired spot in the rules
        list and then update the priority of the rules.
        """
        rule_to_reorder = self.get_rule(subreddit, short_name)
        if not rule_to_reorder:
            return False

        self._remove(subreddit._id36, [short_name])
        rules = self.get_rules(subreddit)

        priority = min(priority, len(rules))
        current_priority_index = 0
        blobs = {}
        blobs.update(self.get_rule_blob(
                short_name=rule_to_reorder["short_name"],
                description=rule_to_reorder["description"],
                priority=priority,
                kind=rule_to_reorder.get("kind"),
                created_utc=rule_to_reorder["created_utc"],
        ))

        for rule in rules:
            # Placeholder for rule_to_reorder's new priority
            if priority == current_priority_index:
                current_priority_index += 1

            if rule["priority"] != current_priority_index:
                blobs.update(self.get_rule_blob(
                    short_name=rule["short_name"],
                    description=rule["description"],
                    priority=current_priority_index,
                    kind=rule.get("kind"),
                    created_utc=rule["created_utc"],
                ))
            current_priority_index += 1
        self._set_values(subreddit._id36, blobs)

    @classmethod
    def get_rule(self, subreddit, short_name):
        """Return rule associated with short_name or None."""
        try:
            rules = self._cf.get(subreddit._id36)
        except tdb_cassandra.NotFoundException:
            return None
        rule = rules.get(short_name, None)
        if not rule:
            return None
        rule = json.loads(rule)
        rule["short_name"] = short_name
        return rule

    @classmethod
    def get_rules(self, subreddit, kind=None):
        """Return list of rules sorted by priority.

        If kind is empty, then all the rules apply.
        """
        try:
            query = self._cf.get(subreddit._id36)
        except tdb_cassandra.NotFoundException:
            return []

        result = []
        for uuid, json_blob in query.iteritems():
            payload = json.loads(json_blob)
            if not payload.get("created_utc"):
                payload["created_utc"] = time.mktime(
                    datetime.strptime(
                        payload.pop("when")[:-6], "%Y-%m-%d %H:%M:%S.%f"
                    ).timetuple())
            payload["short_name"] = uuid

            if not kind:
                result.append(payload)
            elif kind in payload.get("kind", kind):
                result.append(payload)

        return sorted(result, key=lambda t: t["priority"])
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.manager import tp_manager
from r2.lib.jsontemplates import *

tpm = tp_manager.tp_manager()

def api(type, cls):
    tpm.add_handler(type, 'api', cls())
    tpm.add_handler(type, 'api-html', cls())
    tpm.add_handler(type, 'api-compact', cls())


def register_api_templates(template_name, template_class):
    for style in ('api', 'api-html', 'api-compact'):
        tpm.add_handler(
            name=template_name,
            style=style,
            handler=template_class,
        )


# blanket fallback rule
api('templated', NullJsonTemplate)

# class specific overrides
api('link',          LinkJsonTemplate)
api('promotedlink',  PromotedLinkJsonTemplate)
api('message',       MessageJsonTemplate)
api('subreddit',     SubredditJsonTemplate)
api('labeledmulti',  LabeledMultiJsonTemplate)
api('reddit',        RedditJsonTemplate)
api('panestack',     PanestackJsonTemplate)
api('htmlpanestack', NullJsonTemplate)
api('listing',       ListingJsonTemplate)
api('searchlisting', SearchListingJsonTemplate)
api('userlisting',   UserListingJsonTemplate)
api('usertableitem', UserTableItemJsonTemplate)
api('account',       AccountJsonTemplate)

api('reltableitem', RelTableItemJsonTemplate)
api('bannedtableitem', BannedTableItemJsonTemplate)
api('mutedtableitem', MutedTableItemJsonTemplate)
api('invitedmodtableitem', InvitedModTableItemJsonTemplate)
api('friendtableitem', FriendTableItemJsonTemplate)

api('organiclisting',       OrganicListingJsonTemplate)
api('subreddittraffic', TrafficJsonTemplate)
api('takedownpane', TakedownJsonTemplate)
api('policyview', PolicyViewJsonTemplate)

api('wikibasepage', WikiJsonTemplate)
api('wikipagerevisions', WikiJsonTemplate)
api('wikiview', WikiViewJsonTemplate)
api('wikirevision', WikiRevisionJsonTemplate)

api('wikipagelisting', WikiPageListingJsonTemplate)
api('wikipagediscussions', WikiJsonTemplate)
api('wikipagesettings', WikiSettingsJsonTemplate)

api('flairlist', FlairListJsonTemplate)
api('flaircsv', FlairCsvJsonTemplate)
api('flairselector', FlairSelectorJsonTemplate)

api('subredditstylesheet', StylesheetTemplate)
api('subredditstylesheetsource', StylesheetTemplate)
api('createsubreddit', SubredditSettingsTemplate)
api('uploadedimage', UploadedImageJsonTemplate)

api('modaction', ModActionTemplate)

api('trophy', TrophyJsonTemplate)
api('rules', RulesJsonTemplate)


register_api_templates('comment', CommentJsonTemplate)
register_api_templates('morerecursion', MoreCommentJsonTemplate)
register_api_templates('morechildren', MoreCommentJsonTemplate)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""Pylons middleware initialization"""
import importlib
import re
import urllib
import tempfile
import urlparse
from threading import Lock
import itertools
import simplejson

from paste.cascade import Cascade
from paste.errordocument import StatusBasedForward
from paste.recursive import RecursiveMiddleware
from paste.registry import RegistryManager
from paste.urlparser import StaticURLParser
from paste.deploy.converters import asbool
from paste.request import path_info_split
from pylons import response
from pylons.middleware import ErrorHandler
from pylons.wsgiapp import PylonsApp
from routes.middleware import RoutesMiddleware

from r2.config import hooks
from r2.config.environment import load_environment
from r2.config.extensions import extension_mapping, set_extension
from r2.lib.utils import is_subdomain, is_language_subdomain
from r2.lib import csrf, filters


# patch in WebOb support for HTTP 429 "Too Many Requests"
import webob.exc
import webob.util

class HTTPTooManyRequests(webob.exc.HTTPClientError):
    code = 429
    title = 'Too Many Requests'
    explanation = ('The server has received too many requests from the client.')

webob.exc.status_map[429] = HTTPTooManyRequests
webob.util.status_reasons[429] = HTTPTooManyRequests.title

# patch out SSRFable/XSSable endpoints in older versions of weberror
import weberror.evalexception


# We could probably just set `.exposed = False`, but this makes me feel better
def _stub(*args, **kwargs):
    pass

weberror.evalexception.EvalException.post_traceback = _stub
weberror.evalexception.EvalException.relay = _stub


def error_mapper(code, message, environ, global_conf=None, **kw):
    if environ.get('pylons.error_call'):
        return None
    else:
        environ['pylons.error_call'] = True

    from pylons import tmpl_context as c

    if global_conf is None:
        global_conf = {}
    codes = [304, 400, 401, 403, 404, 409, 415, 429, 503]
    if not asbool(global_conf.get('debug')):
        codes.append(500)
    if code in codes:
        # StatusBasedForward expects a relative URL (no SCRIPT_NAME)
        d = dict(code = code, message = message)

        exception = environ.get('r2.controller.exception')
        if exception:
            d['explanation'] = exception.explanation
            error_data = getattr(exception, 'error_data', None)
            if error_data:
                environ['extra_error_data'] = error_data

        if environ.get('REDDIT_NAME'):
            d['srname'] = environ.get('REDDIT_NAME')
        if environ.get('REDDIT_TAKEDOWN'):
            d['takedown'] = environ.get('REDDIT_TAKEDOWN')
        if environ.get('REDDIT_ERROR_NAME'):
            d['error_name'] = environ.get('REDDIT_ERROR_NAME')

        # preserve x-frame-options when 304ing
        if code == 304:
            d['allow_framing'] = 1 if c.allow_framing else 0

        extension = environ.get("extension")
        if extension:
            url = '/error/document/.%s?%s' % (extension, urllib.urlencode(d))
        else:
            url = '/error/document/?%s' % (urllib.urlencode(d))
        return url


# from pylons < 1.0
def ErrorDocuments(app, global_conf, mapper, **kw):
    """Wraps the app in error docs using Paste RecursiveMiddleware and
    ErrorDocumentsMiddleware
    """
    if global_conf is None:
        global_conf = {}

    return RecursiveMiddleware(StatusBasedForward(
        app, global_conf=global_conf, mapper=mapper, **kw))


class ProfilingMiddleware(object):
    def __init__(self, app, directory):
        self.app = app
        self.directory = directory

    def __call__(self, environ, start_response):
        import cProfile

        try:
            tmpfile = tempfile.NamedTemporaryFile(prefix='profile',
                                                  dir=self.directory,
                                                  delete=False)

            profile = cProfile.Profile()
            result = profile.runcall(self.app, environ, start_response)
            profile.dump_stats(tmpfile.name)

            return result
        finally:
            tmpfile.close()


class DomainMiddleware(object):

    def __init__(self, app, config):
        self.app = app
        self.config = config

    def __call__(self, environ, start_response):
        g = self.config['pylons.app_globals']
        http_host = environ.get('HTTP_HOST', 'localhost').lower()
        domain, s, port = http_host.partition(':')

        # remember the port
        try:
            environ['request_port'] = int(port)
        except ValueError:
            pass

        # localhost is exempt so paster run/shell will work
        # media_domain doesn't need special processing since it's just ads
        is_media_only_domain = (is_subdomain(domain, g.media_domain) and
                                g.domain != g.media_domain)
        if domain == "localhost" or is_media_only_domain:
            return self.app(environ, start_response)

        # tell reddit_base to redirect to the appropriate subreddit for
        # a legacy CNAME
        if not is_subdomain(domain, g.domain):
            environ['legacy-cname'] = domain
            return self.app(environ, start_response)

        # How many characters to chop off the end of the hostname before
        # we start looking at subdomains
        ignored_suffix_len = len(g.domain)

        # figure out what subdomain we're on, if any
        subdomains = domain[:-ignored_suffix_len - 1].split('.')

        sr_redirect = None
        prefix_parts = []
        for subdomain in subdomains[:]:
            extension = g.extension_subdomains.get(subdomain)
            # These subdomains are reserved, don't treat them as SR
            # or language subdomains.
            if subdomain in g.reserved_subdomains:
                # Some subdomains are reserved, but also can't be mixed into
                # the domain prefix for various reasons (permalinks will be
                # broken, etc.)
                if subdomain in g.ignored_subdomains:
                    continue
                prefix_parts.append(subdomain)
            elif extension:
                environ['reddit-domain-extension'] = extension
            elif is_language_subdomain(subdomain):
                environ['reddit-prefer-lang'] = subdomain
            else:
                sr_redirect = subdomain
                subdomains.remove(subdomain)

        if 'reddit-prefer-lang' in environ:
            prefix_parts.insert(0, environ['reddit-prefer-lang'])
        if prefix_parts:
            environ['reddit-domain-prefix'] = '.'.join(prefix_parts)

        # if there was a subreddit subdomain, redirect
        if sr_redirect and environ.get("FULLPATH"):
            if not subdomains and g.domain_prefix:
                subdomains.append(g.domain_prefix)
            subdomains.append(g.domain)
            redir = "%s/r/%s/%s" % ('.'.join(subdomains),
                                    sr_redirect, environ['FULLPATH'])
            redir = g.default_scheme + "://" + redir.replace('//', '/')

            start_response("301 Moved Permanently", [("Location", redir)])
            return [""]

        return self.app(environ, start_response)


class SubredditMiddleware(object):
    sr_pattern = re.compile(r'^/r/([^/]{2,})')

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        sr = self.sr_pattern.match(path)
        if sr:
            environ['subreddit'] = sr.groups()[0]
            environ['PATH_INFO'] = self.sr_pattern.sub('', path) or '/'
        return self.app(environ, start_response)


class DomainListingMiddleware(object):
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if not environ.has_key('subreddit'):
            path = environ['PATH_INFO']
            domain, rest = path_info_split(path)
            if domain == "domain" and rest:
                domain, rest = path_info_split(rest)
                environ['domain'] = domain
                environ['PATH_INFO'] = rest or '/'
        return self.app(environ, start_response)


class ExtensionMiddleware(object):
    ext_pattern = re.compile(r'\.([^/]+)\Z')

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        fname, sep, path_ext = path.rpartition('.')
        domain_ext = environ.get('reddit-domain-extension')

        ext = None
        if path_ext in extension_mapping:
            ext = path_ext
            # Strip off the extension.
            environ['PATH_INFO'] = path[:-(len(ext) + 1)]
        elif domain_ext in extension_mapping:
            ext = domain_ext

        if ext:
            set_extension(environ, ext)
        else:
            environ['render_style'] = 'html'
            environ['content_type'] = 'text/html; charset=UTF-8'

        return self.app(environ, start_response)

class FullPathMiddleware(object):
    # Debt: we have a lot of middleware which (unfortunately) modify the
    # global URL PATH_INFO string. To work with the original request URL, we
    # save it to a different location here.
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        environ['FULLPATH'] = environ.get('PATH_INFO')
        qs = environ.get('QUERY_STRING')
        if qs:
            environ['FULLPATH'] += '?' + qs
        return self.app(environ, start_response)

class StaticTestMiddleware(object):
    def __init__(self, app, static_path, domain):
        self.app = app
        self.static_path = static_path
        self.domain = domain

    def __call__(self, environ, start_response):
        if environ['HTTP_HOST'] == self.domain:
            environ['PATH_INFO'] = self.static_path.rstrip('/') + environ['PATH_INFO']
            return self.app(environ, start_response)
        raise webob.exc.HTTPNotFound()


def _wsgi_json(start_response, status_int, message=""):
    status_message = webob.util.status_reasons[status_int]
    message = message or status_message

    start_response(
        "%s %s" % (status_int, status_message),
        [("Content-Type", "application/json")])

    data = simplejson.dumps({
        "error": status_int,
        "message": message
    })
    return [filters.websafe_json(data).encode("utf-8")]


class LimitUploadSize(object):
    """
    Middleware for restricting the size of uploaded files (such as
    image files for the CSS editing capability).
    """
    def __init__(self, app, max_size=1024*500):
        self.app = app
        self.max_size = max_size

    def __call__(self, environ, start_response):
        cl_key = 'CONTENT_LENGTH'
        is_error = environ.get("pylons.error_call", False)
        is_api = environ.get("render_style").startswith("api")
        if not is_error and environ['REQUEST_METHOD'] == 'POST':
            if cl_key not in environ:

                if is_api:
                    return _wsgi_json(start_response, 411)
                else:
                    start_response("411 Length Required", [])
                    return ['<html><body>length required</body></html>']

            try:
                cl_int = int(environ[cl_key])
            except ValueError:
                if is_api:
                    return _wsgi_json(start_response, 400)
                else:
                    start_response("400 Bad Request", [])
                    return ['<html><body>bad request</body></html>']

            if cl_int > self.max_size:
                error_msg = "too big. keep it under %d KiB" % (
                    self.max_size / 1024)

                if is_api:
                    return _wsgi_json(start_response, 413, error_msg)
                else:
                    start_response("413 Too Big", [])
                    return ["<html>"
                            "<head>"
                            "<script type='text/javascript'>"
                            "parent.completedUploadImage('failed',"
                            "'',"
                            "'',"
                            "[['BAD_CSS_NAME', ''], ['IMAGE_ERROR', '", error_msg,"']],"
                            "'');"
                            "</script></head><body>you shouldn\'t be here</body></html>"]

        return self.app(environ, start_response)

# TODO CleanupMiddleware seems to exist because cookie headers are being duplicated
# somewhere in the response processing chain. It should be removed as soon as we
# find the underlying issue.
class CleanupMiddleware(object):
    """
    Put anything here that should be called after every other bit of
    middleware. This currently includes the code for removing
    duplicate headers (such as multiple cookie setting).  The behavior
    here is to disregard all but the last record.
    """
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        def custom_start_response(status, headers, exc_info = None):
            fixed = []
            seen = set()
            for head, val in reversed(headers):
                head = head.lower()
                key = (head, val.split("=", 1)[0])
                if key not in seen:
                    fixed.insert(0, (head, val))
                    seen.add(key)
            return start_response(status, fixed, exc_info)
        return self.app(environ, custom_start_response)


class SafetyMiddleware(object):
    """Clean up any attempts at response splitting in headers."""

    has_bad_characters = re.compile("[\r\n]")
    sanitizer = re.compile("[\r\n]+[ \t]*")

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        def safe_start_response(status, headers, exc_info=None):
            sanitized = []
            for name, value in headers:
                if self.has_bad_characters.search(value):
                    value = self.sanitizer.sub("", value)
                sanitized.append((name, value))
            return start_response(status, sanitized, exc_info)
        return self.app(environ, safe_start_response)


class RedditApp(PylonsApp):

    test_mode = False

    def __init__(self, *args, **kwargs):
        super(RedditApp, self).__init__(*args, **kwargs)
        self._loading_lock = Lock()
        self._controllers = None
        self._hooks_registered = False

    def setup_app_env(self, environ, start_response):
        PylonsApp.setup_app_env(self, environ, start_response)

        if not self.test_mode:
            if self._controllers and self._hooks_registered:
                return

            with self._loading_lock:
                self.load_controllers()
                self.register_hooks()

    def _check_csrf_prevention(self):
        from r2 import controllers
        from pylons import app_globals as g

        if not g.running_as_script:
            controllers_iter = itertools.chain(
                controllers._reddit_controllers.itervalues(),
                controllers._plugin_controllers.itervalues(),
            )
            for controller in controllers_iter:
                csrf.check_controller_csrf_prevention(controller)

    def load_controllers(self):
        if self._controllers:
            return

        controllers = importlib.import_module(self.package_name +
                                              '.controllers')
        controllers.load_controllers()
        self.config['r2.plugins'].load_controllers()
        self._controllers = controllers
        self._check_csrf_prevention()

    def register_hooks(self):
        if self._hooks_registered:
            return

        hooks.register_hooks()
        self._hooks_registered = True

    def find_controller(self, controller_name):
        if controller_name in self.controller_classes:
            return self.controller_classes[controller_name]

        controller_cls = self._controllers.get_controller(controller_name)
        self.controller_classes[controller_name] = controller_cls
        return controller_cls

def make_app(global_conf, full_stack=True, **app_conf):
    """Create a Pylons WSGI application and return it

    `global_conf`
        The inherited configuration for this application. Normally from the
        [DEFAULT] section of the Paste ini file.

    `full_stack`
        Whether or not this application provides a full WSGI stack (by default,
        meaning it handles its own exceptions and errors). Disable full_stack
        when this application is "managed" by another WSGI middleware.

    `app_conf`
        The application's local configuration. Normally specified in the
        [app:<name>] section of the Paste ini file (where <name> defaults to
        main).
    """

    # Configure the Pylons environment
    config = load_environment(global_conf, app_conf)
    g = config['pylons.app_globals']

    # The Pylons WSGI app
    app = RedditApp(config=config)
    app = RoutesMiddleware(app, config["routes.map"])

    # CUSTOM MIDDLEWARE HERE (filtered by the error handling middlewares)

    # last thing first from here down
    app = CleanupMiddleware(app)

    app = LimitUploadSize(app)

    profile_directory = g.config.get('profile_directory')
    if profile_directory:
        app = ProfilingMiddleware(app, profile_directory)

    app = DomainListingMiddleware(app)
    app = SubredditMiddleware(app)
    app = ExtensionMiddleware(app)
    app = DomainMiddleware(app, config=config)

    if asbool(full_stack):
        # Handle Python exceptions
        app = ErrorHandler(app, global_conf, **config['pylons.errorware'])

        # Display error documents for 401, 403, 404 status codes (and 500 when
        # debug is disabled)
        app = ErrorDocuments(app, global_conf, error_mapper, **app_conf)

    # Establish the Registry for this application
    app = RegistryManager(app)

    # Static files
    static_app = StaticURLParser(config['pylons.paths']['static_files'])
    static_cascade = [static_app, app]

    if config['r2.plugins'] and g.config['uncompressedJS']:
        plugin_static_apps = Cascade([StaticURLParser(plugin.static_dir)
                                      for plugin in config['r2.plugins']])
        static_cascade.insert(0, plugin_static_apps)
    app = Cascade(static_cascade)

    app = FullPathMiddleware(app)

    if not g.config['uncompressedJS'] and g.config['debug']:
        static_fallback = StaticTestMiddleware(static_app, g.config['static_path'], g.config['static_domain'])
        app = Cascade([static_fallback, app])

    app = SafetyMiddleware(app)

    app.config = config

    return app
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Setup your Routes options here
"""
from routes import Mapper


def not_in_sr(environ, results):
    return ('subreddit' not in environ and
            'sub_domain' not in environ and
            'domain' not in environ)


# FIXME: submappers with path prefixes are broken in Routes 1.11. Once we
# upgrade, we should be able to replace this ugliness with submappers.
def partial_connect(mc, **override_args):
    def connect(path, **kwargs):
        if 'path_prefix' in override_args:
            path = override_args['path_prefix'] + path
        kwargs.update(override_args)
        mc(path, **kwargs)
    return connect


def make_map(config):
    map = Mapper(explicit=False)
    map.minimization = True
    mc = map.connect

    # Username-relative userpage redirects, need to be defined here in case
    # a plugin defines a `/user/:name` handler.
    mc('/user/me', controller='user', action='rel_user_redirect')
    mc('/user/me/*rest', controller='user', action='rel_user_redirect')

    for plugin in reversed(config['r2.plugins']):
        plugin.add_routes(mc)

    mc('/admin/', controller='awards')

    mc('/robots.txt', controller='robots', action='robots')
    mc('/crossdomain', controller='robots', action='crossdomain')

    mc('/login', controller='forms', action='login')
    mc('/register', controller='forms', action='register')
    mc('/logout', controller='forms', action='logout')
    mc('/verify', controller='forms', action='verify')
    mc('/adminon', controller='forms', action='adminon')
    mc('/adminoff', controller='forms', action='adminoff')
    mc('/submit', controller='front', action='submit')

    # redirect old urls to the new
    ABOUT_BASE = "https://about.reddit.com/"
    mc('/about', controller='redirect', action='redirect', dest=ABOUT_BASE, 
       conditions={'function':not_in_sr})
    mc('/about/values', controller='redirect', action='redirect', dest=ABOUT_BASE)
    mc('/about/team', controller='redirect', action='redirect',
       dest=ABOUT_BASE)
    mc('/about/alien', controller='redirect', action='redirect',
       dest=ABOUT_BASE + "press")
    mc('/jobs', controller='redirect', action='redirect',
       dest=ABOUT_BASE + "careers")

    mc('/over18', controller='post', action='over18')
    mc('/quarantine', controller='post', action='quarantine')
    mc('/quarantine_optout', controller='api', action='quarantine_optout')

    mc('/traffic', controller='front', action='site_traffic')
    mc('/traffic/languages/:langcode', controller='front',
       action='lang_traffic', langcode='')
    mc('/traffic/adverts/:code', controller='front',
       action='advert_traffic', code='')
    mc('/traffic/subreddits/report', controller='front',
       action='subreddit_traffic_report')
    mc('/account-activity', controller='front', action='account_activity')

    mc('/subreddits/create', controller='front', action='newreddit')
    mc('/subreddits/search', controller='front', action='search_reddits')
    mc('/subreddits/login', controller='forms', action='login')
    mc('/subreddits/:where', controller='reddits', action='listing',
       where='popular', conditions={'function':not_in_sr},
       requirements=dict(where="popular|new|banned|employee|gold|default|"
                               "quarantine|featured"))
    # If no subreddit is specified, might as well show a list of 'em.
    mc('/r', controller='redirect', action='redirect', dest='/subreddits')

    mc('/subreddits/mine/:where', controller='myreddits', action='listing',
       where='subscriber', conditions={'function':not_in_sr},
       requirements=dict(where='subscriber|contributor|moderator'))

    # These routes are kept for backwards-compatibility reasons
    # Using the above /subreddits/ ones instead is preferable
    mc('/reddits/create', controller='front', action='newreddit')
    mc('/reddits/search', controller='front', action='search_reddits')
    mc('/reddits/login', controller='forms', action='login')
    mc('/reddits/:where', controller='reddits', action='listing',
       where='popular', conditions={'function':not_in_sr},
       requirements=dict(where="popular|new|banned"))

    mc('/reddits/mine/:where', controller='myreddits', action='listing',
       where='subscriber', conditions={'function':not_in_sr},
       requirements=dict(where='subscriber|contributor|moderator'))

    mc('/buttons', controller='buttons', action='button_demo_page')

    #/button.js and buttonlite.js - the embeds
    mc('/button', controller='buttons', action='button_embed')
    mc('/buttonlite', controller='buttons', action='button_lite')

    mc('/widget', controller='buttons', action='widget_demo_page')

    mc('/awards', controller='front', action='awards')
    mc('/awards/confirm/:code', controller='front',
       action='confirm_award_claim')
    mc('/awards/claim/:code', controller='front', action='claim_award')
    mc('/awards/received', controller='front', action='received_award')

    mc('/i18n', controller='redirect', action='redirect',
       dest='https://www.reddit.com/r/i18n')
    mc('/feedback', controller='redirect', action='redirect',
       dest='/contact')
    mc('/contact', controller='frontunstyled', action='contact_us')

    mc('/admin/awards', controller='awards')
    mc('/admin/awards/:awardcn/:action', controller='awards',
       requirements=dict(action="give|winners"))

    mc('/admin/creddits', controller='admintool', action='creddits')
    mc('/admin/gold', controller='admintool', action='gold')

    mc('/user/:username/about', controller='user', action='about',
       where='overview')
    mc('/user/:username/trophies', controller='user', action='trophies')
    mc('/user/:username/:where', controller='user', action='listing',
       where='overview')
    mc('/user/:username/saved/:category', controller='user', action='listing',
       where='saved')

    multi_prefixes = (
       partial_connect(mc, path_prefix='/user/:username/m/:multipath'),
       partial_connect(mc, path_prefix='/me/m/:multipath', my_multi=True),
       partial_connect(mc, path_prefix='/me/f/:filtername'),
    )

    for connect in multi_prefixes:
       connect('/', controller='hot', action='listing')
       connect('/submit', controller='front', action='submit')
       connect('/:sort', controller='browse', sort='top',
          action='listing', requirements=dict(sort='top|controversial'))
       connect('/:controller', action='listing',
          requirements=dict(controller="hot|new|rising|randomrising|ads"))

    mc('/user/:username/:where/:show', controller='user', action='listing')
    
    mc('/explore', controller='front', action='explore')
    mc('/api/recommend/feedback', controller='api', action='rec_feedback')

    mc("/newsletter", controller="newsletter", action="newsletter")

    mc("/gtm/jail", controller="googletagmanager", action="jail")
    mc("/gtm", controller="googletagmanager", action="gtm")

    mc('/oembed', controller='oembed', action='oembed')

    mc('/about/rules', controller='front', action='rules')
    mc('/about/sidebar', controller='front', action='sidebar')
    mc('/about/sticky', controller='front', action='sticky')
    mc('/about/flair', controller='front', action='flairlisting')
    mc('/about', controller='front', action='about')
    for connect in (mc,) + multi_prefixes:
       connect('/about/message/:where', controller='message',
          action='listing')
       connect('/about/log', controller='front', action='moderationlog')
       connect('/about/:location', controller='front',
          action='spamlisting',
          requirements=dict(location='reports|spam|modqueue|unmoderated|edited'))
       connect('/about/:where', controller='userlistlisting',
          requirements=dict(where='contributors|banned|muted|wikibanned|'
              'wikicontributors|moderators'), action='listing')
       connect('/about/:location', controller='front', action='editreddit',
          requirements=dict(location='edit|stylesheet|traffic|about'))
       connect('/comments', controller='comments', action='listing')
       connect('/comments/gilded', action='listing', controller='gilded')
       connect('/gilded', action='listing', controller='gilded')
       connect('/search', controller='front', action='search')

    mc('/u/:username', controller='redirect', action='user_redirect')
    mc('/u/:username/*rest', controller='redirect', action='user_redirect')

    # preserve timereddit URLs from 4/1/2012
    mc('/t/:timereddit', controller='redirect', action='timereddit_redirect')
    mc('/t/:timereddit/*rest', controller='redirect',
       action='timereddit_redirect')

    # /prefs/friends is also aliased to /api/v1/me/friends
    mc('/prefs/:where', controller='userlistlisting',
        action='user_prefs', requirements=dict(where='blocked|friends'))
    mc('/prefs/:location', controller='forms', action='prefs',
       location='options')

    mc('/info/0:article/*rest', controller='front',
       action='oldinfo', dest='comments', type='ancient')
    mc('/info/:article/:dest/:comment', controller='front',
       action='oldinfo', type='old', dest='comments', comment=None)


    mc('/related/:article/:title', controller='front',
       action='related', title=None)
    mc('/details/:article/:title', controller='front',
       action='details', title=None)
    mc('/traffic/:link/:campaign', controller='front', action='traffic',
       campaign=None)
    mc('/comments/:article/:title/:comment', controller='front',
       action='comments', title=None, comment=None)
    mc('/duplicates/:article/:title', controller='front',
       action='duplicates', title=None)

    mc('/mail/optout', controller='forms', action='optout')
    mc('/mail/optin', controller='forms', action='optin')
    mc('/mail/unsubscribe/:user/:key', controller='forms',
       action='unsubscribe_emails')
    mc('/stylesheet', controller='front', action='stylesheet')

    mc('/share/close', controller='front', action='share_close')

    # sponsor endpoints
    mc('/sponsor/report', controller='sponsor', action='report')
    mc('/sponsor/inventory', controller='sponsor', action='promote_inventory')
    mc('/sponsor/lookup_user', controller='sponsor', action="lookup_user")

    # sponsor listings
    mc('/sponsor/promoted/:sort', controller='sponsorlisting', action='listing',
       requirements=dict(sort="future_promos|pending_promos|unpaid_promos|"
                              "rejected_promos|live_promos|edited_live_promos|"
                              "underdelivered|reported|house|fraud|all|"
                              "unapproved_campaigns|by_platform"))
    mc('/sponsor', controller='sponsorlisting', action="listing",
       sort="all")
    mc('/sponsor/promoted/', controller='sponsorlisting', action="listing",
       sort="all")
    mc('/sponsor/promoted/live_promos/:sr', controller='sponsorlisting',
       sort='live_promos', action='listing')


    # listings of user's promos
    mc('/promoted/:sort', controller='promotelisting', action="listing",
       requirements=dict(sort="future_promos|pending_promos|unpaid_promos|"
                              "rejected_promos|live_promos|edited_live_promos|"
                              "all"))
    mc('/promoted/', controller='promotelisting', action="listing", sort="all")

    # editing endpoints
    mc('/promoted/new_promo', controller='promote', action='new_promo')
    mc('/promoted/edit_promo/:link', controller='promote', action='edit_promo')
    mc('/promoted/pay/:link/:campaign', controller='promote', action='pay')
    mc('/promoted/refund/:link/:campaign', controller='promote',
       action='refund')

    mc('/health', controller='health', action='health')
    mc('/health/ads', controller='health', action='promohealth')
    mc('/health/caches', controller='health', action='cachehealth')

    mc('/', controller='hot', action='listing')

    mc('/:controller', action='listing',
       requirements=dict(controller="hot|new|rising|randomrising|ads"))
    mc('/saved', controller='user', action='saved_redirect')

    mc('/by_id/:names', controller='byId', action='listing')

    mc('/:sort', controller='browse', sort='top', action='listing',
       requirements=dict(sort='top|controversial'))

    mc('/message/compose', controller='message', action='compose')
    mc('/message/messages/:mid', controller='message', action='listing',
       where="messages")
    mc('/message/:where', controller='message', action='listing')
    mc('/message/moderator/:subwhere', controller='message', action='listing',
       where='moderator')

    mc('/thanks', controller='forms', action="claim", secret='')
    mc('/thanks/:secret', controller='forms', action="claim")

    mc('/gold', controller='forms', action="gold", is_payment=False)
    mc('/gold/payment', controller='forms', action="gold", is_payment=True)
    mc('/gold/creditgild/:passthrough', controller='forms', action='creditgild')
    mc('/gold/thanks', controller='front', action='goldthanks')
    mc('/gold/subscription', controller='forms', action='subscription')
    mc('/gilding', controller='front', action='gilding')
    mc('/creddits', controller='redirect', action='redirect', 
       dest='/gold?goldtype=creddits')

    mc('/password', controller='forms', action="password")
    mc('/random', controller='front', action="random")
    mc('/:action', controller='embed',
       requirements=dict(action="blog"))
    mc('/help/gold', controller='redirect', action='redirect',
       dest='/gold/about')

    mc('/help/:page', controller='policies', action='policy_page',
       conditions={'function':not_in_sr},
       requirements={'page':'contentpolicy|privacypolicy|useragreement'})
    mc('/rules', controller='redirect', action='redirect',
        dest='/help/contentpolicy')
    mc('/faq', controller='redirect', action='redirect',
       dest='https://reddit.zendesk.com/')

    mc('/wiki/create/*page', controller='wiki', action='wiki_create')
    mc('/wiki/edit/*page', controller='wiki', action='wiki_revise')
    mc('/wiki/revisions', controller='wiki', action='wiki_recent')
    mc('/wiki/revisions/*page', controller='wiki', action='wiki_revisions')
    mc('/wiki/settings/*page', controller='wiki', action='wiki_settings')
    mc('/wiki/discussions/*page', controller='wiki', action='wiki_discussions')
    mc('/wiki/pages', controller='wiki', action='wiki_listing')

    mc('/api/wiki/edit', controller='wikiapi', action='wiki_edit')
    mc('/api/wiki/hide', controller='wikiapi', action='wiki_revision_hide')
    mc('/api/wiki/delete', controller='wikiapi', action='wiki_revision_delete')
    mc('/api/wiki/revert', controller='wikiapi', action='wiki_revision_revert')
    mc('/api/wiki/alloweditor/:act', controller='wikiapi',
       requirements=dict(act="del|add"), action='wiki_allow_editor')

    mc('/wiki/*page', controller='wiki', action='wiki_page')
    mc('/wiki/', controller='wiki', action='wiki_page')

    mc('/:action', controller='wiki', requirements=dict(action="help"))
    mc('/help/*page', controller='wiki', action='wiki_redirect')
    mc('/w/*page', controller='wiki', action='wiki_redirect')

    mc('/goto', controller='toolbar', action='goto')
    mc('/tb/:link_id', controller='front', action='link_id_redirect')
    mc('/toolbar/*frame', controller='toolbar', action='redirect')

    mc('/c/:comment_id', controller='front', action='comment_by_id')

    mc('/s/*urloid', controller='toolbar', action='s')
    # additional toolbar-related rules just above the catchall

    mc('/resetpassword/:key', controller='forms',
       action='resetpassword')
    mc('/verification/:key', controller='forms',
       action='verify_email')
    mc('/resetpassword', controller='forms',
       action='resetpassword')

    mc('/modify_hsts_grant', controller='front', action='modify_hsts_grant')

    mc('/post/:action/:url_user', controller='post',
       requirements=dict(action="login|reg"))
    mc('/post/:action', controller='post',
       requirements=dict(action="options|over18|unlogged_options|optout"
                         "|optin|login|reg|explore_settings"))

    mc('/api', controller='redirect', action='redirect', dest='/dev/api')
    mc('/api/distinguish/:how', controller='api', action="distinguish")
    mc('/api/spendcreddits', controller='ipn', action="spendcreddits")
    mc('/api/stripecharge/gold', controller='stripe', action='goldcharge')
    mc('/api/modify_subscription', controller='stripe',
       action='modify_subscription')
    mc('/api/cancel_subscription', controller='stripe',
       action='cancel_subscription')
    mc('/api/stripewebhook/gold/:secret', controller='stripe',
       action='goldwebhook')
    mc('/api/coinbasewebhook/gold/:secret', controller='coinbase',
       action='goldwebhook')
    mc('/api/rgwebhook/gold/:secret', controller='redditgifts',
       action='goldwebhook')
    mc('/api/ipn/:secret', controller='ipn', action='ipn')
    mc('/ipn/:secret', controller='ipn', action='ipn')
    mc('/api/:action/:url_user', controller='api',
       requirements=dict(action="login|register"))
    mc('/api/gadget/click/:ids', controller='api', action='gadget',
       type='click')
    mc('/api/gadget/:type', controller='api', action='gadget')
    mc('/api/zendeskreply', controller='mailgunwebhook', action='zendeskreply')
    mc('/api/:action', controller='promoteapi',
       requirements=dict(action=("promote|unpromote|edit_promo|ad_s3_callback|"
                                 "ad_s3_params|freebie|promote_note|update_pay|"
                                 "edit_campaign|delete_campaign|"
                                 "check_inventory|"
                                 "refund_campaign|terminate_campaign|"
                                 "review_fraud|create_promo|"
                                 "toggle_pause_campaign")))
    mc('/api/:action', controller='apiminimal',
       requirements=dict(action="new_captcha"))
    mc('/api/:type', controller='api',
       requirements=dict(type='wikibannednote|bannednote|mutednote'),
       action='relnote')

    # Route /api/multi here to prioritize it over the /api/:action rule
    mc("/api/multi", controller="multiapi", action="multi",
       conditions={"method": ["POST"]})

    mc('/api/:action', controller='api')
    
    mc('/api/recommend/sr/:srnames', controller='api',
       action='subreddit_recommendations')

    mc('/api/server_seconds_visibility', controller='api',
       action='server_seconds_visibility')

    mc("/api/multi/mine", controller="multiapi", action="my_multis")
    mc("/api/multi/user/:username", controller="multiapi", action="list_multis")
    mc("/api/multi/copy", controller="multiapi", action="multi_copy")
    mc("/api/multi/rename", controller="multiapi", action="multi_rename")
    mc("/api/multi/*multipath/r/:srname", controller="multiapi", action="multi_subreddit")
    mc("/api/multi/*multipath/description", controller="multiapi", action="multi_description")
    mc("/api/multi/*multipath", controller="multiapi", action="multi")
    mc("/api/filter/*multipath/r/:srname", controller="multiapi", action="multi_subreddit")
    mc("/api/filter/*multipath", controller="multiapi", action="multi")

    mc("/api/v1/:action", controller="oauth2frontend",
       requirements=dict(action="authorize"))
    mc("/api/v1/:action", controller="oauth2access",
       requirements=dict(action="access_token|revoke_token"))
    mc("/api/v1/:action", controller="apiv1scopes",
       requirements=dict(action="scopes"))
    mc("/api/v1/user/:username/trophies",
       controller="apiv1user", action="usertrophies")
    mc("/api/v1/:action", controller="apiv1login",
       requirements=dict(action="register|login"))
    mc("/api/v1/:action", controller="apiv1user")
    # Same controller/action as /prefs/friends
    mc("/api/v1/me/:where", controller="userlistlisting",
        action="user_prefs", requirements=dict(where="friends"))
    mc("/api/v1/me/:action", controller="apiv1user")
    mc("/api/v1/me/:action/:username", controller="apiv1user")

    mc("/api/v1/gold/gild/:fullname", controller="apiv1gold", action="gild")
    mc("/api/v1/gold/give/:username", controller="apiv1gold", action="give")

    mc('/dev', controller='redirect', action='redirect', dest='/dev/api')
    mc('/dev/api', controller='apidocs', action='docs')
    mc('/dev/api/:mode', controller='apidocs', action='docs',
       requirements=dict(mode="oauth"))

    mc("/button_info", controller="api", action="url_info", limit=1)

    mc('/captcha/:iden', controller='captcha', action='captchaimg')

    mc('/mediaembed/:link/:credentials',
       controller="mediaembed", action="mediaembed", credentials=None)

    mc('/code', controller='redirect', action='redirect',
       dest='http://github.com/reddit/')

    mc('/socialite', controller='redirect', action='redirect',
       dest='https://addons.mozilla.org/firefox/addon/socialite/')

    # Used for showing ads
    mc("/ads/", controller="ad", action="ad")

    mc("/try", controller="forms", action="try_compact")

    mc("/web/timings", controller="weblog", action="timings")

    mc("/web/log/:level", controller="weblog", action="message",
       requirements=dict(level="error"))

    mc("/web/poisoning", controller="weblog", action="report_cache_poisoning")

    # This route handles displaying the error page and
    # graphics used in the 404/500
    # error pages. It should likely stay at the top
    # to ensure that the error page is
    # displayed properly.
    mc('/error/document/:id', controller='error', action="document")

    # these should be near the buttom, because they should only kick
    # in if everything else fails. It's the attempted catch-all
    # reddit.com/http://... and reddit.com/34fr
    mc('/:link_id', controller='front', action='link_id_redirect',
       requirements=dict(link_id='[0-9a-z]{1,6}'))
    mc('/:urloid', controller='toolbar', action='s',
       requirements=dict(urloid=r'(\w+\.\w{2,}|https?).*'))

    mc("/*url", controller='front', action='catchall')

    return map
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.utils import tup


__all__ = ["MessageQueue", "declare_queues"]


class Queues(dict):
    """A container for queue declarations."""
    def __init__(self, queues):
        dict.__init__(self)
        self.__dict__ = self
        self.bindings = set()
        self.declare(queues)

    def __iter__(self):
        for name, queue in self.iteritems():
            if name != "bindings":
                yield queue

    def declare(self, queues):
        for name, queue in queues.iteritems():
            queue.name = name
            queue.bindings = self.bindings
            if queue.bind_to_self:
                queue._bind(name)
        self.update(queues)


class MessageQueue(object):
    """A representation of an AMQP message queue.

    This class is solely intended for use with the Queues class above.

    """
    def __init__(self, durable=True, exclusive=False,
                 auto_delete=False, bind_to_self=False):
        self.durable = durable
        self.exclusive = exclusive
        self.auto_delete = auto_delete
        self.bind_to_self = bind_to_self

    def _bind(self, routing_key):
        self.bindings.add((self.name, routing_key))

    def __lshift__(self, routing_keys):
        """Register bindings from routing keys to this queue."""
        routing_keys = tup(routing_keys)
        for routing_key in routing_keys:
            self._bind(routing_key)


def declare_queues(g):
    queues = Queues({
        "scraper_q": MessageQueue(bind_to_self=True),
        "newcomments_q": MessageQueue(),
        "commentstree_q": MessageQueue(bind_to_self=True),
        "commentstree_fastlane_q": MessageQueue(bind_to_self=True),
        "vote_link_q": MessageQueue(bind_to_self=True),
        "vote_comment_q": MessageQueue(bind_to_self=True),
        "cloudsearch_changes": MessageQueue(bind_to_self=True),
        "butler_q": MessageQueue(),
        "markread_q": MessageQueue(),
        "del_account_q": MessageQueue(),
        "automoderator_q": MessageQueue(),
        "event_collector": MessageQueue(bind_to_self=True),
        "event_collector_failed": MessageQueue(bind_to_self=True),
        "modmail_email_q": MessageQueue(bind_to_self=True),
        "author_query_q": MessageQueue(bind_to_self=True),
        "subreddit_query_q": MessageQueue(bind_to_self=True),
        "domain_query_q": MessageQueue(bind_to_self=True),
    })

    if g.shard_commentstree_queues:
        sharded_commentstree_queues = {"commentstree_%d_q" % i :
                                       MessageQueue(bind_to_self=True)
                                       for i in xrange(10)}
        queues.declare(sharded_commentstree_queues)

    if g.shard_author_query_queues:
        sharded_author_query_queues = {
            "author_query_%d_q" % i: MessageQueue(bind_to_self=True)
            for i in xrange(10)
        }
        queues.declare(sharded_author_query_queues)

    if g.shard_subreddit_query_queues:
        sharded_subreddit_query_queues = {
            "subreddit_query_%d_q" % i: MessageQueue(bind_to_self=True)
            for i in xrange(10)
        }
        queues.declare(sharded_subreddit_query_queues)

    if g.shard_domain_query_queues:
        sharded_domain_query_queues = {
            "domain_query_%d_q" % i: MessageQueue(bind_to_self=True)
            for i in xrange(10)
        }
        queues.declare(sharded_domain_query_queues)

    queues.cloudsearch_changes << "search_changes"
    queues.scraper_q << ("new_link", "link_text_edited")
    queues.newcomments_q << "new_comment"
    queues.butler_q << ("new_comment",
                        "comment_text_edited")
    queues.markread_q << "mark_all_read"
    queues.del_account_q << "account_deleted"
    queues.automoderator_q << (
        "auto_removed",
        "new_link",
        "new_comment",
        "new_media_embed",
        "new_report",
        "link_text_edited",
        "comment_text_edited",
    )
    queues.event_collector << "event_collector_test"

    return queues
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import tmpl_context as c

def api_type(subtype = ''):
    return 'api-' + subtype if subtype else 'api'

def is_api(subtype = ''):
    return c.render_style and c.render_style.startswith(api_type(subtype))

def get_api_subtype():
    if is_api() and c.render_style.startswith('api-'):
        return c.render_style[4:]

extension_mapping = {
    "rss": ("xml", "application/atom+xml; charset=UTF-8"),
    "xml": ("xml", "application/atom+xml; charset=UTF-8"),
    "js": ("js", "text/javascript; charset=UTF-8"),
    "embed": ("htmllite", "text/javascript; charset=UTF-8"),
    "mobile": ("mobile", "text/html; charset=UTF-8"),
    "png": ("png", "image/png"),
    "css": ("css", "text/css"),
    "csv": ("csv", "text/csv; charset=UTF-8"),
    "api": (api_type(), "application/json; charset=UTF-8"),
    "json-html": (api_type("html"), "application/json; charset=UTF-8"),
    "json-compact": (api_type("compact"), "application/json; charset=UTF-8"),
    "compact": ("compact", "text/html; charset=UTF-8"),
    "json": (api_type(), "application/json; charset=UTF-8"),
    "i": ("compact", "text/html; charset=UTF-8"),
}

API_TYPES = ('api', 'json')
RSS_TYPES = ('rss', 'xml')

def set_extension(environ, ext):
    environ["extension"] = ext
    environ["render_style"], environ["content_type"] = extension_mapping[ext]
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import mimetypes

from mako.lookup import TemplateLookup
from pylons.error import handle_mako_error
from pylons.configuration import PylonsConfig

import r2.lib.helpers
from r2.config.paths import (
    get_r2_path,
    get_built_statics_path,
    get_raw_statics_path,
)
from r2.config.routing import make_map
from r2.lib.app_globals import Globals
from r2.lib.configparse import ConfigValue


mimetypes.init()


def load_environment(global_conf={}, app_conf={}, setup_globals=True):
    r2_path = get_r2_path()
    root_path = os.path.join(r2_path, 'r2')

    paths = {
        'root': root_path,
        'controllers': os.path.join(root_path, 'controllers'),
        'templates': [os.path.join(root_path, 'templates')],
    }

    if ConfigValue.bool(global_conf.get('uncompressedJS')):
        paths['static_files'] = get_raw_statics_path()
    else:
        paths['static_files'] = get_built_statics_path()

    config = PylonsConfig()

    config.init_app(global_conf, app_conf, package='r2', paths=paths)

    # don't put action arguments onto c automatically
    config['pylons.c_attach_args'] = False

    # when accessing non-existent attributes on c, return "" instead of dying
    config['pylons.strict_tmpl_context'] = False

    g = Globals(config, global_conf, app_conf, paths)
    config['pylons.app_globals'] = g

    if setup_globals:
        config['r2.import_private'] = \
            ConfigValue.bool(global_conf['import_private'])
        g.setup()
        g.plugins.declare_queues(g.queues)

    g.plugins.load_plugins(config)
    config['r2.plugins'] = g.plugins
    g.startup_timer.intermediate("plugins")

    config['pylons.h'] = r2.lib.helpers
    config['routes.map'] = make_map(config)

    #override the default response options
    config['pylons.response_options']['headers'] = {}

    # when mako loads a previously compiled template file from its cache, it
    # doesn't check that the original template path matches the current path.
    # in the event that a new plugin defines a template overriding a reddit
    # template, unless the mtime newer, mako doesn't update the compiled
    # template. as a workaround, this makes mako store compiled templates with
    # the original path in the filename, forcing it to update with the path.
    if "cache_dir" in app_conf:
        module_directory = os.path.join(app_conf['cache_dir'], 'templates')

        def mako_module_path(filename, uri):
            filename = filename.lstrip('/').replace('/', '-')
            path = os.path.join(module_directory, filename + ".py")
            return os.path.abspath(path)
    else:
        # disable caching templates since we don't know where they should go.
        module_directory = mako_module_path = None

    # set up the templating system
    config["pylons.app_globals"].mako_lookup = TemplateLookup(
        directories=paths["templates"],
        error_handler=handle_mako_error,
        module_directory=module_directory,
        input_encoding="utf-8",
        default_filters=["conditional_websafe"],
        filesystem_checks=getattr(g, "reload_templates", False),
        imports=[
            "from r2.lib.filters import websafe, unsafe, conditional_websafe",
            "from pylons import request",
            "from pylons import tmpl_context as c",
            "from pylons import app_globals as g",
            "from pylons.i18n import _, ungettext",
        ],
        modulename_callable=mako_module_path,
    )

    if setup_globals:
        g.setup_complete()

    return config
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os.path


def get_r2_path():
    # we know this file is at r2/r2/config/paths.py
    this_path = os.path.abspath(__file__)
    # walk up 3 directories to r2
    r2_path = os.path.dirname(os.path.dirname(os.path.dirname(this_path)))
    return r2_path


def get_built_statics_path():
    """Return the path for built (compiled/compressed) statics."""
    r2_path = get_r2_path()
    return os.path.join(r2_path, 'build', 'public')


def get_raw_statics_path():
    """Return the path for the raw (under version control) statics"""
    r2_path = get_r2_path()
    return os.path.join(r2_path, 'r2', 'public')
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


def register_hooks():
    """Register all known non-plugin hooks. Called on app setup."""
    from r2.config.feature.feature import feature_hooks
    feature_hooks.register_all()

    from r2.models.admintools import admintools_hooks
    admintools_hooks.register_all()

    from r2.models.account import trylater_hooks
    trylater_hooks.register_all()

    from r2.models import subreddit
    subreddit.trylater_hooks.register_all()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import logging
import json
import hashlib

from pylons import tmpl_context as c
from pylons import app_globals as g


class FeatureState(object):
    """A FeatureState is the state of a feature and its condition in the world.

    It determines if this feature is enabled given the world provided.
    """

    # Special values for globally enabled properties - no need to interrogate
    # the world for these values.
    GLOBALLY_ON = "on"
    GLOBALLY_OFF = "off"

    # constant config blocks
    DISABLED_CFG = {"enabled": GLOBALLY_OFF}
    ENABLED_CFG = {"enabled": GLOBALLY_ON}

    # The number of buckets to use for any bucketing operations.  Should always
    # be evenly divisible by 100.  Each factor of 10 over 100 gives us an
    # additional digit of precision.
    NUM_BUCKETS = 1000

    # The variant definition for control groups that are added by default.
    DEFAULT_CONTROL_GROUPS = {'control_1': 10, 'control_2': 10}

    def __init__(self, name, world, config_name=None, config_str=None):
        self.name = name
        self.world = world
        self.config = self._parse_config(name, config_name, config_str)

    def _parse_config(self, name, config_name=None, config_str=None):
        """Find and parse a config from our live config with this given name.

        :param name string - a given feature name
        :return dict - a dictionary with at least "enabled". May include more
                       depending on the enabled type.
        """
        if not config_name:
            config_name = "feature_%s" % name

        if not config_str:
            config_str = self.world.live_config(config_name)

        if not config_str or config_str == FeatureState.GLOBALLY_OFF:
            return self.DISABLED_CFG

        if config_str == FeatureState.GLOBALLY_ON:
            return self.ENABLED_CFG

        try:
            config = json.loads(config_str)
        except (ValueError, TypeError) as e:
            g.log.warning("Could not load config for name %r - %r",
                          config_name, e)
            return self.DISABLED_CFG

        if not isinstance(config, dict):
            g.log.warning("Config not dict, on or off: %r", config_name)
            return self.DISABLED_CFG

        return config

    @staticmethod
    def get_all(world):
        """Return FeatureState objects for all features in live_config.

        Creates a FeatureState object for every config entry prefixed with
        "feature_".

        :param world - World proxy object to the app/request state.
        """
        features = []
        for (key, config_str) in world.live_config_iteritems():
            if key.startswith('feature_'):
                feature_state = FeatureState(key[8:], world, key, config_str)
                features.append(feature_state)
        return features

    @staticmethod
    def is_user_experiment(experiment):
        return not FeatureState.is_page_experiment(experiment)

    @staticmethod
    def is_page_experiment(experiment):
        return experiment.get('page')

    def _calculate_bucket(self, seed, experiment_seed=None):
        """Sort something into one of self.NUM_BUCKETS buckets.

        :param seed -- a string used for shifting the deterministic bucketing
                       algorithm.  In most cases, this will be an Account's
                       _fullname.
        :return int -- a bucket, 0 <= bucket < self.NUM_BUCKETS
        """
        # Mix the feature name in with the seed so the same users don't get
        # selected for ramp-ups for every feature.
        hashed = hashlib.sha1(self.name + seed)
        bucket = long(hashed.hexdigest(), 16) % self.NUM_BUCKETS
        return bucket

    @classmethod
    def _choose_variant(cls, bucket, variants):
        """Deterministically choose a percentage-based variant.

        The algorithm satisfies two conditions:

        1. It's deterministic (that is, every call with the same bucket and
           variants will result in the same answer).
        2. An increase in any of the variant percentages will keep the same
           buckets in the same variants as at the smaller percentage (that is,
           all buckets previously put in variant A will still be in variant A,
           all buckets previously put in variant B will still be in variant B,
           etc. and the increased percentages will be made of up buckets
           previously not assigned to a bucket).

        These attributes make it suitable for use in A/B experiments that may
        see an increase in their variant percentages post-enabling.

        :param bucket -- an integer bucket representation
        :param variants -- a dictionary of
                           <string:variant name>:<float:percentage> pairs.  If
                           any percentage exceeds 1/n percent, where n is the
                           number of variants, the percentage will be capped to
                           1/n.  These variants will be added to
                           DEFAULT_CONTROL_GROUPS to create the effective
                           variant set.
        :return string -- the variant name, or None if bucket doesn't fall into
                          any of the variants
        """
        # We want to always include two control groups, but allow overriding of
        # their percentages.
        all_variants = dict(cls.DEFAULT_CONTROL_GROUPS)
        all_variants.update(variants)

        # Say we have an experiment with two new things we're trying out for 2%
        # of users (A and B), a control group with 5% (C), and a pool of
        # excluded users (x).  The buckets will be assigned like so:
        #
        #     A B C A B C x x C x x C x x C x x x x x x x x x...
        #
        # This scheme allows us to later increase the size of A and B to 7%
        # while keeping the experience consistent for users in any group other
        # than excluded users:
        #
        #     A B C A B C A B C A B C A B C A B x A B x x x x...
        #
        # Rather than building this entire structure out in memory, we can use
        # a little bit of math to figure out just the one bucket's value.
        num_variants = len(all_variants)
        variant_names = sorted(all_variants.keys())
        # If the variants took up the entire set of buckets, which bucket would
        # we be in?
        candidate_variant = variant_names[bucket % num_variants]
        # Log a warning if this variant is capped, to help us prevent user (us)
        # error.  It's not the most correct to only check the one, but it's
        # easy and quick, and anything with that high a percentage should be
        # selected quite often.
        variant_fraction = all_variants[candidate_variant] / 100.0
        variant_cap = 1.0 / num_variants
        if variant_fraction > variant_cap:
            g.log.warning(
                'Variant %s exceeds allowable percentage (%.2f > %.2f)',
                candidate_variant,
                variant_fraction,
                variant_cap,
            )
        # Variant percentages are expressed as numeric percentages rather than
        # a fraction of 1 (that is, 1.5 means 1.5%, not 150%); thus, at 100
        # buckets, buckets and percents map 1:1 with each other.  Since we may
        # have more than 100 buckets (causing each bucket to represent less
        # than 1% each), we need to scale up how far "right" we move for each
        # variant percent.
        bucket_multiplier = cls.NUM_BUCKETS / 100
        # Now check to see if we're far enough left to be included in the
        # variant percentage.
        if bucket < (all_variants[candidate_variant] * num_variants *
                     bucket_multiplier):
            return candidate_variant
        else:
            return None

    @classmethod
    def _is_variant_enabled(cls, variant):
        """Determine if a variant is "enabled", as returned by is_enabled."""
        # The excluded experimental group will have a `None` variant and
        # this feature should be disabled.
        # For users in control groups, the feature is considered "not
        # enabled" because they should get the same behavior as ineligible
        # users.
        return (
            variant is not None and
            variant not in cls.DEFAULT_CONTROL_GROUPS
        )

    def is_enabled(self, user=None, subreddit=None, subdomain=None,
                   oauth_client=None):
        """Determine if a feature is enabled.

        For experiments, this induces a bucketing event by calling
        self._is_experiment_enabled.
        """
        cfg = self.config
        kw = dict(
            user=user,
            subreddit=subreddit,
            subdomain=subdomain,
            oauth_client=oauth_client
        )
        # first, test if the config would be enabled without an experiment
        if self._is_config_enabled(cfg, **kw):
            return True

        # next, test if the config is enabled fractionally
        if self._is_percent_enabled(cfg, user=user):
            return True

        # lastly, check experiment
        experiment = self.config.get('experiment')
        if self._is_config_enabled(experiment, **kw):
            return self._is_experiment_enabled(experiment, user=user)

        # Unknown value, default to off.
        return False

    def _is_config_enabled(
        self, cfg, user=None, subreddit=None, subdomain=None,
        oauth_client=None
    ):
        world = self.world

        if not cfg:
            return False

        if cfg.get('enabled') == self.GLOBALLY_ON:
            return True

        if cfg.get('enabled') == self.GLOBALLY_OFF:
            return False

        url_flag = cfg.get('url')
        if url_flag:
            if isinstance(url_flag, dict):
                for feature in world.url_features():
                    if feature in url_flag:
                        return self._is_variant_enabled(url_flag[feature])
            elif url_flag in world.url_features():
                return True

        if cfg.get('admin') and world.is_admin(user):
            return True

        if cfg.get('employee') and world.is_employee(user):
            return True

        if cfg.get('beta') and world.user_has_beta_enabled(user):
            return True

        if cfg.get('gold') and world.has_gold(user):
            return True

        loggedin = world.is_user_loggedin(user)
        if cfg.get('loggedin') and loggedin:
            return True

        if cfg.get('loggedout') and not loggedin:
            return True

        users = [u.lower() for u in cfg.get('users', [])]
        if users and user and user.name.lower() in users:
            return True

        subreddits = [s.lower() for s in cfg.get('subreddits', [])]
        if subreddits and subreddit and subreddit.lower() in subreddits:
            return True

        subdomains = [s.lower() for s in cfg.get('subdomains', [])]
        if subdomains and subdomain and subdomain.lower() in subdomains:
            return True

        clients = set(cfg.get('oauth_clients', []))
        if clients and oauth_client and oauth_client in clients:
            return True

    def _is_percent_enabled(self, cfg, user=None):
        loggedin = self.world.is_user_loggedin(user)
        percent_loggedin = cfg.get('percent_loggedin', 0)
        if percent_loggedin and loggedin:
            bucket = self._calculate_bucket(user._fullname)
            scaled_percent = bucket / (self.NUM_BUCKETS / 100)
            if scaled_percent < percent_loggedin:
                return True

        percent_loggedout = cfg.get('percent_loggedout', 0)
        if percent_loggedout and not loggedin:
            # We want this to match the JS function for bucketing loggedout
            # users, and JS doesn't make it easy to mix the feature name in
            # with the LOID. Just look at the last 4 chars of the LOID.
            loid = self.world.current_loid()
            if loid:
                try:
                    bucket = int(loid[-4:], 36) % 100
                    if bucket < percent_loggedout:
                        return True
                except ValueError:
                    pass

    def _is_experiment_enabled(self, experiment, user=None):
        """ Determine if there's an active variant of the specified experiment
        for the current user.

        Sends a bucketing event.
        """
        if not experiment.get('enabled', True):
            return False

        variant = None
        if FeatureState.is_user_experiment(experiment):
            variant = self._get_user_experiment_variant(experiment, user)
        elif FeatureState.is_page_experiment(experiment):
            content_id, _ = FeatureState.get_content_id()
            variant = self._get_page_experiment_variant(experiment)

        # We only want to send this event once per request, because that's
        # an easy way to get rid of extraneous events.
        if not c.have_sent_bucketing_event:
            c.have_sent_bucketing_event = set()

        if variant is not None and self.world.valid_experiment_request():
            if FeatureState.is_user_experiment(experiment):
                loid = self.world.current_loid()
                if self.world.is_user_loggedin(user):
                    bucketing_id = user._id
                else:
                    bucketing_id = loid

                key = ('user', self.name, bucketing_id)

                if (
                    g.running_as_script or
                    key not in c.have_sent_bucketing_event
                ):
                    g.events.bucketing_event(
                        experiment_id=experiment.get('experiment_id'),
                        experiment_name=self.name,
                        variant=variant,
                        user=user,
                        loid=self.world.current_loid_obj(),
                    )
                    c.have_sent_bucketing_event.add(key)
            else:
                # This is a page experiment, so we know we have a content_id
                key = ('page', self.name, content_id)
                if (
                    g.running_as_script or
                    key not in c.have_sent_bucketing_event
                ):
                    g.events.page_bucketing_event(
                        experiment_id=experiment.get('experiment_id'),
                        experiment_name=self.name,
                        variant=variant,
                        content_id=content_id,
                        request=request,
                        context=c,
                    )
                    c.have_sent_bucketing_event.add(key)

        return self._is_variant_enabled(variant)

    def variant(self, user):
        """ Determine which variant of this experiment, if any, is active.

        Does not send a bucketing event.
        """
        url_flag = self.config.get('url')
        # We only care about the dict-type 'url_flag's, since those are the
        # only ones that can specify a variant.
        if url_flag and isinstance(url_flag, dict):
            for feature in self.world.url_features():
                try:
                    return url_flag[feature]
                except KeyError:
                    pass

        experiment = self.config.get('experiment')
        if not experiment:
            return None

        if FeatureState.is_user_experiment(experiment):
            return self._get_user_experiment_variant(experiment, user)
        return self._get_page_experiment_variant(experiment)

    def _get_user_experiment_variant(self, experiment, user):
        # for logged in users, bucket based on the User's fullname
        if self.world.is_user_loggedin(user):
            bucket = self._calculate_bucket(user._fullname)
        # for logged out users, bucket based on the loid if we have one
        elif g.enable_loggedout_experiments:
            loid = self.world.current_loid()
            # we can't run an experiment if we have no id to vary on.
            if not loid:
                return None
            bucket = self._calculate_bucket(loid)
        # if logged out experiments are disabled, bail.
        else:
            return None

        variant = self._choose_variant(bucket, experiment.get('variants', {}))
        return variant

    @staticmethod
    def get_content_id():
        from r2.lib import utils
        thing = utils.url_to_thing(request.fullurl)

        if not thing:
            return None, None

        content_id = None
        type_name = getattr(thing, '_type_name', None)
        if type_name == 'comment':
            # We use the parent link for comment permalink pages, since
            # they share a canonical URL
            link = getattr(thing, 'link', thing.link_slow)
            content_id = link._fullname
        elif type_name == 'link':
            content_id = thing._fullname
        elif type_name == 'subreddit':
            content_id = thing._fullname
        return content_id, type_name

    def _get_page_experiment_variant(self, experiment):
        content_id, type_name = FeatureState.get_content_id()

        if content_id is None:
            return None

        # If we've restricted the experiment to certain page types, make sure
        # the request is for one of those
        if (experiment.get('subreddit_only', False) and
                type_name != 'subreddit'):
            return None

        if (experiment.get('link_only', False) and
                (type_name != 'link' and type_name != 'comment')):
            # We treat comment permalink pages like general comments pages
            return None

        experiment_seed = experiment.get('experiment_seed', None)
        bucket = self._calculate_bucket(content_id, experiment_seed)
        variant = self._choose_variant(bucket, experiment.get('variants', {}))
        return variant
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.config.feature.feature import is_enabled, variant, all_enabled
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g


class World(object):
    """A World is the proxy to the app/request state for Features.

    Proxying through World allows for easy testing and caching if needed.
    """

    @staticmethod
    def stacked_proxy_safe_get(stacked_proxy, key, default=None):
        """Get a field from a StackedObjectProxy

        Always succeeds, even if the proxy has not yet been initialized.
        Normally, if the proxy hasn't been initialized, a `TypeError` is
        raised to indicate a programming error. To avoid crashing on feature
        checks that are done too early (e.g., during initial DB set-up of
        the pylons environment), this function will instead return `default`
        for an uninitialized proxy.

        (Initialized proxies ALWAYS return a value, either a set value
        or an empty string)

        """
        try:
            return getattr(stacked_proxy, key)
        except TypeError:
            return default

    def current_user(self):
        if c.user_is_loggedin:
            return self.stacked_proxy_safe_get(c, 'user')

    def current_subreddit(self):
        site = self.stacked_proxy_safe_get(c, 'site')
        if not site:
            # In non-request code (eg queued jobs), there isn't necessarily a
            # site name (or other request-type data).  In those cases, we don't
            # want to trigger any subreddit-specific code.
            return ''
        return site.name

    def current_subdomain(self):
        return self.stacked_proxy_safe_get(c, 'subdomain')

    def current_oauth_client(self):
        client = self.stacked_proxy_safe_get(c, 'oauth2_client', None)
        return getattr(client, '_id', None)

    def current_loid_obj(self):
        return self.stacked_proxy_safe_get(c, 'loid')

    def current_loid(self):
        loid = self.current_loid_obj()
        if not loid:
            return None
        return loid.loid

    def is_admin(self, user):
        if not user or not hasattr(user, 'name'):
            return False

        return user.name in self.stacked_proxy_safe_get(g, 'admins', [])

    def is_employee(self, user):
        if not user:
            return False
        return user.employee

    def user_has_beta_enabled(self, user):
        if not user:
            return False
        return user.pref_beta

    def has_gold(self, user):
        if not user:
            return False

        return user.gold

    def is_user_loggedin(self, user):
        if not (user or self.current_user()):
            return False
        return True

    def url_features(self):
        return set(request.GET.getall('feature'))

    def live_config(self, name):
        live = self.stacked_proxy_safe_get(g, 'live_config', {})
        return live.get(name)

    def live_config_iteritems(self):
        live = self.stacked_proxy_safe_get(g, 'live_config', {})
        return live.iteritems()

    def simple_event(self, name):
        stats = self.stacked_proxy_safe_get(g, 'stats', None)
        if stats:
            return stats.simple_event(name)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.config.feature.state import FeatureState
from r2.config.feature.world import World
from r2.lib.hooks import HookRegistrar

feature_hooks = HookRegistrar()

_world = World()
_featurestate_cache = {}


def is_enabled(name, user=None, subreddit=None):
    """Test and return whether a given feature is enabled for this request.

    If `feature` is not found, returns False.

    The optional arguments allow overriding that you generally don't want, but
    is useful outside of request contexts - cron jobs and the like.

    :param name string - a given feature name
    :param user - (optional) an Account
    :param subreddit - (optional) a Subreddit
    :return bool
    """
    if not user:
        user = _world.current_user()
    if not subreddit:
        subreddit = _world.current_subreddit()
    subdomain = _world.current_subdomain()
    oauth_client = _world.current_oauth_client()

    return _get_featurestate(name).is_enabled(
        user=user,
        subreddit=subreddit,
        subdomain=subdomain,
        oauth_client=oauth_client,
    )

def variant(name, user=None):
    """Return which variant of an experiment a user is part of.

    If the experiment is not found, has no variants, or the user is not part of
    any of them (control), return None.

    :param name string - an experiment (feature) name
    :param user - (optional) an Account.  Defaults to the currently signed in
                  user.
    :return string, or None if not part of an experiment
    """
    if not user:
        user = _world.current_user()

    return _get_featurestate(name).variant(user)

def all_enabled(user=None):
    """Return a list of enabled features and experiments for the user.
    
    Provides the user's assigned variant and the experiment ID for experiments.

    This does not trigger bucketing events, so it should not be used for
    feature flagging purposes on the server. It is meant to let clients
    condition features on experiment variants. Those clients should manually
    send the appropriate bucketing events.

    This does not include page-based experiments, which operate independently
    of the particular user.

    :param user - (optional) an Account. Defaults to None, for which we
                  determine logged-out features.
    :return dict - a dictionary mapping enabled feature keys to True or to the
                   experiment/variant information
    """
    features = FeatureState.get_all(_world)

    # Get enabled features and experiments
    active = {}
    for feature in features:
        experiment = feature.config.get('experiment')
        # Exclude page experiments
        if experiment and FeatureState.is_user_experiment(experiment):
            # Get experiment names, ids, and assigned variants, leaving out
            # experiments for which this user is excluded
            variant = feature.variant(user)
            if variant:
                active[feature.name] = {
                    'experiment_id': experiment.get('experiment_id'),
                    'variant': variant
                }
        elif feature.is_enabled(user):
                active[feature.name] = True

    return active

@feature_hooks.on('worker.live_config.update')
def clear_featurestate_cache():
    global _featurestate_cache
    _featurestate_cache = {}


def _get_featurestate(name):
    """Get a FeatureState object for this feature, creating it if necessary.

    :param name string - a given feature name
    :return FeatureState
    """

    featurestate = _featurestate_cache.get(name, None)
    if featurestate is None:
        featurestate = FeatureState(name, _world)
        _featurestate_cache[name] = featurestate

    return featurestate
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import calendar
from collections import defaultdict

from utils import to36, tup, iters
from wrapped import Wrapped, StringTemplate, CacheStub, Templated
from mako.template import Template
from r2.config import feature
from r2.config.extensions import get_api_subtype
from r2.lib import hooks
from r2.lib.filters import spaceCompress, safemarkdown, _force_unicode
from r2.models import (
    Account,
    Comment,
    Link,
    Report,
    Subreddit,
    SubredditUserRelations,
    Trophy,
)
from r2.models.token import OAuth2Scope, extra_oauth2_scope
import time, pytz
from pylons import response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _

from r2.models.wiki import ImagesByWikiPage


def make_typename(typ):
    return 't%s' % to36(typ._type_id)

def make_fullname(typ, _id):
    return '%s_%s' % (make_typename(typ), to36(_id))


class ObjectTemplate(StringTemplate):
    def __init__(self, d):
        self.d = d

    def update(self, kw):
        def _update(obj):
            if isinstance(obj, (str, unicode)):
                return _force_unicode(obj)
            elif isinstance(obj, dict):
                return dict((k, _update(v)) for k, v in obj.iteritems())
            elif isinstance(obj, (list, tuple)):
                return map(_update, obj)
            elif isinstance(obj, CacheStub) and kw.has_key(obj.name):
                return kw[obj.name]
            else:
                return obj
        res = _update(self.d)
        return ObjectTemplate(res)

    def finalize(self, kw = {}):
        return self.update(kw).d


class JsonTemplate(Template):
    def __init__(self): pass

    def render(self, thing = None, *a, **kw):
        return ObjectTemplate({})


class TakedownJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return thing.explanation


class ThingTemplate(object):
    @classmethod
    def render(cls, thing):
        """
        Return a JSON representation of a Wrapped Thing object.

        The Thing object should be Wrapped and been run through add_props just
        like is required for regular HTML rendering. The return value is an
        ObjectTemplate wrapped dictionary.

        """

        api_subtype = get_api_subtype()

        # the argument is named `thing` due to specifics of wrapped
        item = thing

        if api_subtype:
            # special handling for rendering a nested template as a different
            # style (usually html)
            data = cls.get_rendered(item, render_style=api_subtype)
        else:
            data = cls.get_json(item)

        d = {
            "kind": cls.get_kind(item),
            "data": data,
        }
        return ObjectTemplate(d)

    @classmethod
    def get_kind(cls, item):
        thing = item.lookups[0]
        return make_typename(thing.__class__)

    @classmethod
    def get_json(cls, item):
        data = {
            "created": time.mktime(item._date.timetuple()),
            "created_utc": time.mktime(
                item._date.astimezone(pytz.UTC).timetuple()) - time.timezone,
            "id": item._id36,
            "name": item._fullname,
        }
        return data

    @classmethod
    def get_rendered(cls, item, render_style):
        data = {
            "id": item._fullname,
            "content": item.render(style=render_style),
        }
        return data


class ThingJsonTemplate(JsonTemplate):
    _data_attrs_ = dict(
        created="created",
        created_utc="created_utc",
        id="_id36",
        name="_fullname",
    )

    @classmethod
    def data_attrs(cls, **kw):
        d = cls._data_attrs_.copy()
        d.update(kw)
        return d
    
    def kind(self, wrapped):
        """
        Returns a string literal which identifies the type of this
        thing.  For subclasses of Thing, it will be 't's + kind_id.
        """
        _thing = wrapped.lookups[0] if isinstance(wrapped, Wrapped) else wrapped
        return make_typename(_thing.__class__)

    def rendered_data(self, thing):
        """
        Called only when get_api_type is non-None (i.e., a JSON
        request has been made with partial rendering of the object to
        be returned)

        Canonical Thing data representation for JS, which is currently
        a dictionary of three elements (translated into a JS Object
        when sent out).  The elements are:

         * id : Thing _fullname of thing.
         * content : rendered  representation of the thing by
           calling render on it using the style of get_api_subtype().
        """
        res =  dict(id = thing._fullname,
                    content = thing.render(style=get_api_subtype()))
        return res

    def raw_data(self, thing):
        """
        Complement to rendered_data.  Called when a dictionary of
        thing data attributes is to be sent across the wire.
        """
        attrs = dict(self._data_attrs_)
        if hasattr(self, "_optional_data_attrs"):
            for attr, attrv in self._optional_data_attrs.iteritems():
                if hasattr(thing, attr):
                    attrs[attr] = attrv

        return dict((k, self.thing_attr(thing, v))
                    for k, v in attrs.iteritems())

    def thing_attr(self, thing, attr):
        """
        For the benefit of subclasses, to lookup attributes which may
        require more work than a simple getattr (for example, 'author'
        which has to be gotten from the author_id attribute on most
        things).
        """
        if attr == "author":
            if thing.author._deleted:
                return "[deleted]"
            return thing.author.name

        if attr == "created":
            return time.mktime(thing._date.timetuple())
        elif attr == "created_utc":
            return (time.mktime(thing._date.astimezone(pytz.UTC).timetuple())
                    - time.timezone)
        elif attr == "child":
            child = getattr(thing, "child", None)
            if child:
                return child.render()
            else:
                return ""

        if attr == 'distinguished':
            distinguished = getattr(thing, attr, 'no')
            if distinguished == 'no':
                return None
            return distinguished

        return getattr(thing, attr, None)

    def data(self, thing):
        if get_api_subtype():
            return self.rendered_data(thing)
        else:
            return self.raw_data(thing)

    def render(self, thing = None, action = None, *a, **kw):
        return ObjectTemplate(dict(kind = self.kind(thing),
                                   data = self.data(thing)))

class SubredditJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        accounts_active="accounts_active_count",
        banner_img="banner_img",
        banner_size="banner_size",
        collapse_deleted_comments="collapse_deleted_comments",
        comment_score_hide_mins="comment_score_hide_mins",
        community_rules="community_rules",
        description="description",
        description_html="description_html",
        display_name="name",
        header_img="header",
        header_size="header_size",
        header_title="header_title",
        icon_img="icon_img",
        icon_size="icon_size",
        # key_color="key_color",
        lang="lang",
        over18="over_18",
        public_description="public_description",
        public_description_html="public_description_html",
        public_traffic="public_traffic",
        # related_subreddits="related_subreddits",
        hide_ads="hide_ads",
        quarantine="quarantine",
        show_media="show_media",
        show_media_preview="show_media_preview",
        submission_type="link_type",
        submit_link_label="submit_link_label",
        submit_text_label="submit_text_label",
        submit_text="submit_text",
        submit_text_html="submit_text_html",
        subreddit_type="type",
        subscribers="_ups",
        suggested_comment_sort="suggested_comment_sort",
        title="title",
        url="path",
        user_is_banned="is_banned",
        user_is_muted="is_muted",
        user_is_contributor="is_contributor",
        user_is_moderator="is_moderator",
        user_is_subscriber="is_subscriber",
        user_sr_theme_enabled="user_sr_style_enabled",
        wiki_enabled="wiki_enabled",
    )

    # subreddit *attributes* (right side of the equals)
    # that are accessible even if the user can't view the subreddit
    _public_attrs = {
        "_id36",
        # subreddit ID with prefix
        "_fullname",
        # Creation date
        "created",
        "created_utc",
        # Canonically-cased subreddit name
        "name",
        # Canonical subreddit URL, relative to reddit.com
        "path",
        # Text shown on the access denied page
        "public_description",
        "public_description_html",
        # Title shown in search
        "title",
        # Type of subreddit, so people know that it's private
        "type",
    }

    def raw_data(self, thing):
        data = ThingJsonTemplate.raw_data(self, thing)

        # remove this when feature is enabled and use _data_attrs instead
        if feature.is_enabled('mobile_settings'):
            data['key_color'] = self.thing_attr(thing, 'key_color')
        if feature.is_enabled('related_subreddits'):
            data['related_subreddits'] = self.thing_attr(thing, 'related_subreddits')

        permissions = getattr(thing, 'mod_permissions', None)
        if permissions:
            permissions = [perm for perm, has in permissions.iteritems() if has]
            data['mod_permissions'] = permissions

        return data

    def thing_attr(self, thing, attr):
        if attr not in self._public_attrs and not thing.can_view(c.user):
            return None

        if (attr == "_ups" and
                (thing.hide_subscribers or thing.hide_num_users_info)):
            return 0
        elif attr == 'description_html':
            return safemarkdown(thing.description)
        elif attr == 'public_description_html':
            return safemarkdown(thing.public_description)
        elif attr == "is_moderator":
            if c.user_is_loggedin:
                return thing.moderator
            return None
        elif attr == "is_contributor":
            if c.user_is_loggedin:
                return thing.contributor
            return None
        elif attr == "is_subscriber":
            if c.user_is_loggedin:
                return thing.subscriber
            return None
        elif attr == 'is_banned':
            if c.user_is_loggedin:
                return thing.banned
            return None
        elif attr == 'is_muted':
            if c.user_is_loggedin:
                return thing.muted
            return None
        elif attr == 'submit_text_html':
            return safemarkdown(thing.submit_text)
        elif attr == 'user_sr_style_enabled':
            if c.user_is_loggedin:
                return c.user.use_subreddit_style(thing)
            else:
                return True
        elif attr == 'wiki_enabled':
            is_admin_or_mod = c.user_is_loggedin and (
                c.user_is_admin or thing.is_moderator_with_perms(c.user, 'wiki')
            )

            return thing.wikimode == 'anyone' or (thing.wikimode == 'modonly' and is_admin_or_mod)
        else:
            return ThingJsonTemplate.thing_attr(self, thing, attr)


class LabeledMultiDescriptionJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        body_html="description_html",
        body_md="description_md",
    )

    def kind(self, wrapped):
        return "LabeledMultiDescription"

    def thing_attr(self, thing, attr):
        if attr == "description_html":
            # if safemarkdown is passed a falsy string it returns None :/
            description_html = safemarkdown(thing.description_md) or ''
            return description_html
        else:
            return ThingJsonTemplate.thing_attr(self, thing, attr)


class LabeledMultiJsonTemplate(LabeledMultiDescriptionJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        can_edit="can_edit",
        copied_from="copied_from",
        description_html="description_html",
        description_md="description_md",
        display_name="display_name",
        key_color="key_color",
        icon_name="icon_id",
        icon_url="icon_url",
        name="name",
        path="path",
        subreddits="srs",
        visibility="visibility",
        weighting_scheme="weighting_scheme",
    )
    del _data_attrs_["id"]

    def __init__(self, expand_srs=False):
        super(LabeledMultiJsonTemplate, self).__init__()
        self.expand_srs = expand_srs

    def kind(self, wrapped):
        return "LabeledMulti"

    @classmethod
    def sr_props(cls, thing, srs, expand=False):
        sr_props = dict(thing.sr_props)
        if expand:
            sr_dicts = get_trimmed_sr_dicts(srs, c.user)
            for sr in srs:
                sr_props[sr._id]["data"] = sr_dicts[sr._id]
        return [dict(sr_props[sr._id], name=sr.name) for sr in srs]

    def thing_attr(self, thing, attr):
        if attr == "srs":
            return self.sr_props(thing, thing.srs, expand=self.expand_srs)
        elif attr == "can_edit":
            return c.user_is_loggedin and thing.can_edit(c.user)
        elif attr == "copied_from":
            if thing.can_edit(c.user):
                return thing.copied_from
            else:
                return None
        elif attr == "display_name":
            return thing.display_name or thing.name
        else:
            super_ = super(LabeledMultiJsonTemplate, self)
            return super_.thing_attr(thing, attr)


def get_trimmed_sr_dicts(srs, user):
    if c.user_is_loggedin:
        sr_user_relations = Subreddit.get_sr_user_relations(user, srs)
    else:
        # backwards compatibility: for loggedout users don't return boolean,
        # instead return None for all relations.
        NO_SR_USER_RELATIONS = SubredditUserRelations(
            subscriber=None,
            moderator=None,
            contributor=None,
            banned=None,
            muted=None,
        )
        sr_user_relations = defaultdict(lambda: NO_SR_USER_RELATIONS)

    ret = {}
    for sr in srs:
        relations = sr_user_relations[sr._id]
        can_view = sr.can_view(user)
        subscribers = sr._ups if not sr.hide_subscribers else 0

        data = dict(
            name=sr._fullname,
            display_name=sr.name,
            url=sr.path,
            banner_img=sr.banner_img if can_view else None,
            banner_size=sr.banner_size if can_view else None,
            header_img=sr.header if can_view else None,
            header_size=sr.header_size if can_view else None,
            icon_img=sr.icon_img if can_view else None,
            icon_size=sr.icon_size if can_view else None,
            key_color=sr.key_color if can_view else None,
            subscribers=subscribers if can_view else None,
            user_is_banned=relations.banned if can_view else None,
            user_is_muted=relations.muted if can_view else None,
            user_is_contributor=relations.contributor if can_view else None,
            user_is_moderator=relations.moderator if can_view else None,
            user_is_subscriber=relations.subscriber if can_view else None,
        )

        if feature.is_enabled('mobile_settings'):
            data["key_color"] = sr.key_color if can_view else None

        ret[sr._id] = data
    return ret


class IdentityJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        comment_karma="comment_karma",
        has_verified_email="email_verified",
        is_gold="gold",
        is_mod="is_mod",
        link_karma="link_karma",
        name="name",
        hide_from_robots="pref_hide_from_robots",
    )
    _private_data_attrs = dict(
        inbox_count="inbox_count",
        over_18="pref_over_18",
        gold_creddits="gold_creddits",
        gold_expiration="gold_expiration",
        is_suspended="in_timeout",
        suspension_expiration_utc="timeout_expiration_utc",
        features="features",
    )
    _public_attrs = {
        "name",
        "is_suspended",
    }

    def raw_data(self, thing):
        viewable = True
        attrs = self._data_attrs_.copy()
        if c.user_is_loggedin and thing._id == c.user._id:
            attrs.update(self._private_data_attrs)
        # Add a public indication when a user is permanently in timeout.
        elif (thing.in_timeout and thing.timeout_expiration is None):
            attrs.update({"is_suspended": "in_timeout"})
            viewable = False

        if thing.pref_hide_from_robots:
            response.headers['X-Robots-Tag'] = 'noindex, nofollow'

        data = {k: self.thing_attr(thing, v) for k, v in attrs.iteritems()
                if viewable or k in self._public_attrs}
        try:
            self.add_message_data(data, thing)
        except OAuth2Scope.InsufficientScopeError:
            # No access to privatemessages, but the rest of
            # the identity information is sufficient.
            pass

        # Add as private data attributes states about this user. This is used
        # for feature flagging by user state on first-party API clients.
        if c.user_is_loggedin and thing._id == c.user._id:
            data['is_employee'] = thing.employee
            data['in_beta'] = thing.pref_beta

        return data

    @extra_oauth2_scope("privatemessages")
    def add_message_data(self, data, thing):
        if c.user_is_loggedin and thing._id == c.user._id:
            data['has_mail'] = self.thing_attr(thing, 'has_mail')
            data['has_mod_mail'] = self.thing_attr(thing, 'has_mod_mail')

    def thing_attr(self, thing, attr):
        from r2.lib.template_helpers import (
            display_comment_karma, display_link_karma)
        if attr == "is_mod":
            t = thing.lookups[0] if isinstance(thing, Wrapped) else thing
            return t.is_moderator_somewhere
        elif attr == "has_mail":
            return bool(c.have_messages)
        elif attr == "has_mod_mail":
            return bool(c.have_mod_messages)
        elif attr == "comment_karma":
            return display_comment_karma(thing.comment_karma)
        elif attr == "link_karma":
            return display_link_karma(thing.link_karma)
        elif attr == "gold_expiration":
            if not thing.gold:
                return None
            return calendar.timegm(thing.gold_expiration.utctimetuple())
        elif attr == "timeout_expiration_utc":
            expiration_date = thing.timeout_expiration
            if not expiration_date:
                return None

            return calendar.timegm(expiration_date.utctimetuple())
        elif attr == "features":
            return feature.all_enabled(c.user)

        return ThingJsonTemplate.thing_attr(self, thing, attr)


class AccountJsonTemplate(IdentityJsonTemplate):
    _data_attrs_ = IdentityJsonTemplate.data_attrs(is_friend="is_friend")
    _private_data_attrs = dict(
        modhash="modhash",
        **IdentityJsonTemplate._private_data_attrs
    )

    def thing_attr(self, thing, attr):
        if attr == "is_friend":
            return c.user_is_loggedin and thing._id in c.user.friends
        elif attr == "modhash":
            return c.modhash
        return IdentityJsonTemplate.thing_attr(self, thing, attr)



class PrefsJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict((k[len("pref_"):], k) for k in
            Account._preference_attrs)

    def __init__(self, fields=None):
        if fields is not None:
            _data_attrs_ = {}
            for field in fields:
                if field not in self._data_attrs_:
                    raise KeyError(field)
                _data_attrs_[field] = self._data_attrs_[field]
            self._data_attrs_ = _data_attrs_

    def thing_attr(self, thing, attr):
        if attr == "pref_clickgadget":
            return bool(thing.pref_clickgadget)
        return ThingJsonTemplate.thing_attr(self, thing, attr)


def get_mod_attributes(item):
    data = {}
    if c.user_is_loggedin and item.can_ban:
        data["num_reports"] = item.reported
        data["report_reasons"] = Report.get_reasons(item)

        ban_info = getattr(item, "ban_info", {})
        if item._spam:
            data["approved_by"] = None
            if ban_info.get('moderator_banned'):
                data["banned_by"] = ban_info.get("banner")
            else:
                data["banned_by"] = True
        else:
            data["approved_by"] = ban_info.get("unbanner")
            data["banned_by"] = None
    else:
        data["num_reports"] = None
        data["report_reasons"] = None
        data["approved_by"] = None
        data["banned_by"] = None
    return data


def get_author_attributes(item):
    data = {}
    if not item.author._deleted:
        author = item.author
        sr_id = item.subreddit._id

        data["author"] = author.name

        if author.flair_enabled_in_sr(sr_id):
            flair_text = getattr(author, 'flair_%s_text' % sr_id, None)
            flair_css = getattr(author, 'flair_%s_css_class' % sr_id, None)
        else:
            flair_text = None
            flair_css = None
        data["author_flair_text"] = flair_text
        data["author_flair_css_class"] = flair_css

    else:
        data["author"] = "[deleted]"
        data["author_flair_text"] = None
        data["author_flair_css_class"] = None
    return data


def get_distinguished_attributes(item):
    data = {}
    distinguished = getattr(item, "distinguished", "no")
    data["distinguished"] = distinguished if distinguished != "no" else None
    return data


def get_edited_attributes(item):
    data = {}
    if isinstance(item.editted, bool):
        data["edited"] = item.editted
    else:
        editted_timetuple = item.editted.astimezone(pytz.UTC).timetuple()
        data["edited"] = time.mktime(editted_timetuple) - time.timezone
    return data


def get_report_reason_attributes(item):
    if c.user_is_loggedin and c.user.in_timeout:
        data = {
            "user_reports": [],
            "mod_reports": [],
        }
    else:
        data = {
            "user_reports": item.user_reports,
            "mod_reports": item.mod_reports,
        }
    return data


def get_removal_reason_attributes(item):
    data = {}
    if getattr(item, "admin_takedown", None):
        data["removal_reason"] = "legal"
    else:
        data["removal_reason"] = None
    return data


def get_media_embed_attributes(item):
    from r2.lib.media import get_media_embed

    data = {
        "media_embed": {},
        "secure_media_embed": {},
    }

    media_object = item.media_object
    if media_object and not isinstance(media_object, basestring):
        media_embed = get_media_embed(media_object)
        if media_embed:
            data["media_embed"] = {
                "scrolling": media_embed.scrolling,
                "width": media_embed.width,
                "height": media_embed.height,
                "content": media_embed.content,
            }

    secure_media_object = item.secure_media_object
    if secure_media_object and not isinstance(secure_media_object, basestring):
        secure_media_embed = get_media_embed(secure_media_object)
        if secure_media_embed:
            data["secure_media_embed"] = {
                "scrolling": secure_media_embed.scrolling,
                "width": secure_media_embed.width,
                "height": secure_media_embed.height,
                "content": secure_media_embed.content,
            }
    return data


def get_selftext_attributes(item):
    data = {}
    if not item.expunged:
        data["selftext"] = item.selftext
        data["selftext_html"] = safemarkdown(item.selftext)
    else:
        data["selftext"] = "[removed]"
        data["selftext_html"] = safemarkdown(_("[removed]"))
    return data


def generate_image_links(preview_object, file_type=None, censor_nsfw=False):
    PREVIEW_RESOLUTIONS = (108, 216, 320, 640, 960, 1080)
    PREVIEW_MAX_RATIO = 2

    # Determine which previews would be feasible with our given dims
    source_width = preview_object['width']
    source_height = preview_object['height']
    source_ratio = float(source_height) / source_width

    # previews with a ratio above the max will be cropped to a lower ratio
    max_ratio = float(PREVIEW_MAX_RATIO)
    preview_ratio = min(source_ratio, max_ratio)

    preview_resolutions = []
    for w in PREVIEW_RESOLUTIONS:
        if w > source_width:
            continue

        url = g.image_resizing_provider.resize_image(
            preview_object,
            w,
            file_type=file_type,
            censor_nsfw=censor_nsfw,
            max_ratio=PREVIEW_MAX_RATIO,
        )
        h = int(w * preview_ratio)
        preview_resolutions.append({
            "url": url,
            "width": w,
            "height": h,
        })

    url = g.image_resizing_provider.resize_image(
        preview_object,
        file_type=file_type,
        censor_nsfw=censor_nsfw,
    )

    return {
        "source": {
            "url": url,
            "width": source_width,
            "height": source_height,
        },
        "resolutions": preview_resolutions,
    }


class LinkJsonTemplate(ThingTemplate):
    @classmethod
    def get_json(cls, item):
        data = ThingTemplate.get_json(item)

        data.update(get_mod_attributes(item))
        data.update(get_author_attributes(item))
        data.update(get_distinguished_attributes(item))
        data.update(get_edited_attributes(item))
        data.update(get_media_embed_attributes(item))
        data.update(get_report_reason_attributes(item))
        data.update(get_removal_reason_attributes(item))
        data.update(get_selftext_attributes(item))

        data.update({
            "archived": not item.votable,
            "visited": item.visited,
            "clicked": False,
            "contest_mode": item.contest_mode,
            "domain": item.domain,
            "downs": 0,
            "gilded": item.gildings,
            "hidden": item.hidden,
            "hide_score": item.hide_score,
            "is_self": item.is_self,
            "likes": item.likes,
            "link_flair_css_class": item.flair_css_class,
            "link_flair_text": item.flair_text,
            "locked": item.locked,
            "media": item.media_object,
            "secure_media": item.secure_media_object,
            "num_comments": item.num_comments,
            "over_18": item.over_18,
            "quarantine": item.quarantine,
            "permalink": item.permalink,
            "saved": item.saved,
            "score": item.score,
            "stickied": item.stickied,
            "subreddit": item.subreddit.name,
            "subreddit_id": item.subreddit._fullname,
            "suggested_sort": item.sort_if_suggested(sr=item.subreddit),
            "thumbnail": item.thumbnail,
            "title": item.title,
            "ups": item.score,
            "url": item.url,
        })

        if hasattr(item, "action_type"):
            data["action_type"] = item.action_type

        if hasattr(item, "sr_detail"):
            data["sr_detail"] = item.sr_detail

        if hasattr(item, "show_media"):
            data["show_media"] = item.show_media

        if c.permalink_page:
            data["upvote_ratio"] = item.upvote_ratio

        preview_object = item.preview_image
        if preview_object:
            preview_is_gif = preview_object.get('url', '').endswith('.gif')
            data['preview'] = {}
            data['post_hint'] = item.post_hint
            # For gifs, the default preview should be a static image, with the
            # full gif as a variant
            if preview_is_gif:
                images = generate_image_links(preview_object, file_type="jpg")
            else:
                images = generate_image_links(preview_object)

            images['id'] = preview_object['uid']
            images['variants'] = {}
            if item.nsfw:
                images['variants']['nsfw'] = generate_image_links(
                    preview_object, censor_nsfw=True, file_type="png")
            if preview_is_gif:
                images['variants']['gif'] = generate_image_links(
                    preview_object)
                images['variants']['mp4'] = generate_image_links(
                    preview_object, file_type="mp4")
            data['preview']['images'] = [images]
        return data

    @classmethod
    def get_rendered(cls, item, render_style):
        data = ThingTemplate.get_rendered(item, render_style)
        data.update({
            "sr": item.subreddit._fullname,
        })
        return data


class PromotedLinkJsonTemplate(LinkJsonTemplate):
    @classmethod
    def get_json(cls, item):
        data = LinkJsonTemplate.get_json(item)
        data.update({
            "promoted": item.promoted,
            "imp_pixel": getattr(item, "imp_pixel", None),
            "href_url": item.href_url,
            "adserver_imp_pixel": getattr(item, "adserver_imp_pixel", None),
            "adserver_click_url": getattr(item, "adserver_click_url", None),
            "mobile_ad_url": item.mobile_ad_url,
            "disable_comments": item.disable_comments,
            "third_party_tracking": item.third_party_tracking,
            "third_party_tracking_2": item.third_party_tracking_2,
        })

        del data["subreddit"]
        del data["subreddit_id"]
        return data


class CommentJsonTemplate(ThingTemplate):
    @classmethod
    def get_parent_id(cls, item):
        from r2.models import Comment, Link

        if getattr(item, "parent_id", None):
            return make_fullname(Comment, item.parent_id)
        else:
            return make_fullname(Link, item.link_id)

    @classmethod
    def get_link_name(cls, item):
        from r2.models import Link
        return make_fullname(Link, item.link_id)

    @classmethod
    def render_child(cls, item):
        child = getattr(item, "child", None)
        if child:
            return child.render()
        else:
            return ""

    @classmethod
    def get_json(cls, item):
        from r2.models import Link

        data = ThingTemplate.get_json(item)

        data.update(get_mod_attributes(item))
        data.update(get_author_attributes(item))
        data.update(get_distinguished_attributes(item))
        data.update(get_edited_attributes(item))
        data.update(get_report_reason_attributes(item))
        data.update(get_removal_reason_attributes(item))

        data.update({
            "archived": not item.votable,
            "body": item.body,
            "body_html": spaceCompress(safemarkdown(item.body)),
            "controversiality": 1 if item.is_controversial else 0,
            "downs": 0,
            "gilded": item.gildings,
            "likes": item.likes,
            "link_id": cls.get_link_name(item),
            "saved": item.saved,
            "score": item.score,
            "score_hidden": item.score_hidden,
            "subreddit": item.subreddit.name,
            "subreddit_id": item.subreddit._fullname,
            "ups": item.score,
            "replies": cls.render_child(item),
            "parent_id": cls.get_parent_id(item),
        })

        if feature.is_enabled('sticky_comments'):
            data["stickied"] = item.link.sticky_comment_id == item._id

        if hasattr(item, "action_type"):
            data["action_type"] = item.action_type

        if c.profilepage:
            data["quarantine"] = item.subreddit.quarantine
            data["over_18"] = item.link.is_nsfw

            data["link_title"] = item.link.title
            data["link_author"] = item.link_author.name

            if item.link.is_self:
                link_url = item.link.make_permalink(
                    item.subreddit, force_domain=True)
            else:
                link_url = item.link.url
            data["link_url"] = link_url

        return data

    @classmethod
    def get_rendered(cls, item, render_style):
        data = ThingTemplate.get_rendered(item, render_style)
        data.update({
            "replies": cls.render_child(item),
            "contentText": item.body,
            "contentHTML": spaceCompress(safemarkdown(item.body)),
            "link": cls.get_link_name(item),
            "parent": cls.get_parent_id(item),
        })
        return data


class MoreCommentJsonTemplate(ThingTemplate):
    @classmethod
    def get_kind(cls, item):
        return "more"

    @classmethod
    def get_json(cls, item):
        data = {
            "children": [to36(comment_id) for comment_id in item.children],
            "count": item.count,
            "id": item._id36,
            "name": item._fullname,
            "parent_id": CommentJsonTemplate.get_parent_id(item),
        }
        return data

    @classmethod
    def get_rendered(cls, item, render_style):
        data = ThingTemplate.get_rendered(item, render_style)
        data.update({
            "replies": "",
            "contentText": "",
            "contentHTML": "",
            "link": CommentJsonTemplate.get_link_name(item),
            "parent": CommentJsonTemplate.get_parent_id(item),
        })
        return data


class MessageJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        author="author",
        body="body",
        body_html="body_html",
        context="context",
        created="created",
        dest="dest",
        distinguished="distinguished",
        first_message="first_message",
        first_message_name="first_message_name",
        new="new",
        parent_id="parent_id",
        replies="child",
        subject="subject",
        subreddit="subreddit",
        was_comment="was_comment",
    )

    def thing_attr(self, thing, attr):
        from r2.models import Comment, Link, Message
        if attr == "was_comment":
            return thing.was_comment
        elif attr == "context":
            return ("" if not thing.was_comment
                    else thing.permalink + "?context=3")
        elif attr == "dest":
            if thing.to_id:
                return thing.to.name
            else:
                return "#" + thing.subreddit.name
        elif attr == "subreddit":
            if thing.sr_id:
                return thing.subreddit.name
            return None
        elif attr == "body_html":
            return safemarkdown(thing.body)
        elif attr == "author" and getattr(thing, "hide_author", False):
            return None
        elif attr == "parent_id":
            if thing.was_comment:
                if getattr(thing, "parent_id", None):
                    return make_fullname(Comment, thing.parent_id)
                else:
                    return make_fullname(Link, thing.link_id)
            elif getattr(thing, "parent_id", None):
                return make_fullname(Message, thing.parent_id)
        elif attr == "first_message_name":
            if getattr(thing, "first_message", None):
                return make_fullname(Message, thing.first_message)
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def raw_data(self, thing):
        d = ThingJsonTemplate.raw_data(self, thing)
        if thing.was_comment:
            d['link_title'] = thing.link_title
            d['likes'] = thing.likes
        return d

    def rendered_data(self, wrapped):
        from r2.models import Message
        parent_id = wrapped.parent_id
        if parent_id:
            parent_id = make_fullname(Message, parent_id)
        d = ThingJsonTemplate.rendered_data(self, wrapped)
        d['parent'] = parent_id
        d['contentText'] = self.thing_attr(wrapped, 'body')
        d['contentHTML'] = self.thing_attr(wrapped, 'body_html')
        return d


class RedditJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return ObjectTemplate(thing.content().render() if thing else {})

class PanestackJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        res = [t.render() for t in thing.stack if t] if thing else []
        res = [x for x in res if x]
        if not res:
            return {}
        return ObjectTemplate(res if len(res) > 1 else res[0] )

class NullJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return ""

    def get_def(self, name):
        return self

class ListingJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        after="after",
        before="before",
        children="things",
        modhash="modhash",
    )
    
    def thing_attr(self, thing, attr):
        if attr == "modhash":
            return c.modhash
        elif attr == "things":
            res = []
            for a in thing.things:
                a.childlisting = False
                r = a.render()
                res.append(r)
            return res
        return ThingJsonTemplate.thing_attr(self, thing, attr)
        

    def rendered_data(self, thing):
        return self.thing_attr(thing, "things")
    
    def kind(self, wrapped):
        return "Listing"


class SearchListingJsonTemplate(ListingJsonTemplate):
    def raw_data(self, thing):
        data = ThingJsonTemplate.raw_data(self, thing)

        def format_sr(sr, count):
            return {'name': sr.name, 'url': sr.path, 'count': count}

        facets = {}
        if thing.subreddit_facets:
            facets['subreddits'] = [format_sr(sr, count)
                                    for sr, count in thing.subreddit_facets]
        data['facets'] = facets

        return data


class UserListingJsonTemplate(ListingJsonTemplate):
    def raw_data(self, thing):
        if not thing.nextprev:
            return {"children": self.rendered_data(thing)}
        return ListingJsonTemplate.raw_data(self, thing)

    def kind(self, wrapped):
        return "Listing" if wrapped.nextprev else "UserList"

class UserListJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        children="users",
    )

    def thing_attr(self, thing, attr):
        if attr == "users":
            res = []
            for a in thing.user_rows:
                r = a.render()
                res.append(r)
            return res
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def rendered_data(self, thing):
        return self.thing_attr(thing, "users")

    def kind(self, wrapped):
        return "UserList"


class UserTableItemJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        id="_fullname",
        name="name",
    )

    def thing_attr(self, thing, attr):
        return ThingJsonTemplate.thing_attr(self, thing.user, attr)

    def render(self, thing, *a, **kw):
        return ObjectTemplate(self.data(thing))


class RelTableItemJsonTemplate(UserTableItemJsonTemplate):
    _data_attrs_ = UserTableItemJsonTemplate.data_attrs(
        date="date",
    )

    def thing_attr(self, thing, attr):
        rel_attr, splitter, attr = attr.partition(".")
        if attr == 'note':
            # return empty string instead of None for missing note
            return ThingJsonTemplate.thing_attr(self, thing.rel, attr) or ''
        elif attr:
            return ThingJsonTemplate.thing_attr(self, thing.rel, attr)
        elif rel_attr == 'date':
            # make date UTC
            date = self.thing_attr(thing, 'rel._date')
            date = time.mktime(date.astimezone(pytz.UTC).timetuple())
            return date - time.timezone
        else:
            return UserTableItemJsonTemplate.thing_attr(self, thing, rel_attr)


class FriendTableItemJsonTemplate(RelTableItemJsonTemplate):
    def inject_data(self, thing, d):
        if c.user.gold and thing.type == "friend":
            d["note"] = self.thing_attr(thing, 'rel.note')
        return d

    def rendered_data(self, thing):
        d = RelTableItemJsonTemplate.rendered_data(self, thing)
        return self.inject_data(thing, d)

    def raw_data(self, thing):
        d = RelTableItemJsonTemplate.raw_data(self, thing)
        return self.inject_data(thing, d)


class BannedTableItemJsonTemplate(RelTableItemJsonTemplate):
    _data_attrs_ = RelTableItemJsonTemplate.data_attrs(
        note="rel.note",
    )


class MutedTableItemJsonTemplate(RelTableItemJsonTemplate):
    pass


class InvitedModTableItemJsonTemplate(RelTableItemJsonTemplate):
    _data_attrs_ = RelTableItemJsonTemplate.data_attrs(
        mod_permissions="permissions",
    )

    def thing_attr(self, thing, attr):
        if attr == 'permissions':
            permissions = thing.permissions.items()
            return [perm for perm, has in permissions if has]
        else:
            return RelTableItemJsonTemplate.thing_attr(self, thing, attr)


class OrganicListingJsonTemplate(ListingJsonTemplate):
    def kind(self, wrapped):
        return "OrganicListing"

class TrafficJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        res = {}

        for interval in ("hour", "day", "month"):
            # we don't actually care about the column definitions (used for
            # charting) here, so just pass an empty list.
            interval_data = thing.get_data_for_interval(interval, [])

            # turn the python datetimes into unix timestamps and flatten data
            res[interval] = [(calendar.timegm(date.timetuple()),) + data
                             for date, data in interval_data]

        return ObjectTemplate(res)

class WikiJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        try:
            content = thing.content()
        except AttributeError:
            content = thing.listing
        return ObjectTemplate(content.render() if thing else {})

class WikiPageListingJsonTemplate(ThingJsonTemplate):
    def kind(self, thing):
        return "wikipagelisting"
    
    def data(self, thing):
        pages = [p.name for p in thing.linear_pages]
        return pages

class WikiViewJsonTemplate(ThingJsonTemplate):
    def kind(self, thing):
        return "wikipage"
    
    def data(self, thing):
        edit_date = time.mktime(thing.edit_date.timetuple()) if thing.edit_date else None
        edit_by = None
        if thing.edit_by and not thing.edit_by._deleted:
             edit_by = Wrapped(thing.edit_by).render()
        return dict(content_md=thing.page_content_md,
                    content_html=thing.page_content,
                    revision_by=edit_by,
                    revision_date=edit_date,
                    may_revise=thing.may_revise)

class WikiSettingsJsonTemplate(ThingJsonTemplate):
     def kind(self, thing):
         return "wikipagesettings"
    
     def data(self, thing):
         editors = [Wrapped(e).render() for e in thing.mayedit]
         return dict(permlevel=thing.permlevel,
                     listed=thing.listed,
                     editors=editors)

class WikiRevisionJsonTemplate(ThingJsonTemplate):
    def render(self, thing, *a, **kw):
        timestamp = time.mktime(thing.date.timetuple()) if thing.date else None
        author = thing.get_author()
        if author and not author._deleted:
            author = Wrapped(author).render()
        else:
            author = None
        return ObjectTemplate(dict(author=author,
                                   id=str(thing._id),
                                   timestamp=timestamp,
                                   reason=thing._get('reason'),
                                   page=thing.page))

class FlairListJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        def row_to_json(row):
            if hasattr(row, 'user'):
              return dict(user=row.user.name, flair_text=row.flair_text,
                          flair_css_class=row.flair_css_class)
            else:
              # prev/next link
              return dict(after=row.after, reverse=row.previous)

        json_rows = [row_to_json(row) for row in thing.flair]
        result = dict(users=[row for row in json_rows if 'user' in row])
        for row in json_rows:
            if 'after' in row:
                if row['reverse']:
                    result['prev'] = row['after']
                else:
                    result['next'] = row['after']
        return ObjectTemplate(result)

class FlairCsvJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        return ObjectTemplate([l.__dict__ for l in thing.results_by_line])


class FlairSelectorJsonTemplate(JsonTemplate):
    def _template_dict(self, flair):
        return {"flair_template_id": flair.flair_template_id,
                "flair_position": flair.flair_position,
                "flair_text": flair.flair_text,
                "flair_css_class": flair.flair_css_class,
                "flair_text_editable": flair.flair_text_editable}

    def render(self, thing, *a, **kw):
        """Render a list of flair choices into JSON

        Sample output:
        {
            "choices": [
                {
                    "flair_css_class": "flair-444",
                    "flair_position": "right",
                    "flair_template_id": "5668d204-9388-11e3-8109-080027a38559",
                    "flair_text": "444",
                    "flair_text_editable": true
                },
                {
                    "flair_css_class": "flair-nouser",
                    "flair_position": "right",
                    "flair_template_id": "58e34d7a-9388-11e3-ab01-080027a38559",
                    "flair_text": "nouser",
                    "flair_text_editable": true
                },
                {
                    "flair_css_class": "flair-bar",
                    "flair_position": "right",
                    "flair_template_id": "fb01cc04-9391-11e3-b1d6-080027a38559",
                    "flair_text": "foooooo",
                    "flair_text_editable": true
                }
            ],
            "current": {
                "flair_css_class": "444",
                "flair_position": "right",
                "flair_template_id": "5668d204-9388-11e3-8109-080027a38559",
                "flair_text": "444"
            }
        }

        """
        choices = [self._template_dict(choice) for choice in thing.choices]

        current_flair = {
            "flair_text": thing.text,
            "flair_css_class": thing.css_class,
            "flair_position": thing.position,
            "flair_template_id": thing.matching_template,
        }
        return ObjectTemplate({"current": current_flair, "choices": choices})


class StylesheetTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        images='_images',
        stylesheet='stylesheet_contents',
        subreddit_id='_fullname',
    )

    def kind(self, wrapped):
        return 'stylesheet'

    def images(self):
        sr_images = ImagesByWikiPage.get_images(c.site, "config/stylesheet")
        images = []
        for name, url in sr_images.iteritems():
            images.append({'name': name,
                           'link': 'url(%%%%%s%%%%)' % name,
                           'url': url})
        return images

    def thing_attr(self, thing, attr):
        if attr == '_images':
            return self.images()
        elif attr == '_fullname':
            return c.site._fullname
        return ThingJsonTemplate.thing_attr(self, thing, attr)

class SubredditSettingsTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        allow_images='site.allow_images',
        collapse_deleted_comments='site.collapse_deleted_comments',
        comment_score_hide_mins='site.comment_score_hide_mins',
        content_options='site.link_type',
        default_set='site.allow_top',
        description='site.description',
        domain='site.domain',
        exclude_banned_modqueue='site.exclude_banned_modqueue',
        header_hover_text='site.header_title',
        # key_color='site.key_color',
        language='site.lang',
        over_18='site.over_18',
        public_description='site.public_description',
        public_traffic='site.public_traffic',
        # related_subreddits='site.related_subreddits',
        hide_ads="site.hide_ads",
        show_media='site.show_media',
        show_media_preview='site.show_media_preview',
        submit_link_label='site.submit_link_label',
        submit_text_label='site.submit_text_label',
        submit_text='site.submit_text',
        subreddit_id='site._fullname',
        subreddit_type='site.type',
        suggested_comment_sort="site.suggested_comment_sort",
        title='site.title',
        wiki_edit_age='site.wiki_edit_age',
        wiki_edit_karma='site.wiki_edit_karma',
        wikimode='site.wikimode',
        spam_links='site.spam_links',
        spam_selfposts='site.spam_selfposts',
        spam_comments='site.spam_comments',
    )

    def kind(self, wrapped):
        return 'subreddit_settings'

    def thing_attr(self, thing, attr):
        if attr.startswith('site.') and thing.site:
            return getattr(thing.site, attr[5:])
        if attr == 'related_subreddits' and thing.site:
            # string used for form input
            return '\n'.join(thing.site.related_subreddits)
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def raw_data(self, thing):
        data = ThingJsonTemplate.raw_data(self, thing)

        # remove this when feature is enabled and use _data_attrs instead
        if feature.is_enabled('mobile_settings'):
            data['key_color'] = self.thing_attr(thing, 'key_color')
        if feature.is_enabled('related_subreddits'):
            data['related_subreddits'] = self.thing_attr(thing, 'related_subreddits')

        return data


class UploadedImageJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        return ObjectTemplate({
            "errors": list(k for (k, v) in thing.errors if v),
            "img_src": thing.img_src,
        })


class ModActionTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        action='action',
        created_utc='date',
        description='description',
        details='details',
        id='_fullname',
        mod='moderator',
        mod_id36='mod_id36',
        sr_id36='sr_id36',
        subreddit='subreddit',
        target_author='target_author',
        target_fullname='target_fullname',
        target_permalink='target_permalink',
        target_title='target_title',
        target_body='target_body',
    )

    def thing_attr(self, thing, attr):
        if attr == 'date':
            return (time.mktime(thing.date.astimezone(pytz.UTC).timetuple())
                    - time.timezone)
        elif attr == 'target_author':
            if thing.target_author and thing.target_author._deleted:
                return "[deleted]"
            elif thing.target_author:
                return thing.target_author.name
            return ""
        elif attr == 'target_permalink':
            try:
                return thing.target.make_permalink_slow()
            except AttributeError:
                return None
        elif attr == "moderator":
            return thing.moderator.name
        elif attr == "subreddit":
            return thing.subreddit.name
        elif attr == 'target_title' and isinstance(thing.target, Link):
            return thing.target.title
        elif attr == 'target_body' and isinstance(thing.target, Comment):
            return thing.target.body
        elif (attr == 'target_body' and isinstance(thing.target, Link)
              and getattr(thing.target, 'selftext', None)):
            return thing.target.selftext

        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def kind(self, wrapped):
        return 'modaction'


class PolicyViewJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        body_html="body_html",
        display_rev="display_rev",
        revs="revs",
        toc_html="toc_html",
    )

    def kind(self, wrapped):
        return "Policy"

class KarmaListJsonTemplate(ThingJsonTemplate):
    def data(self, karmas):
        from r2.lib.template_helpers import (
            display_comment_karma, display_link_karma)
        karmas = [{
            'sr': sr,
            'link_karma': display_link_karma(link_karma),
            'comment_karma': display_comment_karma(comment_karma),
        } for sr, (link_karma, comment_karma) in karmas.iteritems()]
        return karmas

    def kind(self, wrapped):
        return "KarmaList"


def get_usertrophies(user):
    trophies = Trophy.by_account(user)
    def visible_trophy(trophy):
        return trophy._thing2.awardtype != 'invisible'
    trophies = filter(visible_trophy, trophies)
    resp = TrophyListJsonTemplate().render(trophies)
    return resp.finalize()


class TrophyJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        award_id="award._id36",
        description="description",
        name="award.title",
        id="_id36",
        icon_40="icon_40",
        icon_70="icon_70",
        url="trophy_url",
    )

    def thing_attr(self, thing, attr):
        if attr == "icon_40":
            return "https:" + thing._thing2.imgurl % 40
        elif attr == "icon_70":
            return "https:" + thing._thing2.imgurl % 70
        rel_attr, splitter, attr = attr.partition(".")
        if attr:
            return ThingJsonTemplate.thing_attr(self, thing._thing2, attr)
        else:
            return ThingJsonTemplate.thing_attr(self, thing, rel_attr)

    def kind(self, thing):
        return ThingJsonTemplate.kind(self, thing._thing2)

class TrophyListJsonTemplate(ThingJsonTemplate):
    def data(self, trophies):
        trophies = [Wrapped(t).render() for t in trophies]
        return dict(trophies=trophies)

    def kind(self, wrapped):
        return "TrophyList"


class RulesJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        rules = {}
        rules['site_rules'] = thing.site_rules
        rules['rules'] = thing.rules

        for rule in rules["rules"]:
            if rule.get("description"):
                rule["description_html"] = safemarkdown(rule["description"])
            if not rule.get("kind"):
                rule["kind"] = "all"

        return ObjectTemplate(rules)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from hashlib import md5
import sys

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.util import PylonsContext, AttribSafeContextObj, ContextObj
import raven
from raven.processors import Processor
from weberror.reporter import Reporter

from r2.lib.app_globals import Globals


def get_operational_exceptions():
    import _pylibmc
    import sqlalchemy.exc
    import pycassa.pool
    import r2.lib.db.thing
    import r2.lib.lock
    import r2.lib.cache

    return (
        SystemExit,  # gunicorn is shutting us down
        _pylibmc.MemcachedError,
        r2.lib.db.thing.NotFound,
        r2.lib.lock.TimeoutExpired,
        sqlalchemy.exc.OperationalError,
        sqlalchemy.exc.IntegrityError,
        pycassa.pool.AllServersUnavailable,
        pycassa.pool.NoConnectionAvailable,
        pycassa.pool.MaximumRetryException,
    )


class SanitizeStackLocalsProcessor(Processor):
    keys_to_remove = (
        "self",
        "__traceback_supplement__",
    )

    classes_to_remove = (
        Globals,
        PylonsContext,
        AttribSafeContextObj,
        ContextObj,
    )

    def filter_stacktrace(self, data, **kwargs):
        def remove_keys(obj):
            if isinstance(obj, dict):
                for k in obj.keys():
                    if k in self.keys_to_remove:
                        obj.pop(k)
                    elif isinstance(obj[k], self.classes_to_remove):
                        obj.pop(k)
                    elif isinstance(obj[k], basestring):
                        contains_forbidden_repr = any(
                            _cls.__name__ in obj[k]
                            for _cls in self.classes_to_remove
                        )
                        if contains_forbidden_repr:
                            obj.pop(k)
                    elif isinstance(obj[k], (list, dict)):
                        remove_keys(obj[k])
            elif isinstance(obj, list):
                for v in obj:
                    if isinstance(v, (list, dict)):
                        remove_keys(v)

        for frame in data.get('frames', []):
            if 'vars' in frame:
                remove_keys(frame['vars'])


class RavenErrorReporter(Reporter):
    @classmethod
    def get_module_versions(cls):
        return {
            repo: commit_hash[:6]
            for repo, commit_hash in g.versions.iteritems()
        }

    @classmethod
    def add_http_context(cls, client):
        """Add request details to the 'request' context

        These fields will be filtered by SanitizePasswordsProcessor
        as long as they are one of 'data', 'cookies', 'headers', 'env', and
        'query_string'.

        """

        HEADER_WHITELIST = (
            "user-agent",
            "host",
            "accept",
            "accept-encoding",
            "accept-language",
            "referer",
        )
        headers = {
            k: v for k, v in request.headers.iteritems()
            if k.lower() in HEADER_WHITELIST
        }

        client.http_context({
            "url": request.path,
            "method": request.method,
            "query_string": request.query_string,
            "data": request.body,
            "headers": headers,
        })

        if "app" in request.GET:
            client.tags_context({"app": request.GET["app"]})

    @classmethod
    def add_reddit_context(cls, client):
        reddit_context = {
            "language": c.lang,
            "render_style": c.render_style,
        }

        if c.site:
            reddit_context["subreddit"] = c.site.name

        client.extra_context(reddit_context)

    @classmethod
    def add_user_context(cls, client):
        user_context = {}

        if c.user_is_loggedin:
            user_context["user"] = c.user._id

        if c.oauth2_client:
            user_context["oauth_client_id"] = c.oauth2_client._id
            user_context["oauth_client_name"] = c.oauth2_client.name

        client.user_context(user_context)

    @classmethod
    def get_raven_client(cls):
        app_path_prefixes = [
            "r2",
            "reddit_",  # plugins such as 'reddit_liveupdate'
            "/opt/",    # scripts may be run from /opt/REPO/scripts
        ]
        release_str = '|'.join(
           "%s:%s" % (repo, commit_hash)
           for repo, commit_hash in sorted(g.versions.items())
        )
        release_hash = md5(release_str).hexdigest()

        RAVEN_CLIENT = raven.Client(
            dsn=g.sentry_dsn,
            # use the default transport to send errors from another thread:
            transport=raven.transport.threaded.ThreadedHTTPTransport,
            include_paths=app_path_prefixes,
            processors=[
                'raven.processors.SanitizePasswordsProcessor',
                'r2.lib.log.SanitizeStackLocalsProcessor',
            ],
            release=release_hash,
            environment=g.pool_name,
            include_versions=False,     # handled by get_module_versions
            install_sys_hook=False,
        )
        return RAVEN_CLIENT

    @classmethod
    def capture_exception(cls, exc_info=None):
        if exc_info is None:
            # if possible exc_info should be captured as close to the exception
            # as possible and passed in because sys.exc_info() can give
            # unexpected behavior
            exc_info = sys.exc_info()

        if issubclass(exc_info[0], get_operational_exceptions()):
            return

        client = cls.get_raven_client()

        if g.running_as_script:
            # scripts are run like:
            # paster run INIFILE -c "python code to execute"
            # OR
            # paster run INIFILE script.py
            # either way sys.argv[-1] will tell us the entry point to the error
            culprit = 'script: "%s"' % sys.argv[-1]
        else:
            cls.add_http_context(client)
            cls.add_reddit_context(client)
            cls.add_user_context(client)

            routes_dict = request.environ["pylons.routes_dict"]
            controller = routes_dict.get("controller", "unknown")
            action = routes_dict.get("action", "unknown")
            culprit = "%s.%s" % (controller, action)

        try:
            client.captureException(
                exc_info=exc_info,
                data={
                    "modules": cls.get_module_versions(),
                    "culprit": culprit,
                },
            )
        finally:
            client.context.clear()

    def report(self, exc_data):
        self.capture_exception()


def write_error_summary(error):
    """Log a single-line summary of the error for easy log grepping."""
    fullpath = request.environ.get('FULLPATH', request.path)
    uid = c.user._id if c.user_is_loggedin else '-'
    g.log.error("E: %s U: %s FP: %s", error, uid, fullpath)


class LoggingErrorReporter(Reporter):
    """ErrorMiddleware-compatible reporter that writes exceptions to g.log."""

    def report(self, exc_data):
        # exception_formatted is the output of traceback.format_exception_only
        exception = exc_data.exception_formatted[-1].strip()

        # First emit a single-line summary.  This is great for grepping the
        # streaming log for errors.
        write_error_summary(exception)

        text, extra = self.format_text(exc_data)
        # TODO: send this all in one burst so that error reports aren't
        # interleaved / individual lines aren't dropped. doing so will take
        # configuration on the syslog side and potentially in apptail as well
        for line in text.splitlines():
            g.log.warning(line)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import hmac
import json

from pylons import app_globals as g
import requests

from r2.lib import amqp
from r2.lib.filters import _force_unicode
from r2.lib.template_helpers import add_sr
from r2.lib.utils import constant_time_compare
from r2.models import (
    Account,
    Message,
    Subreddit,
)


def get_reply_to_address(message):
    """Construct a reply-to address that encodes the message id.

    The address is of the form:
        zendeskreply+{message_id36}-{email_mac}

    where the mac is generated from {message_id36} using the
    `modmail_email_secret`

    The reply address should be configured with the inbound email service so
    that replies to our messages are routed back to the app somehow. For mailgun
    this involves adding a Routes filter for messages sent to
    "zendeskreply\+*@". to be forwarded to POST /api/zendeskreply.

    """

    # all email replies are treated as replies to the first message in the
    # conversation. this is to get around some peculiarities of zendesk
    if message.first_message:
        first_message = Message._byID(message.first_message, data=True)
    else:
        first_message = message
    email_id = first_message._id36

    email_mac = hmac.new(
        g.secrets['modmail_email_secret'], email_id, hashlib.sha256).hexdigest()
    reply_id = "zendeskreply+{email_id}-{email_mac}".format(
        email_id=email_id, email_mac=email_mac)

    sr = Subreddit._byID(message.sr_id, data=True)
    return "r/{subreddit} mail <{reply_id}@{domain}>".format(
        subreddit=sr.name, reply_id=reply_id, domain=g.modmail_email_domain)


def parse_and_validate_reply_to_address(address):
    """Validate the address and parse out and return the message id.

    This is the reverse operation of `get_reply_to_address`.

    """

    recipient, sep, domain = address.partition("@")
    if not sep or not recipient or domain != g.modmail_email_domain:
        return

    main, sep, remainder = recipient.partition("+")
    if not sep or not main or main != "zendeskreply":
        return

    try:
        email_id, email_mac = remainder.split("-")
    except ValueError:
        return

    expected_mac = hmac.new(
        g.secrets['modmail_email_secret'], email_id, hashlib.sha256).hexdigest()

    if not constant_time_compare(expected_mac, email_mac):
        return

    message_id36 = email_id
    return message_id36


def get_message_subject(message):
    sr = Subreddit._byID(message.sr_id, data=True)

    if message.first_message:
        first_message = Message._byID(message.first_message, data=True)
        conversation_subject = first_message.subject
    else:
        conversation_subject = message.subject

    return u"[r/{subreddit} mail]: {subject}".format(
        subreddit=sr.name, subject=_force_unicode(conversation_subject))


def get_email_ids(message):
    parent_email_id = None
    other_email_ids = []
    if message.parent_id:
        parent = Message._byID(message.parent_id, data=True)
        if parent.email_id:
            other_email_ids.append(parent.email_id)
            parent_email_id = parent.email_id

    if message.first_message:
        first_message = Message._byID(message.first_message, data=True)
        if first_message.email_id:
            other_email_ids.append(first_message.email_id)

    return parent_email_id, other_email_ids


def get_system_from_address(sr):
    return "r/{subreddit} mail <{sender_email}>".format(
        subreddit=sr.name, sender_email=g.modmail_system_email)


def send_modmail_email(message):
    if not message.sr_id:
        return

    sr = Subreddit._byID(message.sr_id, data=True)

    forwarding_email = g.live_config['modmail_forwarding_email'].get(sr.name)
    if not forwarding_email:
        return

    sender = Account._byID(message.author_id, data=True)

    if sender.name in g.admins:
        distinguish = "[A]"
    elif sr.is_moderator(sender):
        distinguish = "[M]"
    else:
        distinguish = None

    if distinguish:
        from_address = "u/{username} {distinguish} <{sender_email}>".format(
            username=sender.name, distinguish=distinguish,
            sender_email=g.modmail_sender_email)
    else:
        from_address = "u/{username} <{sender_email}>".format(
            username=sender.name, sender_email=g.modmail_sender_email)

    reply_to = get_reply_to_address(message)
    parent_email_id, other_email_ids = get_email_ids(message)
    subject = get_message_subject(message)

    if message.from_sr and not message.first_message:
        # this is a message from the subreddit to a user. add some text that
        # shows the recipient
        recipient = Account._byID(message.to_id, data=True)
        sender_text = ("This message was sent from r/{subreddit} to "
            "u/{user}").format(subreddit=sr.name, user=recipient.name)
    else:
        userlink = add_sr("/u/{name}".format(name=sender.name), sr_path=False)
        sender_text = "This message was sent by {userlink}".format(
            userlink=userlink,
        )

    reply_footer = ("\n\n-\n{sender_text}\n\n"
        "Reply to this email directly or view it on reddit: {link}")
    reply_footer = reply_footer.format(
        sender_text=sender_text,
        link=message.make_permalink(force_domain=True),
    )
    message_text = message.body + reply_footer

    email_id = g.email_provider.send_email(
        to_address=forwarding_email,
        from_address=from_address,
        subject=subject,
        text=message_text,
        reply_to=reply_to,
        parent_email_id=parent_email_id,
        other_email_ids=other_email_ids,
    )
    if email_id:
        g.log.info("sent %s as %s", message._id36, email_id)
        message.email_id = email_id
        message._commit()
        g.stats.simple_event("modmail_email.outgoing_email")


def send_blocked_muted_email(sr, parent, sender_email, incoming_email_id):
    subject = get_message_subject(parent)
    from_address = get_system_from_address(sr)
    text = "Message was not delivered because recipient is muted."

    email_id = g.email_provider.send_email(
        to_address=sender_email,
        from_address=from_address,
        subject=subject,
        text=text,
        reply_to=from_address,
        parent_email_id=incoming_email_id,
        other_email_ids=[parent.email_id],
    )
    if email_id:
        g.log.info("sent as %s", email_id)


def queue_modmail_email(message):
    amqp.add_item(
        "modmail_email_q",
        json.dumps({
            "event": "new_message",
            "message_id36": message._id36,
        }),
    )


def queue_blocked_muted_email(sr, parent, sender_email, incoming_email_id):
    amqp.add_item(
        "modmail_email_q",
        json.dumps({
            "event": "blocked_muted",
            "subreddit_id36": sr._id36,
            "parent_id36": parent._id36,
            "sender_email": sender_email,
            "incoming_email_id": incoming_email_id,
        }),
    )


def process_modmail_email():
    @g.stats.amqp_processor("modmail_email_q")
    def process_message(msg):
        msg_dict = json.loads(msg.body)
        if msg_dict["event"] == "new_message":
            message_id36 = msg_dict["message_id36"]
            message = Message._byID36(message_id36, data=True)
            send_modmail_email(message)
        elif msg_dict["event"] == "blocked_muted":
            subreddit_id36 = msg_dict["subreddit_id36"]
            sr = Subreddit._byID36(subreddit_id36, data=True)
            parent_id36 = msg_dict["parent_id36"]
            parent = Message._byID36(parent_id36, data=True)
            sender_email = msg_dict["sender_email"]
            incoming_email_id = msg_dict["incoming_email_id"]
            send_blocked_muted_email(sr, parent, sender_email, incoming_email_id)

    amqp.consume_items("modmail_email_q", process_message)
<EOF>
<BOF>
from datetime import datetime
import math
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.controllers.util import abort
import pytz

from r2.controllers.reddit_base import UnloggedUser
from r2.lib import js
from r2.models import Account, NotFound
from r2.models.subreddit import Subreddit

# Note: This template is shared between python and javascript. See underscore
# templating in embed.js for more info. (Specific note: Only %()s is supported
# presently to use underscore templating.)
_COMMENT_EMBED_TEMPLATE = (
    '<div class="reddit-embed" '
        'data-embed-media="%(media)s" '
        'data-embed-parent="%(parent)s" '
        'data-embed-live="%(live)s" '
        'data-embed-created="%(created)s">'
        '<a href="%(comment)s">Comment</a> '
        'from discussion '
        '<a href="%(link)s">%(title)s</a>.'
    '</div>'
)


def get_inject_template(omitscript=False):
    template = _COMMENT_EMBED_TEMPLATE
    if not omitscript:
        script_urls = js.src("comment-embed", absolute=True, mangle_name=False)
        scripts = "".join('<script%s src="%s"></script>' % (
            ' async' if len(script_urls) == 1 else '',
            script_url
        ) for script_url in script_urls)
        template += scripts
    return template


def edited_after(thing, iso_timestamp, showedits):
    if not thing:
        return False

    if not isinstance(getattr(thing, "editted", False), datetime):
        return False

    try:
        created = datetime.strptime(iso_timestamp, "%Y-%m-%dT%H:%M:%S.%fZ")
    except ValueError:
        return not showedits

    created = created.replace(tzinfo=pytz.utc)

    return created < thing.editted


def prepare_embed_request():
    """Given a request, determine if we are embedding. If so, prepare the
    request for embedding.

    Returns the value of the embed GET parameter.
    """
    is_embed = request.GET.get('embed')

    if not is_embed:
        return None

    if request.host != g.media_domain:
        # don't serve up untrusted content except on our
        # specifically untrusted domain
        abort(404)

    c.allow_framing = True

    return is_embed


def set_up_comment_embed(sr, thing, showedits):
    try:
        author = Account._byID(thing.author_id) if thing.author_id else None
    except NotFound:
        author = None

    iso_timestamp = request.GET.get("created", "")

    c.embed_config = {
        "eventtracker_url": g.eventtracker_url or "",
        "anon_eventtracker_url": g.anon_eventtracker_url or "",
        "event_clicktracker_url": g.event_clicktracker_url or "",
        "created": iso_timestamp,
        "showedits": showedits,
        "thing": {
            "id": thing._id,
            "sr_id": sr._id,
            "sr_name": sr.name,
            "edited": edited_after(thing, iso_timestamp, showedits),
            "deleted": thing.deleted or author._deleted,
        },
        "comment_max_height": 200,
    }

    c.render_style = "iframe"
    c.user = UnloggedUser([c.lang])
    c.user_is_loggedin = False
    c.forced_loggedout = True


def is_embed():
    return c.render_style == "iframe"
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import calendar
from collections import namedtuple
import datetime
from decimal import Decimal, ROUND_DOWN, ROUND_UP
import hashlib
import hmac
import itertools
import json
import random
import time
import urllib
import urlparse

from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import ungettext
from pytz import timezone

from r2.lib import (
    authorize,
    emailer,
    hooks,
)
from r2.lib.db.operators import not_
from r2.lib.db import queries
from r2.lib.filters import _force_utf8
from r2.lib.geoip import location_by_ips
from r2.lib.memoize import memoize
from r2.lib.sgm import sgm
from r2.lib.strings import strings
from r2.lib.utils import (
    constant_time_compare,
    to_date,
    weighted_lottery,
)
from r2.models import (
    Account,
    Bid,
    Collection,
    DefaultSR,
    FakeAccount,
    FakeSubreddit,
    Frontpage,
    Link,
    MultiReddit,
    NotFound,
    NO_TRANSACTION,
    PromoCampaign,
    PROMOTE_STATUS,
    PromotedLink,
    PromotionLog,
    PromotionWeights,
    Subreddit,
    traffic,
)
from r2.models.keyvalue import NamedGlobals

PROMO_HEALTH_KEY = 'promotions_last_updated'

def _mark_promos_updated():
    NamedGlobals.set(PROMO_HEALTH_KEY, time.time())


def health_check():
    """Calculate the number of seconds since promotions were last updated"""
    return time.time() - int(NamedGlobals.get(PROMO_HEALTH_KEY, default=0))


def cost_per_mille(spend, impressions):
    """Return the cost-per-mille given ad spend and impressions."""
    if impressions:
        return 1000. * float(spend) / impressions
    else:
        return 0


def cost_per_click(spend, clicks):
    """Return the cost-per-click given ad spend and clicks."""
    if clicks:
        return float(spend) / clicks
    else:
        return 0


def promo_keep_fn(item):
    return (is_promoted(item) and
            not item.hidden and
            (c.over18 or not item.over_18))


# attrs

def _base_host(is_mobile_web=False):
    domain_prefix = "m" if is_mobile_web else g.domain_prefix
    if domain_prefix:
        base_domain = domain_prefix + '.' + g.domain
    else:
        base_domain = g.domain
    return "%s://%s" % (g.default_scheme, base_domain)


def promo_traffic_url(l): # old traffic url
    return "%s/traffic/%s/" % (_base_host(), l._id36)

def promotraffic_url(l): # new traffic url
    return "%s/promoted/traffic/headline/%s" % (_base_host(), l._id36)

def promo_edit_url(l):
    return "%s/promoted/edit_promo/%s" % (_base_host(), l._id36)

def view_live_url(link, campaign, srname):
    is_mobile_web = campaign.platform == "mobile_web"
    host = _base_host(is_mobile_web=is_mobile_web)
    if srname:
        host += '/r/%s' % srname
    return '%s/?ad=%s' % (host, link._fullname)

def payment_url(action, link_id36, campaign_id36):
    path = '/promoted/%s/%s/%s' % (action, link_id36, campaign_id36)
    return urlparse.urljoin(g.payment_domain, path)

def pay_url(l, campaign):
    return payment_url('pay', l._id36, campaign._id36)

def refund_url(l, campaign):
    return payment_url('refund', l._id36, campaign._id36)

# booleans

def is_awaiting_fraud_review(link):
    return link.payment_flagged_reason and link.fraud == None

def is_promo(link):
    return (link and not link._deleted and link.promoted is not None
            and hasattr(link, "promote_status"))

def is_accepted(link):
    return (is_promo(link) and
            link.promote_status != PROMOTE_STATUS.rejected and
            link.promote_status != PROMOTE_STATUS.edited_live and
            link.promote_status >= PROMOTE_STATUS.accepted)

def is_unpaid(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.unpaid

def is_unapproved(link):
    return is_promo(link) and link.promote_status <= PROMOTE_STATUS.unseen

def is_rejected(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.rejected

def is_promoted(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.promoted

def is_edited_live(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.edited_live

def is_finished(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.finished

def is_live_on_sr(link, sr):
    return bool(live_campaigns_by_link(link, sr=sr))

def is_pending(campaign):
    today = promo_datetime_now().date()
    return today < to_date(campaign.start_date)

def update_query(base_url, query_updates, unset=False):
    scheme, netloc, path, params, query, fragment = urlparse.urlparse(base_url)
    query_dict = urlparse.parse_qs(query)
    query_dict.update(query_updates)

    if unset:
        query_dict = dict((k, v) for k, v in query_dict.iteritems() if v is not None)

    query = urllib.urlencode(query_dict, doseq=True)
    return urlparse.urlunparse((scheme, netloc, path, params, query, fragment))


def update_served(items):
    for item in items:
        if not item.promoted:
            continue

        campaign = PromoCampaign._by_fullname(item.campaign)

        if not campaign.has_served:
            campaign.has_served = True
            campaign._commit()


NO_CAMPAIGN = "NO_CAMPAIGN"

def is_valid_click_url(link, click_url, click_hash):
    expected_mac = get_click_url_hmac(link, click_url)

    return constant_time_compare(click_hash, expected_mac)


def get_click_url_hmac(link, click_url):
    secret = g.secrets["adserver_click_url_secret"]
    data = "|".join([link._fullname, click_url])

    return hmac.new(secret, data, hashlib.sha256).hexdigest()


def add_trackers(items, sr, adserver_click_urls=None):
    """Add tracking names and hashes to a list of wrapped promoted links."""
    adserver_click_urls = adserver_click_urls or {}
    for item in items:
        if not item.promoted:
            continue

        if item.campaign is None:
            item.campaign = NO_CAMPAIGN

        tracking_name_fields = [item.fullname, item.campaign]
        if not isinstance(sr, FakeSubreddit):
            tracking_name_fields.append(sr.name)

        tracking_name = '-'.join(tracking_name_fields)

        # construct the impression pixel url
        pixel_mac = hmac.new(
            g.tracking_secret, tracking_name, hashlib.sha1).hexdigest()
        pixel_query = {
            "id": tracking_name,
            "hash": pixel_mac,
            "r": random.randint(0, 2147483647), # cachebuster
        }
        item.imp_pixel = update_query(g.adtracker_url, pixel_query)
        
        if item.third_party_tracking:
            item.third_party_tracking_url = item.third_party_tracking
        if item.third_party_tracking_2:
            item.third_party_tracking_url_2 = item.third_party_tracking_2

        # construct the click redirect url
        item_url = adserver_click_urls.get(item.campaign) or item.url
        url = _force_utf8(item_url)
        hashable = ''.join((url, tracking_name.encode("utf-8")))
        click_mac = hmac.new(
            g.tracking_secret, hashable, hashlib.sha1).hexdigest()
        click_query = {
            "id": tracking_name,
            "hash": click_mac,
            "url": url,
        }
        click_url = update_query(g.clicktracker_url, click_query)

        # overwrite the href_url with redirect click_url
        item.href_url = click_url

        # also overwrite the permalink url with redirect click_url for selfposts
        if item.is_self:
            item.permalink = click_url
        else:
            # add encrypted click url to the permalink for comments->click
            item.permalink = update_query(item.permalink, {
                "click_url": url,
                "click_hash": get_click_url_hmac(item, url),
            })

def update_promote_status(link, status):
    queries.set_promote_status(link, status)
    hooks.get_hook('promote.edit_promotion').call(link=link)


def new_promotion(is_self, title, content, author, ip):
    """
    Creates a new promotion with the provided title, etc, and sets it
    status to be 'unpaid'.
    """
    sr = Subreddit._byID(Subreddit.get_promote_srid())
    l = Link._submit(
        is_self=is_self,
        title=title,
        content=content,
        author=author,
        sr=sr,
        ip=ip,
    )

    l.promoted = True
    l.disable_comments = False
    l.sendreplies = True
    PromotionLog.add(l, 'promotion created')

    update_promote_status(l, PROMOTE_STATUS.unpaid)

    # the user has posted a promotion, so enable the promote menu unless
    # they have already opted out
    if author.pref_show_promote is not False:
        author.pref_show_promote = True
        author._commit()

    # notify of new promo
    emailer.new_promo(l)
    return l


def get_transactions(link, campaigns):
    """Return Bids for specified campaigns on the link.

    A PromoCampaign can have several bids associated with it, but the most
    recent one is recorded on the trans_id attribute. This is the one that will
    be returned.

    """

    campaigns = [c for c in campaigns if (c.trans_id != 0
                                          and c.link_id == link._id)]
    if not campaigns:
        return {}

    bids = Bid.lookup(thing_id=link._id)
    bid_dict = {(b.campaign, b.transaction): b for b in bids}
    bids_by_campaign = {c._id: bid_dict[(c._id, c.trans_id)] for c in campaigns}
    return bids_by_campaign

def new_campaign(link, dates, target, frequency_cap,
                 priority, location, platform,
                 mobile_os, ios_devices, ios_version_range, android_devices,
                 android_version_range, total_budget_pennies, cost_basis,
                 bid_pennies):
    campaign = PromoCampaign.create(link, target, dates[0], dates[1],
                                    frequency_cap, priority,
                                    location, platform, mobile_os, ios_devices,
                                    ios_version_range, android_devices,
                                    android_version_range, total_budget_pennies,
                                    cost_basis, bid_pennies)
    PromotionWeights.add(link, campaign)
    PromotionLog.add(link, 'campaign %s created' % campaign._id)

    if not campaign.is_house:
        author = Account._byID(link.author_id, data=True)
        if getattr(author, "complimentary_promos", False):
            free_campaign(link, campaign, c.user)

    hooks.get_hook('promote.new_campaign').call(link=link, campaign=campaign)
    return campaign


def free_campaign(link, campaign, user):
    auth_campaign(link, campaign, user, freebie=True)


def edit_campaign(link, campaign, dates, target, frequency_cap,
                  priority, location,
                  total_budget_pennies, cost_basis, bid_pennies,
                  platform='desktop', mobile_os=None, ios_devices=None,
                  ios_version_range=None, android_devices=None,
                  android_version_range=None):
    changed = {}
    if dates[0] != campaign.start_date or dates[1] != campaign.end_date:
        original = '%s to %s' % (campaign.start_date, campaign.end_date)
        edited = '%s to %s' % (dates[0], dates[1])
        changed['dates'] = (original, edited)
        campaign.start_date = dates[0]
        campaign.end_date = dates[1]
    if target != campaign.target:
        changed['target'] = (campaign.target, target)
        campaign.target = target
    if frequency_cap != campaign.frequency_cap:
        changed['frequency_cap'] = (campaign.frequency_cap, frequency_cap)
        campaign.frequency_cap = frequency_cap
    if priority != campaign.priority:
        changed['priority'] = (campaign.priority.name, priority.name)
        campaign.priority = priority
    if location != campaign.location:
        changed['location'] = (campaign.location, location)
        campaign.location = location
    if platform != campaign.platform:
        changed["platform"] = (campaign.platform, platform)
        campaign.platform = platform
    if mobile_os != campaign.mobile_os:
        changed["mobile_os"] = (campaign.mobile_os, mobile_os)
        campaign.mobile_os = mobile_os
    if ios_devices != campaign.ios_devices:
        changed['ios_devices'] = (campaign.ios_devices, ios_devices)
        campaign.ios_devices = ios_devices
    if android_devices != campaign.android_devices:
        changed['android_devices'] = (campaign.android_devices, android_devices)
        campaign.android_devices = android_devices
    if ios_version_range != campaign.ios_version_range:
        changed['ios_version_range'] = (campaign.ios_version_range,
                                        ios_version_range)
        campaign.ios_version_range = ios_version_range
    if android_version_range != campaign.android_version_range:
        changed['android_version_range'] = (campaign.android_version_range,
                                            android_version_range)
        campaign.android_version_range = android_version_range
    if total_budget_pennies != campaign.total_budget_pennies:
        void_campaign(link, campaign, reason='changed_budget')
        campaign.total_budget_pennies = total_budget_pennies
    if cost_basis != campaign.cost_basis:
        changed['cost_basis'] = (campaign.cost_basis, cost_basis)
        campaign.cost_basis = cost_basis
    if bid_pennies != campaign.bid_pennies:
        changed['bid_pennies'] = (campaign.bid_pennies,
                                        bid_pennies)
        campaign.bid_pennies = bid_pennies

    change_strs = map(lambda t: '%s: %s -> %s' % (t[0], t[1][0], t[1][1]),
                      changed.iteritems())
    change_text = ', '.join(change_strs)
    campaign._commit()

    # update the index
    PromotionWeights.reschedule(link, campaign)

    if not campaign.is_house:
        # make it a freebie, if applicable
        author = Account._byID(link.author_id, True)
        if getattr(author, "complimentary_promos", False):
            free_campaign(link, campaign, c.user)

    # record the changes
    if change_text:
        PromotionLog.add(link, 'edited %s: %s' % (campaign, change_text))

    hooks.get_hook('promote.edit_campaign').call(link=link, campaign=campaign)


def terminate_campaign(link, campaign):
    if not is_live_promo(link, campaign):
        return

    now = promo_datetime_now()
    original_end = campaign.end_date
    dates = [campaign.start_date, now]

    # NOTE: this will delete PromotionWeights after and including now.date()
    edit_campaign(
        link=link,
        campaign=campaign,
        dates=dates,
        target=campaign.target,
        frequency_cap=campaign.frequency_cap,
        priority=campaign.priority,
        location=campaign.location,
        total_budget_pennies=campaign.total_budget_pennies,
        cost_basis=campaign.cost_basis,
        bid_pennies=campaign.bid_pennies,
    )

    campaigns = list(PromoCampaign._by_link(link._id))
    is_live = any(is_live_promo(link, camp) for camp in campaigns
                                            if camp._id != campaign._id)
    if not is_live:
        update_promote_status(link, PROMOTE_STATUS.finished)
        all_live_promo_srnames(_update=True)

    msg = 'terminated campaign %s (original end %s)' % (campaign._id,
                                                        original_end.date())
    PromotionLog.add(link, msg)


def delete_campaign(link, campaign):
    PromotionWeights.delete(link, campaign)
    void_campaign(link, campaign, reason='deleted_campaign')
    campaign.delete()
    PromotionLog.add(link, 'deleted campaign %s' % campaign._id)
    hooks.get_hook('promote.delete_campaign').call(link=link, campaign=campaign)


def toggle_pause_campaign(link, campaign, should_pause):
    campaign.paused = should_pause
    campaign._commit()

    action = 'paused' if should_pause else 'resumed'
    PromotionLog.add(link, '%s campaign %s' % (action, campaign._id))

    hooks.get_hook('promote.edit_campaign').call(link=link,
        campaign=campaign)


def void_campaign(link, campaign, reason):
    transactions = get_transactions(link, [campaign])
    bid_record = transactions.get(campaign._id)
    if bid_record:
        a = Account._byID(link.author_id)
        authorize.void_transaction(a, bid_record.transaction, campaign._id)
        campaign.trans_id = NO_TRANSACTION
        campaign._commit()
        text = ('voided transaction for %s: (trans_id: %d)'
                % (campaign, bid_record.transaction))
        PromotionLog.add(link, text)

        if bid_record.transaction > 0:
            # notify the user that the transaction was voided if it was not
            # a freebie
            emailer.void_payment(
                link,
                campaign,
                reason=reason,
                total_budget_dollars=campaign.total_budget_dollars
            )


def auth_campaign(link, campaign, user, pay_id=None, freebie=False):
    """
    Authorizes (but doesn't charge) a budget with authorize.net.
    Args:
    - link: promoted link
    - campaign: campaign to be authorized
    - user: Account obj of the user doing the auth (usually the currently
        logged in user)
    - pay_id: customer payment profile id to use for this transaction. (One
        user can have more than one payment profile if, for instance, they have
        more than one credit card on file.) Set pay_id to -1 for freebies.

    Returns: (True, "") if successful or (False, error_msg) if not. 
    """
    void_campaign(link, campaign, reason='changed_payment')

    if freebie:
        trans_id, reason = authorize.auth_freebie_transaction(
            campaign.total_budget_dollars, user, link, campaign._id)
    else:
        trans_id, reason = authorize.auth_transaction(
            campaign.total_budget_dollars, user, pay_id, link, campaign._id)

    if trans_id and not reason:
        text = ('updated payment and/or budget for campaign %s: '
                'SUCCESS (trans_id: %d, amt: %0.2f)' %
                (campaign._id, trans_id, campaign.total_budget_dollars))
        PromotionLog.add(link, text)
        if trans_id < 0:
            PromotionLog.add(link, 'FREEBIE (campaign: %s)' % campaign._id)

        if trans_id:
            if is_finished(link):
                # When a finished promo gets a new paid campaign it doesn't
                # need to go through approval again and is marked accepted
                new_status = PROMOTE_STATUS.accepted
            else:
                new_status = max(PROMOTE_STATUS.unseen, link.promote_status)
        else:
            new_status = max(PROMOTE_STATUS.unpaid, link.promote_status)
        update_promote_status(link, new_status)

        if user and (user._id == link.author_id) and trans_id > 0:
            emailer.promo_total_budget(link,
                campaign.total_budget_dollars,
                campaign.start_date)

    else:
        text = ("updated payment and/or budget for campaign %s: FAILED ('%s')"
                % (campaign._id, reason))
        PromotionLog.add(link, text)
        trans_id = 0

    campaign.trans_id = trans_id
    campaign._commit()

    return bool(trans_id), reason



# dates are referenced to UTC, while we want promos to change at (roughly)
# midnight eastern-US.
# TODO: make this a config parameter
timezone_offset = -5 # hours
timezone_offset = datetime.timedelta(0, timezone_offset * 3600)
def promo_datetime_now(offset=None):
    now = datetime.datetime.now(g.tz) + timezone_offset
    if offset is not None:
        now += datetime.timedelta(offset)
    return now


# campaigns can launch the following day if they're created before 17:00 PDT
DAILY_CUTOFF = datetime.time(17, tzinfo=timezone("US/Pacific"))

def get_date_limits(link, is_sponsor=False):
    promo_today = promo_datetime_now().date()

    if is_sponsor:
        min_start = promo_today
    elif is_accepted(link):
        # link is already accepted--let user create a campaign starting
        # tomorrow because it doesn't need to be re-reviewed
        min_start = promo_today + datetime.timedelta(days=1)
    else:
        # campaign and link will need to be reviewed before they can launch.
        # review can happen until DAILY_CUTOFF PDT Monday through Friday and
        # Sunday. Any campaign created after DAILY_CUTOFF is treated as if it
        # were created the following day.
        now = datetime.datetime.now(tz=timezone("US/Pacific"))
        now_today = now.date()
        too_late_for_review = now.time() > DAILY_CUTOFF

        if too_late_for_review and now_today.weekday() == calendar.FRIDAY:
            # no review late on Friday--earliest review is Sunday to launch
            # on Monday
            min_start = now_today + datetime.timedelta(days=3)
        elif now_today.weekday() == calendar.SATURDAY:
            # no review any time on Saturday--earliest review is Sunday to
            # launch on Monday
            min_start = now_today + datetime.timedelta(days=2)
        elif too_late_for_review:
            # no review late in the day--earliest review is tomorrow to
            # launch the following day
            min_start = now_today + datetime.timedelta(days=2)
        else:
            # review will happen today so can launch tomorrow
            min_start = now_today + datetime.timedelta(days=1)

    if is_sponsor:
        max_end = promo_today + datetime.timedelta(days=366)
    else:
        max_end = promo_today + datetime.timedelta(days=93)

    if is_sponsor:
        max_start = max_end - datetime.timedelta(days=1)
    else:
        # authorization hold happens now but expires after 30 days. charge
        # happens 1 day before the campaign launches. the latest a campaign
        # can start is 30 days from now (it will get charged in 29 days).
        max_start = promo_today + datetime.timedelta(days=30)

    return min_start, max_start, max_end


def accept_promotion(link):
    was_edited_live = is_edited_live(link)
    update_promote_status(link, PROMOTE_STATUS.accepted)

    if link._spam:
        link._spam = False
        link._commit()

    if not was_edited_live:
        emailer.accept_promo(link)

    # if the link has campaigns running now charge them and promote the link
    now = promo_datetime_now()
    campaigns = list(PromoCampaign._by_link(link._id))
    is_live = False
    for camp in campaigns:
        if is_accepted_promo(now, link, camp):
            # if link was edited live, do not check against Authorize.net
            if not was_edited_live:
                charge_campaign(link, camp)
            if charged_or_not_needed(camp):
                promote_link(link, camp)
                is_live = True

    if is_live:
        all_live_promo_srnames(_update=True)


def flag_payment(link, reason):
    # already determined to be fraud or already flagged for that reason.
    if link.fraud or reason in link.payment_flagged_reason:
        return

    if link.payment_flagged_reason:
        link.payment_flagged_reason += (", %s" % reason)
    else:
        link.payment_flagged_reason = reason

    link._commit()
    PromotionLog.add(link, "payment flagged: %s" % reason)
    queries.set_payment_flagged_link(link)


def review_fraud(link, is_fraud):
    link.fraud = is_fraud
    link._commit()
    PromotionLog.add(link, "marked as fraud" if is_fraud else "resolved as not fraud")
    queries.unset_payment_flagged_link(link)

    if is_fraud:
        reject_promotion(link, "fraud", notify_why=False)
        hooks.get_hook("promote.fraud_identified").call(link=link, sponsor=c.user)


def reject_promotion(link, reason=None, notify_why=True):
    if is_rejected(link):
        return

    was_live = is_promoted(link)
    update_promote_status(link, PROMOTE_STATUS.rejected)
    if reason:
        PromotionLog.add(link, "rejected: %s" % reason)

    # Send a rejection email (unless the advertiser requested the reject)
    if not c.user or c.user._id != link.author_id:
        emailer.reject_promo(link, reason=(reason if notify_why else None))

    if was_live:
        all_live_promo_srnames(_update=True)


def unapprove_promotion(link):
    if is_unpaid(link):
        return
    elif is_finished(link):
        # when a finished promo is edited it is bumped down to unpaid so if it
        # eventually gets a paid campaign it can get upgraded to unseen and
        # reviewed
        update_promote_status(link, PROMOTE_STATUS.unpaid)
    else:
        update_promote_status(link, PROMOTE_STATUS.unseen)


def edited_live_promotion(link):
    update_promote_status(link, PROMOTE_STATUS.edited_live)
    emailer.edited_live_promo(link)


def authed_or_not_needed(campaign):
    authed = campaign.trans_id != NO_TRANSACTION
    needs_auth = not campaign.is_house
    return authed or not needs_auth


def charged_or_not_needed(campaign):
    # True if a campaign has a charged transaction or doesn't need one
    charged = authorize.is_charged_transaction(campaign.trans_id, campaign._id)
    needs_charge = not campaign.is_house
    return charged or not needs_charge


def is_served_promo(date, link, campaign):
    return (campaign.start_date <= date < campaign.end_date and
            campaign.has_served)


def is_accepted_promo(date, link, campaign):
    return (campaign.start_date <= date < campaign.end_date and
            is_accepted(link) and
            authed_or_not_needed(campaign))


def is_scheduled_promo(date, link, campaign):
    return (is_accepted_promo(date, link, campaign) and 
            charged_or_not_needed(campaign))


def is_live_promo(link, campaign):
    now = promo_datetime_now()
    return is_promoted(link) and is_scheduled_promo(now, link, campaign)


def is_complete_promo(link, campaign):
    return (campaign.is_paid and 
        not (is_live_promo(link, campaign) or is_pending(campaign)))


def _is_geotargeted_promo(link):
    campaigns = live_campaigns_by_link(link)
    geotargeted = filter(lambda camp: camp.location, campaigns)
    city_target = any(camp.location.metro for camp in geotargeted)
    return bool(geotargeted), city_target


def is_geotargeted_promo(link):
    key = 'geopromo:%s' % link._id
    from_cache = g.gencache.get(key)
    if not from_cache:
        ret = _is_geotargeted_promo(link)
        g.gencache.set(key, ret, time=60)
        return ret
    else:
        return from_cache


def get_promos(date, sr_names=None, link=None):
    campaign_ids = PromotionWeights.get_campaign_ids(
        date, sr_names=sr_names, link=link)
    campaigns = PromoCampaign._byID(campaign_ids, data=True, return_dict=False)
    link_ids = {camp.link_id for camp in campaigns}
    links = Link._byID(link_ids, data=True)
    for camp in campaigns:
        yield camp, links[camp.link_id]


def get_accepted_promos(offset=0):
    date = promo_datetime_now(offset=offset)
    for camp, link in get_promos(date):
        if is_accepted_promo(date, link, camp):
            yield camp, link


def get_scheduled_promos(offset=0):
    date = promo_datetime_now(offset=offset)
    for camp, link in get_promos(date):
        if is_scheduled_promo(date, link, camp):
            yield camp, link


def get_served_promos(offset=0):
    date = promo_datetime_now(offset=offset)
    for camp, link in get_promos(date):
        if is_served_promo(date, link, camp):
            yield camp, link


def charge_campaign(link, campaign):
    if charged_or_not_needed(campaign):
        return

    user = Account._byID(link.author_id)
    success, reason = authorize.charge_transaction(user, campaign.trans_id,
                                                   campaign._id)

    if not success:
        if reason == authorize.TRANSACTION_NOT_FOUND:
            # authorization hold has expired
            original_trans_id = campaign.trans_id
            campaign.trans_id = NO_TRANSACTION
            campaign._commit()
            text = ('voided expired transaction for %s: (trans_id: %d)'
                    % (campaign, original_trans_id))
            PromotionLog.add(link, text)
        return

    hooks.get_hook('promote.edit_campaign').call(link=link, campaign=campaign)

    if not is_promoted(link):
        update_promote_status(link, PROMOTE_STATUS.pending)

    emailer.queue_promo(link,
        campaign.total_budget_dollars,
        campaign.trans_id)
    text = ('auth charge for campaign %s, trans_id: %d' %
            (campaign._id, campaign.trans_id))
    PromotionLog.add(link, text)


def charge_pending(offset=1):
    for camp, link in get_accepted_promos(offset=offset):
        charge_campaign(link, camp)


def live_campaigns_by_link(link, sr=None):
    if not is_promoted(link):
        return []

    sr_names = [sr.name] if sr else None
    now = promo_datetime_now()
    return [camp for camp, link in get_promos(now, sr_names=sr_names,
                                              link=link)
            if is_live_promo(link, camp)]


def promote_link(link, campaign):
    if (not link.over_18 and
        not link.over_18_override and
        any(sr.over_18 for sr in campaign.target.subreddits_slow)):
        link.over_18 = True
        link._commit()

    if not is_promoted(link):
        update_promote_status(link, PROMOTE_STATUS.promoted)
        emailer.live_promo(link)


def make_daily_promotions():
    # charge campaigns so they can go live
    charge_pending(offset=0)
    charge_pending(offset=1)

    # promote links and record ids of promoted links
    link_ids = set()
    for campaign, link in get_scheduled_promos(offset=0):
        link_ids.add(link._id)
        promote_link(link, campaign)

    # expire finished links
    q = Link._query(Link.c.promote_status == PROMOTE_STATUS.promoted, data=True)
    q = q._filter(not_(Link.c._id.in_(link_ids)))
    for link in q:
        update_promote_status(link, PROMOTE_STATUS.finished)
        emailer.finished_promo(link)

    # update subreddits with promos
    all_live_promo_srnames(_update=True)

    _mark_promos_updated()
    finalize_completed_campaigns(daysago=1)
    hooks.get_hook('promote.make_daily_promotions').call(offset=0)


def finalize_completed_campaigns(daysago=1):
    # PromoCampaign.end_date is utc datetime with year, month, day only
    now = datetime.datetime.now(g.tz)
    date = now - datetime.timedelta(days=daysago)
    date = date.replace(hour=0, minute=0, second=0, microsecond=0)

    q = PromoCampaign._query(PromoCampaign.c.end_date == date,
                             # exclude no transaction
                             PromoCampaign.c.trans_id != NO_TRANSACTION,
                             data=True)
    # filter out freebies
    campaigns = filter(lambda camp: camp.trans_id > NO_TRANSACTION, q)

    if not campaigns:
        return

    # check that traffic is up to date
    earliest_campaign = min(campaigns, key=lambda camp: camp.start_date)
    start, end = get_total_run(earliest_campaign)
    missing_traffic = traffic.get_missing_traffic(start.replace(tzinfo=None),
                                                  date.replace(tzinfo=None))
    if missing_traffic:
        raise ValueError("Can't finalize campaigns finished on %s."
                         "Missing traffic from %s" % (date, missing_traffic))

    links = Link._byID([camp.link_id for camp in campaigns], data=True)
    underdelivered_campaigns = []

    for camp in campaigns:
        if hasattr(camp, 'refund_amount'):
            continue

        link = links[camp.link_id]
        billable_impressions = get_billable_impressions(camp)
        billable_amount = get_billable_amount(camp, billable_impressions)

        if billable_amount >= camp.total_budget_pennies:
            if hasattr(camp, 'cpm'):
                text = '%s completed with $%s billable (%s impressions @ $%s).'
                text %= (camp, billable_amount, billable_impressions,
                    camp.bid_dollars)
            else:
                text = '%s completed with $%s billable (pre-CPM).'
                text %= (camp, billable_amount) 
            PromotionLog.add(link, text)
            camp.refund_amount = 0.
            camp._commit()
        elif charged_or_not_needed(camp):
            underdelivered_campaigns.append(camp)

        if underdelivered_campaigns:
            queries.set_underdelivered_campaigns(underdelivered_campaigns)


def get_refund_amount(camp, billable):
    existing_refund = getattr(camp, 'refund_amount', 0.)
    charge = camp.total_budget_dollars - existing_refund
    refund_amount = charge - billable
    refund_amount = Decimal(str(refund_amount)).quantize(Decimal('.01'),
                                                    rounding=ROUND_UP)
    return max(float(refund_amount), 0.)


def refund_campaign(link, camp, refund_amount, billable_amount,
        billable_impressions):
    owner = Account._byID(camp.owner_id, data=True)
    success, reason = authorize.refund_transaction(
        owner, camp.trans_id, camp._id, refund_amount)
    if not success:
        text = ('%s $%s refund failed' % (camp, refund_amount))
        PromotionLog.add(link, text)
        g.log.debug(text + ' (reason: %s)' % reason)

        return False

    if billable_impressions:
        text = ('%s completed with $%s billable (%s impressions @ $%s).'
                ' %s refunded.' % (camp, billable_amount,
                                   billable_impressions,
                                   camp.bid_pennies / 100.,
                                   refund_amount))
    else:
        text = ('%s completed with $%s billable. %s refunded' % (camp,
            billable_amount, refund_amount))

    PromotionLog.add(link, text)
    camp.refund_amount = refund_amount
    camp._commit()
    queries.unset_underdelivered_campaigns(camp)
    emailer.refunded_promo(link)

    return True


PromoTuple = namedtuple('PromoTuple', ['link', 'weight', 'campaign'])


@memoize('all_live_promo_srnames', stale=True)
def all_live_promo_srnames():
    now = promo_datetime_now()
    srnames = itertools.chain.from_iterable(
        camp.target.subreddit_names for camp, link in get_promos(now)
                                    if is_live_promo(link, camp)
    )
    return set(srnames)

@memoize('get_nsfw_collections_srnames', time=(60*60), stale=True)
def get_nsfw_collections_srnames():
    all_collections = Collection.get_all()
    nsfw_collections = [col for col in all_collections if col.over_18]
    srnames = itertools.chain.from_iterable(
        col.sr_names for col in nsfw_collections
    )

    return set(srnames)


def is_site_over18(site):
    # a site should be considered nsfw if it's included in a
    # nsfw collection because nsfw ads can target nsfw collections.
    nsfw_collection_srnames = get_nsfw_collections_srnames()
    return site.over_18 or site.name in nsfw_collection_srnames


def srnames_from_site(user, site, include_subscriptions=True):
    is_logged_in = user and not isinstance(user, FakeAccount)
    over_18 = is_site_over18(site)
    srnames = set()

    if not isinstance(site, FakeSubreddit):
        srnames.add(site.name)
    elif isinstance(site, MultiReddit):
        srnames = srnames | {sr.name for sr in site.srs}
    else:
        srnames.add(Frontpage.name)

        if is_logged_in and include_subscriptions:
            subscriptions = Subreddit.user_subreddits(
                user,
                ids=False,
            )

            # only use subreddits that aren't quarantined and have the same
            # age gate as the subreddit being viewed.
            subscriptions = filter(
                lambda sr: not sr.quarantine and sr.over_18 == over_18,
                subscriptions,
            )

            subscription_srnames = {sr.name for sr in subscriptions}

            # remove any subscriptions that may have nsfw ads targeting
            # them because they're apart of a nsfw collection.
            nsfw_collection_srnames = get_nsfw_collections_srnames()

            if not over_18:
                subscription_srnames = (subscription_srnames -
                    nsfw_collection_srnames)

            srnames = srnames | subscription_srnames

    return srnames


def keywords_from_context(
        user, site,
        include_subscriptions=True,
        live_promos_only=True,
    ):

    keywords = srnames_from_site(
        user, site,
        include_subscriptions,
    )

    # if the ad was created by selfserve then we know
    # whether or not there exists an ad for that keyword
    # and can remove un-targeted keywords accordingly.
    if live_promos_only:
        live_srnames = all_live_promo_srnames()
        keywords = live_srnames.intersection(keywords)

    if (not isinstance(site,FakeSubreddit) and
            site._downs > g.live_config["ads_popularity_threshold"]):
        keywords.add("s.popular")

    if is_site_over18(site):
        keywords.add("s.nsfw")
    else:
        keywords.add("s.sfw")

    if c.user_is_loggedin:
        keywords.add("loggedin")
    else:
        keywords.add("loggedout")

    return keywords


# special handling for memcache ascii protocol
SPECIAL_NAMES = {" reddit.com": "_reddit.com"}
REVERSED_NAMES = {v: k for k, v in SPECIAL_NAMES.iteritems()}


def _get_live_promotions(sanitized_names):
    now = promo_datetime_now()
    sr_names = [REVERSED_NAMES.get(name, name) for name in sanitized_names]
    ret = {sr_name: [] for sr_name in sanitized_names}
    for camp, link in get_promos(now, sr_names=sr_names):
        if is_live_promo(link, camp):
            weight = (camp.total_budget_dollars / camp.ndays)
            pt = PromoTuple(link=link._fullname, weight=weight,
                            campaign=camp._fullname)
            for sr_name in camp.target.subreddit_names:
                if sr_name in sr_names:
                    sanitized_name = SPECIAL_NAMES.get(sr_name, sr_name)
                    ret[sanitized_name].append(pt)
    return ret


def get_live_promotions(sr_names):
    sanitized_names = [SPECIAL_NAMES.get(name, name) for name in sr_names]
    promos_by_sanitized_name = sgm(
        cache=g.gencache,
        keys=sanitized_names,
        miss_fn=_get_live_promotions,
        prefix='srpromos:',
        time=60,
        stale=True,
    )
    promos_by_srname = {
        REVERSED_NAMES.get(name, name): val
        for name, val in promos_by_sanitized_name.iteritems()
    }
    return itertools.chain.from_iterable(promos_by_srname.itervalues())


def lottery_promoted_links(sr_names, n=10):
    """Run weighted_lottery to order and choose a subset of promoted links."""
    promo_tuples = get_live_promotions(sr_names)

    # house priority campaigns have weight of 0, use some small value
    # so they'll show if there are no other campaigns
    weights = {p: p.weight or 0.001 for p in promo_tuples}
    selected = []
    while weights and len(selected) < n:
        s = weighted_lottery(weights)
        del weights[s]
        selected.append(s)
    return selected


def get_total_run(thing):
    """Return the total time span this link or campaign will run.

    Starts at the start date of the earliest campaign and goes to the end date
    of the latest campaign.

    """
    campaigns = []
    if isinstance(thing, Link):
        campaigns = PromoCampaign._by_link(thing._id)
    elif isinstance(thing, PromoCampaign):
        campaigns = [thing]
    else:
        campaigns = []

    earliest = None
    latest = None
    for campaign in campaigns:
        if not charged_or_not_needed(campaign):
            continue

        if not earliest or campaign.start_date < earliest:
            earliest = campaign.start_date

        if not latest or campaign.end_date > latest:
            latest = campaign.end_date

    # a manually launched promo (e.g., sr discovery) might not have campaigns.
    if not earliest or not latest:
        latest = datetime.datetime.utcnow()
        earliest = latest - datetime.timedelta(days=30)  # last month

    # ugh this stuff is a mess. they're stored as "UTC" but actually mean UTC-5.
    earliest = earliest.replace(tzinfo=g.tz) - timezone_offset
    latest = latest.replace(tzinfo=g.tz) - timezone_offset

    return earliest, latest


def get_traffic_dates(thing):
    """Retrieve the start and end of a Promoted Link or PromoCampaign."""
    now = datetime.datetime.now(g.tz).replace(minute=0, second=0, microsecond=0)
    start, end = get_total_run(thing)
    end = min(now, end)
    return start, end


def get_billable_impressions(campaign):
    start, end = get_traffic_dates(campaign)
    if start > datetime.datetime.now(g.tz):
        return 0

    traffic_lookup = traffic.TargetedImpressionsByCodename.promotion_history
    imps = traffic_lookup(campaign._fullname, start.replace(tzinfo=None),
                          end.replace(tzinfo=None))
    billable_impressions = sum(imp for date, (imp,) in imps)
    return billable_impressions


def get_billable_amount(camp, impressions):
    if not camp.is_auction:
        value_delivered = impressions / 1000. * camp.bid_dollars
        billable_amount = min(camp.total_budget_dollars, value_delivered)
    else:
        # pre-CPM campaigns are charged in full regardless of impressions
        billable_amount = camp.total_budget_dollars

    billable_amount = Decimal(str(billable_amount)).quantize(Decimal('.01'),
                                                        rounding=ROUND_DOWN)
    return float(billable_amount)


def get_spent_amount(campaign):
    if campaign.is_house:
        spent = 0.
    elif hasattr(campaign, 'refund_amount'):
        # no need to calculate spend if we've already refunded
        spent = campaign.total_budget_dollars - campaign.refund_amount
    elif campaign.is_auction:
        spent = campaign.adserver_spent_pennies / 100.
    else:
        billable_impressions = get_billable_impressions(campaign)
        spent = get_billable_amount(campaign, billable_impressions)
    return spent


def successful_payment(link, campaign, ip, address):
    if not address:
        return

    campaign.trans_ip = ip
    campaign.trans_billing_country = address.country

    location = location_by_ips(ip)

    if location:
        campaign.trans_ip_country = location.get("country_name")

        countries_match = (campaign.trans_billing_country.lower() ==
            campaign.trans_ip_country.lower())
        campaign.trans_country_match = countries_match

    campaign._commit()


def new_payment_method(user, ip, address, link):
    user._incr('num_payment_methods')
    hooks.get_hook('promote.new_payment_method').call(user=user, ip=ip, address=address, link=link)


def failed_payment_method(user, link):
    user._incr('num_failed_payments')
    hooks.get_hook('promote.failed_payment').call(user=user, link=link)


def Run(verbose=True):
    """reddit-job-update_promos: Intended to be run hourly to pull in
    scheduled changes to ads

    """

    if verbose:
        print "%s promote.py:Run() - make_daily_promotions()" % datetime.datetime.now(g.tz)

    make_daily_promotions()

    if verbose:
        print "%s promote.py:Run() - finished" % datetime.datetime.now(g.tz)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


def _force_unicode(text):
    if text == None:
        return u''

    if isinstance(text, unicode):
        return text

    try:
        text = unicode(text, 'utf-8')
    except UnicodeDecodeError:
        text = unicode(text, 'latin1')
    except TypeError:
        text = unicode(text)
    return text


def _force_utf8(text):
    return str(_force_unicode(text).encode('utf8'))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons.i18n import _, N_

from r2.models import Account, Message
from r2.lib.db import queries
from r2.lib.utils import blockquote_text


user_added_messages = {
    "moderator": {
        "pm": {
            "subject": N_("you are a moderator"),
            "msg": N_("you have been added as a moderator to [%(title)s](%(url)s)."),
        },
    },
    "moderator_invite": {
        "pm": {
            "subject": N_("invitation to moderate %(url)s"),
            "msg": N_("**gadzooks! you are invited to become a moderator of [%(title)s](%(url)s)!**\n\n"
                      "*to accept*, visit the [moderators page for %(url)s](%(url)s/about/moderators) and click \"accept\".\n\n"
                      "*otherwise,* if you did not expect to receive this, you can simply ignore this invitation or report it."),
        },
        "modmail": {
            "subject": N_("moderator invited"),
            "msg": N_("%(user)s has been invited by %(author)s to moderate %(url)s."),
        },
    },
    "accept_moderator_invite": {
        "modmail": {
            "subject": N_("moderator added"),
            "msg": N_("%(user)s has accepted an invitation to become moderator of %(url)s."),
        },
    },
    "contributor": {
        "pm": {
            "subject": N_("you are an approved submitter"),
            "msg": N_("you have been added as an approved submitter to [%(title)s](%(url)s)."),
        },
    },
    "traffic": {
        "pm": {
            "subject": N_("you can view traffic on a promoted link"),
            "msg": N_('you have been added to the list of users able to see [traffic for the sponsored link "%(title)s"](%(traffic_url)s).'),
        },
    },
}


def notify_user_added(rel_type, author, user, target):
    msgs = user_added_messages.get(rel_type)
    if not msgs:
        return

    srname = target.path.rstrip("/")
    d = {
        "url": srname,
        "title": "%s: %s" % (srname, target.title),
        "author": "/u/" + author.name,
        "user": "/u/" + user.name,
    }

    if "pm" in msgs and author != user:
        subject = msgs["pm"]["subject"] % d
        msg = msgs["pm"]["msg"] % d

        if rel_type in ("moderator_invite", "contributor"):
            # send the message from the subreddit
            item, inbox_rel = Message._new(
                author, user, subject, msg, request.ip, sr=target, from_sr=True,
                can_send_email=False)
        else:
            item, inbox_rel = Message._new(
                author, user, subject, msg, request.ip, can_send_email=False)

        queries.new_message(item, inbox_rel, update_modmail=False)

    if "modmail" in msgs:
        subject = msgs["modmail"]["subject"] % d
        msg = msgs["modmail"]["msg"] % d

        if rel_type == "moderator_invite":
            modmail_author = Account.system_user()
        else:
            modmail_author = author

        item, inbox_rel = Message._new(modmail_author, target, subject, msg,
                                       request.ip, sr=target)
        queries.new_message(item, inbox_rel)


def send_mod_removal_message(subreddit, mod, user):
    sr_name = "/r/" + subreddit.name
    u_name = "/u/" + user.name
    subject = "%(user)s has been removed as a moderator from %(subreddit)s"
    message = (
        "%(user)s: You have been removed as a moderator from %(subreddit)s.  "
        "If you have a question regarding your removal, you can "
        "contact the moderator team for %(subreddit)s by replying to this "
        "message."
    )
    subject %= {"subreddit": sr_name, "user": u_name}
    message %= {"subreddit": sr_name, "user": user.name}

    item, inbox_rel = Message._new(
        mod, user, subject, message, request.ip,
        sr=subreddit,
        from_sr=True,
        can_send_email=False,
    )
    queries.new_message(item, inbox_rel, update_modmail=True)


def send_ban_message(subreddit, mod, user, note=None, days=None, new=True):
    sr_name = "/r/" + subreddit.name
    if days:
        subject = "You've been temporarily banned from participating in %(subreddit)s"
        message = ("You have been temporarily banned from participating in "
            "%(subreddit)s. This ban will last for %(duration)s days. ")
    else:
        subject = "You've been banned from participating in %(subreddit)s"
        message = "You have been banned from participating in %(subreddit)s. "

    message += ("You can still view and subscribe to %(subreddit)s, but you "
                "won't be able to post or comment.")

    if not new:
        subject = "Your ban from %(subreddit)s has changed"

    subject %= {"subreddit": sr_name}
    message %= {"subreddit": sr_name, "duration": days}

    if note:
        message += "\n\n" + 'Note from the moderators:'
        message += "\n\n" + blockquote_text(note)

    message += "\n\n" + ("If you have a question regarding your ban, you can "
        "contact the moderator team for %(subreddit)s by replying to this "
        "message.") % {"subreddit": sr_name}

    message += "\n\n" + ("**Reminder from the Reddit staff**: If you use "
        "another account to circumvent this subreddit ban, that will be "
        "considered a violation of [the Content Policy](/help/contentpolicy#section_prohibited_behavior) "
        "and can result in your account being [suspended](https://reddit.zendesk.com/hc/en-us/articles/205687686) "
        "from the site as a whole.")

    item, inbox_rel = Message._new(
        mod, user, subject, message, request.ip, sr=subreddit, from_sr=True,
        can_send_email=False)
    queries.new_message(item, inbox_rel, update_modmail=False)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
from cStringIO import StringIO
import datetime
import gzip
import hashlib
import hmac
import itertools
import json
import pytz
import random
import requests
import time

import httpagentparser
import time

from pylons import app_globals as g
from uuid import uuid4
from wsgiref.handlers import format_date_time

from r2.lib import amqp, hooks
from r2.lib.language import charset_summary
from r2.lib.geoip import (
    get_request_location,
    location_by_ips,
)
from r2.lib.cache_poisoning import cache_headers_valid
from r2.lib.utils import (
    domain,
    to_epoch_milliseconds,
    sampled,
    squelch_exceptions,
    to36,
)


def _make_http_date(when=None):
    if when is None:
        when = datetime.datetime.now(pytz.UTC)
    return format_date_time(time.mktime(when.timetuple()))


# XXX External dependencies!
_datetime_to_millis = to_epoch_milliseconds


def parse_agent(ua):
    agent_summary = {}
    parsed = httpagentparser.detect(ua)
    for attr in ("browser", "os", "platform"):
        d = parsed.get(attr)
        if d:
            for subattr in ("name", "version"):
                if subattr in d:
                    key = "%s_%s" % (attr, subattr)
                    agent_summary[key] = d[subattr]

    agent_summary['bot'] = parsed.get('bot')

    return agent_summary


class EventQueue(object):
    def __init__(self, queue=amqp):
        self.queue = queue

    def save_event(self, event):
        if event.testing:
            queue_name = "event_collector_test"
        else:
            queue_name = "event_collector"

        # send info about truncatable field as a header, separate from the
        # actual event data
        headers = None
        if event.truncatable_field:
            headers = {"truncatable_field": event.truncatable_field}

        self.queue.add_item(queue_name, event.dump(), headers=headers)

    @squelch_exceptions
    @sampled("events_collector_vote_sample_rate")
    def vote_event(self, vote):
        """Create a 'vote' event for event-collector

        vote: An r2.models.vote Vote object
        """

        # For mapping vote directions to readable names used by data team
        def get_vote_direction_name(vote):
            if vote.is_upvote:
                return "up"
            elif vote.is_downvote:
                return "down"
            else:
                return "clear"

        event = Event(
            topic="vote_server",
            event_type="server_vote",
            time=vote.date,
            data=vote.event_data["context"],
            obfuscated_data=vote.event_data["sensitive"],
        )

        event.add("vote_direction", get_vote_direction_name(vote))

        if vote.previous_vote:
            event.add("prev_vote_direction",
                get_vote_direction_name(vote.previous_vote))
            event.add(
                "prev_vote_ts",
                to_epoch_milliseconds(vote.previous_vote.date)
            )

        if vote.is_automatic_initial_vote:
            event.add("auto_self_vote", True)

        for name, value in vote.effects.serializable_data.iteritems():
            # rename the "notes" field to "details_text" for the event
            if name == "notes":
                name = "details_text"

            event.add(name, value)

        # add the note codes separately as "process_notes"
        event.add("process_notes", ", ".join(vote.effects.note_codes))

        event.add_subreddit_fields(vote.thing.subreddit_slow)
        event.add_target_fields(vote.thing)

        # add the rank of the vote if we have it (passed in through the API)
        rank = vote.data.get('rank')
        if rank:
            event.add("target_rank", rank)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_submit_sample_rate")
    def submit_event(self, new_post, request=None, context=None):
        """Create a 'submit' event for event-collector

        new_post: An r2.models.Link object
        request, context: Should be pylons.request & pylons.c respectively

        """
        event = Event(
            topic="submit_events",
            event_type="ss.submit",
            time=new_post._date,
            request=request,
            context=context,
            truncatable_field="post_body",
        )

        event.add("post_id", new_post._id)
        event.add("post_fullname", new_post._fullname)
        event.add_text("post_title", new_post.title)

        event.add("user_neutered", new_post.author_slow._spam)

        if new_post.is_self:
            event.add("post_type", "self")
            event.add_text("post_body", new_post.selftext)
        else:
            event.add("post_type", "link")
            event.add("post_target_url", new_post.url)
            event.add("post_target_domain", new_post.link_domain())

        event.add_subreddit_fields(new_post.subreddit_slow)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_comment_sample_rate")
    def comment_event(self, new_comment, request=None, context=None):
        """Create a 'comment' event for event-collector.

        new_comment: An r2.models.Comment object
        request, context: Should be pylons.request & pylons.c respectively
        """
        from r2.models import Comment, Link

        event = Event(
            topic="comment_events",
            event_type="ss.comment",
            time=new_comment._date,
            request=request,
            context=context,
            truncatable_field="comment_body",
        )

        event.add("comment_id", new_comment._id)
        event.add("comment_fullname", new_comment._fullname)

        event.add_text("comment_body", new_comment.body)

        post = Link._byID(new_comment.link_id)
        event.add("post_id", post._id)
        event.add("post_fullname", post._fullname)
        event.add("post_created_ts", to_epoch_milliseconds(post._date))
        if post.promoted:
            event.add("post_is_promoted", bool(post.promoted))

        if new_comment.parent_id:
            parent = Comment._byID(new_comment.parent_id)
        else:
            # If this is a top-level comment, parent is the same as the post
            parent = post
        event.add("parent_id", parent._id)
        event.add("parent_fullname", parent._fullname)
        event.add("parent_created_ts", to_epoch_milliseconds(parent._date))

        event.add("user_neutered", new_comment.author_slow._spam)

        event.add_subreddit_fields(new_comment.subreddit_slow)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_poison_sample_rate")
    def cache_poisoning_event(self, poison_info, request=None, context=None):
        """Create a 'cache_poisoning_server' event for event-collector

        poison_info: Details from the client about the poisoning event
        request, context: Should be pylons.request & pylons.c respectively

        """
        poisoner_name = poison_info.pop("poisoner_name")

        event = Event(
            topic="cache_poisoning_events",
            event_type="ss.cache_poisoning",
            request=request,
            context=context,
            data=poison_info,
            truncatable_field="resp_headers",
        )

        event.add("poison_blame_guess", "proxy")

        resp_headers = poison_info["resp_headers"]
        if resp_headers:
            # Check if the caching headers we got back match the current policy
            cache_policy = poison_info["cache_policy"]
            headers_valid = cache_headers_valid(cache_policy, resp_headers)

            event.add("cache_headers_valid", headers_valid)

        # try to determine what kind of poisoning we're dealing with

        if poison_info["source"] == "web":
            # Do we think they logged in the usual way, or do we think they
            # got poisoned with someone else's session cookie?
            valid_login_hook = hooks.get_hook("poisoning.guess_valid_login")
            if valid_login_hook.call_until_return(poisoner_name=poisoner_name):
                # Maybe a misconfigured local Squid proxy + multiple
                # clients?
                event.add("poison_blame_guess", "local_proxy")
                event.add("poison_credentialed_guess", False)
            elif (context.user_is_loggedin and
                  context.user.name == poisoner_name):
                # Guess we got poisoned with a cookie-bearing response.
                event.add("poison_credentialed_guess", True)
            else:
                event.add("poison_credentialed_guess", False)
        elif poison_info["source"] == "mweb":
            # All mweb responses contain an OAuth token, so we have to assume
            # whoever got this response can perform actions as the poisoner
            event.add("poison_credentialed_guess", True)
        else:
            raise Exception("Unsupported source in cache_poisoning_event")

        # Check if the CF-Cache-Status header is present (this header is not
        # present if caching is disallowed.) If it is, the CDN caching rules
        # are all jacked up.
        if resp_headers and "cf-cache-status" in resp_headers:
            event.add("poison_blame_guess", "cdn")

        self.save_event(event)

    @squelch_exceptions
    def muted_forbidden_event(self, details_text, subreddit=None,
            parent_message=None, target=None, request=None, context=None):
        """Create a mute-related 'forbidden_event' for event-collector.

        details_text: "muted" if a muted user is trying to message the
            subreddit or "muted mod" if the subreddit mod is attempting
            to message the muted user
        subreddit: The Subreddit of the mod messaging the muted user
        parent_message: Message that is being responded to
        target: The intended recipient (Subreddit or Account)
        request, context: Should be pylons.request & pylons.c respectively;

        """
        event = Event(
            topic="forbidden_actions",
            event_type="ss.forbidden_message_attempt",
            request=request,
            context=context,
        )
        event.add("details_text", details_text)

        if parent_message:
            event.add("parent_message_id", parent_message._id)
            event.add("parent_message_fullname", parent_message._fullname)

        event.add_subreddit_fields(subreddit)
        event.add_target_fields(target)

        self.save_event(event)

    @squelch_exceptions
    def timeout_forbidden_event(self, action_name, details_text,
            target=None, target_fullname=None, subreddit=None,
            request=None, context=None):
        """Create a timeout-related 'forbidden_actions' for event-collector.

        action_name: the action taken by a user in timeout
        details_text: this provides more details about the action
        target: The intended item the action was to be taken on
        target_fullname: The fullname used to convert to a target
        subreddit: The Subreddit the action was taken in. If target is of the
            type Subreddit, then this won't be passed in
        request, context: Should be pylons.request & pylons.c respectively;

        """
        if not action_name:
            request_vars = request.environ["pylons.routes_dict"]
            action_name = request_vars.get('action_name')

            # type of vote
            if action_name == "vote":
                direction = int(request.POST.get("dir", 0))
                if direction == 1:
                    action_name = "upvote"
                elif direction == -1:
                    action_name = "downvote"
                else:
                    action_name = "clearvote"
            # set or unset for contest mode and subreddit sticky
            elif action_name in ("set_contest_mode", "set_subreddit_sticky"):
                action_name = action_name.replace("_", "")
                if request.POST.get('state') == "False":
                    action_name = "un" + action_name
            # set or unset for suggested sort
            elif action_name == "set_suggested_sort":
                action_name = action_name.replace("_", "")
                if request.POST.get("sort") in ("", "clear"):
                    action_name = "un" + action_name
            # action for viewing /about/reports, /about/spam, /about/modqueue
            elif action_name == "spamlisting":
                action_name = "pageview"
                details_text = request_vars.get("location")
            elif action_name == "clearflairtemplates":
                action_name = "editflair"
                details_text = "flair_clear_template"
            elif action_name in ("flairconfig", "flaircsv", "flairlisting"):
                details_text = action_name.replace("flair", "flair_")
                action_name = "editflair"

        if not target:
            if not target_fullname:
                if action_name in ("wiki_settings", "wiki_edit"):
                    target = context.site
                elif action_name in ("wiki_allow_editor"):
                    target = Account._by_name(request.POST.get("username"))
                elif action_name in ("delete_sr_header", "delete_sr_icon",
                        "delete_sr_banner"):
                    details_text = "%s" % action_name.replace("ete_sr", "")
                    action_name = "editsettings"
                    target = context.site
                elif action_name in ("bannedlisting", "mutedlisting",
                        "wikibannedlisting", "wikicontributorslisting"):
                    target = context.site

            if target_fullname:
                from r2.models import Thing
                target = Thing._by_fullname(
                    target_fullname,
                    return_dict=False,
                    data=True,
            )

        event = Event(
            topic="forbidden_actions",
            event_type="ss.forbidden_%s" % action_name,
            request=request,
            context=context,
        )
        event.add("details_text", details_text)
        event.add("process_notes", "IN_TIMEOUT")

        from r2.models import Comment, Link, Subreddit
        if not subreddit:
            if isinstance(context.site, Subreddit):
                subreddit = context.site
            elif isinstance(target, (Comment, Link)):
                subreddit = target.subreddit_slow
            elif isinstance(target, Subreddit):
                subreddit = target

        event.add_subreddit_fields(subreddit)
        event.add_target_fields(target)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_mod_sample_rate")
    def mod_event(self, modaction, subreddit, mod, target=None,
            request=None, context=None):
        """Create a 'mod' event for event-collector.

        modaction: An r2.models.ModAction object
        subreddit: The Subreddit the mod action is being performed in
        mod: The Account that is performing the mod action
        target: The Thing the mod action was applied to
        request, context: Should be pylons.request & pylons.c respectively

        """
        event = Event(
            topic="mod_events",
            event_type=modaction.action,
            time=modaction.date,
            uuid=modaction._id,
            request=request,
            context=context,
        )

        event.add("details_text", modaction.details_text)

        # Some jobs that perform mod actions (for example, AutoModerator) are
        # run without actually logging into the account that performs the
        # the actions. In that case, set the user data based on the mod that's
        # performing the action.
        if not event.get("user_id"):
            event["user_id"] = mod._id
            event["user_name"] = mod.name

        event.add_subreddit_fields(subreddit)
        event.add_target_fields(target)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_report_sample_rate")
    def report_event(self, reason=None, details_text=None,
            subreddit=None, target=None, request=None, context=None,
                     event_type="ss.report"):
        """Create a 'report' event for event-collector.

        process_notes: Type of rule (pre-defined report reasons or custom)
        details_text: The report reason
        subreddit: The Subreddit the action is being performed in
        target: The Thing the action was applied to
        request, context: Should be pylons.request & pylons.c respectively

        """
        from r2.models.rules import OLD_SITEWIDE_RULES, SITEWIDE_RULES, SubredditRules

        event = Event(
            topic="report_events",
            event_type=event_type,
            request=request,
            context=context,
        )
        if reason in OLD_SITEWIDE_RULES or reason in SITEWIDE_RULES:
            process_notes = "SITE_RULES"
        else:
            if subreddit and SubredditRules.get_rule(subreddit, reason):
                process_notes = "SUBREDDIT_RULES"
            else:
                process_notes = "CUSTOM"

        event.add("process_notes", process_notes)
        event.add("details_text", details_text)

        event.add_subreddit_fields(subreddit)
        event.add_target_fields(target)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_quarantine_sample_rate")
    def quarantine_event(self, event_type, subreddit,
            request=None, context=None):
        """Create a 'quarantine' event for event-collector.

        event_type: quarantine_interstitial_view, quarantine_opt_in,
            quarantine_opt_out, quarantine_interstitial_dismiss
        subreddit: The quarantined subreddit
        request, context: Should be pylons.request & pylons.c respectively

        """
        event = Event(
            topic="quarantine",
            event_type=event_type,
            request=request,
            context=context,
        )

        if context:
            if context.user_is_loggedin:
                event.add("verified_email", context.user.email_verified)
            else:
                event.add("verified_email", False)

        # Due to the redirect, the request object being sent isn't the
        # original, so referrer and action data is missing for certain events
        if request and (event_type == "quarantine_interstitial_view" or
                 event_type == "quarantine_opt_out"):
            request_vars = request.environ["pylons.routes_dict"]
            event.add("sr_action", request_vars.get("action", None))

            # The thing_id the user is trying to view is a comment
            if request.environ["pylons.routes_dict"].get("comment", None):
                thing_id36 = request_vars.get("comment", None)
            # The thing_id is a link
            else:
                thing_id36 = request_vars.get("article", None)

            if thing_id36:
                event.add("thing_id", int(thing_id36, 36))

        event.add_subreddit_fields(subreddit)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_modmail_sample_rate")
    def modmail_event(self, message, request=None, context=None):
        """Create a 'modmail' event for event-collector.

        message: An r2.models.Message object
        request: pylons.request of the request that created the message
        context: pylons.tmpl_context of the request that created the message

        """

        from r2.models import Account, Message

        sender = message.author_slow
        sr = message.subreddit_slow
        sender_is_moderator = sr.is_moderator_with_perms(sender, "mail")

        if message.first_message:
            first_message = Message._byID(message.first_message, data=True)
        else:
            first_message = message

        event = Event(
            topic="message_events",
            event_type="ss.send_message",
            time=message._date,
            request=request,
            context=context,
            data={
                # set these manually rather than allowing them to be set from
                # the request context because the loggedin user might not
                # be the message sender
                "user_id": sender._id,
                "user_name": sender.name,
            },
        )

        if sender == Account.system_user():
            sender_type = "automated"
        elif sender_is_moderator:
            sender_type = "moderator"
        else:
            sender_type = "user"

        event.add("sender_type", sender_type)
        event.add("sr_id", sr._id)
        event.add("sr_name", sr.name)
        event.add("message_id", message._id)
        event.add("message_kind", "modmail")
        event.add("message_fullname", message._fullname)

        event.add_text("message_body", message.body)
        event.add_text("message_subject", message.subject)

        event.add("first_message_id", first_message._id)
        event.add("first_message_fullname", first_message._fullname)

        if request and request.POST.get("source", None):
            source = request.POST["source"]
            if source in {"compose", "permalink", "modmail", "usermail"}:
                event.add("page", source)

        if message.sent_via_email:
            event.add("is_third_party", True)
            event.add("third_party_metadata", "mailgun")

        if not message.to_id:
            target = sr
        else:
            target = Account._byID(message.to_id, data=True)

        event.add_target_fields(target)

        self.save_event(event)

    @squelch_exceptions
    @sampled("events_collector_message_sample_rate")
    def message_event(self, message, event_type="ss.send_message",
                      request=None, context=None):
        """Create a 'message' event for event-collector.

        message: An r2.models.Message object
        request: pylons.request of the request that created the message
        context: pylons.tmpl_context of the request that created the message

        """

        from r2.models import Account, Message

        sender = message.author_slow

        if message.first_message:
            first_message = Message._byID(message.first_message, data=True)
        else:
            first_message = message

        event = Event(
            topic="message_events",
            event_type=event_type,
            time=message._date,
            request=request,
            context=context,
            data={
                # set these manually rather than allowing them to be set from
                # the request context because the loggedin user might not
                # be the message sender
                "user_id": sender._id,
                "user_name": sender.name,
            },
        )

        if sender == Account.system_user():
            sender_type = "automated"
        else:
            sender_type = "user"

        event.add("sender_type", sender_type)
        event.add("message_kind", "message")
        event.add("message_id", message._id)
        event.add("message_fullname", message._fullname)

        event.add_text("message_body", message.body)
        event.add_text("message_subject", message.subject)

        event.add("first_message_id", first_message._id)
        event.add("first_message_fullname", first_message._fullname)

        if request and request.POST.get("source", None):
            source = request.POST["source"]
            if source in {"compose", "permalink", "usermail"}:
                event.add("page", source)

        if message.sent_via_email:
            event.add("is_third_party", True)
            event.add("third_party_metadata", "mailgun")

        target = Account._byID(message.to_id, data=True)

        event.add_target_fields(target)

        self.save_event(event)

    def loid_event(self, loid, action_name, request=None, context=None):
        """Create a 'loid' event for event-collector.

        loid: the created/modified loid
        action_name: create_loid (only allowed value currently)
        """
        event = Event(
            topic="loid_events",
            event_type='ss.%s' % action_name,
            request=request,
            context=context,
        )
        event.add("request_url", request.fullpath)
        for k, v in loid.to_dict().iteritems():
            event.add(k, v)
        self.save_event(event)

    def login_event(self, action_name, error_msg,
                    user_name=None, email=None,
                    remember_me=None, newsletter=None, email_verified=None,
                    signature=None, request=None, context=None):
        """Create a 'login' event for event-collector.

        action_name: login_attempt, register_attempt, password_reset
        error_msg: error message string if there was an error
        user_name: user entered username string
        email: user entered email string (register, password reset)
        remember_me:  boolean state of remember me checkbox (login, register)
        newsletter: boolean state of newsletter checkbox (register only)
        email_verified: boolean value for email verification state, requires
            email (password reset only)
        request, context: Should be pylons.request & pylons.c respectively

        """
        event = Event(
            topic="login_events",
            event_type='ss.%s' % action_name,
            request=request,
            context=context,
        )

        if error_msg:
            event.add('successful', False)
            event.add('process_notes', error_msg)
        else:
            event.add('successful', True)

        event.add('user_name', user_name)
        event.add('email', email)
        event.add('remember_me', remember_me)
        event.add('newsletter', newsletter)
        event.add('email_verified', email_verified)
        if signature:
            event.add("signed", True)
            event.add("signature_platform", signature.platform)
            event.add("signature_version", signature.version)
            event.add("signature_valid", signature.is_valid())
            sigerror = ", ".join(
                "%s_%s" % (field, code) for code, field in signature.errors
            )
            event.add("signature_errors", sigerror)
            if signature.epoch:
                event.add("signature_age", int(time.time()) - signature.epoch)

        self.save_event(event)

    def bucketing_event(
        self, experiment_id, experiment_name, variant, user, loid
    ):
        """Send an event recording an experiment bucketing.

        experiment_id: an integer representing the experiment
        experiment_name: a human-readable name representing the experiment
        variant: a string representing the variant name
        user: the Account that has been put into the variant
        """
        event = Event(
            topic='bucketing_events',
            event_type='bucket',
        )
        event.add('experiment_id', experiment_id)
        event.add('experiment_name', experiment_name)
        event.add('variant', variant)
        # if the user is logged out, we won't have a user_id or name
        if user is not None:
            event.add('user_id', user._id)
            event.add('user_name', user.name)
        if loid:
            for k, v in loid.to_dict().iteritems():
                event.add(k, v)
        self.save_event(event)

    def page_bucketing_event(
        self, experiment_id, experiment_name, variant, content_id,
        request, context=None
    ):
        """Send an event recording bucketing of a page for a page-based
        experiment.

        experiment_id: an integer representing the experiment
        experiment_name: a human-readable name representing the experiment
        variant: a string representing the variant name
        content_id: the primary content fullname for the page being bucketed
        """
        event = Event(
            topic='bucketing_events',
            event_type='bucket_page',
            request=request,
            context=context,
        )
        event.add('experiment_id', experiment_id)
        event.add('experiment_name', experiment_name)
        event.add('variant', variant)
        event.add('bucketing_fullname', content_id)
        event.add('crawler_name', g.pool_name)
        event.add('url', request.fullurl)
        self.save_event(event)


class Event(object):
    def __init__(self, topic, event_type,
            time=None, uuid=None, request=None, context=None, testing=False,
            data=None, obfuscated_data=None, truncatable_field=None):
        """Create a new event for event-collector.

        topic: Used to filter events into appropriate streams for processing
        event_type: Used for grouping and sub-categorizing events
        time: Should be a datetime.datetime object in UTC timezone
        uuid: Should be a UUID object
        request, context: Should be pylons.request & pylons.c respectively
        testing: Whether to send the event to the test endpoint
        data: A dict of field names/values to initialize the payload with
        obfuscated_data: Same as `data`, but fields that need obfuscation
        truncatable_field: Field to truncate if the event is too large
        """
        self.topic = topic
        self.event_type = event_type
        self.testing = testing or g.debug
        self.truncatable_field = truncatable_field

        if not time:
            time = datetime.datetime.now(pytz.UTC)
        self.timestamp = _datetime_to_millis(time)

        if not uuid:
            uuid = uuid4()
        self.uuid = str(uuid)

        self.payload = {}
        if data:
            self.payload.update(data)
        self.obfuscated_data = {}
        if obfuscated_data:
            self.obfuscated_data.update(obfuscated_data)

        if context and request:
            # Since we don't want to override any of these values that callers
            # might have set, we have to do a bit of finagling to filter out
            # the values that've already been set. Variety of other solutions
            # here: http://stackoverflow.com/q/6354436/120999
            context_data = self.get_context_data(request, context)
            new_context_data = {k: v for (k, v) in context_data.items()
                                if k not in self.payload}
            self.payload.update(new_context_data)

            context_data = self.get_sensitive_context_data(request, context)
            new_context_data = {k: v for (k, v) in context_data.items()
                                if k not in self.obfuscated_data}
            self.obfuscated_data.update(new_context_data)

    def add(self, field, value, obfuscate=False):
        # There's no need to send null/empty values, the collector will act
        # the same whether they're sent or not. Zeros are important though,
        # so we can't use a simple boolean truth check here.
        if value is None or value == "":
            return

        if obfuscate:
            self.obfuscated_data[field] = value
        else:
            self.payload[field] = value

    def add_text(self, key, value, obfuscate=False):
        self.add(key, value, obfuscate=obfuscate)
        for k, v in charset_summary(value).iteritems():
            self.add("{}_{}".format(key, k), v)

    def add_target_fields(self, target):
        if not target:
            return
        from r2.models import Comment, Link, Message

        self.add("target_id", target._id)
        self.add("target_fullname", target._fullname)
        self.add("target_age_seconds", target._age.total_seconds())

        target_type = target.__class__.__name__.lower()
        if target_type == "link" and target.is_self:
            target_type = "self"
        self.add("target_type", target_type)

        # If the target is an Account or Subreddit (or has a "name" attr),
        # add the target_name
        if hasattr(target, "name"):
            self.add("target_name", target.name)

        # Add info about the target's author for comments, links, & messages
        if isinstance(target, (Comment, Link, Message)):
            author = target.author_slow
            if target._deleted or author._deleted:
                self.add("target_author_id", 0)
                self.add("target_author_name", "[deleted]")
            else:
                self.add("target_author_id", author._id)
                self.add("target_author_name", author.name)

        # Add info about the url being linked to for link posts
        if isinstance(target, Link):
            self.add_text("target_title", target.title)
            if not target.is_self:
                self.add("target_url", target.url)
                self.add("target_url_domain", target.link_domain())

        # Add info about the link being commented on for comments
        if isinstance(target, Comment):
            link_fullname = Link._fullname_from_id36(to36(target.link_id))
            self.add("link_id", target.link_id)
            self.add("link_fullname", link_fullname)

        # Add info about when target was originally posted for links/comments
        if isinstance(target, (Comment, Link)):
            self.add("target_created_ts", to_epoch_milliseconds(target._date))

        hooks.get_hook("eventcollector.add_target_fields").call(
            event=self,
            target=target,
        )

    def add_subreddit_fields(self, subreddit):
        if not subreddit:
            return

        self.add("sr_id", subreddit._id)
        self.add("sr_name", subreddit.name)

    def get(self, field, obfuscated=False):
        if obfuscated:
            return self.obfuscated_data.get(field, None)
        else:
            return self.payload.get(field, None)

    @classmethod
    def get_context_data(self, request, context):
        """Extract common data from the current request and context

        This is generally done explicitly in `__init__`, but is done by hand for
        votes before the request context is lost by the queuing.

        request, context: Should be pylons.request & pylons.c respectively
        """
        data = {}

        if context.user_is_loggedin:
            data["user_id"] = context.user._id
            data["user_name"] = context.user.name
        else:
            if context.loid:
                data.update(context.loid.to_dict())

        oauth2_client = getattr(context, "oauth2_client", None)
        if oauth2_client:
            data["oauth2_client_id"] = oauth2_client._id
            data["oauth2_client_name"] = oauth2_client.name
            data["oauth2_client_app_type"] = oauth2_client.app_type

        data["geoip_country"] = get_request_location(request, context)
        data["domain"] = request.host
        data["user_agent"] = request.user_agent
        data["user_agent_parsed"] = request.parsed_agent.to_dict()

        http_referrer = request.headers.get("Referer", None)
        if http_referrer:
            data["referrer_url"] = http_referrer
            data["referrer_domain"] = domain(http_referrer)

        hooks.get_hook("eventcollector.context_data").call(
            data=data,
            user=context.user,
            request=request,
            context=context,
        )

        return data

    @classmethod
    def get_sensitive_context_data(self, request, context):
        data = {}
        ip = getattr(request, "ip", None)
        if ip:
            data["client_ip"] = ip
            # since we obfuscate IP addresses in the DS pipeline, we can't
            # extract the subnet for analysis after this step. So, pre-generate
            # (and separately obfuscate) the subnets.
            if "." in ip:
                octets = ip.split(".")
                data["client_ipv4_24"] = ".".join(octets[:3])
                data["client_ipv4_16"] = ".".join(octets[:2])

        return data

    def dump(self):
        """Returns the JSON representation of the event."""
        data = {
            "event_topic": self.topic,
            "event_type": self.event_type,
            "event_ts": self.timestamp,
            "uuid": self.uuid,
            "payload": self.payload,
        }
        if self.obfuscated_data:
            data["payload"]["obfuscated_data"] = self.obfuscated_data

        return json.dumps(data)


class PublishableEvent(object):
    def __init__(self, data, truncatable_field=None):
        self.data = data
        self.truncatable_field = truncatable_field

    def __len__(self):
        return len(self.data)

    def truncate_data(self, target_len):
        if not self.truncatable_field:
            return

        if len(self.data) <= target_len:
            return

        # this will over-truncate with unicode characters, but it shouldn't be
        # important to cut it as close as possible
        oversize_by = len(self.data) - target_len

        # make space for the is_truncated field we're going to add
        oversize_by += len('"is_truncated": true, ')

        deserialized_data = json.loads(self.data)

        original = deserialized_data["payload"][self.truncatable_field]
        truncated = original[:-oversize_by]
        deserialized_data["payload"][self.truncatable_field] = truncated
        deserialized_data["payload"]["is_truncated"] = True

        self.data = json.dumps(deserialized_data)

        g.stats.simple_event("eventcollector.oversize_truncated")


class EventPublisher(object):
    # The largest JSON string for a single event in bytes (but it's encoded
    # to ASCII, so this is the same as character length)
    MAX_EVENT_SIZE = 100 * 1024

    # The largest combined total JSON string that can be sent (multiple events)
    MAX_CONTENT_LENGTH = 500 * 1024

    def __init__(self, url, signature_key, secret, user_agent, stats,
            max_event_size=MAX_EVENT_SIZE, max_content_length=MAX_CONTENT_LENGTH,
            timeout=None):
        self.url = url
        self.signature_key = signature_key
        self.secret = secret
        self.user_agent = user_agent
        self.timeout = timeout
        self.stats = stats
        self.max_event_size = max_event_size
        self.max_content_length = max_content_length

        self.session = requests.Session()

    def _make_signature(self, payload):
        mac = hmac.new(self.secret, payload, hashlib.sha256).hexdigest()
        return "key={key}, mac={mac}".format(key=self.signature_key, mac=mac)

    def _publish(self, events):
        # Note: If how the JSON payload is created is changed,
        # update the content-length estimations in `_chunk_events`
        data = "[" + ", ".join(events) + "]"

        headers = {
            "Date": _make_http_date(),
            "User-Agent": self.user_agent,
            "Content-Type": "application/json",
            "X-Signature": self._make_signature(data),
        }

        # Gzip body
        use_gzip = (g.live_config.get("events_collector_use_gzip_chance", 0) >
                    random.random())
        if use_gzip:
            f = StringIO()
            gzip.GzipFile(fileobj=f, mode='wb').write(data)
            data = f.getvalue()
            headers["Content-Encoding"] = "gzip"

        # Post events
        with self.stats.get_timer("providers.event_collector"):
            resp = self.session.post(self.url, data=data,
                                     headers=headers, timeout=self.timeout)
            return resp

    def _chunk_events(self, events):
        """Break a PublishableEvent list into chunks to obey size limits.

        Note that this yields lists of strings (the serialized data) to
        publish directly, not PublishableEvent objects.

        """
        to_send = []
        send_size = 0

        for event in events:
            # make sure the event is inside the size limit, and drop it if
            # truncation wasn't possible (or didn't make it small enough)
            event.truncate_data(self.max_event_size)
            if len(event) > self.max_event_size:
                g.log.warning("Event too large (%s); dropping", len(event))
                g.log.warning("%r", event.data)
                g.stats.simple_event("eventcollector.oversize_dropped")
                continue

            # increase estimated content-length by length of message,
            # plus the length of the `, ` used to join the events JSON
            # if there will be more than one event in the list
            send_size += len(event)
            if len(to_send) > 0:
                send_size += len(", ")

            # If adding this event would put us over the batch limit, yield
            # the current set of events first. Note that we add 2 chars to the
            # send_size to account for the square brackets around the list of
            # events when serialized to JSON
            if send_size + 2 >= self.max_content_length:
                yield to_send
                to_send = []
                send_size = len(event)

            to_send.append(event.data)

        if to_send:
            yield to_send

    def publish(self, events):
        for some_events in self._chunk_events(events):
            resp = self._publish(some_events)
            # read from resp.content, so that the connection can be re-used
            # http://docs.python-requests.org/en/latest/user/advanced/#keep-alive
            ignored = resp.content
            yield resp, some_events


def _get_reason(response):
    return (getattr(response, "reason", None) or
            getattr(response.raw, "reason", "{unknown}"))


def process_events(g, timeout=5.0, **kw):
    publisher = EventPublisher(
        g.events_collector_url,
        g.secrets["events_collector_key"],
        g.secrets["events_collector_secret"],
        g.useragent,
        g.stats,
        timeout=timeout,
    )
    test_publisher = EventPublisher(
        g.events_collector_test_url,
        g.secrets["events_collector_key"],
        g.secrets["events_collector_secret"],
        g.useragent,
        g.stats,
        timeout=timeout,
    )

    @g.stats.amqp_processor("event_collector")
    def processor(msgs, chan):
        events = []
        test_events = []

        for msg in msgs:
            headers = msg.properties.get("application_headers", {})
            truncatable_field = headers.get("truncatable_field")

            event = PublishableEvent(msg.body, truncatable_field)
            if msg.delivery_info["routing_key"] == "event_collector_test":
                test_events.append(event)
            else:
                events.append(event)

        to_publish = itertools.chain(
            publisher.publish(events),
            test_publisher.publish(test_events),
        )
        for response, sent in to_publish:
            if response.ok:
                g.log.info("Published %s events", len(sent))
            else:
                g.log.warning(
                    "Event send failed %s - %s",
                    response.status_code,
                    _get_reason(response),
                )
                g.log.warning("Response headers: %r", response.headers)

                # if the events were too large, move them into a separate
                # queue to get them out of here, since they'll always fail
                if response.status_code == 413:
                    for event in sent:
                        amqp.add_item("event_collector_failed", event)
                else:
                    response.raise_for_status()

    amqp.handle_items("event_collector", processor, **kw)
<EOF>
<BOF>
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import httplib
import json
import os
import socket
import urllib2

from pylons import app_globals as g

from r2.lib.sgm import sgm
from r2.lib.utils import in_chunks, tup

# If the geoip service has nginx in front of it there is a default limit of 8kb:
#   http://wiki.nginx.org/NginxHttpCoreModule#large_client_header_buffers
# >>> len('GET /geoip/' + '+'.join(['255.255.255.255'] * 500) + ' HTTP/1.1')
# 8019
MAX_IPS_PER_GROUP = 500

GEOIP_CACHE_TIME = datetime.timedelta(days=7).total_seconds()

def _location_by_ips(ips):
    if not hasattr(g, 'geoip_location'):
        g.log.warning("g.geoip_location not set. skipping GeoIP lookup.")
        return {}

    ret = {}
    for batch in in_chunks(ips, MAX_IPS_PER_GROUP):
        ip_string = '+'.join(batch)
        url = os.path.join(g.geoip_location, 'geoip', ip_string)

        try:
            response = urllib2.urlopen(url=url, timeout=3)
            json_data = response.read()
        except (urllib2.URLError, httplib.HTTPException, socket.error) as e:
            g.log.warning("Failed to fetch GeoIP information: %r" % e)
            continue

        try:
            ret.update(json.loads(json_data))
        except ValueError, e:
            g.log.warning("Invalid JSON response for GeoIP lookup: %r" % e)
            continue
    return ret


def _organization_by_ips(ips):
    if not hasattr(g, 'geoip_location'):
        g.log.warning("g.geoip_location not set. skipping GeoIP lookup.")
        return {}

    ip_string = '+'.join(set(ips))
    url = os.path.join(g.geoip_location, 'org', ip_string)

    try:
        response = urllib2.urlopen(url=url, timeout=3)
        json_data = response.read()
    except urllib2.URLError, e:
        g.log.warning("Failed to fetch GeoIP information: %r" % e)
        return {}

    try:
        return json.loads(json_data)
    except ValueError, e:
        g.log.warning("Invalid JSON response for GeoIP lookup: %r" % e)
        return {}


def location_by_ips(ips):
    ips, is_single = tup(ips, ret_is_single=True)
    location_by_ip = sgm(
        cache=g.gencache,
        keys=ips,
        miss_fn=_location_by_ips,
        prefix='geoip:loc_',
        time=GEOIP_CACHE_TIME,
        ignore_set_errors=True,
    )
    if is_single and location_by_ip:
        return location_by_ip[ips[0]]
    else:
        return location_by_ip


def organization_by_ips(ips):
    ips, is_single = tup(ips, ret_is_single=True)
    organization_by_ip = sgm(
        cache=g.gencache,
        keys=ips,
        miss_fn=_organization_by_ips,
        prefix='geoip:org_',
        time=GEOIP_CACHE_TIME,
        ignore_set_errors=True,
    )
    if is_single and organization_by_ip:
        return organization_by_ip[ips[0]]
    else:
        return organization_by_ip


def get_request_location(request, context):
    """Determine country of origin of the `request` for the given `context`

    This is done by:
     * checking the CDN headers for country of origin if set
     * falling back on geocoding request.ip address against the geocoder service
    The resulting location is memoized on context on `context.location`

    request, context: Should be pylons.request & pylons.c respectively;
    """
    if context.location != '':
        # unset context attributes have the value ''
        return context.location

    context.location = None

    if getattr(request, 'via_cdn', False):
        g.stats.simple_event('geoip.cdn_request')
        cdn_geoinfo = g.cdn_provider.get_client_location(request.environ)
        if cdn_geoinfo:
            context.location = cdn_geoinfo
    elif getattr(request, 'ip', None):
        g.stats.simple_event('geoip.non_cdn_request')
        timer = g.stats.get_timer("providers.geoip.location_by_ips")
        timer.start()
        location = location_by_ips(request.ip)
        if location:
            context.location = location.get('country_code', None)
        timer.stop()

    return context.location
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from __future__ import absolute_import

import random, string

from pylons import app_globals as g

from Captcha.Base import randomIdentifier
from Captcha.Visual import Text, Backgrounds, Distortions, ImageCaptcha


IDEN_LENGTH = 32
SOL_LENGTH = 6

class RandCaptcha(ImageCaptcha):
    defaultSize = (120, 50)
    fontFactory = Text.FontFactory(18, "vera/VeraBd.ttf")

    def getLayers(self, solution="blah"):
        self.addSolution(solution)
        return ((Backgrounds.Grid(size=8, foreground="white"),
                 Distortions.SineWarp(amplitudeRange=(5,9))),
                (Text.TextLayer(solution,
                               textColor = 'white',
                               fontFactory = self.fontFactory),
                 Distortions.SineWarp()))

def get_iden():
    return randomIdentifier(length=IDEN_LENGTH)

def make_solution():
    return randomIdentifier(alphabet=string.ascii_letters, length = SOL_LENGTH).upper()

def get_image(iden):
    key = "captcha:%s" % iden
    solution = g.gencache.get(key)
    if not solution:
        solution = make_solution()
        g.gencache.set(key, solution, time=300)
    return RandCaptcha(solution=solution).render()


def valid_solution(iden, solution):
    key = "captcha:%s" % iden

    if (not iden or
            not solution or
            len(iden) != IDEN_LENGTH or
            len(solution) != SOL_LENGTH or
            solution.upper() != g.gencache.get(key)):
        # the guess was wrong so make a new solution for the next attempt--the
        # client will need to refresh the image before guessing again
        solution = make_solution()
        g.gencache.set(key, solution, time=300)
        return False
    else:
        g.gencache.delete(key)
        return True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from webob.exc import HTTPBadRequest, HTTPForbidden, status_map
from r2.lib.utils import Storage, tup
from pylons import request
from pylons import app_globals as g
from pylons.i18n import _
from copy import copy


error_list = dict((
        ('USER_REQUIRED', _("Please log in to do that.")),
        ('MOD_REQUIRED', _("You must be a moderator to do that.")),
        ('HTTPS_REQUIRED', _("this page must be accessed using https")),
        ('WRONG_DOMAIN', _("you can't do that on this domain")),
        ('VERIFIED_USER_REQUIRED', _("you need to set a valid email address to do that.")),
        ('NO_URL', _('a url is required')),
        ('BAD_URL', _('you should check that url')),
        ('INVALID_SCHEME', _('URI scheme must be one of: %(schemes)s')),
        ('BAD_CAPTCHA', _('care to try these again?')),
        ('BAD_USERNAME', _('invalid user name')),
        ('USERNAME_TOO_SHORT', _('username must be between %(min)d and %(max)d characters')),
        ('USERNAME_INVALID_CHARACTERS', _('username must contain only letters, numbers, "-", and "_"')),
        ('USERNAME_TAKEN', _('that username is already taken')),
        ('USERNAME_TAKEN_DEL', _('that username is taken by a deleted account')),
        ('USER_BLOCKED', _("you can't send to a user that you have blocked")),
        ('NO_THING_ID', _('id not specified')),
        ('TOO_MANY_THING_IDS', _('you provided too many ids')),
        ('NOT_AUTHOR', _("you can't do that")),
        ('NOT_USER', _("You are not logged in as that user.")),
        ('NOT_FRIEND', _("you are not friends with that user")),
        ('LOGGED_IN', _("You are already logged in.")),
        ('DELETED_COMMENT', _('that comment has been deleted')),
        ('DELETED_THING', _('that element has been deleted')),
        ('SHORT_PASSWORD', _('the password must be at least %(chars)d characters')),
        ('BAD_PASSWORD', _('that password is unacceptable')),
        ('WRONG_PASSWORD', _('wrong password')),
        ('BAD_PASSWORD_MATCH', _('passwords do not match')),
        ('NO_NAME', _('please enter a name')),
        ('NO_EMAIL', _('please enter an email address')),
        ('NO_EMAIL_FOR_USER', _('no email address for that user')),
        ('NO_VERIFIED_EMAIL', _('no verified email address for that user')),
        ('NO_TO_ADDRESS', _('send it to whom?')),
        ('NO_SUBJECT', _('please enter a subject')),
        ('USER_DOESNT_EXIST', _("that user doesn't exist")),
        ('NO_USER', _('please enter a username')),
        ('INVALID_PREF', _("that preference isn't valid")),
        ('NON_PREFERENCE', _("'%(choice)s' is not a user preference field")),
        ('INVALID_LANG', _("that language is not available")),
        ('BAD_NUMBER', _("that number isn't in the right range (%(range)s)")),
        ('BAD_STRING', _("you used a character here that we can't handle")),
        ('BAD_BUDGET', _("your budget must be at least $%(min)d and no more than $%(max)d.")),
        ('BAD_BID', _('your bid must be at least $%(min)s and no more than $%(max)s.')),
        ('ALREADY_SUB', _("that link has already been submitted")),
        ('SUBREDDIT_EXISTS', _('that subreddit already exists')),
        ('SUBREDDIT_NOEXIST', _('that subreddit doesn\'t exist')),
        ('SUBREDDIT_NOTALLOWED', _("you aren't allowed to post there.")),
        ('SUBREDDIT_NO_ACCESS', _("you aren't allowed access to this subreddit")),
        ('SUBREDDIT_REQUIRED', _('you must specify a subreddit')),
        ('SUBREDDIT_DISABLED_ADS', _('this subreddit has chosen to disable their ads at this time')),
        ('BAD_SR_NAME', _('that name isn\'t going to work')),
        ('COLLECTION_NOEXIST', _('that collection doesn\'t exist')),
        ('INVALID_TARGET', _('that target type is not valid')),
        ('INVALID_NSFW_TARGET', _('nsfw ads must target nsfw content')),
        ('INVALID_OS_VERSION', _('that version range is not valid')),
        ('RATELIMIT', _('you are doing that too much. try again in %(time)s.')),
        ('SUBREDDIT_RATELIMIT', _("you are doing that too much. try again later.")),
        ('EXPIRED', _('your session has expired')),
        ('DRACONIAN', _('you must accept the terms first')),
        ('BANNED_IP', "IP banned"),
        ('BAD_CNAME', "that domain isn't going to work"),
        ('USED_CNAME', "that domain is already in use"),
        ('INVALID_OPTION', _('that option is not valid')),
        ('BAD_EMAIL', _('that email is invalid')),
        ('BAD_EMAILS', _('the following emails are invalid: %(emails)s')),
        ('NO_EMAILS', _('please enter at least one email address')),
        ('TOO_MANY_EMAILS', _('please only share to %(num)s emails at a time.')),
        ('NEWSLETTER_NO_EMAIL', _('where should we send that weekly newsletter?')),
        ('SPONSOR_NO_EMAIL', _('advertisers are required to supply an email')),
        ('NEWSLETTER_EMAIL_UNACCEPTABLE', _('That email could not be added. Check your email for an existing confirmation email.')),
        ('OVERSOLD', _('that subreddit has already been oversold on %(start)s to %(end)s. Please pick another subreddit or date.')),
        ('OVERSOLD_DETAIL', _("We have insufficient inventory to fulfill your requested budget, target, and dates. Only %(available)s impressions available on %(target)s from %(start)s to %(end)s.")),
        ('BAD_DATE', _('please provide a date of the form mm/dd/yyyy')),
        ('BAD_DATE_RANGE', _('the dates need to be in order and not identical')),
        ('DATE_TOO_LATE', _('please enter a date %(day)s or earlier')),
        ('DATE_TOO_EARLY', _('please enter a date %(day)s or later')),
        ('START_DATE_CANNOT_CHANGE', _('start date cannot be changed')),
        ('COST_BASIS_CANNOT_CHANGE', _('this campaign was created prior to auction and cannot be edited')),
        ('BAD_ADDRESS', _('address problem: %(message)s')),
        ('BAD_CARD', _('card problem: %(message)s')),
        ('TOO_LONG', _("this is too long (max: %(max_length)s)")),
        ('NO_TEXT', _('we need something here')),
        ('TOO_SHORT', _("this is too short (min: %(min_length)s)")),
        ('INVALID_CODE', _("we've never seen that code before")),
        ('CLAIMED_CODE', _("that code has already been claimed -- perhaps by you?")),
        ('NO_SELFS', _("that subreddit doesn't allow text posts")),
        ('NO_LINKS', _("that subreddit only allows text posts")),
        ('TOO_OLD', _("that's a piece of history now; it's too late to reply to it")),
        ('THREAD_LOCKED', _("Comments are locked.")),
        ('BAD_CSS_NAME', _('invalid css name')),
        ('BAD_CSS', _('invalid css')),
        ('BAD_COLOR', _('invalid color')),
        ('BAD_REVISION', _('invalid revision ID')),
        ('TOO_MUCH_FLAIR_CSS', _('too many flair css classes')),
        ('BAD_FLAIR_TARGET', _('not a valid flair target')),
        ('OAUTH2_INVALID_CLIENT', _('invalid client id')),
        ('OAUTH2_INVALID_REDIRECT_URI', _('invalid redirect_uri parameter')),
        ('OAUTH2_INVALID_RESPONSE_TYPE', _('invalid response type')),
        ('OAUTH2_INVALID_SCOPE', _('invalid scope requested')),
        ('OAUTH2_INVALID_REFRESH_TOKEN', _('invalid refresh token')),
        ('OAUTH2_ACCESS_DENIED', _('access denied by the user')),
        ('OAUTH2_NO_REFRESH_TOKENS_ALLOWED', _('refresh tokens are not allowed for this response_type')),
        ('OAUTH2_CONFIDENTIAL_TOKEN', _('confidential clients can not request tokens directly')),
        ('CONFIRM', _("please confirm the form")),
        ('CONFLICT', _("conflict error while saving")),
        ('NO_API', _('cannot perform this action via the API')),
        ('DOMAIN_BANNED', _('%(domain)s is not allowed on reddit: %(reason)s')),
        ('NO_OTP_SECRET', _('you must enable two-factor authentication')),
        ('OTP_ALREADY_ENABLED', _('two-factor authentication is already enabled')),
        ('BAD_IMAGE', _('image problem')),
        ('DEVELOPER_ALREADY_ADDED', _('already added')),
        ('TOO_MANY_DEVELOPERS', _('too many developers')),
        ('DEVELOPER_FIRST_PARTY_APP', _('this app can not be modified from this interface')),
        ('DEVELOPER_PRIVILEGED_ACCOUNT', _('you cannot add this account from this interface')),
        ('INVALID_MODHASH', _("invalid modhash")),
        ('ALREADY_MODERATOR', _('that user is already a moderator')),
        ('CANT_RESTRICT_MODERATOR', _("You can't perform that action because that user is a moderator.")),
        ('NO_INVITE_FOUND', _('there is no pending invite for that subreddit')),
        ('BUDGET_LIVE', _('you cannot edit the budget of a live ad')),
        ('TOO_MANY_CAMPAIGNS', _('you have too many campaigns for that promotion')),
        ('BAD_JSONP_CALLBACK', _('that jsonp callback contains invalid characters')),
        ('INVALID_PERMISSION_TYPE', _("permissions don't apply to that type of user")),
        ('INVALID_PERMISSIONS', _('invalid permissions string')),
        ('BAD_MULTI_PATH', _('invalid multi path')),
        ('BAD_MULTI_NAME', _('%(reason)s')),
        ('MULTI_NOT_FOUND', _('that multireddit doesn\'t exist')),
        ('MULTI_EXISTS', _('that multireddit already exists')),
        ('MULTI_CANNOT_EDIT', _('you can\'t change that multireddit')),
        ('MULTI_TOO_MANY_SUBREDDITS', _('no more space for subreddits in that multireddit')),
        ('MULTI_SPECIAL_SUBREDDIT', _("can't add special subreddit %(path)s")),
        ('TOO_MANY_SUBREDDITS', _('maximum %(max)s subreddits')),
        ('JSON_PARSE_ERROR', _('unable to parse JSON data')),
        ('JSON_INVALID', _('unexpected JSON structure')),
        ('JSON_MISSING_KEY', _('JSON missing key: "%(key)s"')),
        ('NO_CHANGE_KIND', _("can't change post type")),
        ('INVALID_LOCATION', _("invalid location")),
        ('INVALID_FREQUENCY_CAP', _("invalid values for frequency cap")),
        ('FREQUENCY_CAP_TOO_LOW', _('frequency cap must be at least %(min)d')),
        ('BANNED_FROM_SUBREDDIT', _('that user is banned from the subreddit')),
        ('IN_TIMEOUT', _("You can't do that while suspended.")),
        ('GOLD_REQUIRED', _('you must have an active reddit gold subscription to do that')),
        ('INSUFFICIENT_CREDDITS', _("insufficient creddits")),
        ('GILDING_NOT_ALLOWED', _("gilding is not allowed in this subreddit")),
        ('SCRAPER_ERROR', _("unable to scrape provided url")),
        ('NO_SR_TO_SR_MESSAGE', _("can't send a message from a subreddit to another subreddit")),
        ('USER_BLOCKED_MESSAGE', _("can't send message to that user")),
        ('ADMIN_REQUIRED', _("you must be in admin mode for this")),
        ('CANT_CONVERT_TO_GOLD_ONLY', _("to convert an existing subreddit to gold only, send a message to %(admin_modmail)s") 
            % dict(admin_modmail=g.admin_message_acct)),
        ('GOLD_ONLY_SR_REQUIRED', _("this subreddit must be 'gold only' to select this")),
        ('CANT_CREATE_SR', _("your account is too new or you do not have enough karma to create a subreddit. please contact the admins to request an exemption.")),
        ('BAD_PROMO_MOBILE_OS', _("you must select at least one mobile OS to target")),
        ('BAD_PROMO_MOBILE_DEVICE', _("you must select at least one device per OS to target")),
        ('USER_MUTED', _("You have been muted from this subreddit.")),
        ('MUTED_FROM_SUBREDDIT', _("This user has been muted from the subreddit.")),
        ('COMMENT_NOT_STICKYABLE', _("This comment is not stickyable. Ensure that it is a top level comment.")),
        ('SR_RULE_EXISTS', _("A subreddit rule by that name already exists.")),
        ('SR_RULE_DOESNT_EXIST', _("No subreddit rule by that name exists.")),
        ('SR_RULE_TOO_MANY', _("This subreddit already has the maximum number of rules.")),
        ('COMMENT_NOT_ACCESSIBLE', _("Cannot access this comment.")),
        ('POST_NOT_ACCESSIBLE', _("Cannot access this post.")),
    ))

errors = Storage([(e, e) for e in error_list.keys()])


def add_error_codes(new_codes):
    """Add error codes to the error enumeration.

    It is assumed that the incoming messages are marked for translation but not
    yet translated, so they can be declared before pylons.i18n is ready.

    """
    for code, message in new_codes.iteritems():
        error_list[code] = _(message)
        errors[code] = code


class RedditError(Exception):
    name = None
    fields = None
    code = None

    def __init__(self, name=None, msg_params=None, fields=None, code=None):
        Exception.__init__(self)

        if name is not None:
            self.name = name

        self.i18n_message = error_list.get(self.name)
        self.msg_params = msg_params or {}

        if fields is not None:
            # list of fields in the original form that caused the error
            self.fields = tup(fields)

        if code is not None:
            self.code = code

    @property
    def message(self):
        return _(self.i18n_message) % self.msg_params

    def __iter__(self):
        yield ('name', self.name)
        yield ('message', _(self.message))

    def __repr__(self):
        return '<RedditError: %s>' % self.name

    def __str__(self):
        return repr(self)


class ErrorSet(object):
    def __init__(self):
        self.errors = {}

    def __contains__(self, pair):
        """Expects an (error_name, field_name) tuple and checks to
        see if it's in the errors list."""
        return self.errors.has_key(pair)

    def get(self, name, default=None):
        return self.errors.get(name, default)

    def get_first(self, field_name, *error_names):
        error = None

        for error_name in error_names:
            error = self.get((error_name, field_name))
            if error:
                return error

    def __getitem__(self, name):
        return self.errors[name]

    def __repr__(self):
        return "<ErrorSet %s>" % list(self)

    def __iter__(self):
        for x in self.errors:
            yield x

    def __len__(self):
        return len(self.errors)

    def add(self, error_name, msg_params=None, field=None, code=None):
        for field_name in tup(field):
            e = RedditError(error_name, msg_params, fields=field_name,
                            code=code)
            self.add_error(e)

    def add_error(self, error):
        for field_name in tup(error.fields):
            self.errors[(error.name, field_name)] = error

    def remove(self, pair):
        """Expects an (error_name, field_name) tuple and removes it
        from the errors list."""
        if self.errors.has_key(pair):
            del self.errors[pair]


class ForbiddenError(HTTPForbidden):
    def __init__(self, error_name):
        HTTPForbidden.__init__(self)
        self.explanation = error_list[error_name]


class BadRequestError(HTTPBadRequest):
    def __init__(self, error_name):
        HTTPBadRequest.__init__(self)
        self.error_data = {
            'reason': error_name,
            'explanation': error_list[error_name],
        }
        self.explanation = error_list[error_name]


def reddit_http_error(code=400, error_name='UNKNOWN_ERROR', **data):
    exc = status_map[code]()

    data['reason'] = exc.explanation = error_name
    if 'explanation' not in data and error_name in error_list:
        data['explanation'] = exc.explanation = error_list[error_name]

    # omit 'fields' json attribute if it is empty
    if 'fields' in data and not data['fields']:
        del data['fields']

    exc.error_data = data
    return exc


class UserRequiredException(RedditError):
    name = errors.USER_REQUIRED
    code = 403


class VerifiedUserRequiredException(RedditError):
    name = errors.VERIFIED_USER_REQUIRED
    code = 403


class MessageError(Exception): pass
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Helper functions

All names available in this module will be available under the Pylons h object.
"""
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from itertools import chain
import math
import random
from collections import defaultdict
from datetime import timedelta
from operator import itemgetter
from pycassa.types import LongType

from r2.lib import rising
from r2.lib.db import operators, tdb_cassandra
from r2.lib.pages import ExploreItem
from r2.lib.normalized_hot import normalized_hot
from r2.lib.utils import roundrobin, tup, to36
from r2.models import Link, Subreddit
from r2.models.builder import CommentBuilder
from r2.models.listing import NestedListing
from r2.models.recommend import (
    AccountSRPrefs,
    AccountSRFeedback,
)

from pylons import app_globals as g
from pylons.i18n import _

# recommendation sources
SRC_MULTIREDDITS = 'mr'
SRC_EXPLORE = 'e'  # favors lesser known srs

# explore item types
TYPE_RISING = _("rising")
TYPE_DISCOVERY = _("discovery")
TYPE_HOT = _("hot")
TYPE_COMMENT = _("comment")


def get_recommendations(srs,
                        count=10,
                        source=SRC_MULTIREDDITS,
                        to_omit=None,
                        match_set=True,
                        over18=False):
    """Return subreddits recommended if you like the given subreddits.

    Args:
    - srs is one Subreddit object or a list of Subreddits
    - count is total number of results to return
    - source is a prefix telling which set of recommendations to use
    - to_omit is a single or list of subreddit id36s that should not be
        be included. (Useful for omitting recs that were already rejected.)
    - match_set=True will return recs that are similar to each other, useful
        for matching the "theme" of the original set
    - over18 content is filtered unless over18=True or one of the original srs
        is over18

    """
    srs = tup(srs)
    to_omit = tup(to_omit) if to_omit else []

    # fetch more recs than requested because some might get filtered out
    rec_id36s = SRRecommendation.for_srs([sr._id36 for sr in srs],
                                          to_omit,
                                          count * 2,
                                          source,
                                          match_set=match_set)

    # always check for private subreddits at runtime since type might change
    rec_srs = Subreddit._byID36(rec_id36s, return_dict=False)
    filtered = [sr for sr in rec_srs if is_visible(sr)]

    # don't recommend adult srs unless one of the originals was over_18
    if not over18 and not any(sr.over_18 for sr in srs):
        filtered = [sr for sr in filtered if not sr.over_18]

    return filtered[:count]


def get_recommended_content_for_user(account,
                                     settings,
                                     record_views=False,
                                     src=SRC_EXPLORE):
    """Wrapper around get_recommended_content() that fills in user info.

    If record_views == True, the srs will be noted in the user's preferences
    to keep from showing them again too soon.

    settings is an ExploreSettings object that controls what types of content
    will be included.

    Returns a list of ExploreItems.

    """
    prefs = AccountSRPrefs.for_user(account)
    recs = get_recommended_content(prefs, src, settings)
    if record_views:
        # mark as seen so they won't be shown again too soon
        sr_data = {r.sr: r.src for r in recs}
        AccountSRFeedback.record_views(account, sr_data)
    return recs


def get_recommended_content(prefs, src, settings):
    """Get a mix of content from subreddits recommended for someone with
    the given preferences (likes and dislikes.)

    Returns a list of ExploreItems.

    """
    # numbers chosen empirically to give enough results for explore page
    num_liked = 10  # how many liked srs to use when generating the recs
    num_recs = 20  # how many recommended srs to ask for
    num_discovery = 2  # how many discovery-related subreddits to mix in
    num_rising = 4  # how many rising links to mix in
    num_items = 20  # total items to return
    rising_items = discovery_items = comment_items = hot_items = []

    # make a list of srs that shouldn't be recommended
    default_srid36s = [to36(srid) for srid in Subreddit.default_subreddits()]
    omit_srid36s = list(prefs.likes.union(prefs.dislikes,
                                          prefs.recent_views,
                                          default_srid36s))
    # pick random subset of the user's liked srs
    liked_srid36s = random_sample(prefs.likes, num_liked) if settings.personalized else []
    # pick random subset of discovery srs
    candidates = set(get_discovery_srid36s()).difference(prefs.dislikes)
    discovery_srid36s = random_sample(candidates, num_discovery)
    # multiget subreddits
    to_fetch = liked_srid36s + discovery_srid36s
    srs = Subreddit._byID36(to_fetch)
    liked_srs = [srs[sr_id36] for sr_id36 in liked_srid36s]
    discovery_srs = [srs[sr_id36] for sr_id36 in discovery_srid36s]
    if settings.personalized:
        # generate recs from srs we know the user likes
        recommended_srs = get_recommendations(liked_srs,
                                              count=num_recs,
                                              to_omit=omit_srid36s,
                                              source=src,
                                              match_set=False,
                                              over18=settings.nsfw)
        random.shuffle(recommended_srs)
        # split list of recommended srs in half
        midpoint = len(recommended_srs) / 2
        srs_slice1 = recommended_srs[:midpoint]
        srs_slice2 = recommended_srs[midpoint:]
        # get hot links plus top comments from one half
        comment_items = get_comment_items(srs_slice1, src)
        # just get hot links from the other half
        hot_items = get_hot_items(srs_slice2, TYPE_HOT, src)
    if settings.discovery:
        # get links from subreddits dedicated to discovery
        discovery_items = get_hot_items(discovery_srs, TYPE_DISCOVERY, 'disc')
    if settings.rising:
        # grab some (non-personalized) rising items
        omit_sr_ids = set(int(id36, 36) for id36 in omit_srid36s)
        rising_items = get_rising_items(omit_sr_ids, count=num_rising)
    # combine all items and randomize order to get a mix of types
    all_recs = list(chain(rising_items,
                          comment_items,
                          discovery_items,
                          hot_items))
    random.shuffle(all_recs)
    # make sure subreddits aren't repeated
    seen_srs = set()
    recs = []
    for r in all_recs:
        if not settings.nsfw and r.is_over18():
            continue
        if not is_visible(r.sr):  # could happen in rising items
            continue
        if r.sr._id not in seen_srs:
            recs.append(r)
            seen_srs.add(r.sr._id)
        if len(recs) >= num_items:
            break
    return recs


def get_hot_items(srs, item_type, src):
    """Get hot links from specified srs."""
    hot_srs = {sr._id: sr for sr in srs}  # for looking up sr by id
    hot_link_fullnames = normalized_hot([sr._id for sr in srs])
    hot_links = Link._by_fullname(hot_link_fullnames, return_dict=False)
    hot_items = []
    for l in hot_links:
        hot_items.append(ExploreItem(item_type, src, hot_srs[l.sr_id], l))
    return hot_items


def get_rising_items(omit_sr_ids, count=4):
    """Get links that are rising right now."""
    all_rising = rising.get_all_rising()
    candidate_sr_ids = {sr_id for link, score, sr_id in all_rising}.difference(omit_sr_ids)
    link_fullnames = [link for link, score, sr_id in all_rising if sr_id in candidate_sr_ids]
    link_fullnames_to_show = random_sample(link_fullnames, count)
    rising_links = Link._by_fullname(link_fullnames_to_show,
                                     return_dict=False,
                                     data=True)
    rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l)
                   for l in rising_links]
    return rising_items


def get_comment_items(srs, src, count=4):
    """Get hot links from srs, plus top comment from each link."""
    link_fullnames = normalized_hot([sr._id for sr in srs])
    hot_links = Link._by_fullname(link_fullnames[:count], return_dict=False)
    top_comments = []
    for link in hot_links:
        builder = CommentBuilder(link,
                                 operators.desc('_confidence'),
                                 comment=None,
                                 context=None,
                                 num=1,
                                 load_more=False)
        listing = NestedListing(builder, parent_name=link._fullname).listing()
        top_comments.extend(listing.things)
    srs = Subreddit._byID([com.sr_id for com in top_comments])
    links = Link._byID([com.link_id for com in top_comments])
    comment_items = [ExploreItem(TYPE_COMMENT,
                                 src,
                                 srs[com.sr_id],
                                 links[com.link_id],
                                 com) for com in top_comments]
    return comment_items


def get_discovery_srid36s():
    """Get list of srs that help people discover other srs."""
    srs = Subreddit._by_name(g.live_config['discovery_srs'])
    return [sr._id36 for sr in srs.itervalues()]


def random_sample(items, count):
    """Safe random sample that won't choke if len(items) < count."""
    sample_size = min(count, len(items))
    return random.sample(items, sample_size)


def is_visible(sr):
    """True if sr is visible to regular users, false if private or banned."""
    return (
        sr.type not in Subreddit.private_types and
        not sr._spam and
        sr.discoverable
    )


class SRRecommendation(tdb_cassandra.View):
    _use_db = True

    _compare_with = LongType()

    # don't keep these around if a run hasn't happened lately, or if the last
    # N runs didn't generate recommendations for a given subreddit
    _ttl = timedelta(days=7, hours=12)

    # we know that we mess with these but it's okay
    _warn_on_partial_ttl = False

    @classmethod
    def for_srs(cls, srid36, to_omit, count, source, match_set=True):
        # It's usually better to use get_recommendations() than to call this
        # function directly because it does privacy filtering.

        srid36s = tup(srid36)
        to_omit = set(to_omit)
        to_omit.update(srid36s)  # don't show the originals
        rowkeys = ['%s.%s' % (source, srid36) for srid36 in srid36s]

        # fetch multiple sets of recommendations, one for each input srid36
        rows = cls._byID(rowkeys, return_dict=False)

        if match_set:
            sorted_recs = cls._merge_and_sort_by_count(rows)
            # heuristic: if input set is large, rec should match more than one
            min_count = math.floor(.1 * len(srid36s))
            sorted_recs = (rec[0] for rec in sorted_recs if rec[1] > min_count)
        else:
            sorted_recs = cls._merge_roundrobin(rows)
        # remove duplicates and ids listed in to_omit
        filtered = []
        for r in sorted_recs:
            if r not in to_omit:
                filtered.append(r)
                to_omit.add(r)
        return filtered[:count]

    @classmethod
    def _merge_roundrobin(cls, rows):
        """Combine multiple sets of recs, preserving order.

        Picks items equally from each input sr, which can be useful for
        getting a diverse set of recommendations instead of one that matches
        a theme. Preserves ordering, so all rank 1 recs will be listed first,
        then all rank 2, etc.

        Returns a list of id36s.

        """
        return roundrobin(*[row._values().itervalues() for row in rows])

    @classmethod
    def _merge_and_sort_by_count(cls, rows):
        """Combine and sort multiple sets of recs.

        Combines multiple sets of recs and sorts by number of times each rec
        appears, the reasoning being that an item recommended for several of
        the original srs is more likely to match the "theme" of the set.

        """
        # combine recs from all input srs
        rank_id36_pairs = chain.from_iterable(row._values().iteritems()
                                              for row in rows)
        ranks = defaultdict(list)
        for rank, id36 in rank_id36_pairs:
            ranks[id36].append(rank)
        recs = [(id36, len(ranks), max(ranks))
                for id36, ranks in ranks.iteritems()]
        # first, sort ascending by rank
        recs = sorted(recs, key=itemgetter(2))
        # next, sort descending by number of times the rec appeared. since
        # python sort is stable, tied items will still be ordered by rank
        return sorted(recs, key=itemgetter(1), reverse=True)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from __future__ import with_statement
from time import sleep
from datetime import datetime
from threading import local
from pylons import app_globals as g
import os
import socket
import random

from _pylibmc import MemcachedError

from r2.lib.utils import simple_traceback

# thread-local storage for detection of recursive locks
locks = local()

reddit_host = socket.gethostname()
reddit_pid  = os.getpid()

class TimeoutExpired(Exception): pass

class MemcacheLock(object):
    """A simple global lock based on the memcache 'add' command. We
    attempt to grab a lock by 'adding' the lock name. If the response
    is True, we have the lock. If it's False, someone else has it."""

    def __init__(self, stats, group, key, cache,
                 time=30, timeout=30, verbose=True):
        # get a thread-local set of locks that we own
        self.locks = locks.locks = getattr(locks, 'locks', set())

        self.stats = stats
        self.group = group
        self.key = key
        self.cache = cache
        self.time = time
        self.timeout = timeout
        self.have_lock = False
        self.owns_lock = False
        self.verbose = verbose

    def __enter__(self):
        self.acquire()
        return self

    def __exit__(self, type, value, tb):
        self.release()

    def acquire(self):
        start = datetime.now()

        self.nonce = (reddit_host, reddit_pid, simple_traceback(limit=7))

        # if this thread already has this lock, move on
        if self.key in self.locks:
            self.have_lock = True
            return

        timer = self.stats.get_timer("lock_wait")
        timer.start()

        # try and fetch the lock, looping until it's available
        lock = None
        while not lock:
            # catch all exceptions here because we can't trust the memcached
            # protocol. The add for the lock may have actually succeeded.
            try:
                lock = self.cache.add(self.key, self.nonce, time = self.time)
            except MemcachedError as e:
                if self.cache.get(self.key) == self.nonce:
                    g.log.error(
                        'Memcached add succeeded, but threw an exception for key %r %s',
                        self.key, e)
                    break

            if not lock:
                if (datetime.now() - start).seconds > self.timeout:
                    if self.verbose:
                        info = self.cache.get(self.key)
                        if info:
                            info = "%s %s\n%s" % info
                        else:
                            info = "(nonexistent)"
                        msg = ("\nSome jerk is hogging %s:\n%s" %
                                         (self.key, info))
                        msg += "^^^ that was the stack trace of the lock hog, not me."
                    else:
                        msg = "Timed out waiting for %s" % self.key
                    raise TimeoutExpired(msg)
                else:
                    # this should prevent unnecessary spam on highly contended locks.
                    sleep(random.uniform(0.1, 1))

        timer.stop(subname=self.group)

        self.owns_lock = True
        self.have_lock = True

        # tell this thread we have this lock so we can avoid deadlocks
        # of requests for the same lock in the same thread
        self.locks.add(self.key)

    def release(self):
        # only release the lock if we acquired it in the first place (are owner)
        if self.owns_lock:
            # verify that our lock did not expire before we could release it
            if self.cache.get(self.key) == self.nonce:
                self.cache.delete(self.key)
            else:
                g.log.error("Lock expired before completion at key %r: %s",
                            self.key, self.nonce)
            self.locks.remove(self.key)
            self.have_lock = False
            self.owns_lock = False
            self.nonce = None


def make_lock_factory(cache, stats):
    def factory(group, key, **kw):
        return MemcacheLock(stats, group, key, cache, **kw)
    return factory
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import app_globals as g

class SupportTicketError(Exception):
    pass

class SupportTickerNotFoundError(SupportTicketError):
    pass

def create_support_ticket(subject,
                          comment_body,
                          comment_is_public=False,
                          group=None,
                          requester_email=None, 
                          product=None,
                          ):
    requester_id = None
    if requester_email == 'contact@reddit.com':
        requester_id = g.live_config['ticket_contact_user_id']
        
    custom_fields = []
    if product:
        custom_fields.append({
            'id': g.live_config['ticket_user_fields']['Product'],
            'value': product,
        })
        
    return g.ticket_provider.create(
        requester_id=requester_id,
        subject=subject,
        comment_body=comment_body,
        comment_is_public=comment_is_public,
        group_id=g.live_config['ticket_groups'][group],
        custom_fields=custom_fields,
    )

def get_support_ticket(ticket_id):
    return g.ticket_provider.get(ticket_id)

def get_support_ticket_url(ticket_id):
    return g.ticket_provider.build_ticket_url_from_id(ticket_id)

def update_support_ticket(ticket=None, ticket_id=None,
                          status=None,
                          comment_body=None,
                          comment_is_public=False,
                          tag_list=None,
                          ):
    if not ticket and not ticket_id:
        raise SupportTickerNotFoundError(
            'No ticket provided to update.'
        )
        
    if not ticket:
        ticket = get_support_ticket(ticket_id)
    
    return g.ticket_provider.update(
            ticket=ticket,
            status=status,
            comment_body=comment_body,
            comment_is_public=comment_is_public,
            tag_list=tag_list,
        )
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import Link, Subreddit
from r2.lib import utils
from r2.lib.db.operators import desc
from pylons import config
from pylons import app_globals as g


count_period = g.rising_period

#stubs

def incr_counts(wrapped):
    pass

def get_link_counts(period = count_period):
    links = Link._query(Link.c._date >= utils.timeago(period),
                        limit=50, data = True)
    return dict((l._fullname, (0, l.sr_id)) for l in links)

def get_sr_counts():
    srs = utils.fetch_things2(Subreddit._query(sort=desc("_date")))

    return dict((sr._fullname, sr._ups) for sr in srs)


if config['r2.import_private']:
    from r2admin.lib.count import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Run a Gunicorn WSGI container under Einhorn.

[Einhorn] is a language/protocol-agnostic socket and worker manager. We're
using it elsewhere for Baseplate services (where something WSGI-specific
wouldn't work on Thrift-based services) and its graceful reload logic is more
friendly than Gunicorn's*. However, for non-gevent WSGI, we still need
something to parse HTTP and provide a WSGI container. Gunicorn is excellent at
this. This module adapts Gunicorn to work under Einhorn as a single worker
process.

To run a paste-based application (like r2) under Einhorn using Gunicorn as the
WSGI container, run this module. All of gunicorn's command line arguments are
supported, though some may be meaningless (like worker count) because Gunicorn
isn't managing workers.

    einhorn -n 4 -b 0.0.0.0:8080 python -m r2.lib.einhorn example.ini

[Einhorn]: https://github.com/stripe/einhorn

*: In particular, when told to gracefully reload, Gunicorn will gracefully
terminate all workers immediately and then replace them. Einhorn starts up a
new worker, waits for it to acknowledge it is up and running, and then reaps an
old worker.

"""
import os
import signal
import sys

from baseplate.server import einhorn
from gunicorn import util
from gunicorn.app.pasterapp import PasterApplication
from gunicorn.workers.sync import SyncWorker


class EinhornSyncWorker(SyncWorker):
    def __init__(self, cfg, app):
        listener = einhorn.get_socket()
        super(EinhornSyncWorker, self).__init__(
            age=0,
            ppid=os.getppid(),
            sockets=[listener],
            app=app,
            timeout=None,
            cfg=cfg,
            log=cfg.logger_class(cfg),
        )

    def init_signals(self):
        # reset signal handlers to defaults
        [signal.signal(s, signal.SIG_DFL) for s in self.SIGNALS]

        # einhorn will send SIGUSR2 to request a graceful shutdown
        signal.signal(signal.SIGUSR2, self.start_graceful_shutdown)
        signal.siginterrupt(signal.SIGUSR2, False)

    def start_graceful_shutdown(self, signal_number, frame):
        # gunicorn changed the meaning of its signals in 8124190. by being
        # explicit what we mean here, we avoid woes when upgrading later.
        self.alive = False


def run_gunicorn_worker():
    if not einhorn.is_worker():
        print >> sys.stderr, "This process does not appear to be running under Einhorn."
        sys.exit(1)

    app = PasterApplication()
    util._setproctitle("worker [%s]" % app.cfg.proc_name)
    worker = EinhornSyncWorker(app.cfg, app)
    worker.init_process()


if __name__ == "__main__":
    run_gunicorn_worker()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict
from itertools import chain


class SimpleCampaign(object):
    def __init__(self, name, target_names, impressions):
        self.name = name
        self.target_names = target_names
        self.impressions = impressions

    def __repr__(self):
        s = "<%s %s: %s impressions in %s>"
        return s % (self.__class__.__name__, self.name, self.impressions,
                    ', '.join(self.target_names))


class SimpleTarget(object):
    def __init__(self, name, impressions):
        self.name = name
        self.impressions = impressions

    def __repr__(self):
        return "<%s %s: %s impressions>" % (self.__class__.__name__, self.name,
                                            self.impressions)


class System(object):
    """Take a set of campaigns and a set of targets and allocate the
    inventory of each target to the campaigns in such a way to maximize
    the free inventory in the priority target or targets.

    """

    def __init__(self, campaigns, targets, priority_target_names):
        self.priority_target_names = priority_target_names
        self.campaigns, self.targets = self.simplify(campaigns, targets)

    def __repr__(self):
        max_names = ', '.join(self.priority_target_names)
        all_names = ', '.join("%s (%s)" % (target.name, target.impressions)
                              for target in self.targets)
        return "<%s: max %s in %s>" % (self.__class__.__name__, max_names,
                                       all_names)

    def combine_campaigns(self, campaigns):
        """Combine campaigns with the same target."""
        campaigns_by_target = defaultdict(list)
        for campaign in campaigns:
            target_names_tuple = tuple(sorted(campaign.target_names))
            campaigns_by_target[target_names_tuple].append(campaign)

        combined_campaigns = []
        changed = False
        for target_names_tuple, campaigns in campaigns_by_target.iteritems():
            if len(campaigns) > 1:
                changed = True
                name = ','.join(camp.name for camp in campaigns)
                target_names = list(target_names_tuple)
                impressions = sum(camp.impressions for camp in campaigns)
                combined = SimpleCampaign(name, target_names, impressions)
                combined_campaigns.append(combined)
            else:
                combined_campaigns.extend(campaigns)
        return changed, combined_campaigns

    def reduce_campaigns(self, campaigns, targets):
        """Remove campaigns.

        Find campaigns with only a single target and subtract their required
        impressions from the target and remove the campaign.

        """

        targets_by_name = {target.name: target for target in targets}
        changed = False
        reduced_campaigns = []
        for campaign in campaigns:
            if len(campaign.target_names) == 1:
                changed = True
                target_name = campaign.target_names[0]
                target_impressions = targets_by_name[target_name].impressions
                target_impressions -= campaign.impressions
                new_target = SimpleTarget(target_name, target_impressions)
                targets_by_name[target_name] = new_target
            else:
                reduced_campaigns.append(campaign)
        reduced_targets = targets_by_name.values()
        return changed, reduced_campaigns, reduced_targets

    def reduce_targets(self, campaigns, targets):
        """Remove targets.

        Remove non-priority targets that have only a single campaign or that
        have enough inventory to satisfy all their campaigns. As a result may
        end up removing campaigns if they're fully satisfied.

        """

        campaign_names_by_target = defaultdict(list)
        for campaign in campaigns:
            for target_name in campaign.target_names:
                if target_name not in self.priority_target_names:
                    campaign_names_by_target[target_name].append(campaign.name)

        campaigns_by_name = {campaign.name: campaign for campaign in campaigns}
        targets_by_name = {target.name: target for target in targets}
        changed = False
        for target_name, campaign_names in campaign_names_by_target.iteritems():
            target = targets_by_name[target_name]
            campaign_impressions = sum(
                campaigns_by_name[name].impressions for name in campaign_names)
            fully_satisfied = campaign_impressions <= target.impressions
            single_campaign = len(campaign_names) == 1

            if not (fully_satisfied or single_campaign):
                continue

            changed = True
            for campaign_name in campaign_names:
                campaign = campaigns_by_name[campaign_name]
                if fully_satisfied:
                    # the target has enough impressions to cover all campaigns
                    impressions = 0
                else:
                    # assign all of target's inventory to its single campaign
                    target_impressions = max(0, target.impressions)
                    impressions = campaign_impressions - target_impressions
                target_names = campaign.target_names[:]
                target_names.remove(target_name)

                # update (rewrite) the SimpleCampaign object in the lookup dict
                campaigns_by_name[campaign_name] = SimpleCampaign(
                    campaign_name, target_names, impressions)

            # no need to adjust the inventory of the target--it's not one
            # we care about and there are no campaigns targeting it any more
            # so we can just delete it
            del targets_by_name[target_name]

        reduced_campaigns = []
        for campaign in campaigns_by_name.itervalues():
            if campaign.impressions > 0:
                reduced_campaigns.append(campaign)
        reduced_targets = targets_by_name.values()
        return changed, reduced_campaigns, reduced_targets

    def simplify(self, campaigns, targets):
        changed = False
        first_run = True

        while changed or first_run:
            first_run = False
            changed_1, campaigns = self.combine_campaigns(campaigns)
            changed_2, campaigns, targets = self.reduce_campaigns(
                campaigns, targets)
            changed_3, campaigns, targets = self.reduce_targets(
                campaigns, targets)

            # only re-run if changed_2 or changed_2 because then we need to
            # repeat the earlier steps
            changed = changed_2 or changed_3

        return campaigns, targets

    def get_free_impressions(self):
        """Run through algorithm to solve for maximum free impressions.

        Choose how to allocate inventory to each campaign by first mapping out
        the distance of each target from the targets we're trying to maximize
        inventory of, and then assigning inventory to each campaign
        preferring to choose the targets that are farthest away.

        """

        campaigns_by_target = defaultdict(list)
        for campaign in self.campaigns:
            for target_name in campaign.target_names:
                campaigns_by_target[target_name].append(campaign)

        # map out distance from targets we want to maximize
        level = 0
        level_by_target_name = {}
        next_level_target_names = set(self.priority_target_names)
        while next_level_target_names:
            target_names = next_level_target_names

            for target_name in target_names:
                level_by_target_name[target_name] = level

            campaigns = chain.from_iterable(
                campaigns_by_target[target_name] for target_name in target_names
            )
            next_level_target_names = {
                target_name for campaign in campaigns
                            for target_name in campaign.target_names
                            # skip any targets we've already seen
                            if target_name not in level_by_target_name
            }
            level += 1

        # assign any unconnected targets maximum level (although they should
        # probably have been excluded before getting to the optimization)
        for target in self.targets:
            if target.name not in level_by_target_name:
                level_by_target_name[target.name] = level

        target_names_by_level = defaultdict(list)
        for target_name, level in level_by_target_name.iteritems():
            target_names_by_level[level].append(target_name)

        # iterate over targets, starting at the highest level, and assign
        # their inventory to campaigns
        unassigned_by_campaign = {
            campaign.name: campaign.impressions for campaign in self.campaigns}
        impressions_by_target = {
            target.name: target.impressions for target in self.targets}

        for level in sorted(target_names_by_level.iterkeys(), reverse=True):
            target_names = target_names_by_level[level]
            campaigns = chain.from_iterable(
                campaigns_by_target[target_name] for target_name in target_names
            )

            def sort_val(campaign):
                # campaigns can have a maximum of 2 levels of targets,
                # prioritize those with lower level and fewer targets
                val = sum(
                    level_by_target_name[name] + 1
                    for name in campaign.target_names
                    if level_by_target_name[name] <= level
                )
                return val

            for campaign in sorted(campaigns, key=sort_val):
                campaign_targets = [name for name in target_names
                                    if name in campaign.target_names]
                for target_name in campaign_targets:
                    unassigned = unassigned_by_campaign[campaign.name]

                    if unassigned > 0:
                        available = max(0, impressions_by_target[target_name])
                        assigned = min(unassigned, available)
                        unassigned_by_campaign[campaign.name] -= assigned
                        impressions_by_target[target_name] -= assigned

        # check if any campaigns didn't get fully allocated (this means a target
        # is oversold)
        penalty = 0
        for campaign in self.campaigns:
            unassigned = unassigned_by_campaign[campaign.name]
            if unassigned > 0:
                if not campaign.target_names:
                    # all the campaign's targets were already allocated
                    continue

                # allocate inventory from the lowest level target
                target_name = min(campaign.target_names,
                                  key=lambda name: level_by_target_name[name])
                unassigned_by_campaign[campaign.name] -= unassigned
                impressions_by_target[target_name] -= unassigned

                # we screwed up, reduce the free impressions. using the penalty
                # may end up underestimating the free impressions, but better
                # safe than sorry.
                penalty += unassigned

        free_impressions = sum(
            impressions_by_target[target_name] for target_name
                                               in self.priority_target_names)
        return free_impressions - penalty


def campaign_to_simple_campaign(campaign):
    name = campaign._fullname
    target_names = campaign.target.subreddit_names
    impressions = campaign.impressions / campaign.ndays
    return SimpleCampaign(name, target_names, impressions)


def get_maximized_pageviews(priority_sr_names, booked_by_target,
                            pageviews_by_sr_name):
    targets = [SimpleTarget(sr_name, pageviews) for sr_name, pageviews
                                                in pageviews_by_sr_name.iteritems()]
    campaigns = [
        SimpleCampaign(', '.join(sr_names), list(sr_names), impressions)
        for sr_names, impressions in booked_by_target.iteritems()
    ]
    system = System(campaigns, targets, priority_sr_names)
    return system.get_free_impressions()


def run_tests():
    # example 1: maximize impressions in a subreddit that is also targeted
    # by a collection
    pageviews_by_sr_name = {
        'leagueoflegends': 50000,
        'dota2': 50000,
        'hearthstone': 50000,
        'games': 50000,
    }
    targets = [SimpleTarget(sr_name, pageviews) for sr_name, pageviews
               in pageviews_by_sr_name.iteritems()]

    campaigns = [
        SimpleCampaign('c1', ['leagueoflegends'], 20000),
        SimpleCampaign('c2', ['dota2'], 40000),
        SimpleCampaign('c3', ['games'], 40000),
        SimpleCampaign('c4', ['hearthstone'], 40000),
        SimpleCampaign('c5',
            ['leagueoflegends', 'dota2', 'hearthstone', 'games'], 20000),
    ]
    priority_target_names = ['leagueoflegends']
    system = System(campaigns, targets, priority_target_names)
    impressions = system.get_free_impressions()
    assert impressions == 30000

    # example 2: maximize impressions of a collection
    priority_target_names = ['leagueoflegends', 'dota2', 'hearthstone', 'games']
    system = System(campaigns, targets, priority_target_names)
    impressions = system.get_free_impressions()
    assert impressions == 40000

    # example 3: branching--we don't solve this "correctly", must rely on
    # penalty (algorithm is greedy, see following notes)
    pageviews_by_sr_name = {
        'leagueoflegends': 25000,
        'dota2': 25000,
        'hearthstone': 25000,
        'games': 25000,
        'smashbros': 50000,
    }
    targets = [SimpleTarget(sr_name, pageviews) for sr_name, pageviews
               in pageviews_by_sr_name.iteritems()]
    campaigns = [
        SimpleCampaign('c1', ['leagueoflegends', 'dota2'], 25000),
        SimpleCampaign('c2', ['hearthstone', 'games'], 25000),
        SimpleCampaign('c3', ['dota2', 'smashbros'], 50000),
        SimpleCampaign('c4', ['games', 'smashbros'], 50000),
    ]
    priority_target_names = ['leagueoflegends', 'hearthstone']

    """
    optimal distribution:
    c4: 25000 from smashbros, 25000 from games
    c3: 25000 from smashbros, 25000 from dota2
    c2: 25000 from hearthstone
    c2: 25000 from leagueoflegends

    Current algorithm can't split smashbros because it's too greedy, the first
    of c4 or c3 to be allocated will get all 50000

    Subsequent improvements to the algorithm should allow splitting a target
    and should prioritize campaigns for which the target is their lowest
    level target. Also for campaigns for which the target is their highest
    level target the algorithm should look forward to their lowest level target
    and determin whether that has any chance of satisfying the campaign.

    """

    system = System(campaigns, targets, priority_target_names)
    impressions = system.get_free_impressions()
    assert impressions == 0
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import inspect
import sys
import os.path
import re
import subprocess
import json

from pylons import app_globals as g
from pylons import tmpl_context as c

from r2.config.paths import get_built_statics_path
from r2.lib.permissions import ModeratorPermissionSet
from r2.lib.plugin import PluginLoader
from r2.lib.static import locate_static_file
from r2.lib.translation import (
    extract_javascript_msgids,
    get_catalog,
    iter_langs,
    validate_plural_forms,
)


script_tag = '<script type="text/javascript" src="{src}"></script>\n'
inline_script_tag = '<script type="text/javascript">{content}</script>'


class Uglify(object):
    def compile(self, data, dest):
        process = subprocess.Popen(
            ["/usr/bin/uglifyjs", "-nc"],
            stdin=subprocess.PIPE,
            stdout=dest,
        )

        process.communicate(input=data)

        if process.returncode != 0:
            raise subprocess.CalledProcessError(process.returncode, "uglifyjs")


class Source(object):
    """An abstract collection of JavaScript code."""
    def get_source(self, **kwargs):
        """Return the full JavaScript source code."""
        raise NotImplementedError

    def use(self, **kwargs):
        """Return HTML to insert the JavaScript source inside a template."""
        raise NotImplementedError

    @property
    def dependencies(self):
        raise NotImplementedError

    @property
    def outputs(self):
        raise NotImplementedError


class FileSource(Source):
    """A JavaScript source file on disk."""
    def __init__(self, name):
        self.name = name

    def __eq__(self, other):
        return type(self) is type(other) and self.name == other.name

    def get_source(self, use_built_statics=False):
        if use_built_statics:
            # we are in the build system so we have already copied all files
            # into the static build directory
            built_statics_path = get_built_statics_path()
            path = os.path.join(built_statics_path, "static", "js", self.name)
        else:
            # we are in request so we need to check the pylons static_files
            # path and the static paths for all plugins
            path = locate_static_file(os.path.join("static", "js", self.name))

        with open(path) as f:
            return f.read()

    def url(self, absolute=False, mangle_name=False):
        from r2.lib.template_helpers import static
        path = [g.static_path, self.name]
        if g.uncompressedJS:
            path.insert(1, "js")

        return static(os.path.join(*path), absolute, mangle_name)

    def use(self, **kwargs):
        return script_tag.format(src=self.url(**kwargs))

    @property
    def dependencies(self):
        built_statics_path = get_built_statics_path()
        path = os.path.join(built_statics_path, "static", "js", self.name)
        return [path]


class Module(Source):
    """A module of JS code consisting of a collection of sources."""
    def __init__(self, name, *sources, **kwargs):
        self.name = name
        self.should_compile = kwargs.get('should_compile', True)
        self.wrap = kwargs.get('wrap')
        self.sources = []
        filter_module = kwargs.get('filter_module')
        if isinstance(filter_module, Module):
            self.filter_sources = filter_module.get_flattened_sources([])
        else:
            self.filter_sources = None
        sources = sources or (name,)
        for source in sources:
            if not isinstance(source, Source):
                if 'prefix' in kwargs:
                    source = os.path.join(kwargs['prefix'], source)
                source = self.get_default_source(source)
            self.sources.append(source)

    def get_default_source(self, source):
        return FileSource(source)

    def get_flattened_sources(self, flattened_sources):
        for s in self.sources:
            if s in flattened_sources:
                continue
            elif isinstance(s, Module):
                s.get_flattened_sources(flattened_sources)
            else:
                flattened_sources.append(s)
        if self.filter_sources:
            flattened_sources = [s for s in flattened_sources
                                 if s not in self.filter_sources]
        return flattened_sources

    def get_source(self, use_built_statics=False):
        sources = self.get_flattened_sources([])
        return ";".join(
            s.get_source(use_built_statics=use_built_statics)
            for s in sources
        )

    def extend(self, module):
        self.sources.extend(module.sources)

    @property
    def destination_path(self):
        built_statics_path = get_built_statics_path()
        return os.path.join(built_statics_path, "static", self.name)

    def build(self, minifier):
        with open(self.destination_path, "w") as out:
            source = self.get_source(use_built_statics=True)
            if self.wrap:
                source = self.wrap.format(content=source, name=self.name)

            if self.should_compile:
                print >> sys.stderr, "Compiling {0}...".format(self.name),
                minifier.compile(source, out)
            else:
                print >> sys.stderr, "Concatenating {0}...".format(self.name),
                out.write(source)
        print >> sys.stderr, " done."

    def url(self, absolute=False, mangle_name=True):
        from r2.lib.template_helpers import static
        if g.uncompressedJS:
            return [source.url(absolute=absolute, mangle_name=mangle_name) for source in self.sources]
        else:
            return static(self.name, absolute=absolute, mangle_name=mangle_name)

    def use(self, **kwargs):
        if g.uncompressedJS:
            sources = self.get_flattened_sources([])
            return "".join(source.use(**kwargs) for source in sources)
        else:
            return script_tag.format(src=self.url(**kwargs))

    @property
    def dependencies(self):
        deps = []
        for source in self.sources:
            deps.extend(source.dependencies)
        return deps

    @property
    def outputs(self):
        return [self.destination_path]


class DataSource(Source):
    """A generated source consisting of wrapped JSON data."""
    def __init__(self, wrap, data=None):
        self.wrap = wrap
        self.data = data

    def get_content(self, **kw):
        return self.data

    def get_source(self, use_built_statics=False):
        content = self.get_content(use_built_statics=use_built_statics)
        json_data = json.dumps(content)
        return self.wrap.format(content=json_data) + "\n"

    def use(self):
        from r2.lib.filters import SC_OFF, SC_ON, websafe_json
        escaped_json = websafe_json(self.get_source())
        return (SC_OFF + inline_script_tag.format(content=escaped_json) +
                SC_ON + "\n")

    @property
    def dependencies(self):
        return []


class PermissionsDataSource(DataSource):
    """DataSource for PermissionEditor configuration data."""

    def __init__(self, permission_sets):
        self.permission_sets = permission_sets

    @classmethod
    def _make_marked_json(cls, obj):
        """Return serialized psuedo-JSON with translation support.

        Strings are marked for extraction with r.N_. Dictionaries are
        serialized to JSON objects as normal.

        """
        if isinstance(obj, dict):
            props = []
            for key, value in obj.iteritems():
                value_encoded = cls._make_marked_json(value)
                props.append("%s: %s" % (key, value_encoded))
            return "{%s}" % ",".join(props)
        elif isinstance(obj, basestring):
            return "r.N_(%s)" % json.dumps(obj)
        else:
            raise ValueError, "unsupported type"

    def get_source(self, **kw):
        permission_set_info = {k: v.info for k, v in
                               self.permission_sets.iteritems()}
        permissions = self._make_marked_json(permission_set_info)
        return "r.permissions = _.extend(r.permissions || {}, %s)" % permissions

    @property
    def dependencies(self):
        dependencies = set(super(PermissionsDataSource, self).dependencies)
        for permission_set in self.permission_sets.itervalues():
            dependencies.add(inspect.getsourcefile(permission_set))
        return list(dependencies)


class TemplateFileSource(DataSource, FileSource):
    """A JavaScript template file on disk."""
    def __init__(self, name, wrap="r.templates.set({content})"):
        DataSource.__init__(self, wrap)
        FileSource.__init__(self, name)
        self.name = name

    def get_content(self, use_built_statics=False):
        name, style = os.path.splitext(self.name)

        if use_built_statics:
            built_statics_path = get_built_statics_path()
            path = os.path.join(built_statics_path, 'static', 'js', self.name)
        else:
            path = locate_static_file(os.path.join('static', 'js', self.name))

        with open(path) as f:
            return [{
                "name": name,
                "style": style.lstrip('.'),
                "template": f.read(),
            }]


class LocaleSpecificSource(object):
    def get_localized_source(self, lang):
        raise NotImplementedError


class StringsSource(LocaleSpecificSource):
    """Translations sourced from a gettext catalog."""

    def __init__(self, keys):
        self.keys = keys

    invalid_formatting_specifier_re = re.compile(r"(?<!%)%\w|(?<!%)%\(\w+\)[^s]")
    def _check_formatting_specifiers(self, string):
        if not isinstance(string, basestring):
            return

        if self.invalid_formatting_specifier_re.search(string):
            raise ValueError("Invalid string formatting specifier: %r" % string)

    def get_localized_source(self, lang):
        catalog = get_catalog(lang)

        # relies on pyx files, so it can't be imported at global scope
        from r2.lib.utils import tup

        data = {}
        for key in self.keys:
            key = tup(key)[0]  # because the key for plurals is (sing, plur)
            self._check_formatting_specifiers(key)
            msg = catalog[key]

            if not msg or not msg.string:
                continue

            # jed expects to ignore the first value in the translations array
            # so we'll just make it null
            strings = tup(msg.string)
            data[key] = [None] + list(strings)
        return "r.i18n.addMessages(%s)" % json.dumps(data)


class PluralForms(LocaleSpecificSource):
    def get_localized_source(self, lang):
        catalog = get_catalog(lang)
        validate_plural_forms(catalog.plural_expr)
        return "r.i18n.setPluralForms('%s')" % catalog.plural_expr


class LocalizedModule(Module):
    """A module that generates localized code for each language.

    Strings marked for translation with one of the functions in i18n.js (viz.
    r._, r.P_, and r.N_) are extracted from the source and their translations
    are built into the compiled source.

    """

    def __init__(self, *args, **kwargs):
        self.localized_appendices = kwargs.pop("localized_appendices", [])
        Module.__init__(self, *args, **kwargs)

        for source in self.sources:
            if isinstance(source, LocalizedModule):
                self.localized_appendices.extend(source.localized_appendices)

    @staticmethod
    def languagize_path(path, lang):
        path_name, path_ext = os.path.splitext(path)
        return path_name + "." + lang + path_ext

    def build(self, minifier):
        Module.build(self, minifier)

        with open(self.destination_path) as f:
            reddit_source = f.read()

        localized_appendices = self.localized_appendices
        msgids = extract_javascript_msgids(reddit_source)
        if msgids:
            localized_appendices = localized_appendices + [StringsSource(msgids)]

        print >> sys.stderr, "Creating language-specific files:"
        for lang, unused in iter_langs():
            lang_path = LocalizedModule.languagize_path(
                self.destination_path, lang)

            # make sure we're not rewriting a different mangled file
            # via symlink
            if os.path.islink(lang_path):
                os.unlink(lang_path)

            with open(lang_path, "w") as out:
                print >> sys.stderr, "  " + lang_path
                out.write(reddit_source)
                for appendix in localized_appendices:
                    out.write(appendix.get_localized_source(lang) + ";")

    def use(self, **kwargs):
        from pylons.i18n import get_lang
        from r2.lib.template_helpers import static
        from r2.lib.filters import SC_OFF, SC_ON

        if g.uncompressedJS:
            if c.lang == "en" or c.lang not in g.all_languages:
                # in this case, the msgids *are* the translated strings and we
                # can save ourselves the pricey step of lexing the js source
                return Module.use(self, **kwargs)

            msgids = extract_javascript_msgids(Module.get_source(self))
            localized_appendices = self.localized_appendices + [StringsSource(msgids)]

            lines = [Module.use(self, **kwargs)]
            for appendix in localized_appendices:
                line = SC_OFF + inline_script_tag.format(
                    content=appendix.get_localized_source(c.lang)) + SC_ON
                lines.append(line)
            return "\n".join(lines)
        else:
            langs = get_lang() or [g.lang]
            url = LocalizedModule.languagize_path(self.name, langs[0])
            return script_tag.format(src=static(url), **kwargs)

    @property
    def outputs(self):
        for lang, unused in iter_langs():
            yield LocalizedModule.languagize_path(self.destination_path, lang)


_submodule = {}
module = {}

catch_errors = "try {{ {content} }} catch (err) {{ r.sendError('Error running module', '{name}', ':', err.toString()) }}"


_submodule["config"] = Module("_setup.js",
    "base.js",
    "setup.js",
    "hooks.js",
)

_submodule["utils"] = Module("_utils.js",
    "base.js",
    _submodule["config"],
    "utils.js",
)

_submodule["uibase"] = Module("_uibase.js",
    "base.js",
    "i18n.js",
    _submodule["utils"],
    "uibase.js",
)

_submodule["analytics"] = Module("_analytics.js",
    "base.js",
    _submodule["config"],
    _submodule["utils"],
    "events.js",
    "analytics.js",
)

_submodule["errors"] = Module("_errors.js",
    "base.js",
    "i18n.js",
    "errors.js",
)

_submodule["gate-popup"] = Module("_gate-popup.js",
    "base.js",
    _submodule["uibase"],
    _submodule["errors"],
    "gate-popup.js",
)

_submodule["timeouts"] = Module("_timeouts.js",
    "base.js",
    _submodule["config"],
    _submodule["analytics"],
    _submodule["gate-popup"],
    "access.js",
    "timeouts.js",
)

_submodule["locked"] = Module("_locked.js",
    "base.js",
    "access.js",
    _submodule["gate-popup"],
    "locked.js",
)

_submodule["archived"] = Module("_archived.js",
    "base.js",
    "hooks.js",
    _submodule["gate-popup"],
    "archived.js",
)

module["gtm-jail"] = Module("gtm-jail.js",
    "lib/json2.js",
    "custom-event.js",
    "frames.js",
    "google-tag-manager/gtm-jail-listener.js",
)


module["gtm"] = Module("gtm.js",
    "lib/json2.js",
    "custom-event.js",
    "frames.js",
    "google-tag-manager/gtm-listener.js",
)


module["reddit-embed-base"] = Module("reddit-embed-base.js",
    "lib/es5-shim.js",
    "lib/json2.js",
    "base.js",
    "uuid.js",
    "custom-event.js",
    "frames.js",
    "embed/utils.js",
    "embed/pixel-tracking.js",
)


module["reddit-embed"] = Module("reddit-embed.js",
    module["reddit-embed-base"],
    "embed/embed.js",
)


module["comment-embed"] = Module("comment-embed.js",
    module["reddit-embed-base"],
    "embed/comment-embed.js",
)


module["reddit-init-base"] = LocalizedModule("reddit-init-base.js",
    "lib/modernizr.js",
    "lib/json2.js",
    "lib/underscore-1.4.4-1.js",
    "lib/store.js",
    "lib/jed.js",
    "lib/bootstrap.modal.js",
    "lib/bootstrap.transition.js",
    "lib/bootstrap.tooltip.js",
    "lib/reddit-client-lib.js",
    "lib/jquery.cookie.js",
    "lib/event-tracker.js",
    "lib/hmac-sha256.js",
    "do-not-track.js",
    "bootstrap.tooltip.extension.js",
    "base.js",
    "uuid.js",
    "hooks.js",
    "setup.js",
    "migrate-global-reddit.js",
    "ajax.js",
    "safe-store.js",
    "preload.js",
    "logging.js",
    "client-error-logger.js",
    "voting.js",
    "uibase.js",
    "i18n.js",
    "utils.js",
    "analytics.js",
    "events.js",
    "access.js",
    "reddit-init-hook.js",
    "jquery.reddit.js",
    "stateify.js",
    "validator.js",
    "strength-meter.js",
    "toggles.js",
    "reddit.js",
    "sr-autocomplete.js",
    "spotlight.js",
    localized_appendices=[
        PluralForms(),
    ],
)

module["reddit-init-legacy"] = LocalizedModule("reddit-init-legacy.js",
    "lib/html5shiv.js",
    "lib/jquery-1.11.1.js",
    "lib/es5-shim.js",
    "lib/es5-sham.js",
    module["reddit-init-base"],
    wrap=catch_errors,
)

module["reddit-init"] = LocalizedModule("reddit-init.js",
    "lib/jquery-2.1.1.js",
    "lib/es5-shim.js",
    module["reddit-init-base"],
    wrap=catch_errors,
)

module["expando-nsfw-flow"] = Module("expando-nsfw-flow.js",
    TemplateFileSource('ui/formbar.html'),
    "ui/formbar.js",
    TemplateFileSource('expando/nsfwgate.html'),
    "expando/nsfwflow.js",
)

module["reddit"] = LocalizedModule("reddit.js",
    "lib/jquery.url.js",
    "lib/backbone-1.0.0.js",
    "custom-event.js",
    "frames.js",
    "embed/utils.js",
    "embed/pixel-tracking.js",
    "embed/comment-embed.js",
    "google-tag-manager/gtm.js",
    "backbone-init.js",
    "timings.js",
    "templates.js",
    "scrollupdater.js",
    "timetext.js",
    "ui.js",
    "popup.js",
    "login.js",
    _submodule["locked"],
    _submodule["timeouts"],
    _submodule["archived"],
    "newsletter.js",
    "flair.js",
    "report.js",
    "interestbar.js",
    "visited.js",
    "wiki.js",
    "apps.js",
    "gold.js",
    "multi.js",
    "filter.js",
    "recommender.js",
    "action-forms.js",
    "embed.js",
    "post-sharing.js",
    "expando.js",
    # inline expando-nsfw-flow.js module here when unflagged
    "saved.js",
    "cache-poisoning-detection.js",
    "messages.js",
    "reddit-hook.js",
    "link-click-tracking.js",
    "warn-on-unload.js",
    PermissionsDataSource({
        "moderator": ModeratorPermissionSet,
        "moderator_invite": ModeratorPermissionSet,
    }),
    wrap=catch_errors,
    filter_module=module["reddit-init-base"],
)

module["modtools"] = Module("modtools.js",
    "errors.js",
    "models/validators.js",
    "models/subreddit-rule.js",
    "edit-subreddit-rules.js",
    wrap=catch_errors,
)

module["admin"] = Module("admin.js",
    # include Backbone and timings so they are available early to render admin bar fast.
    "lib/backbone-1.0.0.js",
    "timings.js",
    "adminbar.js",
)

module["mobile"] = LocalizedModule("mobile.js",
    module["reddit"],
    "lib/jquery.lazyload.js",
    "compact.js",
    filter_module=module["reddit-init-base"],
)


module["policies"] = Module("policies.js",
    "policies.js",
)


module["sponsored"] = LocalizedModule("sponsored.js",
    "lib/ui.core.js",
    "lib/ui.datepicker.js",
    "lib/react-with-addons-0.11.2.js",
    "image-upload.js",
    "sponsored.js"
)


module["timeseries"] = Module("timeseries.js",
    "lib/jquery.flot.js",
    "lib/jquery.flot.time.js",
    "timeseries.js",
)


module["timeseries-ie"] = Module("timeseries-ie.js",
    "lib/excanvas.min.js",
    module["timeseries"],
)


module["traffic"] = LocalizedModule("traffic.js",
    "traffic.js",
)


module["qrcode"] = Module("qrcode.js",
    "lib/jquery.qrcode.min.js",
    "qrcode.js",
)


module["highlight"] = Module("highlight.js",
    "lib/highlight.pack.js",
    "highlight.js",
)

module["messagecompose"] = Module("messagecompose.js",
    # jquery, hooks, ajax, preload
    "messagecompose.js")

module["less"] = Module('less.js',
    'lib/less-1.4.2.js',
    should_compile=False,
)

# This needs to be separate module because we need it to load on old / bad
# browsers that choke on reddit.js
module["https-tester"] = Module("https-tester.js",
    "base.js",
    "uuid.js",
    "https-tester.js"
)

def src(*names, **kwargs):
    sources = []

    for name in names:
        urls = module[name].url(**kwargs)

        if isinstance(urls, str) or isinstance(urls, unicode):
            sources.append(urls)
        else:
            for url in list(urls):
                if isinstance(url, list):
                    sources.extend(url)
                else:
                    sources.append(url)

    return sources

def use(*names, **kwargs):
    return "\n".join(module[name].use(**kwargs) for name in names)


def load_plugin_modules(plugins=None):
    if not plugins:
        plugins = PluginLoader()
    for plugin in plugins:
        plugin.add_js(module)


commands = {}
def build_command(fn):
    def wrapped(*args):
        load_plugin_modules()
        fn(*args)
    commands[fn.__name__] = wrapped
    return wrapped


@build_command
def enumerate_modules():
    for name, m in module.iteritems():
        print name


@build_command
def dependencies(name):
    for dep in module[name].dependencies:
        print dep


@build_command
def enumerate_outputs(*names):
    if names:
        modules = [module[name] for name in names]
    else:
        modules = module.itervalues()

    for m in modules:
        for output in m.outputs:
            print output


@build_command
def build_module(name):
    minifier = Uglify()
    module[name].build(minifier)


if __name__ == "__main__":
    commands[sys.argv[1]](*sys.argv[2:])
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
Tools to check if arbitrary HTML fragments would be safe to embed inline
"""


import os
import re
import sys
import urllib
import urlparse

import lxml.etree

from cStringIO import StringIO

valid_link_schemes = (
    '/',
    '#',
    'http://',
    'https://',
    'ftp://',
    'mailto:',
    'steam://',
    'irc://',
    'ircs://',
    'news://',
    'mumble://',
    'ssh://',
    'git://',
    'ts3server://',
)

allowed_tags = {
    'div': {'class'},
    'a': {'href', 'title', 'target', 'nofollow', 'rel'},
    'img': {'src', 'alt', 'title'},
}

markdown_boring_tags = {
    'p', 'em', 'strong', 'br', 'ol', 'ul', 'hr', 'li', 'pre', 'code',
    'blockquote', 'center', 'sup', 'del', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
}

markdown_user_tags = {
    'table', 'th', 'tr', 'td', 'tbody', 'thead', 'tfoot', 'caption',
}

for bt in markdown_boring_tags:
    allowed_tags[bt] = {'id', 'class'}

for bt in markdown_user_tags:
    allowed_tags[bt] = {
        'colspan', 'rowspan', 'cellspacing', 'cellpadding', 'align', 'scope',
    }


def souptest_sniff_node(node):
    """Check that a node from an (X)HTML document passes the sniff test"""
    # Because IE loves conditional comments.
    if node.tag is lxml.etree.Comment:
        # Benign, used to turn space compression on and off.
        if node.text.strip() not in {"SC_ON", "SC_OFF"}:
            raise SoupUnexpectedCommentError(node)
    # Looks like all nodes but `Element` have functions in `.tag`
    elif isinstance(node.tag, basestring):
        # namespaces are tacked onto the front of the tag / attr name if
        # applicable so we don't need to worry about checking for those.
        tag_name = node.tag
        if tag_name not in allowed_tags:
            raise SoupUnsupportedTagError(tag_name)

        for attr, val in node.items():
            if attr not in allowed_tags[tag_name]:
                raise SoupUnsupportedAttrError(attr)

            if tag_name == 'a' and attr == 'href':
                lv = val.lower()
                if not lv.startswith(valid_link_schemes):
                    raise SoupUnsupportedSchemeError(val)
                # work around CRBUG-464270
                parsed_url = urlparse.urlparse(lv)
                if parsed_url.hostname and len(parsed_url.hostname) > 255:
                    raise SoupDetectedCrasherError(parsed_url.hostname)
                # work around for Chrome crash with "%%30%30" - Sep 2015
                if "%00" in urllib.unquote(parsed_url.path):
                    raise SoupDetectedCrasherError(lv)
    else:
        # Processing instructions and friends fall down here.
        raise SoupUnsupportedNodeError(node)


ENTITY_DTD_PATH = os.path.join(
    os.path.dirname(os.path.abspath(__file__)),
    'contrib/dtds/allowed_entities.dtd')

SOUPTEST_DOCTYPE_FMT = '<!DOCTYPE div- [\n %s \n]>'

UNDEFINED_ENTITY_RE = re.compile(r"\AEntity '([^']*)' not defined,.*")


def souptest_fragment(fragment):
    """Check if an HTML fragment is sane and safe to embed.

    For checking markup is safe, and that it will embed properly in both HTML
    and XML documents. In practice, this means we check that all
    tags / attributes are safe, and that the syntax conforms to a restricted
    subset of XHTML.

    No processing instructions, only specific comments, no CDATA sections,
    and a whitelist of named character entities to deal with inconsistencies
    between how parsers handle lookup failures for them.
    """
    # We lazily load this so processes that don't souptest don't have to load
    # it every startup.
    souptest_doctype = getattr(souptest_fragment, 'souptest_doctype', None)
    if souptest_doctype is None:
        # Slurp in all of the entity definitions so we can avoid a read every
        # time we parse
        with open(ENTITY_DTD_PATH, 'r') as ent_file:
            souptest_doctype = SOUPTEST_DOCTYPE_FMT % ent_file.read()
        souptest_fragment.souptest_doctype = souptest_doctype

    # lxml makes it *very* difficult to tell if there's a CDATA node in the
    # tree, even if you tell it not to fold them into adjacent text sections.
    # CDATA sections will be ignored and their innards parsed in most doctypes,
    # so we need this hack to keep them out of markup.
    if "<![CDATA" in fragment:
        raise SoupUnexpectedCDataSectionError(fragment)

    # We need to prepend our doctype + DTD or lxml can't resolve entities.
    # We also need to wrap everything in a div, as lxml throws out
    # comments outside the root tag. This also ensures that attempting an
    # entity declaration or similar shenanigans will cause a syntax error.
    documentized_fragment = "%s<div>%s</div>" % (souptest_doctype, fragment)
    s = StringIO(documentized_fragment)

    try:
        parser = lxml.etree.XMLParser()
        # Can't use a SAX interface because lxml's doesn't give you a handler
        # for comments or entities unless you use Python 3. Oh well.
        for node in lxml.etree.parse(s, parser).iter():
            souptest_sniff_node(node)
    except lxml.etree.XMLSyntaxError:
        # Wrap the exception while keeping the original traceback
        type_, value, trace = sys.exc_info()
        # In XML some characters are illegal even as references, thankfully
        # they're almost all control codes: (`&#x00;`, `&#x1c;`, etc.)
        if value.msg.startswith('xmlParseCharRef: invalid xmlChar '):
            raise SoupUnsupportedEntityError, (value,), trace
        undef_ent = re.match(UNDEFINED_ENTITY_RE, value.msg)
        if undef_ent:
            raise SoupUnsupportedEntityError, (value, undef_ent.group(1)), trace

        raise SoupSyntaxError, (value,), trace


class SoupError(Exception):
    """An error specific to the souptesting process"""
    pass


class SoupReprError(SoupError):
    """Give a class-defined message as well as a repr of the passed object"""
    HUMAN_MESSAGE = None

    def __init__(self, obj):
        self.obj = obj

    def __str__(self):
        return "HAX: %s: %r" % (self.HUMAN_MESSAGE, self.obj)


class SoupSyntaxError(SoupReprError):
    """Found a general syntax error"""
    HUMAN_MESSAGE = "XML Parsing error"


class SoupUnsupportedNodeError(SoupReprError):
    """Found a weird node, like a processing instruction"""
    HUMAN_MESSAGE = "Unsupported node type"


class SoupUnexpectedCommentError(SoupReprError):
    """Found a comment that hasn't been specifically whitelisted"""
    HUMAN_MESSAGE = "Unexpected comment"


class SoupUnexpectedCDataSectionError(SoupReprError):
    """Found a CDATA section, which have no meaning in HTML5"""
    HUMAN_MESSAGE = "Unexpected CDATA section"


class SoupUnsupportedSchemeError(SoupReprError):
    """Found a URL whose scheme hasn't been explicitly whitelisted"""
    HUMAN_MESSAGE = "Unsupported URL scheme"


class SoupUnsupportedAttrError(SoupReprError):
    """Found an element attribute that hasn't been explicitly whitelisted"""
    HUMAN_MESSAGE = "Unsupported attribute"


class SoupUnsupportedTagError(SoupReprError):
    """Found an element that hasn't been explicitly whitelisted"""
    HUMAN_MESSAGE = "Unsupported tag"


class SoupDetectedCrasherError(SoupReprError):
    HUMAN_MESSAGE = "Known crasher posted"


class SoupUnsupportedEntityError(SoupReprError):
    """Found an otherwise well-formed entity that couldn't be accepted"""
    HUMAN_MESSAGE = "Invalid or unrecognized entity"

    # `entity` is optional, because we can't get the passed-in entity in
    # the case of an exception because of invalid numeric entities.
    def __init__(self, obj, entity=None):
        self.entity = entity
        SoupReprError.__init__(self, obj)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import heapq
import itertools
from datetime import datetime, timedelta

from pylons import app_globals as g

from r2.config import feature
from r2.lib.db.queries import _get_links, CachedResults
from r2.lib.db.sorts import epoch_seconds


MAX_PER_SUBREDDIT = 150
MAX_LINKS = 1000


def get_hot_tuples(sr_ids, ageweight=None):
    queries_by_sr_id = {sr_id: _get_links(sr_id, sort='hot', time='all')
                        for sr_id in sr_ids}
    CachedResults.fetch_multi(queries_by_sr_id.values(), stale=True)
    tuples_by_srid = {sr_id: [] for sr_id in sr_ids}

    now_seconds = epoch_seconds(datetime.now(g.tz))

    for sr_id, q in queries_by_sr_id.iteritems():
        if not q.data:
            continue

        hot_factor = get_hot_factor(q.data[0], now_seconds, ageweight)

        for link_name, hot, timestamp in q.data[:MAX_PER_SUBREDDIT]:
            effective_hot = hot / hot_factor
            # heapq.merge sorts from smallest to largest so we need to flip
            # ehot and hot to get the hottest links first
            tuples_by_srid[sr_id].append(
                (-effective_hot, -hot, link_name, timestamp)
            )

    return tuples_by_srid


def get_hot_factor(qdata, now, ageweight):
    """Return a "hot factor" score for a link's hot tuple.

    Recalculate the item's hot score as if it had been submitted
    more recently than it was. This will cause the `effective_hot` value in
    get_hot_tuples to move older first items back

    ageweight should be a float from 0.0 - 1.0, which "scales" how far
    between the original submission time and "now" to use as the base
    for the new hot score. Smaller values will favor older #1 posts in
    multireddits; larger values will drop older posts further in the ranking
    (or possibly off the ranking entirely).

    """
    ageweight = float(ageweight or 0.0)
    link_name, hot, timestamp = qdata
    return max(hot + ((now - timestamp) * ageweight) / 45000.0, 1.0)


def normalized_hot(sr_ids, obey_age_limit=True, ageweight=None):
    timer = g.stats.get_timer("normalized_hot")
    timer.start()

    if not sr_ids:
        return []

    if not feature.is_enabled("scaled_normalized_hot"):
        ageweight = None

    tuples_by_srid = get_hot_tuples(sr_ids, ageweight=ageweight)

    if obey_age_limit:
        cutoff = datetime.now(g.tz) - timedelta(days=g.HOT_PAGE_AGE)
        oldest = epoch_seconds(cutoff)
    else:
        oldest = 0.

    merged = heapq.merge(*tuples_by_srid.values())
    generator = (link_name for ehot, hot, link_name, timestamp in merged
                           if timestamp > oldest)
    ret = list(itertools.islice(generator, MAX_LINKS))
    timer.stop()
    return ret
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

# Known bug: if a given listing hasn't had a submission in the
# allotted time (e.g. the year listing in a subreddit that hasn't had
# a submission in the last year), we won't write out an empty
# list. I'll call it a feature.

import sys

from r2.models import Link, Comment
from r2.lib.db.sorts import epoch_seconds, score, controversy
from r2.lib.db import queries
from r2.lib import mr_tools
from r2.lib.utils import timeago, UrlParser
from r2.lib.jsontemplates import make_fullname # what a strange place
                                               # for this function

thingcls_by_name = {
    "link": Link,
    "comment": Comment,
}
data_fields_by_name = {
    "link": {
        "url": str,
        "sr_id": int,
        "author_id": int,
    },
    "comment": {
        "sr_id": int,
        "author_id": int,
    },
}


def join_things(thing_type):
    mr_tools.join_things(data_fields_by_name[thing_type].keys())


def _get_cutoffs(intervals):
    cutoffs = {}
    for interval in intervals:
        if interval == "all":
            cutoffs["all"] = 0.0
        else:
            cutoffs[interval] = epoch_seconds(timeago("1 %s" % interval))

    return cutoffs


def time_listings(intervals, thing_type):
    cutoff_by_interval = _get_cutoffs(intervals)

    @mr_tools.dataspec_m_thing(*data_fields_by_name[thing_type].items())
    def process(thing):
        if thing.deleted:
            return

        thing_cls = thingcls_by_name[thing.thing_type]
        fname = make_fullname(thing_cls, thing.thing_id)
        thing_score = score(thing.ups, thing.downs)
        thing_controversy = controversy(thing.ups, thing.downs)

        for interval, cutoff in cutoff_by_interval.iteritems():
            if thing.timestamp < cutoff:
                continue

            yield ("user/%s/top/%s/%d" % (thing.thing_type, interval, thing.author_id),
                   thing_score, thing.timestamp, fname)
            yield ("user/%s/controversial/%s/%d" % (thing.thing_type, interval, thing.author_id),
                   thing_controversy, thing.timestamp, fname)

            if thing.spam:
                continue

            if thing.thing_type == "link":
                yield ("sr/link/top/%s/%d" % (interval, thing.sr_id),
                       thing_score, thing.timestamp, fname)
                yield ("sr/link/controversial/%s/%d" % (interval, thing.sr_id),
                       thing_controversy, thing.timestamp, fname)

                if thing.url:
                    try:
                        parsed = UrlParser(thing.url)
                    except ValueError:
                        continue

                    for domain in parsed.domain_permutations():
                        yield ("domain/link/top/%s/%s" % (interval, domain),
                               thing_score, thing.timestamp, fname)
                        yield ("domain/link/controversial/%s/%s" % (interval, domain),
                               thing_controversy, thing.timestamp, fname)

    mr_tools.mr_map(process)


def store_keys(key, maxes):
    category, thing_cls, sort, time, id = key.split("/")

    query = None
    if category == "user":
        if thing_cls == "link":
            query = queries._get_submitted(int(id), sort, time)
        elif thing_cls == "comment":
            query = queries._get_comments(int(id), sort, time)
    elif category == "sr":
        if thing_cls == "link":
            query = queries._get_links(int(id), sort, time)
    elif category == "domain":
        if thing_cls == "link":
            query = queries.get_domain_links(id, sort, time)

    assert query, 'unknown query type for %s' % (key,)

    item_tuples = [tuple([item[-1]] + [float(x) for x in item[:-1]])
                   for item in maxes]

    # we only need locking updates for non-time-based listings, since for time-
    # based ones we're the only ones that ever update it
    lock = time == 'all'

    query._replace(item_tuples, lock=lock)

def write_permacache(fd = sys.stdin):
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=store_keys,
                                   fd = fd)

def reduce_listings(fd=sys.stdin):
    # like write_permacache, but just sends the reduced version of the listing
    # to stdout instead of to the permacache. It's handy for debugging to see
    # the final result before it's written out
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   fd = fd)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""An implementation of the RFC-6238 Time-Based One Time Password algorithm."""

import time
import hmac
import base64
import struct
import hashlib


PERIOD = 30


def make_hotp(secret, counter):
    """Generate an RFC-4226 HMAC-Based One Time Password."""
    key = base64.b32decode(secret)

    # compute the HMAC digest of the counter with the secret key
    counter_encoded = struct.pack(">q", counter)
    hmac_result = hmac.HMAC(key, counter_encoded, hashlib.sha1).digest()

    # do HOTP dynamic truncation (see RFC4226 5.3)
    offset = ord(hmac_result[-1]) & 0x0f
    truncated_hash = hmac_result[offset:offset + 4]
    code_bits, = struct.unpack(">L", truncated_hash)
    htop = (code_bits & 0x7fffffff) % 1000000

    # pad it out as necessary
    return "%06d" % htop


def make_totp(secret, skew=0, timestamp=None):
    """Generate an RFC-6238 Time-Based One Time Password."""
    timestamp = timestamp or time.time()
    counter = timestamp // PERIOD
    return make_hotp(secret, counter - skew)


def generate_secret():
    """Make a secret key suitable for use in TOTP."""
    from Crypto.Random import get_random_bytes
    bytes = get_random_bytes(20)
    encoded = base64.b32encode(bytes)
    return encoded


if __name__ == "__main__":
    # based on RFC-6238 Appendix B (trimmed to six-digit OTPs)
    secret = base64.b32encode("12345678901234567890")
    assert make_totp(secret, timestamp=59) == "287082"
    assert make_totp(secret, timestamp=1111111109) == "081804"
    assert make_totp(secret, timestamp=1111111111) == "050471"
    assert make_totp(secret, timestamp=1234567890) == "005924"
    assert make_totp(secret, timestamp=2000000000) == "279037"
    assert make_totp(secret, timestamp=20000000000) == "353130"
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import functools
import os
import random
import socket
import time
import threading
import random

from pycassa import columnfamily
from pycassa import pool

from r2.lib import baseplate_integration
from r2.lib import cache
from r2.lib import utils

class TimingStatBuffer:
    """Dictionary of keys to cumulative time+count values.

    This provides thread-safe accumulation of pairs of values. Iterating over
    instances of this class yields (key, (total_time, count)) tuples.
    """

    Timing = collections.namedtuple('Timing', ['key', 'start', 'end'])


    def __init__(self):
        # Store data internally as a map of keys to complex values. The real
        # part of the complex value is the total time (in seconds), and the
        # imaginary part is the total count.
        self.data = collections.defaultdict(complex)
        self.log = threading.local()

    def record(self, key, start, end, publish=True):
        if publish:
            # Add to the total time and total count with a single complex value,
            # so as to avoid inconsistency from a poorly timed context switch.
            self.data[key] += (end - start) + 1j

        if getattr(self.log, 'timings', None) is not None:
            self.log.timings.append(self.Timing(key, start, end))

    def flush(self):
        """Yields accumulated timing and counter data and resets the buffer."""
        data, self.data = self.data, collections.defaultdict(complex)
        while True:
            try:
                k, v = data.popitem()
            except KeyError:
                break

            total_time, count = v.real, v.imag
            divisor = count or 1
            mean = total_time / divisor
            yield k, str(mean * 1000) + '|ms'

    def start_logging(self):
        self.log.timings = []

    def end_logging(self):
        timings = getattr(self.log, 'timings', None)
        self.log.timings = None
        return timings


class CountingStatBuffer:
    """Dictionary of keys to cumulative counts."""

    def __init__(self):
        self.data = collections.defaultdict(int)

    def record(self, key, delta):
        self.data[key] += delta

    def flush(self):
        """Yields accumulated counter data and resets the buffer."""
        data, self.data = self.data, collections.defaultdict(int)
        for k, v in data.iteritems():
            yield k, str(v) + '|c'


class StringCountBuffer:
    """Dictionary of keys to counts of various values."""

    def __init__(self):
        self.data = collections.defaultdict(
            functools.partial(collections.defaultdict, int))

    @staticmethod
    def _encode_string(string):
        # escape \ -> \\, | -> \&, : -> \;, and newline -> \n
        return (
            string.replace('\\', '\\\\')
                .replace('\n', '\\n')
                .replace('|', '\\&')
                .replace(':', '\\;'))

    def record(self, key, value, count=1):
        self.data[key][value] += count

    def flush(self):
        new_data = collections.defaultdict(
            functools.partial(collections.defaultdict, int))
        data, self.data = self.data, new_data
        for k, counts in data.iteritems():
            for v, count in counts.iteritems():
                yield k, str(count) + '|s|' + self._encode_string(v)


class StatsdConnection:
    def __init__(self, addr, compress=True):
        if addr:
            self.host, self.port = self._parse_addr(addr)
            self.sock = self._make_socket()
        else:
            self.host = self.port = self.sock = None
        self.compress = compress

    @classmethod
    def _make_socket(cls):
        return socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    @staticmethod
    def _parse_addr(addr):
        host, port_str = addr.rsplit(':', 1)
        return host, int(port_str)

    @staticmethod
    def _compress(lines):
        compressed_lines = []
        previous = ''
        for line in sorted(lines):
            prefix = os.path.commonprefix([previous, line])
            if len(prefix) > 3:
                prefix_len = len(prefix)
                compressed_lines.append(
                    '^%02x%s' % (prefix_len, line[prefix_len:]))
            else:
                compressed_lines.append(line)
            previous = line
        return compressed_lines

    def send(self, data):
        if self.sock is None:
            return
        data = ('%s:%s' % item for item in data)
        if self.compress:
            data = self._compress(data)
        payload = '\n'.join(data)
        self.sock.sendto(payload, (self.host, self.port))


class StatsdClient:
    _data_iterator = iter
    _make_conn = StatsdConnection

    def __init__(self, addr=None, sample_rate=1.0):
        self.sample_rate = sample_rate
        self.timing_stats = TimingStatBuffer()
        self.counting_stats = CountingStatBuffer()
        self.string_counts = StringCountBuffer()
        self.connect(addr)

    def connect(self, addr):
        self.conn = self._make_conn(addr)

    def disconnect(self):
        self.conn = self._make_conn(None)

    def flush(self):
        data = list(self.timing_stats.flush())
        data.extend(self.counting_stats.flush())
        data.extend(self.string_counts.flush())
        self.conn.send(self._data_iterator(data))


def _get_stat_name(*name_parts):
    def to_str(value):
        if isinstance(value, unicode):
            value = value.encode('utf-8', 'replace')
        return value
    return '.'.join(to_str(x) for x in name_parts if x)


class Counter:
    def __init__(self, client, name):
        self.client = client
        self.name = name

    def _send(self, subname, delta):
        name = _get_stat_name(self.name, subname)
        return self.client.counting_stats.record(name, delta)

    def increment(self, subname=None, delta=1):
        self._send(subname, delta)

    def decrement(self, subname=None, delta=1):
        self._send(subname, -delta)

    def __add__(self, delta):
        self.increment(delta=delta)
        return self

    def __sub__(self, delta):
        self.decrement(delta=delta)
        return self


class Timer:
    _time = time.time

    def __init__(self, client, name, publish=True):
        self.client = client
        self.name = name
        self.publish = publish
        self._start = None
        self._last = None
        self._stop = None
        self._timings = []

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, type, value, tb):
        self.stop()

    def flush(self):
        for timing in self._timings:
            self.send(*timing)
        self._timings = []

    def elapsed_seconds(self):
        if self._start is None:
            raise AssertionError("timer hasn't been started")
        if self._stop is None:
            raise AssertionError("timer hasn't been stopped")
        return self._stop - self._start

    def send(self, subname, start, end):
        name = _get_stat_name(self.name, subname)
        self.client.timing_stats.record(name, start, end,
                                        publish=self.publish)

    def start(self):
        self._last = self._start = self._time()

    def intermediate(self, subname):
        if self._last is None:
            raise AssertionError("timer hasn't been started")
        if self._stop is not None:
            raise AssertionError("timer is stopped")
        last, self._last = self._last, self._time()
        self._timings.append((subname, last, self._last))

    def stop(self, subname='total'):
        if self._start is None:
            raise AssertionError("timer hasn't been started")
        if self._stop is not None:
            raise AssertionError('timer is already stopped')
        self._stop = self._time()
        self.flush()
        self.send(subname, self._start, self._stop)


class Stats:
    # Sample rate for recording cache hits/misses, relative to the global
    # sample_rate.
    CACHE_SAMPLE_RATE = 0.01

    CASSANDRA_KEY_SUFFIXES = ['error', 'ok']

    def __init__(self, addr, sample_rate):
        self.client = StatsdClient(addr, sample_rate)

    def get_timer(self, name, publish=True):
        return Timer(self.client, name, publish)

    # Just a convenience method to use timers as context managers clearly
    def quick_time(self, *args, **kwargs):
        return self.get_timer(*args, **kwargs)

    def transact(self, action, start, end):
        timer = self.get_timer('service_time')
        timer.send(action, start, end)

    def get_counter(self, name):
        return Counter(self.client, name)

    def action_count(self, counter_name, name, delta=1):
        counter = self.get_counter(counter_name)
        if counter:
            from pylons import request
            counter.increment('%s.%s' % (request.environ["pylons.routes_dict"]["action"], name), delta=delta)

    def action_event_count(self, event_name, state=None, delta=1, true_name="success", false_name="fail"):
        counter_name = 'event.%s' % event_name
        if state == True:
            self.action_count(counter_name, true_name, delta=delta)
        elif state == False:
            self.action_count(counter_name, false_name, delta=delta)
        self.action_count(counter_name, 'total', delta=delta)

    def simple_event(self, event_name, delta=1):
        parts = event_name.split('.')
        counter = self.get_counter('.'.join(['event'] + parts[:-1]))
        if counter:
            counter.increment(parts[-1], delta=delta)

    def simple_timing(self, event_name, ms):
        self.client.timing_stats.record(event_name, start=0, end=ms)

    def event_count(self, event_name, name, sample_rate=None):
        if sample_rate is None:
            sample_rate = 1.0
        counter = self.get_counter('event.%s' % event_name)
        if counter and random.random() < sample_rate:
            counter.increment(name)
            counter.increment('total')

    def cache_count_multi(self, data, sample_rate=None):
        if sample_rate is None:
            sample_rate = self.CACHE_SAMPLE_RATE
        counter = self.get_counter('cache')
        if counter and random.random() < sample_rate:
            for name, delta in data.iteritems():
                counter.increment(name, delta=delta)

    def amqp_processor(self, queue_name):
        """Decorator for recording stats for amqp queue consumers/handlers."""
        def decorator(processor):
            def wrap_processor(msgs, *args):
                # Work the same for amqp.consume_items and amqp.handle_items.
                msg_tup = utils.tup(msgs)

                metrics_name = "amqp." + queue_name
                start = time.time()
                try:
                    with baseplate_integration.make_server_span(metrics_name):
                        return processor(msgs, *args)
                finally:
                    service_time = (time.time() - start) / len(msg_tup)
                    for n, msg in enumerate(msg_tup):
                        fake_start = start + n * service_time
                        fake_end = fake_start + service_time
                        self.transact(metrics_name, fake_start, fake_end)
                    self.flush()
            return wrap_processor
        return decorator

    def flush(self):
        self.client.flush()

    def start_logging_timings(self):
        self.client.timing_stats.start_logging()

    def end_logging_timings(self):
        return self.client.timing_stats.end_logging()

    def cf_key_iter(self, operation, column_families, suffix):
        if not self.client:
            return
        if not isinstance(column_families, list):
            column_families = [column_families]
        for cf in column_families:
            yield '.'.join(['cassandra', cf, operation, suffix])

    def cassandra_timing(self, operation, column_families, success,
                         start, end):
        suffix = self.CASSANDRA_KEY_SUFFIXES[success]
        for key in self.cf_key_iter(operation, column_families, suffix):
            self.client.timing_stats.record(key, start, end)

    def cassandra_counter(self, operation, column_families, suffix, delta):
        for key in self.cf_key_iter(operation, column_families, suffix):
            self.client.counting_stats.record(key, delta)

    def pg_before_cursor_execute(self, conn, cursor, statement, parameters,
                               context, executemany):
        from pylons import tmpl_context as c

        context._query_start_time = time.time()

        try:
            c.trace
        except TypeError:
            # the tmpl_context global isn't available out of request
            return

        if c.trace:
            context.pg_child_trace = c.trace.make_child("postgres")
            context.pg_child_trace.start()

    def pg_after_cursor_execute(self, conn, cursor, statement, parameters,
                              context, executemany):
        dsn = dict(part.split('=', 1)
                   for part in context.engine.url.query['dsn'].split())

        if getattr(context, "pg_child_trace", None):
            context.pg_child_trace.set_tag("host", dsn["host"])
            context.pg_child_trace.set_tag("db", dsn["dbname"])
            context.pg_child_trace.set_tag("statement", statement)
            context.pg_child_trace.finish()

        start = context._query_start_time
        self.pg_event(dsn['host'], dsn['dbname'], start, time.time())

    def pg_event(self, db_server, db_name, start, end):
        if not self.client:
            return
        key = '.'.join(['pg', db_server.replace('.', '-'), db_name])
        self.client.timing_stats.record(key, start, end)

    def count_string(self, key, value, count=1):
        self.client.string_counts.record(key, str(value), count=count)


class CacheStats:
    def __init__(self, parent, cache_name):
        self.parent = parent
        self.cache_name = cache_name
        self.hit_stat_name = '%s.hit' % self.cache_name
        self.miss_stat_name = '%s.miss' % self.cache_name
        self.total_stat_name = '%s.total' % self.cache_name
        self.hit_stat_template = '%s.%%s.hit' % self.cache_name
        self.miss_stat_template = '%s.%%s.miss' % self.cache_name
        self.total_stat_template = '%s.%%s.total' % self.cache_name

    def cache_hit(self, delta=1, subname=None):
        if delta:
            data = {
                self.hit_stat_name: delta,
                self.total_stat_name: delta,
            }
            if subname:
                data.update({
                    self.hit_stat_template % subname: delta,
                    self.total_stat_template % subname: delta,
                })
            self.parent.cache_count_multi(data)

    def cache_miss(self, delta=1, subname=None):
        if delta:
            data = {
                self.miss_stat_name: delta,
                self.total_stat_name: delta,
            }
            if subname:
                data.update({
                    self.miss_stat_template % subname: delta,
                    self.total_stat_template % subname: delta,
                })
            self.parent.cache_count_multi(data)


class StaleCacheStats(CacheStats):
    def __init__(self, parent, cache_name):
        CacheStats.__init__(self, parent, cache_name)
        self.stale_hit_name = '%s.stale.hit' % self.cache_name
        self.stale_miss_name = '%s.stale.miss' % self.cache_name
        self.stale_total_name = '%s.stale.total' % self.cache_name
        self.stale_hit_stat_template = '%s.stale.%%s.hit' % self.cache_name
        self.stale_miss_stat_template = '%s.stale.%%s.miss' % self.cache_name
        self.stale_total_stat_template = '%s.stale.%%s.total' % self.cache_name

    def stale_hit(self, delta=1, subname=None):
        if delta:
            data = {
                self.stale_hit_name: delta,
                self.stale_total_name: delta,
            }
            if subname:
                data.update({
                    self.stale_hit_stat_template % subname: delta,
                    self.stale_total_stat_template % subname: delta,
                })
            self.parent.cache_count_multi(data)

    def stale_miss(self, delta=1, subname=None):
        if delta:
            data = {
                self.stale_miss_name: delta,
                self.stale_total_name: delta,
            }
            if subname:
                data.update({
                    self.stale_miss_stat_template % subname: delta,
                    self.stale_total_stat_template % subname: delta,
                })
            self.parent.cache_count_multi(data)


class StatsCollectingConnectionPool(pool.ConnectionPool):
    def __init__(self, keyspace, stats=None, *args, **kwargs):
        pool.ConnectionPool.__init__(self, keyspace, *args, **kwargs)
        self.stats = stats

    def _get_new_wrapper(self, server):
        host, sep, port = server.partition(':')
        self.stats.event_count('cassandra.connections', host)

        cf_types = (columnfamily.ColumnParent, columnfamily.ColumnPath)

        def get_cf_name_from_args(args, kwargs):
            for v in args:
                if isinstance(v, cf_types):
                    return v.column_family
            for v in kwargs.itervalues():
                if isinstance(v, cf_types):
                    return v.column_family
            return None

        def get_cf_name_from_batch_mutation(args, kwargs):
            cf_names = set()
            mutation_map = args[0]
            for key_mutations in mutation_map.itervalues():
                cf_names.update(key_mutations)
            return list(cf_names)

        instrumented_methods = dict(
            get=get_cf_name_from_args,
            get_slice=get_cf_name_from_args,
            multiget_slice=get_cf_name_from_args,
            get_count=get_cf_name_from_args,
            multiget_count=get_cf_name_from_args,
            get_range_slices=get_cf_name_from_args,
            get_indexed_slices=get_cf_name_from_args,
            insert=get_cf_name_from_args,
            batch_mutate=get_cf_name_from_batch_mutation,
            add=get_cf_name_from_args,
            remove=get_cf_name_from_args,
            remove_counter=get_cf_name_from_args,
            truncate=lambda args, kwargs: args[0],
        )

        def record_error(method_name, cf_name, start, end):
            if cf_name and self.stats:
                self.stats.cassandra_timing(method_name, cf_name, False,
                                           start, end)

        def record_success(method_name, cf_name, start, end):
            if cf_name and self.stats:
                self.stats.cassandra_timing(method_name, cf_name, True,
                                           start, end)

        def record_size(method_name, cf_name, result, key="size"):
            if cf_name and self.stats:
                # if we don't have easy access to the wire-size, we can
                # proxy because thrift objects have descriptive reprs
                size = len(repr(result))
                self.stats.cassandra_counter(method_name, cf_name, key, size)

        # size_sample determines how often we measure and track the size
        # of the response from cassandra.
        def instrument(f, get_cf_name, size_sample=0.01):
            def call_with_instrumentation(*args, **kwargs):
                from pylons import tmpl_context as c

                cf_name = get_cf_name(args, kwargs)
                method_name = f.__name__

                try:
                    c.trace
                except TypeError:
                    # the tmpl_context global isn't available out of request
                    cassandra_child_trace = utils.SimpleSillyStub()
                else:
                    if c.trace:
                        cassandra_child_trace = c.trace.make_child("cassandra")
                        cassandra_child_trace.set_tag("column_family", cf_name)
                        cassandra_child_trace.set_tag("method", method_name)
                    else:
                        cassandra_child_trace = utils.SimpleSillyStub()

                start = time.time()
                try:
                    with cassandra_child_trace:
                        result = f(*args, **kwargs)
                except:
                    record_error(method_name, cf_name, start, time.time())
                    raise
                else:
                    if random.random() < size_sample:
                        record_size(method_name, cf_name, result)

                    record_success(method_name, cf_name, start, time.time())
                    return result
            return call_with_instrumentation

        wrapper = pool.ConnectionPool._get_new_wrapper(self, server)
        for method_name, get_cf_name in instrumented_methods.iteritems():
            f = getattr(wrapper, method_name)
            setattr(wrapper, method_name, instrument(f, get_cf_name))
        return wrapper
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import re


class ConfigValue(object):
    _bool_map = dict(true=True, false=False)

    @staticmethod
    def str(v, key=None):
        return str(v)

    @staticmethod
    def int(v, key=None):
        return int(v)

    @staticmethod
    def float(v, key=None):
        return float(v)

    @staticmethod
    def bool(v, key=None):
        if v in (True, False, None):
            return bool(v)
        try:
            return ConfigValue._bool_map[v.lower()]
        except KeyError:
            raise ValueError("Unknown value for %r: %r" % (key, v))

    @staticmethod
    def tuple(v, key=None):
        return tuple(ConfigValue.to_iter(v))

    @staticmethod
    def set(v, key=None):
        return set(ConfigValue.to_iter(v))

    @staticmethod
    def set_of(value_type, delim=','):
        def parse(v, key=None):
            return set(value_type(x)
                       for x in ConfigValue.to_iter(v, delim=delim))
        return parse

    @staticmethod
    def tuple_of(value_type, delim=','):
        def parse(v, key=None):
            return tuple(value_type(x)
                         for x in ConfigValue.to_iter(v, delim=delim))
        return parse

    @staticmethod
    def dict(key_type, value_type, delim=',', kvdelim=':'):
        def parse(v, key=None):
            values = (i.partition(kvdelim)
                      for i in ConfigValue.to_iter(v, delim=delim))
            return {key_type(x): value_type(y) for x, _,  y in values}
        return parse

    @staticmethod
    def choice(**choices):
        def parse_choice(v, key=None):
            try:
                return choices[v]
            except KeyError:
                raise ValueError("Unknown option for %r: %r not in %r" % (key, v, choices.keys()))
        return parse_choice

    @staticmethod
    def to_iter(v, delim = ','):
        return (x.strip() for x in v.split(delim) if x)

    @staticmethod
    def timeinterval(v, key=None):
        # this import is at function level because it relies on the cythonized
        # modules being present which is a problem for plugin __init__s that
        # use this module since they are imported in the early stages of the
        # makefile
        from r2.lib.utils import timeinterval_fromstr
        return timeinterval_fromstr(v)

    messages_re = re.compile(r'"([^"]+)"')
    @staticmethod
    def messages(v, key=None):
        return ConfigValue.messages_re.findall(v.decode("string_escape"))

    @staticmethod
    def baseplate(baseplate_parser):
        def adapter(v, key=None):
            return baseplate_parser(v)
        return adapter


class ConfigValueParser(dict):
    def __init__(self, raw_data):
        dict.__init__(self, raw_data)
        self.config_keys = {}
        self.raw_data = raw_data

    def add_spec(self, spec):
        new_keys = []
        for parser, keys in spec.iteritems():
            # keys can be either a list or a dict
            for key in keys:
                assert key not in self.config_keys
                self.config_keys[key] = parser
                new_keys.append(key)
        self._update_values(new_keys)

    def _update_values(self, keys):
        for key in keys:
            if key not in self.raw_data:
                continue

            value = self.raw_data[key]
            if key in self.config_keys:
                parser = self.config_keys[key]
                value = parser(value, key)
            self[key] = value
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import base64
import boto
import hashlib
import hmac
import json
import os
import sys
import time
import datetime
import pytz
from collections import namedtuple

from pylons import app_globals as g


HADOOP_FOLDER_SUFFIX = '_$folder$'

SIGNATURE_V4_ALGORITHM = "AWS4-HMAC-SHA256"


def _to_path(bucket, key):
    if not bucket:
        raise ValueError
    return 's3://%s/%s' % (bucket, key)


def _from_path(path):
    """Return bucket and key names from an s3 path.

    Path of 's3://BUCKET/KEY/NAME' would return 'BUCKET', 'KEY/NAME'.

    """

    if not path.startswith('s3://'):
        raise ValueError('Bad S3 path %s' % path)

    r = path[len('s3://'):].split('/', 1)
    bucket = key = None

    if len(r) == 2:
        bucket, key = r[0], r[1]
    else:
        bucket = r[0]

    if not bucket:
        raise ValueError('Bad S3 path %s' % path)

    return bucket, key


S3Path = namedtuple('S3Path', ['bucket', 'key'])


def parse_s3_path(path):
    return S3Path(*_from_path(path))


def format_expires(expires):
    return expires.strftime(EXPIRES_DATE_FORMAT)


def get_text_from_s3(s3_connection, path):
    """Read a file from S3 and return it as text."""
    bucket_name, key_name = _from_path(path)
    bucket = s3_connection.get_bucket(bucket_name)
    k = boto.s3.Key(bucket)
    k.key = key_name
    txt = k.get_contents_as_string()
    return txt


def mv_file_s3(s3_connection, src_path, dst_path):
    """Move a file within S3."""
    src_bucket_name, src_key_name = _from_path(src_path)
    dst_bucket_name, dst_key_name = _from_path(dst_path)

    src_bucket = s3_connection.get_bucket(src_bucket_name)
    k = boto.s3.Key(src_bucket)
    k.key = src_key_name
    k.copy(dst_bucket_name, dst_key_name)
    k.delete()


def s3_key_exists(s3_connection, path):
    bucket_name, key_name = _from_path(path)
    bucket = s3_connection.get_bucket(bucket_name)
    key = bucket.get_key(key_name)
    return bool(key)


def copy_to_s3(s3_connection, local_path, dst_path, verbose=False):
    def callback(trans, total):
        sys.stdout.write('%s/%s' % trans, total)
        sys.stdout.flush()

    dst_bucket_name, dst_key_name = _from_path(dst_path)
    bucket = s3_connection.get_bucket(dst_bucket_name)

    filename = os.path.basename(local_path)
    if not filename:
        return

    key_name = os.path.join(dst_key_name, filename)
    k = boto.s3.Key(bucket)
    k.key = key_name

    kw = {}
    if verbose:
        print 'Uploading %s to %s' % (local_path, dst_path)
        kw['cb'] = callback

    k.set_contents_from_filename(logfile, **kw)


def get_connection():
    return boto.connect_s3(g.S3KEY_ID or None, g.S3SECRET_KEY or None)


def get_key(bucket_name, key, connection=None):
    connection = connection or get_connection()
    bucket = connection.get_bucket(bucket_name)

    return bucket.get_key(key)

def get_keys(bucket_name, meta=False, connection=None, **kwargs):
    connection = connection or get_connection()
    bucket = connection.get_bucket(bucket_name)
    keys = bucket.get_all_keys(**kwargs)

    if not meta:
        return keys

    return [bucket.get_key(key.name)
            for key in keys]


def delete_keys(bucket_name, prefix, connection=None):
    connection = connection or get_connection()

    keys = get_keys(bucket_name, prefix=prefix, connection=connection)
    return connection.get_bucket(bucket_name).delete_keys(keys)


def _get_v4_credential(aws_access_key_id, date, service_name, region_name):
    return ("%(aws_access_key_id)s/%(datestamp)s/%(region_name)s/%(service_name)s/aws4_request" % {
        "aws_access_key_id": aws_access_key_id,
        "datestamp": date.strftime("%Y%m%d"),
        "region_name": region_name,
        "service_name": service_name,
    })

def _get_upload_policy(
        bucket, key, credential, date, acl,
        ttl=60,
        success_action_redirect=None,
        success_action_status="201",
        content_type=None,
        max_content_length=((1024**2) * 3),
        storage_class="STANDARD",
        region_name=None,
        meta=None,
        connection=None,
    ):

    connection = connection or get_connection()
    meta = meta or {}

    expiration = time.gmtime(int(time.time() + ttl))
    conditions = []

    conditions.append({"bucket": bucket})

    if key.endswith("${filename}"):
        conditions.append(["starts-with", "$key", key[:-len("${filename}")]])
    else:
        conditions.append({"key": key})

    conditions.append({"acl": acl})
    conditions.append({"x-amz-storage-class": storage_class})

    conditions.append({"x-amz-credential": credential})
    conditions.append({"x-amz-algorithm": SIGNATURE_V4_ALGORITHM})
    conditions.append({"x-amz-date": date.strftime("%Y%m%dT%H%M%SZ")})
    conditions.append({"x-amz-security-token": connection.provider.security_token})

    if success_action_redirect:
        conditions.append([
            "starts-with",
            "$success_action_redirect",
            success_action_redirect,
        ])
    else:
        conditions.append({
            "success_action_status": success_action_status,
        })

    conditions.append([
        "content-length-range", 0, max_content_length])

    for key, value in meta.iteritems():
        conditions.append({key: value})

    if content_type:
        conditions.append({"content-type": content_type})

    return base64.b64encode(json.dumps({
        "expiration": time.strftime(boto.utils.ISO8601, expiration),
        "conditions": conditions,
    }))


def _sign(secret, msg):
    return hmac.new(secret, msg.encode("utf-8"), hashlib.sha256).digest()


def _derive_v4_signature_key(secret, date, region_name, service_name):
    key_date = _sign(("AWS4" + secret).encode("utf-8"), date.strftime("%Y%m%d"))
    key_region = _sign(key_date, region_name)
    key_service = _sign(key_region, service_name)
    return _sign(key_service, "aws4_request")


def _get_upload_signature(
        policy,
        date,
        region_name,
        connection=None,
    ):

    connection = connection or get_connection()

    key = connection.provider.secret_key.encode("utf-8")
    v4_key = _derive_v4_signature_key(
        secret=key, date=date, region_name=region_name, service_name="s3")

    return hmac.new(v4_key, policy, hashlib.sha256).hexdigest()


def get_post_args(
        bucket, key,
        acl="public-read",
        success_action_redirect=None,
        success_action_status="201",
        content_type=None,
        storage_class="STANDARD",
        region_name="us-east-1",
        meta=None,
        connection=None,
        **kwargs
    ):

    meta = meta or []
    connection = connection or get_connection()
    algorithm = "AWS4-HMAC-SHA256"
    date = datetime.datetime.now(pytz.utc)
    credential = _get_v4_credential(
        aws_access_key_id=connection.provider.access_key,
        date=date,
        service_name="s3",
        region_name=region_name,
    )
    policy = _get_upload_policy(
        bucket=bucket,
        key=key,
        credential=credential,
        date=date,
        acl=acl,
        success_action_redirect=success_action_redirect,
        success_action_status=success_action_status,
        content_type=content_type,
        storage_class=storage_class,
        region_name=region_name,
        meta=meta,
        connection=connection,
    )
    signature = _get_upload_signature(
        policy=policy,
        date=date,
        region_name=region_name,
        connection=connection,
    )

    fields = []

    fields.append({
        "name": "acl",
        "value": acl,
    })

    fields.append({
        "name": "key",
        "value": key,
    })

    fields.append({
        "name": "X-Amz-Credential",
        "value": credential,
    })

    fields.append({
        "name": "X-Amz-Algorithm",
        "value": SIGNATURE_V4_ALGORITHM,
    })

    fields.append({
        "name": "X-Amz-Date",
        "value": date.strftime("%Y%m%dT%H%M%SZ"),
    })

    if success_action_redirect:
        fields.append({
            "name": "success_action_redirect",
            "value": success_action_redirect,
        })
    else:
        fields.append({
            "name": "success_action_status",
            "value": success_action_status,
        })

    fields.append({
        "name": "content-type",
        "value": content_type,
    })

    fields.append({
        "name": "x-amz-storage-class",
        "value": storage_class,
    })

    for key, value in meta.iteritems():
        fields.append({
            "name": key,
            "value": value,
        })

    fields.append({
        "name": "policy",
        "value": policy,
    })

    fields.append({
        "name": "X-Amz-Signature",
        "value": signature,
    })

    fields.append({
        "name": "x-amz-security-token",
        "value": connection.provider.security_token,
    })

    return {
        "action": "//%s.%s" % (bucket, g.s3_media_domain),
        "fields": fields,
    }
<EOF>
<BOF>
import re
from collections import namedtuple, Counter

CharRange = namedtuple("CharRange", "name start end")

NONCHAR = re.compile(r"\W+")


def charset_name(name, start, end):
    if name.lower() == "undefined":
        return "_".join([name.title(), start, end])
    else:
        return NONCHAR.sub("_", name.title())

CHARSET_RANGES = tuple(
    CharRange(charset_name(name, start, end), int(start, 16), int(end, 16))
    for start, end, name in (
        ("0000", "007F", "Basic Latin"),
        ("0080", "00FF", "C1 Controls and Latin-1 Supplement"),
        ("0100", "017F", "Latin Extended-A"),
        ("0180", "024F", "Latin Extended-B"),
        ("0250", "02AF", "IPA Extensions"),
        ("02B0", "02FF", "Spacing Modifier Letters"),
        ("0300", "036F", "Combining Diacritical Marks"),
        ("0370", "03FF", "Greek/Coptic"),
        ("0400", "04FF", "Cyrillic"),
        ("0500", "052F", "Cyrillic Supplement"),
        ("0530", "058F", "Armenian"),
        ("0590", "05FF", "Hebrew"),
        ("0600", "06FF", "Arabic"),
        ("0700", "074F", "Syriac"),
        ("0750", "077F", "Undefined"),
        ("0780", "07BF", "Thaana"),
        ("07C0", "08FF", "Undefined"),
        ("0900", "097F", "Devanagari"),
        ("0980", "09FF", "Bengali/Assamese"),
        ("0A00", "0A7F", "Gurmukhi"),
        ("0A80", "0AFF", "Gujarati"),
        ("0B00", "0B7F", "Oriya"),
        ("0B80", "0BFF", "Tamil"),
        ("0C00", "0C7F", "Telugu"),
        ("0C80", "0CFF", "Kannada"),
        ("0D00", "0DFF", "Malayalam"),
        ("0D80", "0DFF", "Sinhala"),
        ("0E00", "0E7F", "Thai"),
        ("0E80", "0EFF", "Lao"),
        ("0F00", "0FFF", "Tibetan"),
        ("1000", "109F", "Myanmar"),
        ("10A0", "10FF", "Georgian"),
        ("1100", "11FF", "Hangul Jamo"),
        ("1200", "137F", "Ethiopic"),
        ("1380", "139F", "Undefined"),
        ("13A0", "13FF", "Cherokee"),
        ("1400", "167F", "Unified Canadian Aboriginal Syllabics"),
        ("1680", "169F", "Ogham"),
        ("16A0", "16FF", "Runic"),
        ("1700", "171F", "Tagalog"),
        ("1720", "173F", "Hanunoo"),
        ("1740", "175F", "Buhid"),
        ("1760", "177F", "Tagbanwa"),
        ("1780", "17FF", "Khmer"),
        ("1800", "18AF", "Mongolian"),
        ("18B0", "18FF", "Undefined"),
        ("1900", "194F", "Limbu"),
        ("1950", "197F", "Tai Le"),
        ("1980", "19DF", "Undefined"),
        ("19E0", "19FF", "Khmer Symbols"),
        ("1A00", "1CFF", "Undefined"),
        ("1D00", "1D7F", "Phonetic Extensions"),
        ("1D80", "1DFF", "Undefined"),
        ("1E00", "1EFF", "Latin Extended Additional"),
        ("1F00", "1FFF", "Greek Extended"),
        ("2000", "206F", "General Punctuation"),
        ("2070", "209F", "Superscripts and Subscripts"),
        ("20A0", "20CF", "Currency Symbols"),
        ("20D0", "20FF", "Combining Diacritical Marks for Symbols"),
        ("2100", "214F", "Letterlike Symbols"),
        ("2150", "218F", "Number Forms"),
        ("2190", "21FF", "Arrows"),
        ("2200", "22FF", "Mathematical Operators"),
        ("2300", "23FF", "Miscellaneous Technical"),
        ("2400", "243F", "Control Pictures"),
        ("2440", "245F", "Optical Character Recognition"),
        ("2460", "24FF", "Enclosed Alphanumerics"),
        ("2500", "257F", "Box Drawing"),
        ("2580", "259F", "Block Elements"),
        ("25A0", "25FF", "Geometric Shapes"),
        ("2600", "26FF", "Miscellaneous Symbols"),
        ("2700", "27BF", "Dingbats"),
        ("27C0", "27EF", "Miscellaneous Mathematical Symbols-A"),
        ("27F0", "27FF", "Supplemental Arrows-A"),
        ("2800", "28FF", "Braille Patterns"),
        ("2900", "297F", "Supplemental Arrows-B"),
        ("2980", "29FF", "Miscellaneous Mathematical Symbols-B"),
        ("2A00", "2AFF", "Supplemental Mathematical Operators"),
        ("2B00", "2BFF", "Miscellaneous Symbols and Arrows"),
        ("2C00", "2E7F", "Undefined"),
        ("2E80", "2EFF", "CJK Radicals Supplement"),
        ("2F00", "2FDF", "Kangxi Radicals"),
        ("2FE0", "2FEF", "Undefined"),
        ("2FF0", "2FFF", "Ideographic Description Characters"),
        ("3000", "303F", "CJK Symbols and Punctuation"),
        ("3040", "309F", "Hiragana"),
        ("30A0", "30FF", "Katakana"),
        ("3100", "312F", "Bopomofo"),
        ("3130", "318F", "Hangul Compatibility Jamo"),
        ("3190", "319F", "Kanbun (Kunten)"),
        ("31A0", "31BF", "Bopomofo Extended"),
        ("31C0", "31EF", "Undefined"),
        ("31F0", "31FF", "Katakana Phonetic Extensions"),
        ("3200", "32FF", "Enclosed CJK Letters and Months"),
        ("3300", "33FF", "CJK Compatibility"),
        ("3400", "4DBF", "CJK Unified Ideographs Extension A"),
        ("4DC0", "4DFF", "Yijing Hexagram Symbols"),
        ("4E00", "9FAF", "CJK Unified Ideographs"),
        ("9FB0", "9FFF", "Undefined"),
        ("A000", "A48F", "Yi Syllables"),
        ("A490", "A4CF", "Yi Radicals"),
        ("A4D0", "ABFF", "Undefined"),
        ("AC00", "D7AF", "Hangul Syllables"),
        ("D7B0", "D7FF", "Undefined"),
        ("D800", "DBFF", "High Surrogate Area"),
        ("DC00", "DFFF", "Low Surrogate Area"),
        ("E000", "F8FF", "Private Use Area"),
        ("F900", "FAFF", "CJK Compatibility Ideographs"),
        ("FB00", "FB4F", "Alphabetic Presentation Forms"),
        ("FB50", "FDFF", "Arabic Presentation Forms-A"),
        ("FE00", "FE0F", "Variation Selectors"),
        ("FE10", "FE1F", "Undefined"),
        ("FE20", "FE2F", "Combining Half Marks"),
        ("FE30", "FE4F", "CJK Compatibility Forms"),
        ("FE50", "FE6F", "Small Form Variants"),
        ("FE70", "FEFF", "Arabic Presentation Forms-B"),
        ("FF00", "FFEF", "Halfwidth and Fullwidth Forms"),
        ("FFF0", "FFFF", "Specials"),
        ("10000", "1007F", "Linear B Syllabary"),
        ("10080", "100FF", "Linear B Ideograms"),
        ("10100", "1013F", "Aegean Numbers"),
        ("10140", "102FF", "Undefined"),
        ("10300", "1032F", "Old Italic"),
        ("10330", "1034F", "Gothic"),
        ("10380", "1039F", "Ugaritic"),
        ("10400", "1044F", "Deseret"),
        ("10450", "1047F", "Shavian"),
        ("10480", "104AF", "Osmanya"),
        ("104B0", "107FF", "Undefined"),
        ("10800", "1083F", "Cypriot Syllabary"),
        ("10840", "1CFFF", "Undefined"),
        ("1D000", "1D0FF", "Byzantine Musical Symbols"),
        ("1D100", "1D1FF", "Musical Symbols"),
        ("1D200", "1D2FF", "Undefined"),
        ("1D300", "1D35F", "Tai Xuan Jing Symbols"),
        ("1D360", "1D3FF", "Undefined"),
        ("1D400", "1D7FF", "Mathematical Alphanumeric Symbols"),
        ("1D800", "1FFFF", "Undefined"),
        ("20000", "2A6DF", "CJK Unified Ideographs Extension B"),
        ("2A6E0", "2F7FF", "Undefined"),
        ("2F800", "2FA1F", "CJK Compatibility Ideographs Supplement"),
        ("2FAB0", "DFFFF", "Unused"),
        ("E0000", "E007F", "Tags"),
        ("E0080", "E00FF", "Unused"),
        ("E0100", "E01EF", "Variation Selectors Supplement"),
        ("E01F0", "EFFFF", "Unused"),
        ("F0000", "FFFFD", "Supplementary Private Use Area-A"),
        ("FFFFE", "FFFFF", "Unused"),
        ("100000", "10FFFD", "Supplementary Private Use Area-B"),
    )
)


def symbology(s):
    """Return a count of what unicode charsets the string contains."""
    symbols = sorted(ord(c) for c in s)
    current_charset = 0
    char_tally = Counter()
    for symbol in symbols:
        while CHARSET_RANGES[current_charset].end < symbol:
            current_charset += 1
        if CHARSET_RANGES[current_charset].start <= symbol:
            name = CHARSET_RANGES[current_charset].name
        else:
            name = "Unknown"
        char_tally[name] += 1

    return char_tally


def charset_summary(s, prefix=""):
    res = {}
    charsets = symbology(s)
    if charsets:
        res["charset"] = charsets.most_common(1)[0][0]
        res["all_charsets"] = dict(charsets.most_common())
    return res
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


from collections import defaultdict, OrderedDict
from datetime import datetime, timedelta
import re

from itertools import chain
from sqlalchemy import func

from r2.lib.inventory_optimization import get_maximized_pageviews
from r2.lib.memoize import memoize
from r2.lib.utils import to_date, tup
from r2.models import (
    Bid,
    FakeSubreddit,
    LocalizedDefaultSubreddits,
    Location,
    NO_TRANSACTION,
    PromoCampaign,
    PromotionWeights,
    Subreddit,
    traffic,
)
from r2.models.promo_metrics import LocationPromoMetrics, PromoMetrics
from r2.models.subreddit import DefaultSR

NDAYS_TO_QUERY = 14  # how much history to use in the estimate
MIN_DAILY_CASS_KEY = 'min_daily_pageviews.GET_listing'
PAGEVIEWS_REGEXP = re.compile('(.*)-GET_listing')
INVENTORY_FACTOR = 1.00
DEFAULT_INVENTORY_FACTOR = 5.00
# For `PERCENT_MOBILE`:
# if `0`, 100% of inventory will be displayed no matter the platform;
# if not `0`:
# - `all` is 100% of inventory
# - `mobile` is `all` * (PERCENT_MOBILE / 100)
# - `desktop` is `all` - `mobile`
PERCENT_MOBILE = 0


def update_prediction_data():
    """Fetch prediction data and write it to cassandra."""
    min_daily_by_sr = _min_daily_pageviews_by_sr(NDAYS_TO_QUERY)

    # combine front page values (sometimes frontpage gets '' for its name)
    if '' in min_daily_by_sr:
        fp = DefaultSR.name.lower()
        min_daily_by_sr[fp] = min_daily_by_sr.get(fp, 0) + min_daily_by_sr['']
        del min_daily_by_sr['']

    filtered = {sr_name: num for sr_name, num in min_daily_by_sr.iteritems()
                if num > 100}
    PromoMetrics.set(MIN_DAILY_CASS_KEY, filtered)


def _min_daily_pageviews_by_sr(ndays=NDAYS_TO_QUERY, end_date=None):
    """Return dict mapping sr_name to min_pageviews over the last ndays."""
    if not end_date:
        last_modified = traffic.get_traffic_last_modified()
        end_date = last_modified - timedelta(days=1)
    stop = end_date
    start = stop - timedelta(ndays)
    time_points = traffic.get_time_points('day', start, stop)
    cls = traffic.PageviewsBySubredditAndPath
    q = (traffic.Session.query(cls.srpath, func.min(cls.pageview_count))
                               .filter(cls.interval == 'day')
                               .filter(cls.date.in_(time_points))
                               .filter(cls.srpath.like('%-GET_listing'))
                               .group_by(cls.srpath))

    # row looks like: ('lightpainting-GET_listing', 16)
    retval = {}
    for row in q:
        m = PAGEVIEWS_REGEXP.match(row[0])
        if m:
            retval[m.group(1)] = row[1]
    return retval


def get_date_range(start, end):
    start, end = map(to_date, [start, end])
    dates = [start + timedelta(i) for i in xrange((end - start).days)]
    return dates


def get_campaigns_by_date(srs, start, end, ignore=None):
    srs = tup(srs)
    sr_names = [sr.name for sr in srs]
    campaign_ids = PromotionWeights.get_campaign_ids(
        start, end=end, sr_names=sr_names)
    if ignore:
        campaign_ids.discard(ignore._id)
    campaigns = PromoCampaign._byID(campaign_ids, data=True, return_dict=False)

    # filter out deleted campaigns that didn't have their PromotionWeights
    # deleted
    campaigns = filter(lambda camp: not camp._deleted, campaigns)

    transaction_ids = {camp.trans_id for camp in campaigns
                                     if camp.trans_id != NO_TRANSACTION}

    if transaction_ids:
        transactions = Bid.query().filter(Bid.transaction.in_(transaction_ids))
        # index transactions by transaction and campaign id because freebies
        # reuse the same transaction id (they always use -link id)
        transaction_by_id = {
            (bid.transaction, bid.campaign): bid for bid in transactions}
    else:
        transaction_by_id = {}

    dates = set(get_date_range(start, end))
    ret = {date: set() for date in dates}
    for camp in campaigns:
        if camp.trans_id == NO_TRANSACTION:
            continue

        if camp.impressions <= 0:
            # pre-CPM campaign
            continue

        transaction = transaction_by_id[(camp.trans_id, camp._id)]
        if not (transaction.is_auth() or transaction.is_charged()):
            continue

        camp_dates = set(get_date_range(camp.start_date, camp.end_date))
        for date in camp_dates.intersection(dates):
            ret[date].add(camp)
    return ret


def get_predicted_pageviews(srs, location=None):
    """
    Return predicted number of pageviews for sponsored headlines.

    Predicted geotargeted impressions are estimated as:

    geotargeted impressions = (predicted untargeted impressions) *
                                 (fp impressions for location / fp impressions)

    """

    srs, is_single = tup(srs, ret_is_single=True)
    sr_names = [sr.name for sr in srs]

    # default subreddits require a different inventory factor
    default_srids = LocalizedDefaultSubreddits.get_global_defaults()

    if location:
        no_location = Location(None)
        r = LocationPromoMetrics.get(DefaultSR, [no_location, location])
        location_pageviews = r[(DefaultSR, location)]
        all_pageviews = r[(DefaultSR, no_location)]
        if all_pageviews:
            location_factor = float(location_pageviews) / float(all_pageviews)
        else:
            location_factor = 0.
    else:
        location_factor = 1.0

    # prediction does not vary by date
    daily_inventory = PromoMetrics.get(MIN_DAILY_CASS_KEY, sr_names=sr_names)
    ret = {}
    for sr in srs:
        if not isinstance(sr, FakeSubreddit) and sr._id in default_srids:
            default_factor = DEFAULT_INVENTORY_FACTOR
        else:
            default_factor = INVENTORY_FACTOR
        base_pageviews = daily_inventory.get(sr.name, 0)
        ret[sr.name] = int(base_pageviews * default_factor * location_factor)

    if is_single:
        return ret[srs[0].name]
    else:
        return ret


def make_target_name(target):
    name = ("collection: %s" % target.collection.name if target.is_collection
                                           else target.subreddit_name)
    return name


def find_campaigns(srs, start, end, ignore):
    """Get all campaigns in srs and pull in campaigns in other targeted srs."""
    all_sr_names = set()
    all_campaigns = set()
    srs = set(srs)

    while srs:
        all_sr_names |= {sr.name for sr in srs}
        new_campaigns_by_date = get_campaigns_by_date(srs, start, end, ignore)
        new_campaigns = set(chain.from_iterable(
            new_campaigns_by_date.itervalues()))
        all_campaigns.update(new_campaigns)
        new_sr_names = set(chain.from_iterable(
            campaign.target.subreddit_names for campaign in new_campaigns
        ))
        new_sr_names -= all_sr_names
        srs = set(Subreddit._by_name(new_sr_names).values())
    return all_campaigns


def get_available_pageviews(targets, start, end, location=None, datestr=False,
                            ignore=None, platform='all'):
    """
    Return the available pageviews by date for the targets and location.

    Available pageviews depends on all equal and higher level locations:
    A location is: subreddit > country > metro

    e.g. if a campaign is targeting /r/funny in USA/Boston we need to check that
    there's enough inventory in:
    * /r/funny (all campaigns targeting /r/funny regardless of location)
    * /r/funny + USA (all campaigns targeting /r/funny and USA with or without
      metro level targeting)
    * /r/funny + USA + Boston (all campaigns targeting /r/funny and USA and
      Boston)
    The available inventory is the smallest of these values.

    """

    # assemble levels of location targeting, None means untargeted
    locations = [None]
    if location:
        locations.append(location)

        if location.metro:
            locations.append(Location(country=location.country))

    # get all the campaigns directly and indirectly involved in our target
    targets, is_single = tup(targets, ret_is_single=True)
    target_srs = list(chain.from_iterable(
        target.subreddits_slow for target in targets))
    all_campaigns = find_campaigns(target_srs, start, end, ignore)

    # get predicted pageviews for each subreddit and location
    all_sr_names = set(sr.name for sr in target_srs)
    all_sr_names |= set(chain.from_iterable(
        campaign.target.subreddit_names for campaign in all_campaigns
    ))
    all_srs = Subreddit._by_name(all_sr_names).values()
    pageviews_dict = {location: get_predicted_pageviews(all_srs, location)
                          for location in locations}

    # determine booked impressions by target and location for each day
    dates = set(get_date_range(start, end))
    booked_dict = {}
    for date in dates:
        booked_dict[date] = {}
        for location in locations:
            booked_dict[date][location] = defaultdict(int)

    for campaign in all_campaigns:
        camp_dates = set(get_date_range(campaign.start_date, campaign.end_date))
        sr_names = tuple(sorted(campaign.target.subreddit_names))
        daily_impressions = campaign.impressions / campaign.ndays

        for location in locations:
            if location and not location.contains(campaign.location):
                # campaign's location is less specific than location
                continue

            for date in camp_dates.intersection(dates):
                booked_dict[date][location][sr_names] += daily_impressions

    # calculate inventory for each target and location on each date
    datekey = lambda dt: dt.strftime('%m/%d/%Y') if datestr else dt

    ret = {}
    for target in targets:
        name = make_target_name(target)
        subreddit_names = target.subreddit_names
        ret[name] = {}
        for date in dates:
            pageviews_by_location = {}
            for location in locations:
                # calculate available impressions for each location
                booked_by_target = booked_dict[date][location]
                pageviews_by_sr_name = pageviews_dict[location]
                pageviews_by_location[location] = get_maximized_pageviews(
                    subreddit_names, booked_by_target, pageviews_by_sr_name)
            # available pageviews is the minimum from all locations
            min_pageviews = min(pageviews_by_location.values())
            if PERCENT_MOBILE != 0:
                mobile_pageviews = min_pageviews * (float(PERCENT_MOBILE) / 100)
                if platform == 'mobile':
                    min_pageviews = mobile_pageviews
                if platform == 'desktop':
                    min_pageviews = min_pageviews - mobile_pageviews
            ret[name][datekey(date)] = max(0, min_pageviews)

    if is_single:
        name = make_target_name(targets[0])
        return ret[name]
    else:
        return ret


def get_oversold(target, start, end, daily_request, ignore=None, location=None):
    available_by_date = get_available_pageviews(target, start, end, location,
                                                datestr=True, ignore=ignore)
    oversold = {}
    for datestr, available in available_by_date.iteritems():
        if available < daily_request:
            oversold[datestr] = available
    return oversold
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, N_

from r2.config import feature
from r2.lib.db import operators
from r2.lib.filters import _force_unicode
from r2.lib.strings import StringHandler, plurals
from r2.lib.utils import  class_property, query_string, timeago
from r2.lib.wrapped import Styled


class MenuHandler(StringHandler):
    """Bastard child of StringHandler and plurals.  Menus are
    typically a single word (and in some cases, a single plural word
    like 'moderators' or 'contributors' so this class first checks its
    own dictionary of string translations before falling back on the
    plurals list."""
    def __getattr__(self, attr):
        try:
            return StringHandler.__getattr__(self, attr)
        except KeyError:
            return getattr(plurals, attr)

# translation strings for every menu on the site
menu =   MenuHandler(hot          = _('hot'),
                     new          = _('new'),
                     old          = _('old'),
                     ups          = _('ups'),
                     downs        = _('downs'),
                     top          = _('top'),
                     more         = _('more'),
                     relevance    = _('relevance'),
                     controversial  = _('controversial'),
                     gilded       = _('gilded'),
                     confidence   = _('best'),
                     random       = _('random'),
                     qa           = _('q&a'),
                     saved        = _('saved {toolbar}'),
                     recommended  = _('recommended'),
                     rising       = _('rising'),
                     admin        = _('admin'),

                     # time sort words
                     hour         = _('past hour'),
                     day          = _('past 24 hours'),
                     week         = _('past week'),
                     month        = _('past month'),
                     year         = _('past year'),
                     all          = _('all time'),

                     # "kind" words
                     spam         = _("spam"),
                     autobanned   = _("autobanned"),

                     # reddit header strings
                     prefs        = _("preferences"),
                     submit       = _("submit"),
                     wiki         = _("wiki"),
                     blog         = _("blog"),
                     logout       = _("logout"),

                     #reddit footer strings
                     reddiquette  = _("reddiquette"),
                     contact      = _("contact us"),
                     buttons      = _("buttons"),
                     widget       = _("widget"),
                     mobile       = _("mobile"),
                     advertising  = _("advertise"),
                     gold         = _('reddit gold'),
                     reddits      = _('subreddits'),
                     rules        = _('site rules'),
                     jobs         = _('jobs'),
                     transparency = _("transparency"),
                     source_code  = _("source code"),
                     values       = _("values"),

                     #preferences
                     options      = _('options'),
                     apps         = _("apps"),
                     feeds        = _("RSS feeds"),
                     friends      = _("friends"),
                     blocked      = _("blocked"),
                     update       = _("password/email"),
                     deactivate   = _("deactivate"),
                     security     = _("security"),

                     # messages
                     compose      = _("send a private message"),
                     inbox        = _("inbox"),
                     sent         = _("sent"),

                     # comments
                     comments     = _("comments {toolbar}"),
                     details      = _("details"),
                     duplicates   = _("other discussions (%(num)s)"),
                     traffic      = _("traffic stats"),
                     stylesheet   = _("stylesheet"),

                     # reddits
                     home         = _("home"),
                     about        = _("about"),
                     edit_subscriptions = _("edit subscriptions"),
                     community_settings = _("subreddit settings"),
                     edit_stylesheet    = _("edit stylesheet"),
                     community_rules    = _("rules"),
                     moderators   = _("moderators"),
                     modmail      = _("moderator mail"),
                     contributors = _("approved submitters"),
                     banned       = _("ban users"),
                     banusers     = _("ban users"),
                     muted        = _("mute users"),
                     flair        = _("edit flair"),
                     log          = _("moderation log"),
                     modqueue     = _("moderation queue"),
                     unmoderated  = _("unmoderated posts"),
                     edited       = _("edited"),
                     employee     = _("employee"),
                     automod      = _("automoderator config"),
                     new_automod  = _("get started with automoderator"),

                     wikibanned        = _("ban wiki contributors"),
                     wikicontributors  = _("add wiki contributors"),

                     wikirecentrevisions = _("recent wiki revisions"),
                     wikipageslist = _("wiki page list"),

                     popular      = _("popular"),
                     create       = _("create"),
                     mine         = _("my subreddits"),
                     quarantine   = _("quarantine"),
                     featured     = _("featured"),

                     i18n         = _("help translate"),
                     errors       = _("errors"),
                     awards       = _("awards"),
                     ads          = _("ads"),
                     promoted     = _("promoted"),
                     sponsor      = _("sponsor"),
                     reporters    = _("reporters"),
                     reports      = _("reports"),
                     reportedauth = _("reported authors"),
                     info         = _("info"),
                     share        = _("share"),

                     overview     = _("overview"),
                     submitted    = _("submitted"),
                     upvoted      = _("upvoted"),
                     downvoted    = _("downvoted"),
                     hidden       = _("hidden {toolbar}"),
                     deleted      = _("deleted"),
                     reported     = _("reported"),
                     voting       = _("voting"),

                     promote        = _('advertising'),
                     new_promo      = _('create promotion'),
                     my_current_promos = _('my promoted links'),
                     current_promos = _('all promoted links'),
                     all_promos     = _('all'),
                     future_promos  = _('unseen'),
                     unapproved_campaigns = _('unapproved campaigns'),
                     inventory      = _('inventory'),
                     live_promos    = _('live'),
                     unpaid_promos  = _('unpaid'),
                     pending_promos = _('pending'),
                     rejected_promos = _('rejected'),
                     edited_live_promos = _('edited live'),

                     sitewide = _('sitewide'),
                     languages = _('languages'),
                     adverts = _('adverts'),

                     whitelist = _("whitelist")
                     )

def menu_style(type):
    """Simple manager function for the styled menus.  Returns a
    (style, css_class) pair given a 'type', defaulting to style =
    'dropdown' with no css_class."""
    default = ('dropdown', '')
    d = dict(lightdrop = ('dropdown', 'lightdrop'),
             tabdrop = ('dropdown', 'tabdrop'),
             srdrop = ('dropdown', 'srdrop'),
             flatlist =  ('flatlist', 'flat-list'),
             tabmenu = ('tabmenu', ''),
             formtab = ('tabmenu', 'formtab'),
             flat_vert = ('flatlist', 'flat-vert'),
             )
    return d.get(type, default)


class NavMenu(Styled):
    """generates a navigation menu.  The intention here is that the
    'style' parameter sets what template/layout to use to differentiate, say,
    a dropdown from a flatlist, while the optional _class, and _id attributes
    can be used to set individualized CSS."""

    def __init__(self, options, default=None, title='', type="dropdown",
                 base_path='', separator='|', _id='', css_class=''):
        self.options = options
        self.default = default
        self.title = title
        self.base_path = base_path
        self.separator = separator

        # add the menu style, but preserve existing css_class parameter
        style, base_css_class = menu_style(type)
        css_class = base_css_class + ((' ' + css_class) if css_class else '')

        # since the menu contains the path info, it's buttons need a
        # configuration pass to get them pointing to the proper urls
        for opt in self.options:
            opt.build(self.base_path)

            # add "choice" css class to each button
            if opt.css_class:
                opt.css_class += " choice"
            else:
                opt.css_class = "choice"

        self.selected = self.find_selected()

        Styled.__init__(self, style, _id=_id, css_class=css_class)

    def find_selected(self):
        maybe_selected = [o for o in self.options if o.is_selected()]
        if maybe_selected:
            # pick the button with the most restrictive pathing
            maybe_selected.sort(lambda x, y:
                                len(y.bare_path) - len(x.bare_path))
            return maybe_selected[0]
        elif self.default:
            #lookup the menu with the 'dest' that matches 'default'
            for opt in self.options:
                if opt.dest == self.default:
                    return opt

    def __iter__(self):
        for opt in self.options:
            yield opt

    def cachable_attrs(self):
        return [
            ('options', self.options),
            ('title', self.title),
            ('selected', self.selected),
            ('separator', self.separator),
        ]


class NavButton(Styled):
    """Smallest unit of site navigation.  A button once constructed
    must also have its build() method called with the current path to
    set self.path.  This step is done automatically if the button is
    passed to a NavMenu instance upon its construction."""

    _style = "plain"

    def __init__(self, title, dest, sr_path=True, aliases=None,
                 target="", use_params=False, css_class='', data=None):
        aliases = aliases or []
        aliases = set(_force_unicode(a.rstrip('/')) for a in aliases)
        if dest:
            aliases.add(_force_unicode(dest.rstrip('/')))

        self.title = title
        self.dest = dest
        self.selected = False

        self.sr_path = sr_path
        self.aliases = aliases
        self.target = target
        self.use_params = use_params
        self.data = data

        Styled.__init__(self, self._style, css_class=css_class)

    def build(self, base_path=''):
        base_path = ("%s/%s/" % (base_path, self.dest)).replace('//', '/')
        self.bare_path = _force_unicode(base_path.replace('//', '/')).lower()
        self.bare_path = self.bare_path.rstrip('/')
        self.base_path = base_path

        if self.use_params:
            base_path += query_string(dict(request.GET))

        # since we've been sloppy of keeping track of "//", get rid
        # of any that may be present
        self.path = base_path.replace('//', '/')

    def is_selected(self):
        stripped_path = _force_unicode(request.path.rstrip('/').lower())

        if not (self.sr_path or c.default_sr):
            return False
        if stripped_path == self.bare_path:
            return True
        site_path = c.site.user_path.lower() + self.bare_path
        if self.sr_path and stripped_path == site_path:
            return True
        if self.bare_path and stripped_path.startswith(self.bare_path):
            return True
        if stripped_path in self.aliases:
            return True

    def selected_title(self):
        """returns the title of the button when selected (for cases
        when it is different from self.title)"""
        return self.title

    def cachable_attrs(self):
        return [
            ('selected', self.selected),
            ('title', self.title),
            ('path', self.path),
            ('sr_path', self.sr_path),
            ('target', self.target),
            ('css_class', self.css_class),
            ('_id', self._id),
            ('data', self.data),
        ]


class QueryButton(NavButton):
    def __init__(self, title, dest, query_param, sr_path=True, aliases=None,
                 target="", css_class='', data=None):
        self.query_param = query_param
        NavButton.__init__(self, title, dest, sr_path=sr_path,
                           aliases=aliases, target=target, use_params=False,
                           css_class=css_class, data=data)

    def build(self, base_path=''):
        params = dict(request.GET)
        if self.dest:
            params[self.query_param] = self.dest
        elif self.query_param in params:
            del params[self.query_param]

        self.base_path = base_path
        base_path += query_string(params)
        self.path = base_path.replace('//', '/')

    def is_selected(self):
        if not self.dest and self.query_param not in dict(request.GET):
            return True
        return dict(request.GET).get(self.query_param, '') in self.aliases


class PostButton(NavButton):
    _style = "post"

    def __init__(self, title, dest, input_name, sr_path=True, aliases=None,
                 target="", css_class='', data=None):
        self.input_name = input_name
        NavButton.__init__(self, title, dest, sr_path=sr_path,
                           aliases=aliases, target=target, use_params=False,
                           css_class=css_class, data=data)

    def build(self, base_path=''):
        self.base_path = base_path
        self.action_params = {self.input_name: self.dest}

    def cachable_attrs(self):
        return [
            ('selected', self.selected),
            ('title', self.title),
            ('base_path', self.base_path),
            ('action_params', self.action_params),
            ('sr_path', self.sr_path),
            ('target', self.target),
            ('css_class', self.css_class),
            ('_id', self._id),
            ('data', self.data),
        ]

    def is_selected(self):
        return False


class ModeratorMailButton(NavButton):
    def is_selected(self):
        if c.default_sr and not self.sr_path:
            return NavButton.is_selected(self)
        elif not c.default_sr and self.sr_path:
            return NavButton.is_selected(self)


class OffsiteButton(NavButton):
    def build(self, base_path=''):
        self.sr_path = False
        self.path = self.bare_path = self.dest

    def cachable_attrs(self):
        return [
            ('path', self.path),
            ('title', self.title),
            ('css_class', self.css_class),
        ]


class SubredditButton(NavButton):
    from r2.models.subreddit import Frontpage, Mod, All, Random, RandomSubscription
    # TRANSLATORS: This refers to /r/mod
    name_overrides = {Mod: N_("mod"),
    # TRANSLATORS: This refers to the user's front page
                      Frontpage: N_("front"),
                      All: N_("all"),
                      Random: N_("random"),
    # TRANSLATORS: Gold feature, "myrandom", a random subreddit from your subscriptions
                      RandomSubscription: N_("myrandom")}

    def __init__(self, sr, css_class='', data=None):
        self.path = sr.path
        name = self.name_overrides.get(sr)
        name = _(name) if name else sr.name
        self.isselected = (c.site == sr)
        NavButton.__init__(self, name, sr.path, sr_path=False,
                           css_class=css_class, data=data)

    def build(self, base_path=''):
        self.bare_path = ""

    def is_selected(self):
        return self.isselected

    def cachable_attrs(self):
        return [
            ('path', self.path),
            ('title', self.title),
            ('isselected', self.isselected),
            ('css_class', self.css_class),
            ('data', self.data),
        ]


class NamedButton(NavButton):
    """Convenience class for handling the majority of NavButtons
    whereby the 'title' is just the translation of 'name' and the
    'dest' defaults to the 'name' as well (unless specified
    separately)."""

    def __init__(self, name, sr_path=True, aliases=None,
                 dest=None, fmt_args={}, use_params=False, css_class='',
                 data=None):
        self.name = name.strip('/')
        menutext = menu[self.name] % fmt_args
        dest = dest if dest is not None else name
        NavButton.__init__(self, menutext, dest, sr_path=sr_path,
                           aliases=aliases,
                           use_params=use_params, css_class=css_class,
                           data=data)


class JsButton(NavButton):
    """A button which fires a JS event and thus has no path and cannot
    be in the 'selected' state"""

    _style = "js"

    def __init__(self, title, tab_name=None, onclick='', css_class='',
                 data=None):
        self.tab_name = tab_name
        self.onclick = onclick
        dest = '#'
        NavButton.__init__(self, title, dest, sr_path=False,
                           css_class=css_class, data=data)

    def build(self, base_path=''):
        if self.tab_name:
            self.path = '#' + self.tab_name
        else:
            self.path = 'javascript:void(0)'

    def is_selected(self):
        return False

    def cachable_attrs(self):
        return [
            ('title', self.title),
            ('path', self.path),
            ('target', self.target),
            ('css_class', self.css_class),
            ('_id', self._id),
            ('tab_name', self.tab_name),
            ('onclick', self.onclick),
            ('data', self.data),
        ]


class PageNameNav(Styled):
    """generates the links and/or labels which live in the header
    between the header image and the first nav menu (e.g., the
    subreddit name, the page name, etc.)"""
    pass


class SortMenu(NavMenu):
    name = 'sort'
    hidden_options = []
    button_cls = QueryButton

    # these are _ prefixed to avoid colliding with NavMenu attributes
    _default = 'hot'
    _options = ('hot', 'new', 'top', 'old', 'controversial')
    _type = 'lightdrop'
    _title = N_("sorted by")

    def __init__(self, default=None, title='', base_path='', separator='|',
                 _id='', css_class=''):
        options = self.make_buttons()
        default = default or self._default
        base_path = base_path or request.path
        title = title or _(self._title)
        NavMenu.__init__(self, options, default=default, title=title,
                         type=self._type, base_path=base_path,
                         separator=separator, _id=_id, css_class=css_class)

    def make_buttons(self):
        buttons = []
        for name in self._options:
            css_class = 'hidden' if name in self.hidden_options else ''
            button = self.button_cls(self.make_title(name), name, self.name,
                                     css_class=css_class)
            buttons.append(button)
        return buttons

    def make_title(self, attr):
        return menu[attr]

    _mapping = {
        "hot": operators.desc('_hot'),
        "new": operators.desc('_date'),
        "old": operators.asc('_date'),
        "top": operators.desc('_score'),
        "controversial": operators.desc('_controversy'),
        "confidence": operators.desc('_confidence'),
        "random": operators.shuffled('_confidence'),
        "qa": operators.desc('_qa'),
    }
    _reverse_mapping = {v: k for k, v in _mapping.iteritems()}

    @classmethod
    def operator(cls, sort):
        return cls._mapping.get(sort)

    @classmethod
    def sort(cls, operator):
        return cls._reverse_mapping.get(operator)


class ProfileSortMenu(SortMenu):
    _default = 'new'
    _options = ('hot', 'new', 'top', 'controversial')


class CommentSortMenu(SortMenu):
    """Sort menu for comments pages"""
    _default = 'confidence'
    _options = ('confidence', 'top', 'new', 'controversial', 'old', 'random',
                'qa',)
    hidden_options = ['random']

    # Links may have a suggested sort of 'blank', which is an explicit None -
    # that is, do not check the subreddit for a suggested sort, either.
    suggested_sort_options = _options + ('blank',)

    def __init__(self, *args, **kwargs):
        self.suggested_sort = kwargs.pop('suggested_sort', None)
        super(CommentSortMenu, self).__init__(*args, **kwargs)

    @classmethod
    def visible_options(cls):
        return set(cls._options) - set(cls.hidden_options)

    def make_title(self, attr):
        title = super(CommentSortMenu, self).make_title(attr)
        if attr == self.suggested_sort:
            return title + ' ' + _('(suggested)')
        else:
            return title


class SearchSortMenu(SortMenu):
    """Sort menu for search pages."""
    _default = 'relevance'
    _options = ('relevance', 'hot', 'top', 'new', 'comments')

    @class_property
    def hidden_options(cls):
        return ['hot']

    def make_buttons(self):
        buttons = super(SearchSortMenu, self).make_buttons()
        if feature.is_enabled('link_relevancy'):
            button = self.button_cls('relevance2', 'relevance2', self.name)
            buttons.append(button)
        return buttons


class SubredditSearchSortMenu(SortMenu):
    """Sort menu for subreddit search pages."""
    _default = 'relevance'
    _options = ('relevance', 'activity')


class RecSortMenu(SortMenu):
    """Sort menu for recommendation page"""
    _default = 'new'
    _options = ('hot', 'new', 'top', 'controversial', 'relevance')


class KindMenu(SortMenu):
    name = 'kind'
    _default = 'all'
    _options = ('links', 'comments', 'messages', 'all')
    _title = N_("kind")

    def make_title(self, attr):
        if attr == "all":
            return _("all")
        return menu[attr]


class TimeMenu(SortMenu):
    """Menu for setting the time interval of the listing (from 'hour' to 'all')"""
    name = 't'
    _default = 'all'
    _options = ('hour', 'day', 'week', 'month', 'year', 'all')
    _title = N_("links from")

    @classmethod
    def operator(self, time):
        from r2.models import Link
        if time != 'all':
            return Link.c._date >= timeago(time)

class CommentsTimeMenu(TimeMenu):
    """Time Menu with the title changed for comments"""
    _title = N_("comments from")


class ProfileOverviewTimeMenu(TimeMenu):
    """Time Menu with the title changed for a user overview"""
    _title = N_("links and comments from")


class ControversyTimeMenu(TimeMenu):
    """time interval for controversial sort.  Make default time 'day' rather than 'all'"""
    _default = 'day'
    button_cls = PostButton


class SubredditMenu(NavMenu):
    def find_selected(self):
        """Always return False so the title is always displayed"""
        return None


class JsNavMenu(NavMenu):
    def find_selected(self):
        """Always return the first element."""
        return self.options[0]

# --------------------
# TODO: move to admin area
class AdminReporterMenu(SortMenu):
    default = 'top'
    options = ('hot', 'new', 'top')

class AdminKindMenu(KindMenu):
    options = ('all', 'links', 'comments', 'spam', 'autobanned')


class AdminTimeMenu(TimeMenu):
    get_param = 't'
    _default = 'day'
    _options = ('hour', 'day', 'week')
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict
from itertools import chain

from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib.sgm import sgm
from r2.lib.utils import tup
from r2.models.comment_tree import CommentTree
from r2.models.link import Comment, Link, CommentScoresByLink

MESSAGE_TREE_SIZE_LIMIT = 15000


def write_comment_scores(link, comments):
    for sort in ("_controversy", "_confidence", "_score", "_qa"):
        scores = calculate_comment_scores(link, sort, comments)
        CommentScoresByLink.set_scores(link, sort, scores)


def add_comments(comments):
    """Add comments to the CommentTree and update scores."""
    from r2.models.builder import write_comment_orders

    link_ids = [comment.link_id for comment in tup(comments)]
    links_by_id = Link._byID(link_ids)

    comments = tup(comments)
    comments_by_link_id = defaultdict(list)
    for comment in comments:
        comments_by_link_id[comment.link_id].append(comment)

    for link_id, link_comments in comments_by_link_id.iteritems():
        link = links_by_id[link_id]

        timer = g.stats.get_timer('comment_tree.add.1')
        timer.start()

        write_comment_scores(link, link_comments)
        timer.intermediate('scores')

        CommentTree.add_comments(link, link_comments)
        timer.intermediate('update')

        write_comment_orders(link)
        timer.intermediate('write_order')

        timer.stop()


def calculate_comment_scores(link, sort, comments):
    if sort in ("_controversy", "_confidence", "_score"):
        scores = {
            comment._id36: getattr(comment, sort)
            for comment in comments
        }
    elif sort == "_qa":
        comment_tree = CommentTree.by_link(link)
        cid_tree = comment_tree.tree
        scores = _calculate_qa_comment_scores(link, cid_tree, comments)
    else:
        raise ValueError("unsupported comment sort %s" % sort)

    return scores


def _calculate_qa_comment_scores(link, cid_tree, comments):
    """Return a dict of comment_id36 -> qa score"""

    # Responder is usually the OP, but there could be support for adding
    # other answerers in the future.
    responder_ids = link.responder_ids

    # An OP response will change the sort value for its parent, so we need
    # to process the parent, too.
    parent_cids = []
    for comment in comments:
        if comment.author_id in responder_ids and comment.parent_id:
            parent_cids.append(comment.parent_id)
    parent_comments = Comment._byID(parent_cids, return_dict=False)
    comments.extend(parent_comments)

    # Fetch the comments in batch to avoid a bunch of separate calls down
    # the line.
    all_child_cids = []
    for comment in comments:
        child_cids = cid_tree.get(comment._id, None)
        if child_cids:
            all_child_cids.extend(child_cids)
    all_child_comments = Comment._byID(all_child_cids)

    comment_sorter = {}
    for comment in comments:
        child_cids = cid_tree.get(comment._id, ())
        child_comments = (all_child_comments[cid] for cid in child_cids)
        sort_value = comment._qa(child_comments, responder_ids)
        comment_sorter[comment._id36] = sort_value

    return comment_sorter


def get_comment_scores(link, sort, comment_ids, timer):
    """Retrieve cached sort values for all comments on a post.

    Arguments:

    * link_id -- id of the Link containing the comments.
    * sort -- a string indicating the attribute on the comments to use for
      generating sort values.

    Returns a dictionary from cid to a numeric sort value.

    """

    from r2.lib.db import queries
    from r2.models import CommentScoresByLink

    if not comment_ids:
        # no comments means no scores
        return {}

    if sort == "_date":
        # comment ids are monotonically increasing, so we can use them as a
        # substitute for creation date
        scores_by_id = {comment_id: comment_id for comment_id in comment_ids}
    else:
        scores_by_id36 = CommentScoresByLink.get_scores(link, sort)

        # we store these id36ed, but there are still bits of the code that
        # want to deal in integer IDs
        scores_by_id = {
            int(id36, 36): score
            for id36, score in scores_by_id36.iteritems()
        }

        scores_needed = set(comment_ids) - set(scores_by_id.keys())
        if scores_needed:
            # some scores were missing from CommentScoresByLink--lookup the
            # comments and calculate the scores.
            g.stats.simple_event('comment_tree_bad_sorter')

            missing = Comment._byID(scores_needed, return_dict=False)
            scores_by_missing_id36 = calculate_comment_scores(
                link, sort, missing)
            scores_by_missing = {
                int(id36, 36): score
                for id36, score in scores_by_missing_id36.iteritems()
            }

            # up to once per minute write the scores to limit writes but
            # eventually return us to the correct state.
            if not g.disallow_db_writes:
                write_key = "lock:score_{link}{sort}".format(
                    link=link._id36,
                    sort=sort,
                )
                should_write = g.lock_cache.add(write_key, "", time=60)
                if should_write:
                    CommentScoresByLink.set_scores(
                        link, sort, scores_by_missing_id36)

            scores_by_id.update(scores_by_missing)
            timer.intermediate('sort')

    return scores_by_id


# message conversation functions
def messages_key(user_id):
    return 'message_conversations_' + str(user_id)

def messages_lock_key(user_id):
    return 'message_conversations_lock_' + str(user_id)

def add_message(message, update_recipient=True, update_modmail=True,
                add_to_user=None):
    with g.make_lock("message_tree", messages_lock_key(message.author_id)):
        add_message_nolock(message.author_id, message)

    if (update_recipient and message.to_id and
            message.to_id != message.author_id):
        with g.make_lock("message_tree", messages_lock_key(message.to_id)):
            add_message_nolock(message.to_id, message)

    if update_modmail and message.sr_id:
        with g.make_lock("modmail_tree", sr_messages_lock_key(message.sr_id)):
            add_sr_message_nolock(message.sr_id, message)

    if add_to_user and add_to_user._id != message.to_id:
        with g.make_lock("message_tree", messages_lock_key(add_to_user._id)):
            add_message_nolock(add_to_user._id, message)

def _add_message_nolock(key, message):
    from r2.models import Account, Message
    trees = g.permacache.get(key)
    if not trees:
        # in case an empty list got written at some point, delete it to
        # force a recompute
        if trees is not None:
            g.permacache.delete(key)
        # no point computing it now.  We'll do it when they go to
        # their message page.
        return

    # if it is a new root message, easy enough
    if message.first_message is None:
        trees.insert(0, (message._id, []))
    else:
        tree_dict = dict(trees)

        # if the tree already has the first message, update the list
        if message.first_message in tree_dict:
            if message._id not in tree_dict[message.first_message]:
                tree_dict[message.first_message].append(message._id)
                tree_dict[message.first_message].sort()
        # we have to regenerate the conversation :/
        else:
            m = Message._query(Message.c.first_message == message.first_message,
                               data = True)
            new_tree = compute_message_trees(m)
            if new_tree:
                trees.append(new_tree[0])
        trees.sort(key = tree_sort_fn, reverse = True)

    # If we have too many messages in the tree, drop the oldest
    # conversation to avoid the permacache size limit
    tree_size = len(trees) + sum(len(convo[1]) for convo in trees)

    if tree_size > MESSAGE_TREE_SIZE_LIMIT:
        del trees[-1]

    # done!
    g.permacache.set(key, trees)


def add_message_nolock(user_id, message):
    return _add_message_nolock(messages_key(user_id), message)

def _conversation(trees, parent):
    from r2.models import Message
    if parent._id in trees:
        convo = trees[parent._id]
        if convo:
            m = Message._byID(convo[0], data = True)
        if not convo or m.first_message == m.parent_id:
            return [(parent._id, convo)]

    # if we get to this point, either we didn't find the conversation,
    # or the first child of the result was not the actual first child.
    # To the database!
    rules = [Message.c.first_message == parent._id]
    if c.user_is_admin:
        rules.append(Message.c._spam == (True, False))
        rules.append(Message.c._deleted == (True, False))
    m = Message._query(*rules, data=True)
    return compute_message_trees([parent] + list(m))

def conversation(user, parent):
    trees = dict(user_messages(user))
    return _conversation(trees, parent)


def user_messages(user, update = False):
    key = messages_key(user._id)
    trees = g.permacache.get(key)
    if not trees or update:
        trees = user_messages_nocache(user)
        g.permacache.set(key, trees)
    return trees


def _load_messages(mlist):
    from r2.models import Message
    m = {}
    ids = [x for x in mlist if not isinstance(x, Message)]
    if ids:
        m = Message._by_fullname(ids, return_dict = True, data = True)
    messages = [m.get(x, x) for x in mlist]
    return messages

def user_messages_nocache(user):
    """
    Just like user_messages, but avoiding the cache
    """
    from r2.lib.db import queries
    inbox = queries.get_inbox_messages(user)
    sent = queries.get_sent(user)
    messages = _load_messages(list(chain(inbox, sent)))
    return compute_message_trees(messages)

def sr_messages_key(sr_id):
    return 'sr_messages_conversation_' + str(sr_id)

def sr_messages_lock_key(sr_id):
    return 'sr_messages_conversation_lock_' + str(sr_id)


def subreddit_messages(sr, update = False):
    key = sr_messages_key(sr._id)
    trees = g.permacache.get(key)
    if not trees or update:
        trees = subreddit_messages_nocache(sr)
        g.permacache.set(key, trees)
    return trees

def moderator_messages(sr_ids):
    from r2.models import Subreddit

    srs = Subreddit._byID(sr_ids)
    sr_ids = [sr_id for sr_id, sr in srs.iteritems()
              if sr.is_moderator_with_perms(c.user, 'mail')]

    def multi_load_tree(sr_ids):
        res = {}
        for sr_id in sr_ids:
            trees = subreddit_messages_nocache(srs[sr_id])
            if trees:
                res[sr_id] = trees
        return res

    res = sgm(g.permacache, sr_ids, miss_fn = multi_load_tree,
              prefix = sr_messages_key(""))

    return sorted(chain(*res.values()), key = tree_sort_fn, reverse = True)

def subreddit_messages_nocache(sr):
    """
    Just like user_messages, but avoiding the cache
    """
    from r2.lib.db import queries
    inbox = queries.get_subreddit_messages(sr)
    messages = _load_messages(inbox)
    return compute_message_trees(messages)


def add_sr_message_nolock(sr_id, message):
    return _add_message_nolock(sr_messages_key(sr_id), message)

def sr_conversation(sr, parent):
    trees = dict(subreddit_messages(sr))
    return _conversation(trees, parent)


def compute_message_trees(messages):
    from r2.models import Message
    roots = set()
    threads = {}
    mdict = {}
    messages = sorted(messages, key = lambda m: m._date, reverse = True)

    for m in messages:
        mdict[m._id] = m
        if m.first_message:
            roots.add(m.first_message)
            threads.setdefault(m.first_message, set()).add(m._id)
        else:
            roots.add(m._id)

    # load any top-level messages which are not in the original list
    missing = [m for m in roots if m not in mdict]
    if missing:
        mdict.update(Message._byID(tup(missing),
                                   return_dict = True, data = True))

    # sort threads in chrono order
    for k in threads:
        threads[k] = list(sorted(threads[k]))

    tree = [(root, threads.get(root, [])) for root in roots]
    tree.sort(key = tree_sort_fn, reverse = True)

    return tree

def tree_sort_fn(tree):
    root, threads = tree
    return threads[-1] if threads else root

def _populate(after_id = None, estimate=54301242):
    from r2.models import desc
    from r2.lib.db import tdb_cassandra
    from r2.lib import utils

    # larger has a chance to decrease the number of Cassandra writes,
    # but the probability is low
    chunk_size = 5000

    q = Comment._query(Comment.c._spam==(True,False),
                       Comment.c._deleted==(True,False),
                       sort=desc('_date'))

    if after_id is not None:
        q._after(Comment._byID(after_id))

    q = utils.fetch_things2(q, chunk_size=chunk_size)
    q = utils.progress(q, verbosity=chunk_size, estimate = estimate)

    for chunk in utils.in_chunks(q, chunk_size):
        chunk = filter(lambda x: hasattr(x, 'link_id'), chunk)
        add_comments(chunk)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request, session, config, response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.controllers import WSGIController
from pylons.i18n import N_, _, ungettext, get_lang
from webob.exc import HTTPException, status_map
from r2.lib.filters import spaceCompress, _force_unicode
from r2.lib.template_helpers import get_domain
from r2.lib.utils import Agent
from utils import string2js, read_http_date

import re, hashlib
from Cookie import CookieError
from urllib import quote
import urllib2
import sys


#TODO hack
import logging
from r2.lib.utils import UrlParser, query_string
logging.getLogger('scgi-wsgi').setLevel(logging.CRITICAL)


def is_local_address(ip):
    # TODO: support the /20 and /24 private networks? make this configurable?
    return ip.startswith('10.') or ip == "127.0.0.1"

def abort(code_or_exception=None, detail="", headers=None, comment=None,
          **kwargs):
    """Raise an HTTPException and save it in environ for use by error pages."""
    # Pylons 0.9.6 makes it really hard to get your raised HTTPException,
    # so this helper implements it manually using a familiar syntax.
    # FIXME: when we upgrade Pylons, we can replace this with raise
    #        and access environ['pylons.controller.exception']
    # NOTE: when we say "upgrade Pylons" we mean to 0.10+
    if isinstance(code_or_exception, HTTPException):
        exc = code_or_exception
    else:
        if type(code_or_exception) is type and issubclass(code_or_exception,
                                                          HTTPException):
            exc_cls = code_or_exception
        else:
            exc_cls = status_map[code_or_exception]
        exc = exc_cls(detail, headers, comment, **kwargs)
    request.environ['r2.controller.exception'] = exc
    raise exc

class BaseController(WSGIController):
    def __before__(self):
        """Perform setup tasks before the controller method/action is executed.

        Called by WSGIController.__call__.

        """

        # we override this here to ensure that this header, and only this
        # header, is trusted to reduce the number of potential
        # misconfigurations between wsgi application servers (e.g. gunicorn
        # which trusts three different headers out of the box for this) and
        # haproxy (which won't clean out bad headers by default)
        forwarded_proto = request.environ.get("HTTP_X_FORWARDED_PROTO", "http")
        forwarded_proto = forwarded_proto.lower()
        assert forwarded_proto in ("http", "https")
        request.environ["wsgi.url_scheme"] = forwarded_proto

        forwarded_for = request.environ.get('HTTP_X_FORWARDED_FOR', ())
        remote_addr = request.environ.get('REMOTE_ADDR')

        request.via_cdn = False
        cdn_ip = g.cdn_provider.get_client_ip(request.environ)
        if cdn_ip:
            request.ip = cdn_ip
            request.via_cdn = True
        elif (g.trust_local_proxies and
                forwarded_for and
                is_local_address(remote_addr)):
            request.ip = forwarded_for.split(',')[-1]
        else:
            request.ip = request.environ['REMOTE_ADDR']

        try:
            # webob can't handle non utf-8 encoded query strings or paths
            request.params
            request.path
        except UnicodeDecodeError:
            abort(400)

        #if x-dont-decode is set, pylons won't unicode all the parameters
        if request.environ.get('HTTP_X_DONT_DECODE'):
            request.charset = None

        request.referer = request.environ.get('HTTP_REFERER')
        request.user_agent = request.environ.get('HTTP_USER_AGENT')
        request.parsed_agent = Agent.parse(request.user_agent)
        request.fullpath = request.environ.get('FULLPATH', request.path)
        request.fullurl = request.host_url + request.fullpath
        request.port = request.environ.get('request_port')

        if_modified_since = request.environ.get('HTTP_IF_MODIFIED_SINCE')
        if if_modified_since:
            request.if_modified_since = read_http_date(if_modified_since)
        else:
            request.if_modified_since = None

        self.fix_cookie_header()
        self.pre()

    def __after__(self):
        self.post()

    def __call__(self, environ, start_response):
        # as defined by routing rules in in routing.py, a request to
        # /api/do_something is routed to the ApiController's do_something()
        # method (action). Rewrite this to include the HTTP verb which is the
        # real name of the controller method: GET_do_something().
        action = request.environ['pylons.routes_dict'].get('action')
        if action:
            meth = request.method.upper()
            if meth == 'HEAD':
                meth = 'GET'

            if (meth == 'OPTIONS' and
                    self._get_action_handler(action, meth) is None):
                handler_name = meth
            else:
                handler_name = meth + '_' + action

            request.environ['pylons.routes_dict']['action_name'] = action
            request.environ['pylons.routes_dict']['action'] = handler_name

        # WSGIController.__call__ will run __before__() and then execute the
        # controller method via environ['pylons.routes_dict']['action']
        return WSGIController.__call__(self, environ, start_response)

    def pre(self): pass
    def post(self): pass

    def fix_cookie_header(self):
        """
        Detect and drop busted `Cookie` headers

        We get all sorts of invalid `Cookie` headers. Just one example:

            Cookie: fo,o=bar; expires=1;

        Normally you'd do this in middleware, but `webob.cookie`'s API
        is fairly volatile while `webob.request`'s isn't. It's easier to
        do this once we've got a valid `Request` object.
        """
        try:
            # Just accessing this will cause `webob` to attempt a parse,
            # telling us if the header's broken.
            request.cookies
        except (CookieError, KeyError):
            # Someone sent a janked up cookie header, and `webob` exploded.
            # just pretend we didn't receive one at all.
            cookie_val = request.environ.get('HTTP_COOKIE', '')
            request.environ['HTTP_COOKIE'] = ''
            g.log.warning("Cleared bad cookie header: %r" % cookie_val)
            g.stats.simple_event("cookie.bad_cookie_header")

    def _get_action_handler(self, name=None, method=None):
        name = name or request.environ["pylons.routes_dict"]["action_name"]
        method = method or request.method
        action = method + "_" + name
        return getattr(self, action, None)

    @classmethod
    def format_output_url(cls, url, **kw):
        """
        Helper method used during redirect to ensure that the redirect
        url (assisted by frame busting code or javasctipt) will point
        to the correct domain and not have any extra dangling get
        parameters.  The extensions are also made to match and the
        resulting url is utf8 encoded.

        Node: for development purposes, also checks that the port
        matches the request port
        """
        preserve_extension = kw.pop("preserve_extension", True)
        u = UrlParser(url)

        if u.is_reddit_url():
            # make sure to pass the port along if not 80
            if not kw.has_key('port'):
                kw['port'] = request.port

            # make sure the extensions agree with the current page
            if preserve_extension and c.extension:
                u.set_extension(c.extension)

        # unparse and encode it un utf8
        rv = _force_unicode(u.unparse()).encode('utf8')
        if "\n" in rv or "\r" in rv:
            abort(400)
        return rv

    @classmethod
    def intermediate_redirect(cls, form_path, sr_path=True, fullpath=None):
        """
        Generates a /login or /over18 redirect from the specified or current
        fullpath, after having properly reformated the path via
        format_output_url.  The reformatted original url is encoded
        and added as the "dest" parameter of the new url.
        """
        from r2.lib.template_helpers import add_sr
        params = dict(dest=cls.format_output_url(fullpath or request.fullurl))
        if c.extension == "widget" and request.GET.get("callback"):
            params['callback'] = request.GET.get("callback")

        path = add_sr(cls.format_output_url(form_path) +
                      query_string(params), sr_path=sr_path)
        abort(302, location=path)

    @classmethod
    def redirect(cls, dest, code=302, preserve_extension=True):
        """
        Reformats the new Location (dest) using format_output_url and
        sends the user to that location with the provided HTTP code.
        """
        dest = cls.format_output_url(dest or "/",
                                     preserve_extension=preserve_extension)
        response.status_int = code
        response.headers['Location'] = dest


class EmbedHandler(urllib2.BaseHandler, urllib2.HTTPHandler,
                   urllib2.HTTPErrorProcessor, urllib2.HTTPDefaultErrorHandler):

    def http_redirect(self, req, fp, code, msg, hdrs):
        to = hdrs['Location']
        h = urllib2.HTTPRedirectHandler()
        r = h.redirect_request(req, fp, code, msg, hdrs, to)
        return embedopen.open(r)

    http_error_301 = http_redirect
    http_error_302 = http_redirect
    http_error_303 = http_redirect
    http_error_307 = http_redirect

embedopen = urllib2.OpenerDirector()
embedopen.add_handler(EmbedHandler())

def proxyurl(url):
    r = urllib2.Request(url, None, {})
    content = embedopen.open(r).read()
    return content
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import re

from pylons import app_globals as g

from r2.models.keyvalue import NamedGlobals
from r2.models import NotFound, Subreddit, Thing

_SUBREDDIT_RE = re.compile(r'/r/(\w+)')
TRENDING_SUBREDDITS_KEY = 'trending_subreddits'


def get_trending_subreddits():
    return NamedGlobals.get(TRENDING_SUBREDDITS_KEY, None)


def update_trending_subreddits():
    try:
        trending_sr = Subreddit._by_name(g.config['trending_sr'])
    except NotFound:
        g.log.info("Unknown trending subreddit %r or trending_sr config "
                   "not set. Not updating.", g.config['trending_sr'])
        return

    link = _get_newest_link(trending_sr)
    if not link:
        g.log.info("Unable to find active link in subreddit %r. Not updating.",
                   g.config['trending_sr'])
        return

    subreddit_names = _SUBREDDIT_RE.findall(link.title)
    trending_data = {
        'subreddit_names': subreddit_names,
        'permalink': link.make_permalink(trending_sr),
        'link_id': link._id,
    }
    NamedGlobals.set(TRENDING_SUBREDDITS_KEY, trending_data)
    g.log.debug("Trending subreddit data set to %r", trending_data)


def _get_newest_link(sr):
    for fullname in sr.get_links('new', 'all'):
        link = Thing._by_fullname(fullname, data=True)
        if not link._spam and not link._deleted:
            return link

    return None
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import requests

from pylons import app_globals as g

def post_takedown_notice_to_external_site(title, 
                          request_type, 
                          date_sent,
                          date_received,
                          source,
                          action_taken,
                          public_description,
                          kind,
                          original_url,
                          infringing_urls,
                          submitter_attributes,
                          sender_name,
                          sender_kind,
                          sender_country,
                          ):
    """This method publicly posts a copy of the takedown notice to 
    https://lumendatabase.org. Posting notices to Lumen is free, and needs to
    be arranged by contacting their team. Read more about Lumen at
    https://www.lumendatabase.org/pages/about
    """
    # API documentation for lumendatabase.org found here:
    # https://github.com/berkmancenter/lumendatabase/blob/master/doc/api_documentation.mkd
    notice_json = {
        'authentication_token': g.secrets['lumendatabase_org_api_key'],
        'notice': {
            'title': title,
            'type': request_type,
            'date_sent': date_sent.strftime('%Y-%m-%d'),
            'date_received': date_received.strftime('%Y-%m-%d'),
            'source': source,
            'jurisdiction_list': 'US, CA',
            'action_taken': action_taken,
            'works_attributes': [
                {
                    'description': public_description,
                    'kind': kind,
                    'copyrighted_urls_attributes': [
                        { 'url': original_url },
                    ],
                    'infringing_urls_attributes': infringing_urls
                }
            ],
            'entity_notice_roles_attributes': [
                {
                    'name': 'recipient',
                    'entity_attributes': submitter_attributes,
                },
                {
                    'name': 'sender',
                    'entity_attributes': {
                        'name': sender_name,
                        'kind': sender_kind,
                        'address_line_1': '',
                        'city': '',
                        'state': '', 
                        'zip': '',
                        'country_code': sender_country,
                    }
                }
            ]
        }
    }
        
    timer = g.stats.get_timer('lumendatabase.takedown_create')
    timer.start()
    response = requests.post(
        '%snotices' % g.live_config['lumendatabase_org_api_base_url'],
        headers={
            'Content-type': 'application/json',
            'Accept': 'application/json',
        },
        data=json.dumps(notice_json)
    )
    timer.stop()
    return response.headers['location']
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


import snappy
import struct


class HadoopStreamDecompressor(object):
    """This class implements the decompressor-side of the hadoop framing
    format.

    Hadoop fraiming format consists of one or more blocks, each of which is
    composed of one or more compressed subblocks. The block size is the
    uncompressed size, while the subblock size is the size of the compressed
    data.

    https://github.com/andrix/python-snappy/pull/35/files
    """

    __slots__ = ["_buf", "_block_size", "_block_read", "_subblock_size"]

    def __init__(self):
        self._buf = b""
        self._block_size = None
        self._block_read = 0
        self._subblock_size = None

    def decompress(self, data):
        self._buf += data
        output = b""
        while True:
            # decompress block will attempt to decompress  any subblocks if it
            # has already read the block size and subblock size.
            buf = self._decompress_block()
            if len(buf) > 0:
                output += buf
            else:
                break
        return output

    def _decompress_block(self):
        if self._block_size is None:
            if len(self._buf) <= 4:
                return b""
            self._block_size = struct.unpack(">i", self._buf[:4])[0]
            self._buf = self._buf[4:]
        output = b""
        while self._block_read < self._block_size:
            buf = self._decompress_subblock()
            if len(buf) > 0:
                output += buf
            else:
                # Buffer doesn't contain full subblock
                break
        if self._block_read == self._block_size:
            # We finished reading this block, so reinitialize.
            self._block_read = 0
            self._block_size = None
        return output

    def _decompress_subblock(self):
        if self._subblock_size is None:
            if len(self._buf) <= 4:
                return b""
            self._subblock_size = struct.unpack(">i", self._buf[:4])[0]
            self._buf = self._buf[4:]
        # Only attempt to decompress complete subblocks.
        if len(self._buf) < self._subblock_size:
            return b""
        compressed = self._buf[:self._subblock_size]
        self._buf = self._buf[self._subblock_size:]
        uncompressed = snappy.uncompress(compressed)
        self._block_read += len(uncompressed)
        self._subblock_size = None
        return uncompressed

    def flush(self):
        if self._buf != b"":
            raise snappy.UncompressError("chunk truncated")
        return b""

    def copy(self):
        copy = HadoopStreamDecompressor()
        copy._buf = self._buf
        copy._block_size = self._block_size
        copy._block_read = self._block_read
        copy._subblock_size = self._subblock_size
        return copy


def hadoop_decompress(src, dst, blocksize=snappy._STREAM_TO_STREAM_BLOCK_SIZE):
    decompressor = HadoopStreamDecompressor()
    while True:
        buf = src.read(blocksize)
        if not buf:
            break
        buf = decompressor.decompress(buf)
        if buf:
            dst.write(buf)
    decompressor.flush()  # makes sure the stream ended well
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import re
import hashlib
from PIL import Image
import subprocess

from r2.lib.static import generate_static_name

SPRITE_PADDING = 6
sprite_line = re.compile(
    r"""background-image:\ *
        url\((?P<filename>.*)\)\ *
        .*
        /\*\ *SPRITE\ *
        (?P<stretch>stretch-x)?\ *
        (?:pixel-ratio=(?P<pixel_ratio>\d))?\ *
        \*/
     """, re.X)


def optimize_png(filename):
    with open(os.path.devnull, 'w') as devnull:
        subprocess.check_call(("/usr/bin/optipng", filename), stdout=devnull)


def _extract_css_info(match):
    image_filename = match.group("filename")
    should_stretch_property = match.group("stretch")
    pixel_ratio_property = match.group("pixel_ratio")
    image_filename = image_filename.strip('"\'')
    should_stretch = (should_stretch_property == 'stretch-x')
    if pixel_ratio_property:
        pixel_ratio = int(pixel_ratio_property)
    else:
        pixel_ratio = 1
    return image_filename, should_stretch, pixel_ratio


class SpritableImage(object):
    def __init__(self, image, should_stretch=False):
        self.image = image
        self.stretch = should_stretch
        self.filenames = []

    @property
    def width(self):
        return self.image.size[0]

    @property
    def height(self):
        return self.image.size[1]

    def stretch_to_width(self, width):
        self.image = self.image.resize((width, self.height))


class SpriteBin(object):
    def __init__(self, bounding_box):
        # the bounding box is a tuple of
        # top-left-x, top-left-y, bottom-right-x, bottom-right-y
        self.bounding_box = bounding_box
        self.offset = 0
        self.height = bounding_box[3] - bounding_box[1]

    def has_space_for(self, image):
        return (self.offset + image.width <= self.bounding_box[2] and
                self.height >= image.height)

    def add_image(self, image):
        image.sprite_location = (self.offset, self.bounding_box[1])
        self.offset += image.width + SPRITE_PADDING


def _load_spritable_images(css_filename):
    css_location = os.path.dirname(os.path.abspath(css_filename))

    images = {}
    with open(css_filename, 'r') as f:
        for line in f:
            m = sprite_line.search(line)
            if not m:
                continue

            image_filename, should_stretch, pixel_ratio = _extract_css_info(m)
            image = Image.open(os.path.join(css_location, image_filename))
            image_hash = hashlib.md5(image.convert("RGBA").tostring()).hexdigest()

            if image_hash not in images:
                images[image_hash] = SpritableImage(image, should_stretch)
            else:
                assert images[image_hash].stretch == should_stretch
            images[image_hash].filenames.append(image_filename)

    # Sort images by filename to group the layout by names when possible.
    return sorted(images.values(), key=lambda i: i.filenames[0])


def _generate_sprite(images, sprite_path):
    sprite_width = max(i.width for i in images)
    sprite_height = 0

    # put all the max-width and stretch-x images together at the top
    small_images = []
    for image in images:
        if image.width == sprite_width or image.stretch:
            if image.stretch:
                image.stretch_to_width(sprite_width)
            image.sprite_location = (0, sprite_height)
            sprite_height += image.height + SPRITE_PADDING
        else:
            small_images.append(image)

    # lay out the remaining images -- done with a greedy algorithm
    small_images.sort(key=lambda i: i.height, reverse=True)
    bins = []

    for image in small_images:
        # find a bin to fit in
        for bin in bins:
            if bin.has_space_for(image):
                break
        else:
            # or give up and create a new bin
            bin = SpriteBin((0, sprite_height, sprite_width, sprite_height + image.height))
            sprite_height += image.height + SPRITE_PADDING
            bins.append(bin)

        bin.add_image(image)

    # generate the image
    sprite_dimensions = (sprite_width, sprite_height)
    background_color = (255, 69, 0, 0)  # transparent orangered
    sprite = Image.new('RGBA', sprite_dimensions, background_color)

    for image in images:
        sprite.paste(image.image, image.sprite_location)

    sprite.save(sprite_path, optimize=True)
    optimize_png(sprite_path)

    # give back the sprite and mangled name
    sprite_base, sprite_name = os.path.split(sprite_path)
    return (sprite, generate_static_name(sprite_name, base=sprite_base))


def _rewrite_css(css_filename, sprite_path, images, sprite_size):
    # map filenames to coordinates
    locations = {}
    for image in images:
        for filename in image.filenames:
            locations[filename] = image.sprite_location

    def rewrite_sprite_reference(match):
        image_filename, should_stretch, pixel_ratio = _extract_css_info(match)
        position = locations[image_filename]

        if pixel_ratio > 1:
            position = (position[0] / pixel_ratio, position[1] / pixel_ratio)

        css_properties = [
            'background-image: url(%s);' % sprite_path,
            'background-position: -%dpx -%dpx;' % position,
            'background-repeat: %s;' % ('repeat' if should_stretch else 'no-repeat'),
        ]

        if pixel_ratio > 1:
            size = (sprite_size[0] / pixel_ratio, sprite_size[1] / pixel_ratio)
            css_properties.append(
                'background-size: %dpx %dpx;' % size,
            )

        return ''.join(css_properties)

    # read in the css and replace sprite references
    with open(css_filename, 'r') as f:
        css = f.read()
    return sprite_line.sub(rewrite_sprite_reference, css)


def spritify(css_filename, sprite_path):
    images = _load_spritable_images(css_filename)
    sprite, sprite_path = _generate_sprite(images, sprite_path)
    return _rewrite_css(css_filename, sprite_path, images, sprite.size)


if __name__ == '__main__':
    import sys
    print spritify(sys.argv[1], sys.argv[2])
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict
from datetime import datetime
import json

from pylons import tmpl_context as c, app_globals as g, request

from r2.lib import amqp, hooks
from r2.lib.eventcollector import Event
from r2.lib.utils import epoch_timestamp, is_subdomain, UrlParser
from r2.models import Account, Comment, Link, Subreddit
from r2.models.last_modified import LastModified
from r2.models.query_cache import CachedQueryMutator
from r2.models.vote import Vote, VotesByAccount

from r2.lib.geoip import organization_by_ips

def prequeued_vote_key(user, item):
    return 'queuedvote:%s_%s' % (user._id36, item._fullname)


def update_vote_lookups(user, thing, direction):
    """Store info about the existence of this vote (before processing)."""
    # set the vote in memcached so the UI gets updated immediately
    key = prequeued_vote_key(user, thing)
    grace_period = int(g.vote_queue_grace_period.total_seconds())
    direction = Vote.serialize_direction(direction)
    g.gencache.set(key, direction, time=grace_period+1)

    # update LastModified immediately to help us cull prequeued_vote lookups
    rel_cls = VotesByAccount.rel(thing.__class__)
    LastModified.touch(user._fullname, rel_cls._last_modified_name)


def cast_vote(user, thing, direction, **data):
    """Register a vote and queue it for processing."""
    if not isinstance(thing, (Link, Comment)):
        return

    update_vote_lookups(user, thing, direction)

    vote_data = {
        "user_id": user._id,
        "thing_fullname": thing._fullname,
        "direction": direction,
        "date": int(epoch_timestamp(datetime.now(g.tz))),
    }

    data['ip'] = getattr(request, "ip", None)
    if data['ip'] is not None:
        data['org'] = organization_by_ips(data['ip'])
    vote_data['data'] = data

    hooks.get_hook("vote.get_vote_data").call(
        data=vote_data["data"],
        user=user,
        thing=thing,
        request=request,
        context=c,
    )

    # The vote event will actually be sent from an async queue processor, so
    # we need to pull out the context data at this point
    if not g.running_as_script:
        vote_data["event_data"] = {
            "context": Event.get_context_data(request, c),
            "sensitive": Event.get_sensitive_context_data(request, c),
        }

    try:
        vote_dump = json.dumps(vote_data)
    except UnicodeDecodeError:
        g.log.error("Got weird unicode in the vote data: %r", vote_data)
        return

    if isinstance(thing, Link):
        queue = "vote_link_q"
    elif isinstance(thing, Comment):
        queue = "vote_comment_q"

    amqp.add_item(queue, vote_dump)


def update_user_liked(vote):
    from r2.lib.db.queries import get_disliked, get_liked

    with CachedQueryMutator() as m:
        # if this is a changed vote, remove from the previous cached
        # query
        if vote.previous_vote:
            if vote.previous_vote.is_upvote:
                m.delete(get_liked(vote.user), [vote.previous_vote])
            elif vote.previous_vote.is_downvote:
                m.delete(get_disliked(vote.user), [vote.previous_vote])

        # and then add to the new cached query
        if vote.is_upvote:
            m.insert(get_liked(vote.user), [vote])
        elif vote.is_downvote:
            m.insert(get_disliked(vote.user), [vote])


def consume_link_vote_queue(qname="vote_link_q"):
    @g.stats.amqp_processor(qname)
    def process_message(msg):
        vote_data = json.loads(msg.body)
        hook = hooks.get_hook('vote.validate_vote_data')
        if hook.call_until_return(msg=msg, vote_data=vote_data) is False:
            # Corrupt records in the queue. Ignore them.
            print "Ignoring invalid vote by %s on %s %s" % (
                    vote_data.get('user_id', '<unknown>'),
                    vote_data.get('thing_fullname', '<unknown>'),
                    vote_data)
            return

        timer = g.stats.get_timer("link_vote_processor")
        timer.start()

        user = Account._byID(vote_data.pop("user_id"))
        link = Link._by_fullname(vote_data.pop("thing_fullname"))

        # create the vote and update the voter's liked/disliked under lock so
        # that the vote state and cached query are consistent
        lock_key = "vote-%s-%s" % (user._id36, link._fullname)
        with g.make_lock("voting", lock_key, timeout=5):
            print "Processing vote by %s on %s %s" % (user, link, vote_data)

            try:
                vote = Vote(
                    user,
                    link,
                    direction=vote_data["direction"],
                    date=datetime.utcfromtimestamp(vote_data["date"]),
                    data=vote_data["data"],
                    event_data=vote_data.get("event_data"),
                )
            except TypeError as e:
                # a vote on an invalid type got in the queue, just skip it
                g.log.exception("Invalid type: %r", e.message)
                return

            vote.commit()
            timer.intermediate("create_vote_object")

            update_user_liked(vote)
            timer.intermediate("voter_likes")

        vote_valid = vote.is_automatic_initial_vote or vote.effects.affects_score
        link_valid = not (link._spam or link._deleted)
        if vote_valid and link_valid:
            add_to_author_query_q(link)
            add_to_subreddit_query_q(link)
            add_to_domain_query_q(link)

        timer.stop()
        timer.flush()

    amqp.consume_items(qname, process_message, verbose=False)


# these sorts can be changed by voting - we don't need to do "new" since that's
# taken care of by new_link and doesn't change afterwards
SORTS = ["hot", "top", "controversial"]


def add_to_author_query_q(link):
    if g.shard_author_query_queues:
        author_shard = link.author_id % 10
        queue_name = "author_query_%s_q" % author_shard
    else:
        queue_name = "author_query_q"
    amqp.add_item(queue_name, link._fullname)


def consume_author_query_queue(qname="author_query_q", limit=1000):
    @g.stats.amqp_processor(qname)
    def process_message(msgs, chan):
        """Update get_submitted(), the Links by author precomputed query.

        get_submitted() is a CachedResult which is stored in permacache. To
        update these objects we need to do a read-modify-write which requires
        obtaining a lock. Sharding these updates by author allows us to run
        multiple consumers (but ideally just one per shard) to avoid lock
        contention.

        """

        from r2.lib.db.queries import add_queries, get_submitted

        link_names = {msg.body for msg in msgs}
        links = Link._by_fullname(link_names, return_dict=False)
        print 'Processing %r' % (links,)

        links_by_author_id = defaultdict(list)
        for link in links:
            links_by_author_id[link.author_id].append(link)

        authors_by_id = Account._byID(links_by_author_id.keys())

        for author_id, links in links_by_author_id.iteritems():
            with g.stats.get_timer("link_vote_processor.author_queries"):
                author = authors_by_id[author_id]
                add_queries(
                    queries=[
                        get_submitted(author, sort, 'all') for sort in SORTS],
                    insert_items=links,
                )

    amqp.handle_items(qname, process_message, limit=limit)


def add_to_subreddit_query_q(link):
    if g.shard_subreddit_query_queues:
        subreddit_shard = link.sr_id % 10
        queue_name = "subreddit_query_%s_q" % subreddit_shard
    else:
        queue_name = "subreddit_query_q"
    amqp.add_item(queue_name, link._fullname)


def consume_subreddit_query_queue(qname="subreddit_query_q", limit=1000):
    @g.stats.amqp_processor(qname)
    def process_message(msgs, chan):
        """Update get_links(), the Links by Subreddit precomputed query.

        get_links() is a CachedResult which is stored in permacache. To
        update these objects we need to do a read-modify-write which requires
        obtaining a lock. Sharding these updates by subreddit allows us to run
        multiple consumers (but ideally just one per shard) to avoid lock
        contention.

        """

        from r2.lib.db.queries import add_queries, get_links

        link_names = {msg.body for msg in msgs}
        links = Link._by_fullname(link_names, return_dict=False)
        print 'Processing %r' % (links,)

        links_by_sr_id = defaultdict(list)
        for link in links:
            links_by_sr_id[link.sr_id].append(link)

        srs_by_id = Subreddit._byID(links_by_sr_id.keys(), stale=True)

        for sr_id, links in links_by_sr_id.iteritems():
            with g.stats.get_timer("link_vote_processor.subreddit_queries"):
                sr = srs_by_id[sr_id]
                add_queries(
                    queries=[get_links(sr, sort, "all") for sort in SORTS],
                    insert_items=links,
                )

    amqp.handle_items(qname, process_message, limit=limit)


def add_to_domain_query_q(link):
    parsed = UrlParser(link.url)
    if not parsed.domain_permutations():
        # no valid domains found
        return

    if g.shard_domain_query_queues:
        domain_shard = hash(parsed.hostname) % 10
        queue_name = "domain_query_%s_q" % domain_shard
    else:
        queue_name = "domain_query_q"
    amqp.add_item(queue_name, link._fullname)


def consume_domain_query_queue(qname="domain_query_q", limit=1000):
    @g.stats.amqp_processor(qname)
    def process_message(msgs, chan):
        """Update get_domain_links(), the Links by domain precomputed query.

        get_domain_links() is a CachedResult which is stored in permacache. To
        update these objects we need to do a read-modify-write which requires
        obtaining a lock. Sharding these updates by domain allows us to run
        multiple consumers (but ideally just one per shard) to avoid lock
        contention.

        """

        from r2.lib.db.queries import add_queries, get_domain_links

        link_names = {msg.body for msg in msgs}
        links = Link._by_fullname(link_names, return_dict=False)
        print 'Processing %r' % (links,)

        links_by_domain = defaultdict(list)
        for link in links:
            parsed = UrlParser(link.url)

            # update the listings for all permutations of the link's domain
            for domain in parsed.domain_permutations():
                links_by_domain[domain].append(link)

        for d, links in links_by_domain.iteritems():
            with g.stats.get_timer("link_vote_processor.domain_queries"):
                add_queries(
                    queries=[
                        get_domain_links(d, sort, "all") for sort in SORTS],
                    insert_items=links,
                )

    amqp.handle_items(qname, process_message, limit=limit)


def consume_comment_vote_queue(qname="vote_comment_q"):
    @g.stats.amqp_processor(qname)
    def process_message(msg):
        from r2.lib.comment_tree import write_comment_scores
        from r2.lib.db.queries import (
            add_queries,
            add_to_commentstree_q,
            get_comments,
        )
        from r2.models.builder import get_active_sort_orders_for_link

        vote_data = json.loads(msg.body)
        hook = hooks.get_hook('vote.validate_vote_data')
        if hook.call_until_return(msg=msg, vote_data=vote_data) is False:
            # Corrupt records in the queue. Ignore them.
            print "Ignoring invalid vote by %s on %s %s" % (
                    vote_data.get('user_id', '<unknown>'),
                    vote_data.get('thing_fullname', '<unknown>'),
                    vote_data)
            return

        timer = g.stats.get_timer("comment_vote_processor")
        timer.start()

        user = Account._byID(vote_data.pop("user_id"))
        comment = Comment._by_fullname(vote_data.pop("thing_fullname"))

        print "Processing vote by %s on %s %s" % (user, comment, vote_data)

        try:
            vote = Vote(
                user,
                comment,
                direction=vote_data["direction"],
                date=datetime.utcfromtimestamp(vote_data["date"]),
                data=vote_data["data"],
                event_data=vote_data.get("event_data"),
            )
        except TypeError as e:
            # a vote on an invalid type got in the queue, just skip it
            g.log.exception("Invalid type: %r", e.message)
            return

        vote.commit()
        timer.intermediate("create_vote_object")

        vote_invalid = (not vote.effects.affects_score and
            not vote.is_automatic_initial_vote)
        comment_invalid = comment._spam or comment._deleted
        if vote_invalid or comment_invalid:
            timer.stop()
            timer.flush()
            return

        author = Account._byID(comment.author_id)
        add_queries(
            queries=[get_comments(author, sort, 'all') for sort in SORTS],
            insert_items=comment,
        )
        timer.intermediate("author_queries")

        update_threshold = g.live_config['comment_vote_update_threshold']
        update_period = g.live_config['comment_vote_update_period']
        skip_score_update = (comment.num_votes > update_threshold and
            comment.num_votes % update_period != 0)

        # skip updating scores if this was the automatic initial vote. those
        # updates will be handled by new_comment. Also only update scores
        # periodically once a comment has many votes.
        if not vote.is_automatic_initial_vote and not skip_score_update:
            # check whether this link is using precomputed sorts, if it is
            # we'll need to push an update to commentstree_q
            link = Link._byID(comment.link_id)
            if get_active_sort_orders_for_link(link):
                # send this comment to commentstree_q where we will update
                # CommentScoresByLink, CommentTree (noop), and CommentOrderer
                add_to_commentstree_q(comment)
            else:
                # the link isn't using precomputed sorts, so just update the
                # scores
                write_comment_scores(link, [comment])
                timer.intermediate("update_scores")

        timer.stop()
        timer.flush()

    amqp.consume_items(qname, process_message, verbose=False)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Utilities for interfacing with the WebSocket server Sutro."""

import datetime
import json
import urllib
import urlparse

from baseplate.crypto import MessageSigner
from pylons import app_globals as g

from r2.lib import amqp
from r2.lib.filters import websafe_json


_WEBSOCKET_EXCHANGE = "sutro"


def send_broadcast(namespace, type, payload):
    """Broadcast an object to all WebSocket listeners in a namespace.

    The message type is used to differentiate between different kinds of
    payloads that may be sent. The payload will be encoded as a JSON object
    before being sent to the client.

    """
    frame = {
        "type": type,
        "payload": payload,
    }
    amqp.add_item(routing_key=namespace, body=json.dumps(frame),
                  exchange=_WEBSOCKET_EXCHANGE)


def make_url(namespace, max_age):
    """Return a signed URL for the client to use for websockets.

    The namespace determines which messages the client receives and max_age is
    the number of seconds the URL is valid for.

    """

    signer = MessageSigner(g.secrets["websocket"])
    signature = signer.make_signature(
        namespace, max_age=datetime.timedelta(seconds=max_age))

    query_string = urllib.urlencode({
        "m": signature,
    })

    return urlparse.urlunparse(("wss", g.websocket_host, namespace,
                               None, query_string, None))
<EOF>
<BOF>
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
import os
import hashlib
import json
import base64
import shutil


def locate_static_file(name):
    from pylons import app_globals as g
    static_dirs = [plugin.static_dir for plugin in g.plugins]
    static_dirs.insert(0, g.paths['static_files'])

    for static_dir in reversed(static_dirs):
        file_path = os.path.join(static_dir, name.lstrip('/'))
        if os.path.exists(file_path):
            return file_path


def static_mtime(name):
    path = locate_static_file(name)
    if path:
        return os.path.getmtime(path)


def generate_static_name(name, base=None):
    """Generate a unique filename.
    
    Unique filenames are generated by base 64 encoding the first 64 bits of
    the SHA1 hash. This hash string is added to the filename before the extension.
    """
    if base:
        path = os.path.join(base, name)
    else:
        path = name

    sha = hashlib.sha1(open(path).read()).digest()
    shorthash = base64.urlsafe_b64encode(sha[0:8]).rstrip("=")
    name, ext = os.path.splitext(name)
    return name + '.' + shorthash + ext


def update_static_names(names_file, files):
    """Generate a unique file name mapping for ``files`` and write it to a
    JSON file at ``names_file``."""
    if os.path.exists(names_file):
        names = json.load(open(names_file))
    else:
        names = {}

    base = os.path.dirname(names_file)
    for path in files:
        name = os.path.relpath(path, base)
        mangled_name = generate_static_name(name, base)
        names[name] = mangled_name

        if not os.path.islink(path):
            mangled_path = os.path.join(base, mangled_name)
            shutil.move(path, mangled_path)
            # When on NFS, cp has a bad habit of turning our symlinks into
            # hardlinks. shutil.move will then call rename which will noop in
            # the case of hardlinks to the same inode.
            if os.path.exists(path):
                os.unlink(path)
            os.symlink(mangled_name, path)

    json_enc = json.JSONEncoder(indent=2, sort_keys=True)
    open(names_file, "w").write(json_enc.encode(names))

    return names


if __name__ == "__main__":
    update_static_names(sys.argv[1], sys.argv[2:])
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""Implements the AutoModerator functionality, a rules system for subreddits.

AutoModerator allows subreddits to define "rules", which are conditions
checked against submissions and comments in that subreddit. If a rule's
conditions are satisfied by a post, actions can be automatically taken on it,
such as removing the post, setting flair on it, posting a comment, etc.

A subreddit's rules are defined through YAML on a mod-only page on the
subreddit's wiki. This collection of multiple rules is implemented with the
Ruleset class. Each individual rule is a Rule object, which is made up of at
least one RuleTarget object, which defines the conditions and/or actions to
apply to an item. Rules may have additional RuleTargets which represent
"related items" that also can have conditions and/or actions. Currently,
support exists for up to two additional RuleTargets, one for the item's
author, and another for the parent link (if the original item was a comment).
"""

from collections import namedtuple
from datetime import datetime
from hashlib import md5
import re
import traceback
import yaml

from pylons import app_globals as g

from r2.lib import amqp
from r2.lib.db import queries
from r2.lib.errors import RedditError
from r2.lib.filters import _force_unicode
from r2.lib.menus import CommentSortMenu
from r2.lib.utils import (
    SimpleSillyStub,
    TimeoutFunction,
    TimeoutFunctionException,
    lowercase_keys_recursively,
    timeinterval_fromstr,
    tup,
)
from r2.lib.validator import VMarkdown
from r2.models import (
    admintools,
    Account,
    Comment,
    DeletedUser,
    Frontpage,
    Inbox,
    LastModified,
    Link,
    Message,
    ModAction,
    Report,
    Subreddit,
    Thing,
    WikiPage,
)
from r2.models.automoderator import PerformedRulesByThing
from r2.models.wiki import wiki_id


if g.automoderator_account:
    ACCOUNT = Account._by_name(g.automoderator_account)
else:
    ACCOUNT = None

DISCLAIMER = "*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/{{subreddit}}) if you have any questions or concerns.*"

rules_by_subreddit = {}

unnumbered_placeholders_regex = re.compile(r"\{\{(match(?:-[^\d-]+?)?)\}\}")
match_placeholders_regex = re.compile(r"\{\{match-(?:([^}]+?)-)?(\d+)\}\}")
def replace_placeholders(string, data, matches):
    """Replace placeholders in the string with appropriate values."""
    item = data["item"]
    replacements = {
        "{{author}}": data["author"].name,
        "{{body}}": getattr(item, "body", ""),
        "{{subreddit}}": data["subreddit"].name,
        "{{author_flair_text}}": data["author"].flair_text(
            data["subreddit"]._id, obey_disabled=True),
        "{{author_flair_css_class}}": data["author"].flair_css_class(
            data["subreddit"]._id, obey_disabled=True),
    }

    if isinstance(item, Comment):
        context = None
        if item.parent_id:
            context = 3
        replacements.update({
            "{{kind}}": "comment",
            "{{permalink}}": item.make_permalink_slow(
                context=context, force_domain=True),
            "{{title}}": data["link"].title,
        })
        media_item = data["link"]
    elif isinstance(item, Link):
        replacements.update({
            "{{kind}}": "submission",
            "{{domain}}": item.link_domain(),
            "{{permalink}}": item.make_permalink_slow(force_domain=True),
            "{{title}}": item.title,
            "{{url}}": item.url,
        })
        media_item = item

    if media_item.media_object:
        oembed = media_item.media_object.get("oembed")
        if oembed:
            replacements.update({
                "{{media_author}}": oembed.get("author_name", ""),
                "{{media_title}}": oembed.get("title", ""),
                "{{media_description}}": oembed.get("description", ""),
                "{{media_author_url}}": oembed.get("author_url", ""),
            })

    for placeholder, replacement in replacements.iteritems():
        string = string.replace(placeholder, _force_unicode(replacement))

    # do the {{match-XX}} and {{match-field-XX}} replacements
    field_replacements = {}
    if matches:
        # replace any unnumbered matches with -1 versions
        string = unnumbered_placeholders_regex.sub(r"{{\1-1}}", string)

        # find all {{match-field-XX}} placeholders
        to_replace = match_placeholders_regex.finditer(string)
        for placeholder in to_replace:
            if placeholder.group(0) in field_replacements:
                continue

            source_match_name = placeholder.group(1)
            if not source_match_name:
                # they didn't specify a source, so just take one arbitrarily
                source_match_name = matches.keys()[0]

            source_match = matches.get(source_match_name, None)
            if not source_match:
                continue

            try:
                group_index = int(placeholder.group(2))
                replacement = source_match.group(group_index)
            except IndexError:
                continue

            field_replacements[placeholder.group(0)] = replacement

    for placeholder, replacement in field_replacements.iteritems():
        string = string.replace(placeholder, _force_unicode(replacement))

    return string


class AutoModeratorSyntaxError(ValueError):
    def __init__(self, message, yaml):
        yaml_lines = yaml.splitlines()
        if len(yaml_lines) > 10:
            yaml = "\n".join(yaml_lines[:10]) + "\n..."
        self.message = "%s in rule:\n\n%s" % (message, yaml)


class AutoModeratorRuleTypeError(AutoModeratorSyntaxError):
    pass


# used in Ruleset.__init__()
RuleDefinition = namedtuple("RuleDefinition", ["yaml", "values"])


class Ruleset(object):
    """A subreddit's collection of Rules."""
    def __init__(self, yaml_text="", timer=None):
        """Create a collection of Rules from YAML documents."""
        if timer is None:
            timer = SimpleSillyStub()

        self.init_time = datetime.now(g.tz)
        self.rules = []

        if not yaml_text:
            return

        # We want to maintain the original YAML source sections, so we need
        # to manually split up the YAML by the document delimiter (line
        # starting with "---") and then try to load each section to see if
        # it's valid
        yaml_sections = [section.strip("\r\n")
            for section in re.split("^---", yaml_text, flags=re.MULTILINE)]

        rule_defs = []

        for section_num, section in enumerate(yaml_sections, 1):
            try:
                parsed = yaml.safe_load(section)
            except Exception as e:
                raise ValueError(
                    "YAML parsing error in section %s: %s" % (section_num, e))

            # only keep the section if the parsed result is a dict (otherwise
            # it's generally just a comment)
            if isinstance(parsed, dict):
                rule_defs.append(RuleDefinition(yaml=section, values=parsed))

        timer.intermediate("yaml_parsing")

        if any("standard" in rule_def.values for rule_def in rule_defs):
            # load standard rules from wiki page
            standard_rules = {}
            try:
                wp = WikiPage.get(Frontpage, "automoderator_standards")
                standard_defs = yaml.safe_load_all(wp.content)
                for standard_def in standard_defs:
                    name = standard_def.pop("standard")
                    standard_rules[name] = standard_def
            except Exception as e:
                g.log.error("Error while loading automod standards: %s", e)
                standard_rules = None

        timer.intermediate("init_standard_rules")

        for rule_def in rule_defs:
            # use standard rule as a base if they defined one
            standard_name = rule_def.values.pop("standard", None)
            if standard_name:
                # error while loading the standards, skip this rule
                if standard_rules is None:
                    continue

                standard_values = None
                if isinstance(standard_name, basestring):
                    standard_values = standard_rules.get(standard_name, None)
                if not standard_values:
                    raise AutoModeratorSyntaxError(
                        "Invalid standard: `%s`" % standard_name,
                        rule_def.yaml,
                    )

                new_values = standard_values.copy()
                new_values.update(rule_def.values)
                rule_def = rule_def._replace(values=new_values)

            type = rule_def.values.get("type", "any")
            if type == "any":
                # try to create two Rules for comments and links
                rule = None
                for type_value in ("comment", "submission"):
                    rule_def.values["type"] = type_value
                    try:
                        rule = Rule(rule_def.values, rule_def.yaml)
                    except AutoModeratorRuleTypeError as type_error:
                        continue

                    # only keep the rule if it had any checks
                    if rule.has_any_checks(targets_only=True):
                        self.rules.append(rule)

                # if both types hit exceptions we should actually error
                if not rule:
                    raise type_error
            else:
                self.rules.append(Rule(rule_def.values, rule_def.yaml))

        timer.intermediate("init_rules")

        # drop any rules that don't have a check and an action
        self.rules = [rule for rule in self.rules
            if rule.has_any_checks() and rule.has_any_actions()]

        self.rules.sort(key=lambda r: r.priority, reverse=True)

    def __iter__(self):
        """Iterate over the rules in the collection."""
        for rule in self.rules:
            yield rule

    def __len__(self):
        return len(self.rules)

    @property
    def removal_rules(self):
        """Iterate over the rules that could cause removal of their target."""
        for rule in self:
            if rule.is_removal_rule:
                yield rule

    @property
    def nonremoval_rules(self):
        """Iterate over the rules that won't cause removal of their target."""
        for rule in self:
            if not rule.is_removal_rule:
                yield rule

    def apply_to_item(self, item):
        # fetch supplemental data to use throughout
        data = {}
        data["item"] = item
        data["subreddit"] = item.subreddit_slow

        author = item.author_slow
        if not author._deleted:
            data["author"] = author
        else:
            data["author"] = DeletedUser()

        if isinstance(item, Comment):
            data["link"] = Link._byID(item.link_id, data=True)
            link_author = data["link"].author_slow
            if not link_author._deleted:
                data["link_author"] = link_author
            else:
                data["link_author"] = DeletedUser()
            data["is_submitter"] = (author == link_author)

        # get the list of rule IDs that have already been performed
        already_performed = PerformedRulesByThing.get_already_performed(item)

        # stop checking removal rules as soon as one triggers
        for rule in self.removal_rules:
            if rule.is_unrepeatable and rule.unique_id in already_performed:
                continue

            if rule.check_item(item, data):
                rule.perform_actions(item, data)
                break

        # check all other rules, regardless of how many trigger
        for rule in self.nonremoval_rules:
            if rule.is_unrepeatable and rule.unique_id in already_performed:
                continue

            if rule.check_item(item, data):
                rule.perform_actions(item, data)


class RuleComponent(object):
    """Data related to individual key/value components making up a rule."""

    def __init__(self, valid_types=None, valid_values=None,
            valid_regex=None, valid_targets=None, default=None,
            component_type=None, aliases=None):
        """
        Keyword arguments:
        valid_types -- valid value types for this key
        valid_values -- if present, a set of valid options for this key
        valid_regex -- if present, a regex the value must satisfy
        valid_targets -- this key can only be defined for these target types
        default -- if this key isn't defined, default to this value
        component_type -- "action" or "check" if relevant for this key
        aliases -- other keys you can use (only if "normal" one isn't used)
        """
        self.valid_types = valid_types
        self.valid_values = valid_values
        if valid_regex:
            self.valid_regex = re.compile(valid_regex)
        else:
            self.valid_regex = None
        self.valid_targets = tup(valid_targets)
        self.default = default
        self.component_type = component_type
        self.aliases = aliases or []

    def validate(self, value):
        """Return whether a value satisfies this key's constraints."""
        if self.valid_types:
            if not isinstance(value, self.valid_types):
                return False
        if self.valid_values:
            if value not in self.valid_values:
                return False
        if self.valid_regex:
            if not self.valid_regex.search(str(value)):
                return False

        return True


class RuleTarget(object):
    """The conditions and actions that apply to an individual item."""

    # valid options for changing how a field value is searched for a match
    _match_regexes = {
        "full-exact": u"^%s$",
        "full-text": ur"^\W*%s\W*$",
        "includes": u"%s",
        "includes-word": ur"(?:^|\W|\b)%s(?:$|\W|\b)",
        "starts-with": u"^%s",
        "ends-with": u"%s$",
    }

    # full list of modifiers that can be applied to a match
    _match_modifiers = set(_match_regexes.keys()) | {
        "case-sensitive",
        "regex",
    }

    # valid fields to match against for each target object type
    _match_fields_by_type = {
        Link: {
            "id",
            "title",
            "domain",
            "url",
            "body",
            "media_author",
            "media_author_url",
            "media_title",
            "media_description",
            "flair_text",
            "flair_css_class",
        },
        Comment: {
            "id",
            "body",
        },
        Account: {
            "id",
            "name",
            "flair_text",
            "flair_css_class",
        }
    }

    # the match regex to default to for particular fields
    _match_field_defaults = {
        "id": "full-exact",
        "url": "includes",
        "media_author": "full-exact",
        "media_author_url": "includes",
        "flair_text": "full-exact",
        "flair_css_class": "full-exact",
    }

    _operator_regex = r"(==?|<|>)"
    _oper_int_regex = r"^(%s)?\s*-?\d+$" % _operator_regex
    _oper_period_regex = r"(%s)?\s*\d+\s*((minute|hour|day|week|month|year)s?)?$" % _operator_regex

    # all the possible components that can be defined for targets
    _potential_components = {
        "reports": RuleComponent(
            valid_types=int,
            valid_targets=(Link, Comment),
            component_type="check",
        ),
        "body_longer_than": RuleComponent(
            valid_types=int,
            valid_targets=(Link, Comment),
            component_type="check",
        ),
        "body_shorter_than": RuleComponent(
            valid_types=int,
            valid_targets=(Link, Comment),
            component_type="check",
        ),
        "ignore_blockquotes": RuleComponent(
            valid_types=bool,
            valid_targets=(Link, Comment),
        ),
        "action": RuleComponent(
            valid_values={"approve", "remove", "spam", "filter", "report"},
            valid_targets=(Link, Comment),
            component_type="action",
        ),
        "action_reason": RuleComponent(
            valid_types=basestring,
            valid_targets=(Link, Comment),
            aliases=["report_reason"],
        ),
        "set_flair": RuleComponent(
            valid_types=(basestring, list),
            valid_targets=(Link, Account),
            component_type="action",
        ),
        "overwrite_flair": RuleComponent(
            valid_types=bool,
            valid_targets=(Link, Account),
            default=False,
        ),
        "is_top_level": RuleComponent(
            valid_types=bool,
            valid_targets=Comment,
            component_type="check",
        ),
        "is_edited": RuleComponent(
            valid_types=bool,
            valid_targets=(Link, Comment),
            component_type="check",
        ),
        "set_locked": RuleComponent(
            valid_types=bool,
            valid_targets=Link,
            component_type="action",
        ),
        "set_sticky": RuleComponent(
            valid_types=(bool, int),
            valid_values=set(
                [True, False] + range(1, Subreddit.MAX_STICKIES+1)),
            valid_targets=Link,
            component_type="action",
        ),
        "set_nsfw": RuleComponent(
            valid_types=bool,
            valid_targets=Link,
            component_type="action",
        ),
        "set_contest_mode": RuleComponent(
            valid_types=bool,
            valid_targets=Link,
            component_type="action",
        ),
        "set_suggested_sort": RuleComponent(
            valid_values=CommentSortMenu.suggested_sort_options + ("best",),
            valid_targets=Link,
            component_type="action",
        ),
        "comment_karma": RuleComponent(
            valid_regex=_oper_int_regex,
            valid_targets=Account,
            component_type="check",
        ),
        "post_karma": RuleComponent(
            valid_regex=_oper_int_regex,
            valid_targets=Account,
            component_type="check",
            aliases=["link_karma"],
        ),
        "combined_karma": RuleComponent(
            valid_regex=_oper_int_regex,
            valid_targets=Account,
            component_type="check",
        ),
        "account_age": RuleComponent(
            valid_regex=_oper_period_regex,
            valid_targets=Account,
            component_type="check",
        ),
        "satisfy_any_threshold": RuleComponent(
            valid_types=bool,
            valid_targets=Account,
            default=False,
        ),
        "is_gold": RuleComponent(
            valid_types=bool,
            valid_targets=Account,
            component_type="check",
        ),
        "is_submitter": RuleComponent(
            valid_types=bool,
            valid_targets=Account,
            component_type="check",
        ),
        "is_contributor": RuleComponent(
            valid_types=bool,
            valid_targets=Account,
            component_type="check",
        ),
        "is_moderator": RuleComponent(
            valid_types=bool,
            valid_targets=Account,
            component_type="check",
        ),
    }

    def __init__(self, target_type, values, parent, approve_banned=True):
        """Create a RuleTarget that applies to objects of type target_type.

        Keyword arguments:
        target_type -- the type of object this will apply to
        values -- a dict of the values for each rule component
        approve_banned -- whether to approve banned users' posts
        
        """
        self.target_type = target_type
        self.parent = parent

        if not values:
            values = {}
        else:
            values = values.copy()

        self.set_values(values)

        # determine patterns that will be matched against fields
        self.match_patterns = self.get_match_patterns(values)
        self.matches = {}

        self.approve_banned = approve_banned

    def set_values(self, values):
        """Set values for all possible rule components on the RuleTarget.

        If a value for a valid component for this target_type was not specified,
        set to the default value for that component. Set attr values to None for
        all components that only apply to other types."""
        self.checks = set()
        self.actions = set()

        for key, component in self._potential_components.iteritems():
            if self.target_type in component.valid_targets:
                # pop the key and all aliases out of the values
                # but only keep the first value we find
                value = None
                sources = [key] + component.aliases
                for source in sources:
                    from_source = values.pop(source, None)
                    if value is None:
                        value = from_source

                if value is not None:
                    if not component.validate(value):
                        raise AutoModeratorSyntaxError(
                            "invalid value for `%s`: `%s`" % (key, value),
                            self.parent.yaml,
                        )
                            
                    setattr(self, key, value)

                    if component.component_type == "check":
                        self.checks.add(key)
                    elif component.component_type == "action":
                        self.actions.add(key)
                else:
                    setattr(self, key, component.default)
            else:
                if key in values:
                    raise AutoModeratorRuleTypeError(
                        "Can't use `%s` on this type" % key,
                        self.parent.yaml,
                    )

                setattr(self, key, None)

        # special handling for set_flair
        if self.set_flair is not None:
            if isinstance(self.set_flair, basestring):
                self.set_flair = [self.set_flair, ""]

            # handle 0 or 1 item lists
            while len(self.set_flair) < 2:
                self.set_flair.append("")

            self.set_flair = {
                "text": self.set_flair[0],
                "class": self.set_flair[1],
            }

        # ugly hack to allow people to use "best" instead of "confidence"
        if self.set_suggested_sort == "best":
            self.set_suggested_sort = "confidence"

    _match_field_key_regex = re.compile(r"^(~?[^\s(]+)\s*(?:\((.+)\))?$")
    def parse_match_fields_key(self, key):
        """Parse a key defining a match against fields into its components."""
        matches = self._match_field_key_regex.match(key)
        if not matches:
            raise AutoModeratorSyntaxError(
                "Invalid search check: `%s`" % key,
                self.parent.yaml,
            )
        parsed = {}
        name = matches.group(1)

        all_valid_fields = set.union(*self._match_fields_by_type.values())
        fields = [field.strip()
            for field in name.lstrip("~").partition("#")[0].split("+")]
        for field in fields:
            if field not in all_valid_fields:
                raise AutoModeratorSyntaxError(
                    "Unknown field: `%s`" % field,
                    self.parent.yaml,
                )

        valid_fields = self._match_fields_by_type[self.target_type]
        fields = {field for field in fields if field in valid_fields}

        if not fields:
            raise AutoModeratorRuleTypeError(
                "Can't search `%s` on this type" % key,
                self.parent.yaml,
            )

        modifiers = matches.group(2)
        if modifiers:
            modifiers = [mod.strip() for mod in modifiers.split(",")]
        else:
            modifiers = []
        for mod in modifiers:
            if mod not in self._match_modifiers:
                raise AutoModeratorSyntaxError(
                    "Unknown modifier `%s` in `%s`" % (mod, key),
                    self.parent.yaml,
                )

        return {
            "name": name,
            "fields": fields,
            "modifiers": modifiers,
            "match_success": not name.startswith("~"),
        }

    def get_match_patterns(self, values):
        """Generate the regexes used to match against fields."""
        self.match_fields = set()
        match_patterns = {}
        
        for key in values:
            parsed_key = self.parse_match_fields_key(key)

            # add fields to the list of fields we're going to check
            self.match_fields |= parsed_key["fields"]

            match_values = values[key]
            if not match_values:
                continue
            if not isinstance(match_values, list):
                match_values = list((match_values,))
            # cast all values to strings in case any numbers were included
            match_values = [unicode(val) for val in match_values]

            # escape regex special chars unless this is a regex
            if "regex" not in parsed_key["modifiers"]:
                match_values = [re.escape(val) for val in match_values]

            value_str = u"(%s)" % "|".join(match_values)

            for mod in parsed_key["modifiers"]:
                if mod in self._match_regexes:
                    match_mod = mod
                    break
            else:
                if len(parsed_key["fields"]) == 1:
                    field = list(parsed_key["fields"])[0]
                    # default to handling subdomains for checks against domain only
                    if field == "domain":
                        value_str = ur"(?:.*?\.)?" + value_str
                    match_mod = self._match_field_defaults.get(
                        field, "includes-word")
                else:
                    match_mod = "includes-word"

            pattern = self._match_regexes[match_mod] % value_str

            flags = re.DOTALL | re.UNICODE
            if "case-sensitive" not in parsed_key["modifiers"]:
                flags |= re.IGNORECASE

            try:
                match_patterns[key] = re.compile(pattern, flags)
            except Exception as e:
                raise AutoModeratorSyntaxError(
                    "Generated an invalid regex for `%s`: %s" % (key, e),
                    self.parent.yaml,
                )

        return match_patterns

    @property
    def needs_media_data(self):
        """Whether the component requires data from the media embed."""
        for key in self.match_patterns:
            fields = self.parse_match_fields_key(key)["fields"]
            if all(field.startswith("media_") for field in fields):
                return True

        # check if any of the fields that support placeholders have media ones
        potential_placeholders = [self.action_reason]
        if self.set_flair:
            potential_placeholders.extend(self.set_flair.values())
        if any(text and "{{media_" in text for text in potential_placeholders):
            return True

        return False

    def check_item(self, item, data):
        """Return whether an item satisfies all of the defined conditions."""
        if not self.check_nonpattern_conditions(item, data):
            return False

        if isinstance(item, Account):
            if not self.check_account_thresholds(item, data):
                return False

        if not self.check_match_patterns(item, data):
            return False
            
        return True

    def check_nonpattern_conditions(self, item, data):
        """Check all the non-regex conditions against the item."""
        # check number of reports if necessary
        if self.reports and item.reported < self.reports:
            return False

        if hasattr(item, "body"):
            body = self.get_field_value_from_item(item, data, "body")

            # check body length restrictions if necessary
            if (self.body_longer_than is not None or
                        self.body_shorter_than is not None):
                # remove non-word chars on either end of the string
                pattern = re.compile(r'^\W+', re.UNICODE)
                body_text = pattern.sub('', body)
                pattern = re.compile(r'\W+$', re.UNICODE)
                body_text = pattern.sub('', body_text)

                if (self.body_longer_than is not None and
                        len(body_text) <= self.body_longer_than):
                    return False
                if (self.body_shorter_than is not None and
                        len(body_text) >= self.body_shorter_than):
                    return False

        # check whether it's a reply or top-level comment if necessary
        if self.is_top_level is not None:
            item_is_top_level = (not item.parent_id)
            if self.is_top_level != item_is_top_level:
                return False

        # check whether it's been edited if necessary
        if self.is_edited is not None:
            if self.is_edited != hasattr(item, "editted"):
                return False

        if self.is_submitter is not None:
            # default to True in case someone happens to check is_submitter
            # on a submission's author by accident
            is_submitter = data.get("is_submitter", True)
            if self.is_submitter != is_submitter:
                return False

        if self.is_moderator is not None:
            is_mod = bool(data["subreddit"].is_moderator(item))
            if is_mod != self.is_moderator:
                return False

        if self.is_contributor is not None:
            is_contrib = bool(data["subreddit"].is_contributor(item))
            if is_contrib != self.is_contributor:
                return False

        if self.is_gold is not None:
            if item.gold != self.is_gold:
                return False

        return True

    def check_account_thresholds(self, account, data):
        """Check karma/age thresholds against an account."""
        thresholds = ["comment_karma", "post_karma", "combined_karma",
            "account_age"]
        # figure out which thresholds/values we need to check against
        checks = {}
        for threshold in thresholds:
            compare_value = getattr(self, threshold, None)
            if compare_value is not None:
                checks[threshold] = str(compare_value)

        # if we don't need to actually check anything, just return True
        if not checks:
            return True

        # banned accounts should never satisfy threshold checks
        if account._spam:
            return False

        for check, compare_value in checks.iteritems():
            match = re.match(self._operator_regex, compare_value)
            if match:
                operator = match.group(1)
                compare_value = compare_value[len(operator):].strip()
            if not match or operator == "==":
                operator = "="

            # special handling for time period comparison value
            if check == "account_age":
                # if it's just a number, default to days
                try:
                    compare_value = int(compare_value)
                    compare_value = "%s days" % compare_value
                except ValueError:
                    pass

                compare_value = timeinterval_fromstr(compare_value)
            else:
                compare_value = int(compare_value)

            value = self.get_field_value_from_item(account, data, check)

            if operator == "<":
                result = value < compare_value
            elif operator == ">":
                result = value > compare_value
            elif operator == "=":
                result = value == compare_value

            # if satisfy_any_threshold is True, we can return True as soon
            # as a single check is successful. If it's False, they all need
            # to be satisfied, so we can return False as soon as one fails.
            if result == self.satisfy_any_threshold:
                return result

        # if we make it to here, the return statement inside the loop was
        # never triggered, so that means that if satisfy_any_threshold is
        # True, all the checks must have been False, and if it's False
        # they all must have been True
        return (not self.satisfy_any_threshold)

    def check_match_patterns(self, item, data):
        """Check all the regex patterns against the item's field values."""
        if len(self.match_patterns) == 0:
            return True

        self.matches = {}
        checked_anything = False
        for key, match_pattern in self.match_patterns.iteritems():
            match = None
            parsed_key = self.parse_match_fields_key(key)

            if isinstance(item, Link) and not item.is_self:
                # don't check body for link submissions
                parsed_key["fields"].discard("body")
            elif isinstance(item, Link) and item.is_self:
                # don't check url for text submissions
                parsed_key["fields"].discard("url")

            for source in parsed_key["fields"]:
                string = self.get_field_value_from_item(item, data, source)
                match = match_pattern.search(string)
                checked_anything = True

                if match:
                    self.matches[parsed_key["name"]] = match
                    break

            if bool(match) != parsed_key["match_success"]:
                return False

        # if we didn't actually check anything, that means that all checks
        # must have been discarded (for example, url checks when the item
        # is a self-post). That should be considered a failure.
        if checked_anything:
            return True
        else:
            return False

    def perform_actions(self, item, data):
        """Execute the defined actions on the item."""
        # only approve if it's currently removed or reported, and hasn't
        # been removed by a moderator
        ban_info = getattr(item, "ban_info", {})
        mod_banned = ban_info.get("moderator_banned")
        should_approve = ((item._spam and not mod_banned) or 
            (self.reports and item.reported))
        if self.action == "approve" and should_approve:
            approvable_author = not data["author"]._spam or self.approve_banned
            if approvable_author:
                # TODO: shouldn't need to set train_spam/insert values
                was_removed = item._spam
                admintools.unspam(item, moderator_unbanned=True,
                    unbanner=ACCOUNT.name, train_spam=True, insert=item._spam)

                log_action = None
                if isinstance(item, Link):
                    log_action = "approvelink"
                elif isinstance(item, Comment):
                    log_action = "approvecomment"

                if log_action:
                    if self.action_reason:
                        reason = replace_placeholders(
                            self.action_reason, data, self.parent.matches)
                    else:
                        reason = "unspam" if was_removed else "approved"
                    ModAction.create(data["subreddit"], ACCOUNT, log_action,
                        target=item, details=reason)

                g.stats.simple_event("automoderator.approve")

        if self.action in {"remove", "spam", "filter"}:
            spam = (self.action == "spam")
            keep_in_modqueue = (self.action == "filter")
            admintools.spam(
                item,
                auto=keep_in_modqueue,
                moderator_banned=True,
                banner=ACCOUNT.name,
                train_spam=spam,
            )

            # TODO: shouldn't need to do all of this here
            log_action = None
            if isinstance(item, Link):
                log_action = "removelink"
            elif isinstance(item, Comment):
                log_action = "removecomment"
                queries.unnotify(item)

            if log_action:
                if self.action_reason:
                    reason = replace_placeholders(
                        self.action_reason, data, self.parent.matches)
                else:
                    reason = "spam" if spam else "remove"
                ModAction.create(data["subreddit"], ACCOUNT, log_action,
                    target=item, details=reason)

            g.stats.simple_event("automoderator.%s" % self.action)

        if self.action == "report":
            if self.action_reason:
                reason = replace_placeholders(
                    self.action_reason, data, self.parent.matches)
            else:
                reason = None
            Report.new(ACCOUNT, item, reason)
            admintools.report(item)

            g.stats.simple_event("automoderator.report")

        if self.set_nsfw is not None:
            if item.over_18 != self.set_nsfw:
                item.over_18 = self.set_nsfw
                item._commit()
                # TODO: shouldn't need to do this here
                log_details = None
                if not self.set_nsfw:
                    log_details = "remove"
                ModAction.create(data["subreddit"], ACCOUNT, "marknsfw",
                    target=item, details=log_details)
                item.update_search_index()

        if self.set_contest_mode is not None:
            if item.contest_mode != self.set_contest_mode:
                item.contest_mode = self.set_contest_mode
                item._commit()

        if self.set_locked is not None:
            if item.locked != self.set_locked:
                item.locked = self.set_locked
                item._commit()

                log_action = 'lock' if self.set_locked else 'unlock'
                ModAction.create(data["subreddit"], ACCOUNT, log_action,
                    target=item)

        if self.set_sticky is not None:
            if item.is_stickied(data["subreddit"]) != bool(self.set_sticky):
                if self.set_sticky:
                    # if set_sticky is a bool, don't specify a slot
                    if isinstance(self.set_sticky, bool):
                        num = None
                    else:
                        num = self.set_sticky

                    data["subreddit"].set_sticky(item, ACCOUNT, num)
                else:
                    data["subreddit"].remove_sticky(item, ACCOUNT)

        if self.set_suggested_sort is not None:
            if not item.suggested_sort:
                item.suggested_sort = self.set_suggested_sort
                item._commit()

                # TODO: shouldn't need to do this here
                ModAction.create(data["subreddit"], ACCOUNT,
                    action="setsuggestedsort", target=item)

        if self.set_flair:
            # don't overwrite existing flair unless that was specified
            can_update_flair = False
            if isinstance(item, Link):
                if item.flair_text or item.flair_css_class:
                    can_update_flair = self.overwrite_flair
                else:
                    can_update_flair = True
            elif isinstance(item, Account):
                if data["subreddit"].is_flair(item):
                    can_update_flair = self.overwrite_flair
                else:
                    can_update_flair = True

            if can_update_flair:
                text = replace_placeholders(
                    self.set_flair["text"], data, self.parent.matches)
                cls = replace_placeholders(
                    self.set_flair["class"], data, self.parent.matches)

                # apply same limits as API to text and class
                text = text[:64]
                cls = re.sub(r"[^\w -]", "", cls)
                classes = cls.split()[:10]
                classes = [cls[:100] for cls in classes]
                cls = " ".join(classes)

                if isinstance(item, Link):
                    item.set_flair(text, cls)
                elif isinstance(item, Account):
                    item.set_flair(data["subreddit"], text, cls)

                g.stats.simple_event("automoderator.set_flair")

    def get_field_value_from_item(self, item, data, field):
        """Get a field value from the item to check against."""
        value = ''
        if field == 'id':
            value = item._id36
        elif field == 'body':
            # pull out the item's body and remove blockquotes if necessary
            body = item.body
            if self.ignore_blockquotes:
                body = '\n'.join(
                    line for line in body.splitlines()
                    if not line.startswith('>') and
                    len(line) > 0)
            value = body
        elif field == 'domain':
            if not item.is_self:
                value = item.link_domain()
            else:
                value = "self." + data["subreddit"].name
        elif (field.startswith('media_') and
                getattr(item, 'media_object', None)):
            try:
                if field == 'media_author':
                    value = item.media_object['oembed']['author_name']
                elif field == 'media_author_url':
                    value = item.media_object['oembed']['author_url']
                elif field == 'media_description':
                    value = item.media_object['oembed']['description']
                elif field == 'media_title':
                    value = item.media_object['oembed']['title']
            except KeyError:
                value = ''
        elif field == "account_age":
            value = item._age
        elif field == "post_karma":
            value = max(item.link_karma, g.link_karma_display_floor)
        elif field == "comment_karma":
            value = max(item.comment_karma, g.comment_karma_display_floor)
        elif field == "combined_karma":
            post = self.get_field_value_from_item(item, data, "post_karma")
            comment = self.get_field_value_from_item(item, data, "comment_karma")
            value = post + comment
        elif field == "flair_text" and isinstance(item, Account):
            value = item.flair_text(data["subreddit"]._id, obey_disabled=True)
        elif field == "flair_css_class" and isinstance(item, Account):
            value = item.flair_css_class(
                data["subreddit"]._id, obey_disabled=True)
        else:
            value = getattr(item, field, "")

        return value


class Rule(object):
    """The overall rule, made up of 1 or more RuleTargets."""
    _valid_type_map = {
        "comment": Comment,
        "submission": Link,
        "link submission": Link,
        "text submission": Link,
    }

    _valid_components = {
        "type": RuleComponent(
            valid_values=set(_valid_type_map.keys() + ["any"]),
            default="any",
            component_type="check",
        ),
        "priority": RuleComponent(valid_types=int, default=0),
        "moderators_exempt": RuleComponent(valid_types=bool),
        "comment": RuleComponent(valid_types=basestring, component_type="action"),
        "comment_stickied": RuleComponent(valid_types=bool, default=False),
        "modmail": RuleComponent(valid_types=basestring, component_type="action"),
        "modmail_subject": RuleComponent(
            valid_types=basestring,
            default="AutoModerator notification",
        ),
        "message": RuleComponent(valid_types=basestring, component_type="action"),
        "message_subject": RuleComponent(
            valid_types=basestring,
            default="AutoModerator notification",
        ),
    }

    def __init__(self, values, yaml_source=None):
        values = lowercase_keys_recursively(values)

        if yaml_source:
            self.yaml = yaml_source
        else:
            self.yaml = yaml.dump(values)

        self.unique_id = md5(self.yaml.encode("utf-8")).hexdigest()

        self.checks = set()
        self.actions = set()

        # pop off the values that are special for the top level
        for key, component in self._valid_components.iteritems():
            if key in values:
                value = values.pop(key)
                if not component.validate(value):
                    raise AutoModeratorSyntaxError(
                        "invalid value for `%s`: `%s`" % (key, value),
                        self.yaml,
                    )
                setattr(self, key, value)

                if component.component_type == "check":
                    self.checks.add(key)
                elif component.component_type == "action":
                    self.actions.add(key)
            else:
                setattr(self, key, component.default)

        self.base_target_type = self._valid_type_map[self.type]

        self.targets = {}

        author = values.pop("author", None)
        if not isinstance(author, dict):
            # if they just specified string(s) for author
            # that's the same as checking against name
            if isinstance(author, (list, basestring)):
                author = {"name": author}
            else:
                author = {}

        # support string(s) for ~author as well
        not_author = values.pop("~author", None)
        if isinstance(not_author, (list, basestring)):
            author["~name"] = not_author

        approve_banned = False
        if author:
            self.targets["author"] = RuleTarget(Account, author, self)
            # only approve banned users' posts if an author name check is done
            approve_banned = ("name" in self.targets["author"].match_fields)

        parent_submission = values.pop("parent_submission", None)
        if parent_submission:
            if self.base_target_type == Comment:
                target = RuleTarget(Link, parent_submission, self)
                self.targets["parent_submission"] = target
            else:
                raise AutoModeratorRuleTypeError(
                    "can't specify `parent_submission` on a submission",
                    self.yaml,
                )

        # TODO: in the future, "imported" rules can also exist here as targets

        # send all the remaining values through to the base target
        self.targets["base"] = RuleTarget(
            self.base_target_type,
            values,
            self,
            approve_banned=approve_banned,
        )

    @property
    def is_removal_rule(self):
        """Whether the rule could result in removing the item."""
        return self.targets["base"].action in {"spam", "remove", "filter"}

    @property
    def is_inapplicable_to_mods(self):
        """Whether the rule should not be applied to moderators' posts."""
        if self.moderators_exempt is not None:
            return self.moderators_exempt

        if self.is_removal_rule or self.targets["base"].action == "report":
            return True

        return False

    @property
    def is_unrepeatable(self):
        """Whether repeating the rule's actions is undesirable."""
        # we don't want to repeatedly execute rules that post or message
        if self.comment or self.modmail or self.message:
            return True

        # duplicate reports won't go through anyway
        if self.targets["base"].action == "report":
            return True

        return False

    @property
    def needs_media_data(self):
        """Whether the rule requires data from the media embed."""
        for attr in ("comment", "modmail", "message"):
            text = getattr(self, attr, None)
            if text and "{{media_" in text:
                return True

        if any(comp.needs_media_data for comp in self.targets.values()):
            return True

        return False

    @property
    def matches(self):
        return self.targets["base"].matches

    def has_any_checks(self, targets_only=False):
        for target in self.targets.values():
            if target.checks or target.match_fields:
                return True

        if targets_only:
            return False

        return bool(self.checks)

    def has_any_actions(self, targets_only=False):
        for target in self.targets.values():
            if target.actions:
                return True

        if targets_only:
            return False

        return bool(self.actions)

    def get_target_item(self, item, data, key):
        """Return the subject for a particular target's conditions/actions."""
        if key == "base":
            return item
        elif key == "author":
            return data["author"]
        elif key == "parent_submission":
            return data["link"]

    def item_is_correct_type(self, item):
        """Check that the item is the correct type to apply this rule to."""
        if not isinstance(item, self.base_target_type):
            return False

        if self.type == "link submission":
            return not item.is_self
        elif self.type == "text submission":
            return item.is_self

        return True

    def should_check_item(self, item, data):
        """Return if it is necessary to check this rule against the item."""
        if not self.item_is_correct_type(item):
            return False

        # don't check comments made by this account to prevent loops
        if isinstance(item, Comment) and data["author"] == ACCOUNT:
            return False

        if (self.is_inapplicable_to_mods and
                data["subreddit"].is_moderator(data["author"])):
            return False

        # if the item is already removed by a moderator, no need to
        # check any rules
        ban_info = getattr(item, "ban_info", {})
        if item._spam and ban_info.get("moderator_banned"):
            return False

        if self.is_removal_rule:
            # don't consider removing items another moderator has
            # already approved
            if (getattr(item, "verdict", "").endswith("-approved") and
                    ban_info.get("unbanner") != ACCOUNT.name):
                return False

        if self.needs_media_data:
            if isinstance(item, Link) and not item.media_object:
                return False
            elif isinstance(item, Comment) and not data["link"].media_object:
                return False

        return True

    def check_item(self, item, data):
        """Return whether the item satisfies all the targets' conditions."""
        if not self.should_check_item(item, data):
            return False

        g.stats.simple_event("automoderator.check_rule")

        for key, target in self.targets.iteritems():
            target_item = self.get_target_item(item, data, key)
            if not target.check_item(target_item, data):
                return False

        return True

    def perform_actions(self, item, data):
        """Execute all the rule's actions against the item."""
        for key, target in self.targets.iteritems():
            target_item = self.get_target_item(item, data, key)
            target.perform_actions(target_item, data)

        if self.comment:
            comment = self.build_message(self.comment, item, data, disclaimer=True)

            # TODO: shouldn't have to do all this manually
            if isinstance(item, Comment):
                link = data["link"]
                parent_comment = item
            else:
                link = item
                parent_comment = None
            new_comment, inbox_rel = Comment._new(
                ACCOUNT, link, parent_comment, comment, None)
            new_comment.distinguished = "yes"
            new_comment.sendreplies = False
            new_comment._commit()

            # If the comment isn't going to be put into the user's inbox
            # due to them having sendreplies disabled, force it. For a normal
            # mod, distinguishing the comment would do this, but it doesn't
            # happen here since we're setting .distinguished directly.
            if isinstance(item, Link) and not inbox_rel:
                inbox_rel = Inbox._add(data["author"], new_comment, "selfreply")

            queries.new_comment(new_comment, inbox_rel)

            if self.comment_stickied:
                try:
                    link.set_sticky_comment(new_comment, set_by=ACCOUNT)
                except RedditError:
                    # This comment isn't valid to set to sticky, ignore
                    pass

            g.stats.simple_event("automoderator.comment")

        if self.modmail:
            message = self.build_message(self.modmail, item, data, permalink=True)
            subject = replace_placeholders(
                self.modmail_subject, data, self.matches)
            subject = subject[:100]

            new_message, inbox_rel = Message._new(ACCOUNT, data["subreddit"],
                subject, message, None)
            new_message.distinguished = "yes"
            new_message._commit()
            queries.new_message(new_message, inbox_rel)

            g.stats.simple_event("automoderator.modmail")

        if self.message and not data["author"]._deleted:
            message = self.build_message(self.message, item, data,
                disclaimer=True, permalink=True)
            subject = replace_placeholders(
                self.message_subject, data, self.matches)
            subject = subject[:100]

            new_message, inbox_rel = Message._new(ACCOUNT, data["author"],
                subject, message, None)
            queries.new_message(new_message, inbox_rel)

            g.stats.simple_event("automoderator.message")

        PerformedRulesByThing.mark_performed(item, self)

    def build_message(self, text, item, data, disclaimer=False, permalink=False):
        """Generate the text to post as a comment or send as a message."""
        message = text
        if disclaimer:
            message = "%s\n\n%s" % (message, DISCLAIMER)
        if permalink and "{{permalink}}" not in message:
            message = "{{permalink}}\n\n%s" % message
        message = replace_placeholders(message, data, self.matches)

        message = VMarkdown('').run(message)

        return message[:10000]


def run():
    @g.stats.amqp_processor("automoderator_q")
    def process_message(msg):
        if not ACCOUNT:
            return

        fullname = msg.body
        with g.make_lock("automoderator", "automod_" + fullname, timeout=5):
            item = Thing._by_fullname(fullname, data=True)
            if not isinstance(item, (Link, Comment)):
                return

            subreddit = item.subreddit_slow
            
            wiki_page_id = wiki_id(subreddit._id36, "config/automoderator")
            wiki_page_fullname = "WikiPage_%s" % wiki_page_id
            last_edited = LastModified.get(wiki_page_fullname, "Edit")
            if not last_edited:
                return

            # initialize rules for the subreddit if we haven't already
            # or if the page has been edited since we last initialized
            need_to_init = False
            if subreddit._id not in rules_by_subreddit:
                need_to_init = True
            else:
                rules = rules_by_subreddit[subreddit._id]
                if last_edited > rules.init_time:
                    need_to_init = True

            if need_to_init:
                timer = g.stats.get_timer("automoderator.init_ruleset")
                timer.start()

                wp = WikiPage.get(subreddit, "config/automoderator")
                timer.intermediate("get_wiki_page")

                try:
                    rules = Ruleset(wp.content, timer)
                except (AutoModeratorSyntaxError, AutoModeratorRuleTypeError):
                    print "ERROR: Invalid config in /r/%s" % subreddit.name
                    return

                rules_by_subreddit[subreddit._id] = rules

                timer.stop()

            if not rules:
                return

            try:
                TimeoutFunction(rules.apply_to_item, 2)(item)
                print "Checked %s from /r/%s" % (item, subreddit.name)
            except TimeoutFunctionException:
                print "Timed out on %s from /r/%s" % (item, subreddit.name)
            except KeyboardInterrupt:
                raise
            except:
                print "Error on %s from /r/%s" % (item, subreddit.name)
                print traceback.format_exc()

    amqp.consume_items('automoderator_q', process_message, verbose=False)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys

import base64
import cStringIO
import hashlib
import json
import math
import os
import re
import subprocess
import tempfile
import traceback
import urllib
import urllib2
import urlparse
import gzip

import BeautifulSoup
from PIL import Image, ImageFile
import lxml.html
import requests

from pylons import app_globals as g

from r2 import models
from r2.config import feature
from r2.lib import amqp, hooks
from r2.lib.db.tdb_cassandra import NotFound
from r2.lib.memoize import memoize
from r2.lib.nymph import optimize_png
from r2.lib.template_helpers import format_html
from r2.lib.utils import (
    TimeoutFunction,
    TimeoutFunctionException,
    UrlParser,
    coerce_url_to_protocol,
    domain,
    extract_urls_from_markdown,
    get_requests_resp_json,
    is_subdomain,
)
from r2.models.link import Link
from r2.models.media_cache import (
    ERROR_MEDIA,
    Media,
    MediaByURL,
)
from urllib2 import (
    HTTPError,
    URLError,
)

_IMAGE_PREVIEW_TEMPLATE = """
<img class="%(css_class)s" src="%(url)s" width="%(width)s" height="%(height)s">
"""


def _image_to_str(image):
    s = cStringIO.StringIO()
    image.save(s, image.format)
    return s.getvalue()


def str_to_image(s):
    s = cStringIO.StringIO(s)
    image = Image.open(s)
    return image


def _image_entropy(img):
    """calculate the entropy of an image"""
    hist = img.histogram()
    hist_size = sum(hist)
    hist = [float(h) / hist_size for h in hist]

    return -sum(p * math.log(p, 2) for p in hist if p != 0)


def _crop_image_vertically(img, target_height):
    """crop image vertically the the specified height. determine
    which pieces to cut off based on the entropy pieces."""
    x,y = img.size

    while y > target_height:
        #slice 10px at a time until square
        slice_height = min(y - target_height, 10)

        bottom = img.crop((0, y - slice_height, x, y))
        top = img.crop((0, 0, x, slice_height))

        #remove the slice with the least entropy
        if _image_entropy(bottom) < _image_entropy(top):
            img = img.crop((0, 0, x, y - slice_height))
        else:
            img = img.crop((0, slice_height, x, y))

        x,y = img.size

    return img


def _square_image(img):
    """if the image is taller than it is wide, square it off."""
    width = img.size[0]
    return _crop_image_vertically(img, width)


def _apply_exif_orientation(image):
    """Update the image's orientation if it has the relevant EXIF tag."""
    try:
        exif_tags = image._getexif() or {}
    except AttributeError:
        # image format with no EXIF tags
        return image

    # constant from EXIF spec
    ORIENTATION_TAG_ID = 0x0112
    orientation = exif_tags.get(ORIENTATION_TAG_ID)

    if orientation == 1:
        # 1 = Horizontal (normal)
        pass
    elif orientation == 2:
        # 2 = Mirror horizontal
        image = image.transpose(Image.FLIP_LEFT_RIGHT)
    elif orientation == 3:
        # 3 = Rotate 180
        image = image.transpose(Image.ROTATE_180)
    elif orientation == 4:
        # 4 = Mirror vertical
        image = image.transpose(Image.FLIP_TOP_BOTTOM)
    elif orientation == 5:
        # 5 = Mirror horizontal and rotate 90 CCW
        image = image.transpose(Image.FLIP_LEFT_RIGHT)
        image = image.transpose(Image.ROTATE_90)
    elif orientation == 6:
        # 6 = Rotate 270 CCW
        image = image.transpose(Image.ROTATE_270)
    elif orientation == 7:
        # 7 = Mirror horizontal and rotate 270 CCW
        image = image.transpose(Image.FLIP_LEFT_RIGHT)
        image = image.transpose(Image.ROTATE_270)
    elif orientation == 8:
        # 8 = Rotate 90 CCW
        image = image.transpose(Image.ROTATE_90)

    return image


def _prepare_image(image):
    image = _apply_exif_orientation(image)

    image = _square_image(image)

    if feature.is_enabled('hidpi_thumbnails'):
        hidpi_dims = [int(d * g.thumbnail_hidpi_scaling) for d in g.thumbnail_size]

        # If the image width is smaller than hidpi requires, set to non-hidpi
        if image.size[0] < hidpi_dims[0]:
            thumbnail_size = g.thumbnail_size
        else:
            thumbnail_size = hidpi_dims
    else:
        thumbnail_size = g.thumbnail_size

    image.thumbnail(thumbnail_size, Image.ANTIALIAS)
    return image


def _clean_url(url):
    """url quotes unicode data out of urls"""
    url = url.encode('utf8')
    url = ''.join(urllib.quote(c) if ord(c) >= 127 else c for c in url)
    return url


def _initialize_request(url, referer, gzip=False):
    url = _clean_url(url)

    if not url.startswith(("http://", "https://")):
        return

    req = urllib2.Request(url)
    if gzip:
        req.add_header('Accept-Encoding', 'gzip')
    if g.useragent:
        req.add_header('User-Agent', g.useragent)
    if referer:
        req.add_header('Referer', referer)
    return req


def _fetch_url(url, referer=None):
    request = _initialize_request(url, referer=referer, gzip=True)
    if not request:
        return None, None
    response = urllib2.urlopen(request)
    response_data = response.read()
    content_encoding = response.info().get("Content-Encoding")
    if content_encoding and content_encoding.lower() in ["gzip", "x-gzip"]:
        buf = cStringIO.StringIO(response_data)
        f = gzip.GzipFile(fileobj=buf)
        response_data = f.read()
    return response.headers.get("Content-Type"), response_data


@memoize('media.fetch_size', time=3600)
def _fetch_image_size(url, referer):
    """Return the size of an image by URL downloading as little as possible."""

    request = _initialize_request(url, referer)
    if not request:
        return None

    parser = ImageFile.Parser()
    response = None
    try:
        response = urllib2.urlopen(request)

        while True:
            chunk = response.read(1024)
            if not chunk:
                break

            parser.feed(chunk)
            if parser.image:
                return parser.image.size
    except urllib2.URLError:
        return None
    finally:
        if response:
            response.close()


def optimize_jpeg(filename):
    with open(os.path.devnull, 'w') as devnull:
        subprocess.check_call(("/usr/bin/jpegoptim", filename), stdout=devnull)


def thumbnail_url(link):
    """Given a link, returns the url for its thumbnail based on its fullname"""
    if link.has_thumbnail:
        if hasattr(link, "thumbnail_url"):
            return link.thumbnail_url
        else:
            return ''
    else:
        return ''


def _filename_from_content(contents):
    hash_bytes = hashlib.sha256(contents).digest()
    return base64.urlsafe_b64encode(hash_bytes).rstrip("=")


def upload_media(image, file_type='.jpg', category='thumbs'):
    """Upload an image to the media provider."""
    f = tempfile.NamedTemporaryFile(suffix=file_type, delete=False)
    try:
        img = image
        do_convert = True
        if isinstance(img, basestring):
            img = str_to_image(img)
            if img.format == "PNG" and file_type == ".png":
                img.verify()
                f.write(image)
                f.close()
                do_convert = False

        if do_convert:
            img = img.convert('RGBA')
            if file_type == ".jpg":
                # PIL does not play nice when converting alpha channels to jpg
                background = Image.new('RGBA', img.size, (255, 255, 255))
                background.paste(img, img)
                img = background.convert('RGB')
                img.save(f, quality=85) # Bug in the JPG encoder with the optimize flag, even if set to false
            else:
                img.save(f, optimize=True)

        if file_type == ".png":
            optimize_png(f.name)
        elif file_type == ".jpg":
            optimize_jpeg(f.name)
        contents = open(f.name).read()
        file_name = _filename_from_content(contents) + file_type
        return g.media_provider.put(category, file_name, contents)
    finally:
        os.unlink(f.name)
    return ""


def upload_stylesheet(content):
    file_name = _filename_from_content(content) + ".css"
    return g.media_provider.put('stylesheets', file_name, content)


def _scrape_media(url, autoplay=False, maxwidth=600, force=False,
                  save_thumbnail=True, use_cache=False, max_cache_age=None,
                  use_youtube_scraper=False):
    media = None
    autoplay = bool(autoplay)
    maxwidth = int(maxwidth)

    # Use media from the cache (if available)
    if not force and use_cache:
        mediaByURL = MediaByURL.get(url,
                                    autoplay=autoplay,
                                    maxwidth=maxwidth,
                                    max_cache_age=max_cache_age)
        if mediaByURL:
            media = mediaByURL.media

    # Otherwise, scrape it if thumbnail is not present
    if not media or not media.thumbnail_url:
        media_object = secure_media_object = None
        thumbnail_image = thumbnail_url = thumbnail_size = None

        scraper = Scraper.for_url(url, autoplay=autoplay,
                                  use_youtube_scraper=use_youtube_scraper)
        try:
            thumbnail_image, preview_object, media_object, secure_media_object = (
                scraper.scrape())
        except (HTTPError, URLError) as e:
            if use_cache:
                MediaByURL.add_error(url, str(e),
                                     autoplay=autoplay,
                                     maxwidth=maxwidth)
            return None

        # the scraper should be able to make a media embed out of the
        # media object it just gave us. if not, null out the media object
        # to protect downstream code
        if media_object and not scraper.media_embed(media_object):
            print "%s made a bad media obj for url %s" % (scraper, url)
            media_object = None

        if (secure_media_object and
            not scraper.media_embed(secure_media_object)):
            print "%s made a bad secure media obj for url %s" % (scraper, url)
            secure_media_object = None

        # If thumbnail can't be found, attempt again using _ThumbnailOnlyScraper
        # This should fix bugs that occur when embed.ly caches links before the 
        # thumbnail is available
        if (not thumbnail_image and 
                not isinstance(scraper, _ThumbnailOnlyScraper)):
            scraper = _ThumbnailOnlyScraper(url)
            try:
                thumbnail_image, preview_object, _, _ = scraper.scrape()
            except (HTTPError, URLError) as e:
                use_cache = False

        if thumbnail_image and save_thumbnail:
            thumbnail_size = thumbnail_image.size
            thumbnail_url = upload_media(thumbnail_image)
        else:
            # don't cache if thumbnail is absent
            use_cache = False

        media = Media(media_object, secure_media_object, preview_object,
                      thumbnail_url, thumbnail_size)

    if use_cache and save_thumbnail and media is not ERROR_MEDIA:
        # Store the media in the cache, possibly extending the ttl
        MediaByURL.add(url,
                       media,
                       autoplay=autoplay,
                       maxwidth=maxwidth)

    return media


def _get_scrape_url(link):
    if not link.is_self:
        sr_name = link.subreddit_slow.name
        if not feature.is_enabled("imgur_gif_conversion", subreddit=sr_name):
            return link.url
        p = UrlParser(link.url)
        # If it's a gif link on imgur, replacing it with gifv should
        # give us the embedly friendly video url
        if is_subdomain(p.hostname, "imgur.com"):
            if p.path_extension().lower() == "gif":
                p.set_extension("gifv")
                return p.unparse()
        return link.url

    urls = extract_urls_from_markdown(link.selftext)
    second_choice = None
    for url in urls:
        p = UrlParser(url)
        if p.is_reddit_url():
            continue
        # If we don't find anything we like better, use the first image.
        if not second_choice:
            second_choice = url
        # This is an optimization for "proof images" in AMAs.
        if is_subdomain(p.netloc, 'imgur.com') or p.has_image_extension():
            return url

    return second_choice


def _set_media(link, force=False, **kwargs):
    sr = link.subreddit_slow
    
    # Do not process thumbnails for quarantined subreddits
    if sr.quarantine:
        return

    if not link.is_self:
        if not force and (link.has_thumbnail or link.media_object):
            return

    if not force and link.promoted:
        return

    scrape_url = _get_scrape_url(link)

    if not scrape_url:
        if link.preview_object:
            # If the user edited out an image from a self post, we need to make
            # sure to remove its metadata.
            link.set_preview_object(None)
            link._commit()
        return

    youtube_scraper = feature.is_enabled("youtube_scraper", subreddit=sr.name)
    media = _scrape_media(scrape_url, force=force,
                          use_youtube_scraper=youtube_scraper, **kwargs)

    if media and not link.promoted:
        # While we want to add preview images to self posts for the new apps,
        # let's not muck about with the old-style thumbnails in case that
        # breaks assumptions.
        if not link.is_self:
            link.thumbnail_url = media.thumbnail_url
            link.thumbnail_size = media.thumbnail_size

            link.set_media_object(media.media_object)
            link.set_secure_media_object(media.secure_media_object)
        link.set_preview_object(media.preview_object)

        link._commit()

        hooks.get_hook("scraper.set_media").call(link=link)

        if media.media_object or media.secure_media_object:
            amqp.add_item("new_media_embed", link._fullname)


def force_thumbnail(link, image_data, file_type=".jpg"):
    image = str_to_image(image_data)
    image = _prepare_image(image)
    thumb_url = upload_media(image, file_type=file_type)

    link.thumbnail_url = thumb_url
    link.thumbnail_size = image.size
    link._commit()


def force_mobile_ad_image(link, image_data, file_type=".jpg"):
    image = str_to_image(image_data)
    image_width = image.size[0]
    x,y = g.mobile_ad_image_size
    max_height = image_width * y / x
    image = _crop_image_vertically(image, max_height)
    image.thumbnail(g.mobile_ad_image_size, Image.ANTIALIAS)
    image_url = upload_media(image, file_type=file_type)

    link.mobile_ad_url = image_url
    link.mobile_ad_size = image.size
    link._commit()


def upload_icon(image_data, size):
    image = str_to_image(image_data)
    image.format = 'PNG'
    image.thumbnail(size, Image.ANTIALIAS)
    icon_data = _image_to_str(image)
    file_name = _filename_from_content(icon_data)
    return g.media_provider.put('icons', file_name + ".png", icon_data)


def allowed_media_preview_url(url):
    p = UrlParser(url)
    if p.has_static_image_extension():
        return True
    for allowed_domain in g.media_preview_domain_whitelist:
        if is_subdomain(p.hostname, allowed_domain):
            return True
    return False


def get_preview_image(preview_object, include_censored=False):
    """Returns a media_object for rendering a media preview image"""
    min_width, min_height = g.preview_image_min_size
    max_width, max_height = g.preview_image_max_size
    source_width = preview_object['width']
    source_height = preview_object['height']

    if source_width <= max_width and source_height <= max_height:
        width = source_width
        height = source_height
    else:
        max_ratio = float(max_height) / max_width
        source_ratio = float(source_height) / source_width
        if source_ratio >= max_ratio:
            height = max_height
            width = int((height * source_width) / source_height)
        else:
            width = max_width
            height = int((width * source_height) / source_width)

    if width < min_width and height < min_height:
        return None

    url = g.image_resizing_provider.resize_image(preview_object, width)
    img_html = format_html(
        _IMAGE_PREVIEW_TEMPLATE,
        css_class="preview",
        url=url,
        width=width,
        height=height,
    )

    if include_censored:
        censored_url = g.image_resizing_provider.resize_image(
            preview_object,
            width,
            censor_nsfw=True,
        )
        censored_img_html = format_html(
            _IMAGE_PREVIEW_TEMPLATE,
            css_class="censored-preview",
            url=censored_url,
            width=width,
            height=height,
        )
        img_html += censored_img_html

    media_object = {
        "type": "media-preview",
        "width": width,
        "height": height,
        "content": img_html,
    }

    return media_object


def _make_custom_media_embed(media_object):
    # this is for promoted links with custom media embeds.
    return MediaEmbed(
        height=media_object.get("height"),
        width=media_object.get("width"),
        content=media_object.get("content"),
    )


def get_media_embed(media_object):
    if not isinstance(media_object, dict):
        return

    embed_hook = hooks.get_hook("scraper.media_embed")
    media_embed = embed_hook.call_until_return(media_object=media_object)
    if media_embed:
        return media_embed

    if media_object.get("type") == "custom":
        return _make_custom_media_embed(media_object)

    if "oembed" in media_object:
        if media_object.get("type") == "youtube.com":
            return _YouTubeScraper.media_embed(media_object)

        return _EmbedlyScraper.media_embed(media_object)


class MediaEmbed(object):
    """A MediaEmbed holds data relevant for serving media for an object."""

    width = None
    height = None
    content = None
    scrolling = False

    def __init__(self, height, width, content, scrolling=False,
                 public_thumbnail_url=None, sandbox=True):
        """Build a MediaEmbed.

        :param height int - The height of the media embed, in pixels
        :param width int - The width of the media embed, in pixels
        :param content string - The content of the media embed - HTML.
        :param scrolling bool - Whether the media embed should scroll or not.
        :param public_thumbnail_url string - The URL of the most representative
            thumbnail for this media. This may be on an uncontrolled domain,
            and is not necessarily our own thumbs domain (and should not be
            served to browsers).
        :param sandbox bool - True if the content should be sandboxed
            in an iframe on the media domain.
        """

        self.height = int(height)
        self.width = int(width)
        self.content = content
        self.scrolling = scrolling
        self.public_thumbnail_url = public_thumbnail_url
        self.sandbox = sandbox


class Scraper(object):
    @classmethod
    def for_url(cls, url, autoplay=False, maxwidth=600, use_youtube_scraper=False):
        scraper = hooks.get_hook("scraper.factory").call_until_return(url=url)
        if scraper:
            return scraper

        if use_youtube_scraper and _YouTubeScraper.matches(url):
            return _YouTubeScraper(url, maxwidth=maxwidth)

        embedly_services = _fetch_embedly_services()
        for service_re in embedly_services:
            if service_re.match(url):
                return _EmbedlyScraper(url,
                                       autoplay=autoplay,
                                       maxwidth=maxwidth)

        return _ThumbnailOnlyScraper(url)

    def scrape(self):
        # should return a 4-tuple of:
        #     thumbnail, preview_object, media_object, secure_media_obj
        raise NotImplementedError

    @classmethod
    def media_embed(cls, media_object):
        # should take a media object and return an appropriate MediaEmbed
        raise NotImplementedError


class _ThumbnailOnlyScraper(Scraper):
    def __init__(self, url):
        self.url = url
        # Having the source document's protocol on hand makes it easier to deal
        # with protocol-relative urls we extract from it.
        self.protocol = UrlParser(url).scheme

    def scrape(self):
        thumbnail_url, image_data = self._find_thumbnail_image()
        if not thumbnail_url:
            return None, None, None, None

        # When isolated from the context of a webpage, protocol-relative URLs
        # are ambiguous, so let's absolutify them now.
        if thumbnail_url.startswith('//'):
            thumbnail_url = coerce_url_to_protocol(thumbnail_url, self.protocol)

        if not image_data:
            _, image_data = _fetch_url(thumbnail_url, referer=self.url)

        if not image_data:
            return None, None, None, None

        uid = _filename_from_content(image_data)
        image = str_to_image(image_data)
        storage_url = upload_media(image, category='previews')
        width, height = image.size
        preview_object = {
            'uid': uid,
            'url': storage_url,
            'width': width,
            'height': height,
        }

        thumbnail = _prepare_image(image)

        return thumbnail, preview_object, None, None

    def _extract_image_urls(self, soup):
        for img in soup.findAll("img", src=True):
            yield urlparse.urljoin(self.url, img["src"])

    def _find_thumbnail_image(self):
        """Find what we think is the best thumbnail image for a link.

        Returns a 2-tuple of image url and, as an optimization, the raw image
        data.  A value of None for the former means we couldn't find an image;
        None for the latter just means we haven't already fetched the image.
        """
        content_type, content = _fetch_url(self.url)

        # if it's an image, it's pretty easy to guess what we should thumbnail.
        if content_type and "image" in content_type and content:
            return self.url, content

        if content_type and "html" in content_type and content:
            soup = BeautifulSoup.BeautifulSoup(content)
        else:
            return None, None

        # Allow the content author to specify the thumbnail using the Open
        # Graph protocol: http://ogp.me/
        og_image = (soup.find('meta', property='og:image') or
                    soup.find('meta', attrs={'name': 'og:image'}))
        if og_image and og_image.get('content'):
            return og_image['content'], None
        og_image = (soup.find('meta', property='og:image:url') or
                    soup.find('meta', attrs={'name': 'og:image:url'}))
        if og_image and og_image.get('content'):
            return og_image['content'], None

        # <link rel="image_src" href="http://...">
        thumbnail_spec = soup.find('link', rel='image_src')
        if thumbnail_spec and thumbnail_spec['href']:
            return thumbnail_spec['href'], None

        # ok, we have no guidance from the author. look for the largest
        # image on the page with a few caveats. (see below)
        max_area = 0
        max_url = None
        for image_url in self._extract_image_urls(soup):
            # When isolated from the context of a webpage, protocol-relative
            # URLs are ambiguous, so let's absolutify them now.
            if image_url.startswith('//'):
                image_url = coerce_url_to_protocol(image_url, self.protocol)
            size = _fetch_image_size(image_url, referer=self.url)
            if not size:
                continue

            area = size[0] * size[1]

            # ignore little images
            if area < 5000:
                g.log.debug('ignore little %s' % image_url)
                continue

            # ignore excessively long/wide images
            if max(size) / min(size) > 1.5:
                g.log.debug('ignore dimensions %s' % image_url)
                continue

            # penalize images with "sprite" in their name
            if 'sprite' in image_url.lower():
                g.log.debug('penalizing sprite %s' % image_url)
                area /= 10

            if area > max_area:
                max_area = area
                max_url = image_url

        return max_url, None


class _EmbedlyScraper(Scraper):
    """Use Embedly to get information about embed info for a url.

    http://embed.ly/docs/api/embed/endpoints/1/oembed
    """
    EMBEDLY_API_URL = "https://api.embed.ly/1/oembed"

    def __init__(self, url, autoplay=False, maxwidth=600):
        self.url = url
        self.maxwidth = int(maxwidth)
        self.embedly_params = {}

        if autoplay:
            self.embedly_params["autoplay"] = "true"

    def _fetch_from_embedly(self, secure):
        param_dict = {
            "url": self.url,
            "format": "json",
            "maxwidth": self.maxwidth,
            "key": g.embedly_api_key,
            "secure": "true" if secure else "false",
        }

        param_dict.update(self.embedly_params)
        params = urllib.urlencode(param_dict)

        timer = g.stats.get_timer("providers.embedly.oembed")
        timer.start()
        content = requests.get(self.EMBEDLY_API_URL + "?" + params).content
        timer.stop()

        return json.loads(content)

    def _make_media_object(self, oembed):
        if oembed.get("type") in ("video", "rich"):
            return {
                "type": domain(self.url),
                "oembed": oembed,
            }
        return None

    def scrape(self):
        oembed = self._fetch_from_embedly(secure=False)
        if not oembed:
            return None, None, None, None

        if oembed.get("type") == "photo":
            thumbnail_url = oembed.get("url")
        else:
            thumbnail_url = oembed.get("thumbnail_url")
        if not thumbnail_url:
            return None, None, None, None

        content_type, content = _fetch_url(thumbnail_url, referer=self.url)
        uid = _filename_from_content(content)
        image = str_to_image(content)
        storage_url = upload_media(image, category='previews')
        width, height = image.size
        preview_object = {
            'uid': uid,
            'url': storage_url,
            'width': width,
            'height': height,
        }

        thumbnail = _prepare_image(image)

        secure_oembed = self._fetch_from_embedly(secure=True)
        if not self.validate_secure_oembed(secure_oembed):
            secure_oembed = {}

        return (
            thumbnail,
            preview_object,
            self._make_media_object(oembed),
            self._make_media_object(secure_oembed),
        )

    def validate_secure_oembed(self, oembed):
        """Check the "secure" embed is safe to embed, and not a placeholder"""
        if not oembed.get("html"):
            return False

        # Get the embed.ly iframe's src
        iframe_src = lxml.html.fromstring(oembed['html']).get('src')
        if not iframe_src:
            return False
        iframe_src_url = UrlParser(iframe_src)

        # Per embed.ly support: If the URL for the provider is HTTP, we're
        # gonna get a placeholder image instead
        provider_src_url = UrlParser(iframe_src_url.query_dict.get('src'))
        return not provider_src_url.scheme or provider_src_url.scheme == "https"

    @classmethod
    def media_embed(cls, media_object):
        oembed = media_object["oembed"]

        html = oembed.get("html")
        width = oembed.get("width")
        height = oembed.get("height")
        public_thumbnail_url = oembed.get('thumbnail_url')
        if not (html and width and height):
            return

        return MediaEmbed(
            width=width,
            height=height,
            content=html,
            public_thumbnail_url=public_thumbnail_url,
        )


class _YouTubeScraper(Scraper):
    OEMBED_ENDPOINT = "https://www.youtube.com/oembed"
    URL_MATCH = re.compile(r"https?://((www\.)?youtube\.com/watch|youtu\.be/)")

    def __init__(self, url, maxwidth):
        self.url = url
        self.maxwidth = maxwidth

    @classmethod
    def matches(cls, url):
        return cls.URL_MATCH.match(url)

    def _fetch_from_youtube(self):
        params = {
            "url": self.url,
            "format": "json",
            "maxwidth": self.maxwidth,
        }

        with g.stats.get_timer("providers.youtube.oembed"):
            content = requests.get(self.OEMBED_ENDPOINT, params=params).content

        return json.loads(content)

    def _make_media_object(self, oembed):
        if oembed.get("type") == "video":
            return {
                "type": "youtube.com",
                "oembed": oembed,
            }
        return None

    def scrape(self):
        oembed = self._fetch_from_youtube()
        if not oembed:
            return None, None, None, None
        thumbnail_url = oembed.get("thumbnail_url")

        if not thumbnail_url:
            return None, None, None, None

        _, content = _fetch_url(thumbnail_url, referer=self.url)
        uid = _filename_from_content(content)
        image = str_to_image(content)
        storage_url = upload_media(image, category='previews')
        width, height = image.size
        preview_object = {
            'uid': uid,
            'url': storage_url,
            'width': width,
            'height': height,
        }

        thumbnail = _prepare_image(image)
        media_object = self._make_media_object(oembed)

        return (
            thumbnail,
            preview_object,
            media_object,
            media_object,
        )

    @classmethod
    def media_embed(cls, media_object):
        oembed = media_object["oembed"]

        html = oembed.get("html")
        width = oembed.get("width")
        height = oembed.get("height")
        public_thumbnail_url = oembed.get('thumbnail_url')

        if not (html and width and height):
            return

        return MediaEmbed(
            width=width,
            height=height,
            content=html,
            public_thumbnail_url=public_thumbnail_url,
        )


@memoize("media.embedly_services2", time=3600)
def _fetch_embedly_service_data():
    resp = requests.get("https://api.embed.ly/1/services/python")
    return get_requests_resp_json(resp)


def _fetch_embedly_services():
    if not g.embedly_api_key:
        if g.debug:
            g.log.info("No embedly_api_key, using no key while in debug mode.")
        else:
            g.log.warning("No embedly_api_key configured. Will not use "
                          "embed.ly.")
            return []

    service_data = _fetch_embedly_service_data()

    return [
        re.compile("(?:%s)" % "|".join(service["regex"]))
        for service in service_data
    ]


def run():
    @g.stats.amqp_processor('scraper_q')
    def process_link(msg):
        fname = msg.body
        link = Link._by_fullname(fname, data=True)

        try:
            TimeoutFunction(_set_media, 30)(link, use_cache=True)
        except TimeoutFunctionException:
            print "Timed out on %s" % fname
        except KeyboardInterrupt:
            raise
        except:
            print "Error fetching %s" % fname
            print traceback.format_exc()

    amqp.consume_items('scraper_q', process_link)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import json
import urllib
import functools
import zlib

from kazoo.client import KazooClient
from kazoo.security import make_digest_acl
from kazoo.exceptions import NoNodeException

from r2.lib import hooks
from r2.lib.contrib import ipaddress


def connect_to_zookeeper(hostlist, credentials):
    """Create a connection to the ZooKeeper ensemble.

    If authentication credentials are provided (as a two-tuple: username,
    password), we will ensure that they are provided to the server whenever we
    establish a connection.

    """

    client = KazooClient(hostlist,
                         timeout=5,
                         max_retries=3,
                         auth_data=[("digest", ":".join(credentials))])

    # convenient helper function for making credentials
    client.make_acl = functools.partial(make_digest_acl, *credentials)

    client.start()
    return client


class LiveConfig(object):
    """A read-only dictionary view of configuration retrieved from ZooKeeper.

    The data will be parsed using the given configuration specs, exactly like
    the ini file based configuration. When data is changed in ZooKeeper, the
    data in this view will automatically update.

    """
    def __init__(self, client, key):
        self.data = {}

        @client.DataWatch(key)
        def watcher(data, stat):
            if data and data.startswith("gzip"):
                data = zlib.decompress(data[len("gzip"):])
            self.data = json.loads(data or '{}')
            hooks.get_hook("worker.live_config.update").call()

    def __getitem__(self, key):
        return self.data[key]

    def get(self, key, default=None):
        return self.data.get(key, default)

    def iteritems(self):
        return self.data.iteritems()

    def __repr__(self):
        return "<LiveConfig %r>" % self.data


class LiveList(object):
    """A mutable set shared by all apps and backed by ZooKeeper."""
    def __init__(self, client, root, map_fn=None, reduce_fn=lambda L: L,
                 watch=True):
        self.client = client
        self.root = root
        self.map_fn = map_fn
        self.reduce_fn = reduce_fn
        self.is_watching = watch

        acl = [self.client.make_acl(read=True, create=True, delete=True)]
        self.client.ensure_path(self.root, acl)

        if watch:
            self.data = []

            @client.ChildrenWatch(root)
            def watcher(children):
                self.data = self._normalize_children(children, reduce=True)

    def _nodepath(self, item):
        escaped = urllib.quote(str(item), safe=":")
        return os.path.join(self.root, escaped)

    def _normalize_children(self, children, reduce):
        unquoted = (urllib.unquote(c) for c in children)
        mapped = map(self.map_fn, unquoted)

        if reduce:
            return list(self.reduce_fn(mapped))
        else:
            return list(mapped)

    def add(self, item):
        path = self._nodepath(item)
        self.client.ensure_path(path)

    def remove(self, item):
        path = self._nodepath(item)

        try:
            self.client.delete(path)
        except NoNodeException:
            raise ValueError("not in list")

    def get(self, reduce=True):
        children = self.client.get_children(self.root)
        return self._normalize_children(children, reduce)

    def __iter__(self):
        if not self.is_watching:
            raise NotImplementedError()
        return iter(self.data)

    def __len__(self):
        if not self.is_watching:
            raise NotImplementedError()
        return len(self.data)

    def __repr__(self):
        return "<LiveList %r (%s)>" % (self.data,
                                       "push" if self.is_watching else "pull")


class ReducedLiveList(object):
    """Store a copy of the reduced data in addition to the full LiveList.

    This is useful for cases where the map/reduce phase is slow and CPU
    intensive. By storing the reduced data separately the map/reduce only needs
    to be done by the process that's updating the list. All other processes
    watch the reduced data node.

    """

    def __init__(self, client, root, reduced_data_node, map_fn=None,
                 reduce_fn=lambda L: L, to_json_fn=None, from_json_fn=None):
        # don't watch the underlying LiveList because all updates are triggered
        # on the reduced data node
        self.live_list = LiveList(
            client, root, map_fn=map_fn, reduce_fn=reduce_fn, watch=False)

        self.client = client
        self.root = root
        self.reduced_data_node = reduced_data_node

        acl = [self.client.make_acl(
            read=True, write=True, create=True, delete=True)]
        self.client.ensure_path(self.reduced_data_node, acl)

        self.data = []
        self.to_json_fn = to_json_fn
        self.from_json_fn = from_json_fn

        @client.DataWatch(self.reduced_data_node)
        def watcher(json_data, stat):
            if json_data and json_data.startswith("gzip"):
                json_data = zlib.decompress(json_data[len("gzip"):])
            self.data = self.from_json_fn(json_data or '{}')

    def update(self):
        data = self.live_list.get(reduce=True)
        json_data = self.to_json_fn(data)
        compressed_data = "gzip" + zlib.compress(json_data)
        self.client.set(self.reduced_data_node, compressed_data)

    def add(self, item):
        self.live_list.add(item)
        self.update()

    def remove(self, item):
        self.live_list.remove(item)
        self.update()

    def get(self, reduce=True):
        if reduce:
            return self.data
        else:
            return self.live_list.get(reduce=False)

    def __iter__(self):
        return iter(self.data)

    def __len__(self):
        return len(self.data)

    def __repr__(self):
        return "<%s %r>" % (self.__class__.__name__, self.data)


class IPNetworkLiveList(ReducedLiveList):
    def __init__(self, client, root, reduced_data_node):
        def ipnetwork_to_json(ipnetwork_list):
            d = json.dumps([str(ipn) for ipn in ipnetwork_list])
            return d

        def json_to_ipnetwork(d):
            ipnetwork_list = [ipaddress.ip_network(s) for s in json.loads(d)]
            return ipnetwork_list

        ReducedLiveList.__init__(
            self, client, root, reduced_data_node,
            map_fn=ipaddress.ip_network,
            reduce_fn=ipaddress.collapse_addresses,
            to_json_fn=ipnetwork_to_json,
            from_json_fn=json_to_ipnetwork,
        )
<EOF>
<BOF>
from datetime import datetime, timedelta
from dateutil.parser import parse as date_parse
import pytz
import string
from urllib import quote, unquote

from pylons import app_globals as g

from . import hooks
from .utils import randstr, to_epoch_milliseconds

LOID_COOKIE = "loid"
LOID_CREATED_COOKIE = "loidcreated"
# how long the cookie should last, by default.
EXPIRES_RELATIVE = timedelta(days=2 * 365)

GLOBAL_VERSION = 0
LOID_LENGTH = 18
LOID_CHARSPACE = string.uppercase + string.lowercase + string.digits


def isodate(d):
    # Python's `isoformat` isn't actually perfectly ISO.  This more
    # closely matches the format we were getting in JS
    d = d.astimezone(pytz.UTC)
    milliseconds = ("%06d" % d.microsecond)[0:3]
    return d.strftime("%Y-%m-%dT%H:%M:%S.") + milliseconds + "Z"


def ensure_unquoted(cookie_str):
    """Keep unquoting.  Never surrender.

    Some of the cookies issued in the first version of this patch ended up
    doubly quote()d.  As a preventative measure, unquote several times.
    [This could be a while loop, because every iteration will cause the str
    to at worst get shorter and at best stay the same and break the loop.  I
    just don't want to replace an escaping error with a possible infinite
    loop.]

    :param str cookie_str: Cookie string.
    """
    for _ in range(3):
        new_str = unquote(cookie_str)
        if new_str == cookie_str:
            return new_str
        cookie_str = new_str


class LoId(object):
    """Container for holding and validating logged out ids.

    The primary accessor functions for this class are:

     * :py:meth:`load` to pull the ``LoId`` out of the request cookies
     * :py:meth:`save` to save an ``LoId`` to cookies
     * :py:meth:`to_dict` to serialize this object's data to the event pipe
    """

    def __init__(
        self,
        request,
        context,
        loid=None,
        new=None,
        version=GLOBAL_VERSION,
        created=None,
        serializable=True
    ):
        self.context = context
        self.request = request

        # is this a newly generated ID?
        self.new = new
        # the unique ID
        self.loid = loid and str(loid)
        # When was this loid created
        self.created = created or datetime.now(pytz.UTC)

        self.version = version

        # should this be persisted as cookie?
        self.serializable = serializable
        # should this be re-written-out even if it's not new.
        self.dirty = new

    def _trigger_event(self, action):
        g.events.loid_event(
            loid=self,
            action_name=action,
            request=self.request,
            context=self.context,
        )

    @classmethod
    def _create(cls, request, context):
        """Create and return a new logged out id and timestamp.

        This also triggers an loid_event in the event pipeline.

        :param request: current :py:module:`pylons` request object
        :param context: current :py:module:`pylons` context object
        :rtype: :py:class:`LoId`
        :returns: new ``LoId``
        """
        loid = cls(
            request=request,
            context=context,
            new=True,
            loid=randstr(LOID_LENGTH, LOID_CHARSPACE),
        )
        loid._trigger_event("create_loid")
        return loid

    @classmethod
    def load(cls, request, context, create=True):
        """Load loid (and timestamp) from cookie or optionally create one.

        :param request: current :py:module:`pylons` request object
        :param context: current :py:module:`pylons` context object
        :param bool create: On failure to load from a cookie,
        :rtype: :py:class:`LoId`
        """
        stub = cls(request, context, serializable=False)

        loid = request.cookies.get(LOID_COOKIE)
        if loid:
            # future-proof to v1 id tracking
            loid, _, _ = unquote(loid).partition(".")
            try:
                created = ensure_unquoted(
                    request.cookies.get(LOID_CREATED_COOKIE, ""))
                created = date_parse(created)
            except ValueError:
                created = None
            return cls(
                request,
                context,
                new=False,
                loid=loid,
                version=0,
                created=created,
            )
        elif create:
            return cls._create(request, context)
        else:
            return stub

    def save(self, **cookie_attrs):
        """Write to cookie if serializable and dirty (generally new).

        :param dict cookie_attrs: additional cookie attrs.
        """
        if self.serializable and self.dirty:
            expires = datetime.utcnow() + EXPIRES_RELATIVE
            for (name, value) in (
                (LOID_COOKIE, self.loid),
                (LOID_CREATED_COOKIE, isodate(self.created)),
            ):
                d = cookie_attrs.copy()
                d.setdefault("expires", expires)
                self.context.cookies.add(name, value, **d)

    def to_dict(self, prefix=None):
        """Serialize LoId, generally for use in the event pipeline."""
        if not self.serializable:
            return {}

        d = {
            "loid": self.loid,
            "loid_created": to_epoch_milliseconds(self.created),
            "loid_new": self.new,
            "loid_version": self.version,
        }
        hook = hooks.get_hook("loid.to_dict")
        hook.call(loid=self, data=d)
        if prefix:
            d = {"{}{}".format(prefix, k): v for k, v in d.iteritems()}

        return d
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
import heapq

from pylons import app_globals as g

from r2.lib import count
from r2.lib.sgm import sgm
from r2.models.link import Link


def calc_rising():
    link_counts = count.get_link_counts()

    links = Link._by_fullname(link_counts.keys(), data=True)

    def score(link):
        count = link_counts[link._fullname][0]
        return float(link._ups) / max(count, 1)

    # build the rising list, excluding items having 1 or less upvotes
    rising = []
    for link in links.values():
        if link._ups > 1:
            rising.append((link._fullname, score(link), link.sr_id))

    # return rising sorted by score
    return sorted(rising, key=lambda x: x[1], reverse=True)


def set_rising():
    g.gencache.set("all:rising", calc_rising())


def get_all_rising():
    return g.gencache.get("all:rising", [], stale=True)


def get_rising(sr):
    rising = get_all_rising()
    return [link for link, score, sr_id in rising if sr.keep_for_rising(sr_id)]


def get_rising_tuples(sr_ids):
    rising = get_all_rising()

    tuples_by_srid = {sr_id: [] for sr_id in sr_ids}
    top_rising = {}

    for link, score, sr_id in rising:
        if sr_id not in sr_ids:
            continue

        if sr_id not in top_rising:
            top_rising[sr_id] = score

        norm_score = score / top_rising[sr_id]
        tuples_by_srid[sr_id].append((-norm_score, -score, link))

    return tuples_by_srid


def normalized_rising(sr_ids):
    if not sr_ids:
        return []

    tuples_by_srid = sgm(
        cache=g.gencache,
        keys=sr_ids,
        miss_fn=get_rising_tuples,
        prefix='rising:',
        time=90,
    )

    merged = heapq.merge(*tuples_by_srid.values())

    return [link_name for norm_score, score, link_name in merged]
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import app_globals as g

import json
import requests

BASE_URL = "https://api.createsend.com/api/v3.1/"
API_KEY = g.secrets['newsletter_api_key']
LIST_ID = g.newsletter_list_id

# under providers even though this is not yet a provider because this will be
# moved to become one, so we want the right stats namespace.
STATS_NAMESPACE = "providers.campaignmonitor"


class NewsletterError(Exception):
    pass


class EmailUnacceptableError(NewsletterError):
    pass


def add_subscriber(email, source=""):
    """Given an email, add this user to our upvoted newsletter.

    Optionally, also provide a source parameter describing where the subscribe
    came from - like "register".

    If the email was unable to be added, throws `NewsletterError`. Returns
    `True` on success.

    This should be used sparingly and outside of high traffic areas, as it
    requires a network call.
    """
    if not API_KEY or not LIST_ID:
        raise NewsletterError("Unable to subscribe user %s to newsletter. "
                              "API key or list ID not set." % email)

    params = {
        "EmailAddress": email
    }
    if source:
        params["CustomFields"] = [{"Key": "source", "Value": source}]

    timer = g.stats.get_timer('%s.add_subscriber' % STATS_NAMESPACE)
    timer.start()
    try:
        r = requests.post(
            "%s/subscribers/%s.json" % (BASE_URL, LIST_ID),
            json.dumps(params),
            timeout=5,
            auth=(API_KEY, 'x'),
        )
    except requests.exceptions.Timeout:
        g.stats.simple_event('%s.request.timeout' % STATS_NAMESPACE)
        raise NewsletterError("Unable to subscribe user %s to newsletter. "
                              "Request timed out." % email)
    except requests.exceptions.SSLError:
        g.stats.simple_event('%s.request.ssl_error' % STATS_NAMESPACE)
        raise NewsletterError("Unable to subscribe user %s to newsletter. "
                              "SSL Error." % email)
    else:
        if r.status_code == 201:
            return True
        elif r.status_code == 400:
            g.stats.simple_event('%s.request.email_unacceptable' %
                                 STATS_NAMESPACE)
            raise EmailUnacceptableError("Could not subscribe user %s to"
                                         "newsletter. Email was unacceptable, "
                                         "likely due to subscription status." %
                                         email)
        else:
            raise NewsletterError("Could not subscribe user %s to "
                                  "newsletter. Status code: %s" %
                                  (email, r.status_code))
    finally:
        timer.stop()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons.i18n import N_


class PermissionSet(dict):
    ALL = 'all'

    info = None

    def __init__(self, *args, **kwargs):
        super(PermissionSet, self).__init__(*args, **kwargs)

    @classmethod
    def loads(cls, encoded, validate=False):
        if not encoded:
            return cls()
        result = cls(((term[1:], term[0] == '+')
                     for term in encoded.split(',')))
        if result.get(cls.ALL) == False:
            del result[cls.ALL]
        if validate and not result.is_valid():
            raise ValueError
        return result

    def dumps(self):
        if self.is_superuser():
            return '+all'
        return ','.join('-+'[bool(v)] + k for k, v in sorted(self.iteritems()))

    def is_superuser(self):
        return bool(super(PermissionSet, self).get(self.ALL))

    def is_valid(self):
        if not self.info:
            return False
        for k in self:
            if k != self.ALL and k not in self.info:
                return False
        return True

    def get(self, key, default=None):
        if self.info and self.is_superuser():
            return True if key in self.info else default
        return super(PermissionSet, self).get(key, default)

    def __getitem__(self, key):
        if self.info and self.is_superuser():
            return key == self.ALL or key in self.info
        return super(PermissionSet, self).get(key, False)


class ModeratorPermissionSet(PermissionSet):
    info = dict(
        access=dict(
            title=N_('access'),
            description=N_('manage the lists of contributors and banned/muted users'),
        ),
        config=dict(
            title=N_('config'),
            description=N_('edit settings, sidebar, css, images, and AutoModerator config'),
        ),
        flair=dict(
            title=N_('flair'),
            description=N_('manage user flair, link flair, and flair templates'),
        ),
        mail=dict(
            title=N_('mail'),
            description=N_('read and reply to moderator mail'),
        ),
        posts=dict(
            title=N_('posts'),
            description=N_(
                'use the approve, remove, spam, distinguish, and nsfw buttons'),
        ),
        wiki=dict(
            title=N_('wiki'),
            description=N_('manage the wiki and access to the wiki'),
        ),
    )

    @classmethod
    def loads(cls, encoded, **kwargs):
        if encoded is None:
            return cls(all=True)
        return super(ModeratorPermissionSet, cls).loads(encoded, **kwargs)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################
"""A rate limit implementation with state in a key-value cache with item expiry.

The standalone functions (get_usage, get_timeslice, record_usage and the _multi
variants) adapt from the building blocks needed to compute a rate limit to the
key-value operations.

This protocol is best-effort, *not* atomic and transactional. Multiple
simultaneous accesses may push the actual rate limit above the intended limit.

The RateLimit class implements the "check a rate limit" and the "record a usage"
operations and a policy of how the rate limit configuration is loaded from the
application configuration.
"""

import collections
import time

import pylibmc

from pylons import app_globals as g

# AKA, a half open interval.
class TimeSlice(collections.namedtuple("TimeSlice", ["beginning", "end"])):

  @property
  def remaining(self):
      return self.end - int(time.time())


class RatelimitError(Exception):
    def __init__(self, e):
        self.wrapped = e

    def __str__(self):
        return str(self.wrapped)


def _make_ratelimit_cache_key(key_prefix, time_slice):
    # Short term rate limits: use a timestamp that's only valid for a day.
    fmt = '-%H%M%S'
    # Long term rate limits: use an unambigious timestamp.
    if time_slice.end - time_slice.beginning >= 86400:
        fmt = '-@%s'

    # enforce the "rl:" prefix for mcrouter
    prefix = "rl:" + key_prefix
    return prefix + time.strftime(fmt, time.gmtime(time_slice.beginning))


def get_timeslice(slice_seconds):
    """Return tuple describing the current slice given slice width.

    The elements of the tuple are:

    - `beginning`: seconds since epoch to beginning of time period
    - `end`: seconds since epoch to end of time period
    """

    now = int(time.time())
    slice_count = now // slice_seconds
    slice_start = int(slice_count * slice_seconds)
    slice_end = slice_start + slice_seconds
    return TimeSlice(slice_start, slice_end)


def record_usage(key_prefix, time_slice):
    """Record usage of a ratelimit for the specified time slice.

    The total usage (including this one) of the ratelimit is returned or
    RatelimitError is raised if something went wrong during the process.

    """

    key = _make_ratelimit_cache_key(key_prefix, time_slice)

    try:
        g.ratelimitcache.add(key, 0, time=time_slice.remaining)

        try:
            return g.ratelimitcache.incr(key)
        except pylibmc.NotFound:
            # Previous round of ratelimiting fell out in the
            # time between calling `add` and calling `incr`.
            now = int(time.time())
            if now < time_slice.end:
                g.ratelimitcache.add(key, 1, time=time_slice.end - now + 1)
                g.stats.simple_event("ratelimit.eviction")
            return 1
    except pylibmc.Error as e:
        raise RatelimitError(e)


def record_usage_multi(prefix_slices):
    """Record usage of multiple rate limits.

    If any of the of the rate limits expire during the processing of the
    function, the usage counts may be inaccurate and it is not defined
    which, if any, of the keys have been updated in the underlying cache.

    Arguments:
        prefix_slices: A list of (prefix, timeslice)

    Returns:
        A list of the usage counts in the same order as prefix_slices.

    Raises:
        RateLimitError if anything goes wrong.
        It is not defined which, if any, of the keys have been updated if this
        happens.

    """

    keys = [_make_ratelimit_cache_key(k, t) for k, t in prefix_slices]

    try:
        # Can't use add_multi because the various timeslices may be different.
        now = int(time.time())
        for key, (_, time_slice) in zip(keys, prefix_slices):
            g.ratelimitcache.add(key, 0, time=time_slice.end - now + 1)

        try:
            recent_usage = g.ratelimitcache.incr_multi(keys)
        except pylibmc.NotFound:
            # Some part of the previous round of ratelimiting fell out in the
            # time between calling `add` and calling `incr`.
            now = int(time.time())
            if now < time_slice.end:
                recent_usage = []
                for key, (_, time_slice) in zip(keys, prefix_slices):
                    if g.ratelimitcache.add(key, 1,
                                            time=time_slice.end - now + 1):
                        recent_usage.append(1)
                        g.stats.simple_event("ratelimit.eviction")
                    else:
                        recent_usage.append(g.ratelimitcache.get(key))
        return recent_usage
    except pylibmc.Error as e:
        raise RatelimitError(e)


def get_usage(key_prefix, time_slice):
    """Return the current usage of a ratelimit for the specified time slice."""

    key = _make_ratelimit_cache_key(key_prefix, time_slice)

    try:
        return g.ratelimitcache.get(key)
    except pylibmc.NotFound:
        return 0
    except pylibmc.Error as e:
        raise RatelimitError(e)


def get_usage_multi(prefix_slices):
    """Return the current usage of several rate limits.

    Arguments:
        prefix_slices: A list of (prefix, timeslice)

    Returns:
        A list of usages in the same order as prefix_slices
    """
    keys = [_make_ratelimit_cache_key(k, t) for k, t in prefix_slices]

    try:
        values = g.ratelimitcache.get_multi(keys)
        return [values.get(k, 0) for k in keys]
    except pylibmc.Error as e:
        raise RatelimitError(e)


class RateLimit(object):
    """A general purpose whole system rate limit.

    This class takes several basically-static properties for configuring the
    rate limit and logging and provides the basic operations of checking the
    limit and using the limit.

    The limit data is stored as a cluster of keys with a common prefix in the
    g.ratelimitcache memcached cluster.

    Subclasses should set the the parameter properties somehow. @property and in
    __init__ work fine if necessary.  This class is designed for ease of static
    declaration in the subclass definition while still allowing subclasses to
    use dynamic values when necessary.

    Properties:
        sample_rate: How frequently to log to g.stats, probabalistically.
            Defaults to 0.1.
        event_name: The g.stats event prefix.
        event_type: The g.stats event suffix.
        key: The ratelimitcache key prefix.
        limit: The count of events per self.seconds to allow.
        seconds: The length of the interval over which to limit rates.

    Example:
        class MyRateLimit(RateLimit):
            event_name = 'my_system'
            event_type = 'thing'
            key = 'my-rate-limit'
            # Limit to 10 times every 5 minutes
            limit = 10
            seconds = 300

        LIMIT = MyRateLimit()

        if LIMIT.check():
            do_something()
            LIMIT.record_usage()

        This code logs (at self.sample_rate)
            my_system.check_my_thing
            (if limited) my_system.thing_limit_hit
            (if not limited) my_system.set_thing_limit
    """
    sample_rate = 0.1

    def _record_event(self, event_type_template):
        g.stats.event_count(
            self.event_name,
            event_type_template.format(event_type=self.event_type),
            sample_rate=self.sample_rate)

    @property
    def timeslice(self):
        return get_timeslice(self.seconds)

    def _check(self, usage):
        self._record_event('check_{event_type}')
        below_limit = usage is None or usage < self.limit
        if not below_limit:
            self._record_event('{event_type}_limit_hit')
        return below_limit

    def check(self):
        """Check that the usage in the current timeslice is below the limit."""
        return self._check(get_usage(self.key, self.timeslice))

    @staticmethod
    def check_multi(ratelimits):
        """Check that all of the ratelimits can allow more usage."""
        usage = get_usage_multi([(r.key, r.timeslice) for r in ratelimits])
        return all(r._check(u) for u, r in zip(usage, ratelimits))

    def get_usage(self):
        """Get the usage of this limit. You should probably be using check()."""
        return get_usage(self.key, self.timeslice)

    def record_usage(self):
        """Record a new usage within the current timeslice."""
        self._record_event('set_{event_type}_limit')
        return record_usage(self.key, self.timeslice)

    @staticmethod
    def record_multi(ratelimits):
        """Record a new usage of all of the ratelimits.
        See record_usage_multi for everything that could go wrong.
        """
        for r in ratelimits:
            r._record_event('set_{event_type}_limit')
        return record_usage_multi([(r.key, r.timeslice) for r in ratelimits])


class LiveConfigRateLimit(RateLimit):
    """A RateLimit that derives its parameters from values in g.live_config.

    Properties:
        limit_live_key: The g.live_config key for the number per timespan
        seconds_live_key: The g.live_config key for the timespan over which the
            rate is limited, in seconds.
    """

    @property
    def seconds(self):
        return g.live_config[self.seconds_live_key]

    @property
    def limit(self):
        return g.live_config[self.limit_live_key]


class SimpleRateLimit(RateLimit):
    """Simple ratelimiting class.

    Useful for cases where we just want to be able to call record_usage() and
    check(). Does not record events to g.stats.

    """

    def __init__(self, name, seconds, limit):
        self.key = name
        self.seconds = seconds
        self.limit = limit

    def _record_event(self, event_type_template):
        # make this a no-op
        pass

    def record_and_check(self):
        self.record_usage()
        return self.check()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import Subreddit
from r2.lib.memoize import memoize
from r2.lib.db.operators import desc
from r2.lib import utils
from r2.lib.db import tdb_cassandra
from r2.lib.cache import CL_ONE

class SubredditsByPartialName(tdb_cassandra.View):
    _use_db = True
    _value_type = 'pickle'
    _connection_pool = 'main'
    _read_consistency_level = CL_ONE

def load_all_reddits():
    query_cache = {}

    q = Subreddit._query(Subreddit.c.type == 'public',
                         Subreddit.c._spam == False,
                         Subreddit.c._downs > 1,
                         sort = (desc('_downs'), desc('_ups')),
                         data = True)
    for sr in utils.fetch_things2(q):
        if sr.quarantine:
            continue
        name = sr.name.lower()
        for i in xrange(len(name)):
            prefix = name[:i + 1]
            names = query_cache.setdefault(prefix, [])
            if len(names) < 10:
                names.append((sr.name, sr.over_18))

    for name_prefix, subreddits in query_cache.iteritems():
        SubredditsByPartialName._set_values(name_prefix, {'tups': subreddits})

def search_reddits(query, include_over_18=True):
    query = str(query.lower())

    try:
        result = SubredditsByPartialName._byID(query)
        return [name for (name, over_18) in getattr(result, 'tups', [])
                if not over_18 or include_over_18]
    except tdb_cassandra.NotFound:
        return []

@memoize('popular_searches', stale=True, time=3600)
def popular_searches(include_over_18=True):
    top_reddits = Subreddit._query(Subreddit.c.type == 'public',
                                   sort = desc('_downs'),
                                   limit = 100,
                                   data = True)
    top_searches = {}
    for sr in top_reddits:
        if sr.quarantine:
            continue
        if sr.over_18 and not include_over_18:
            continue
        name = sr.name.lower()
        for i in xrange(min(len(name), 3)):
            query = name[:i + 1]
            r = search_reddits(query, include_over_18)
            top_searches[query] = r
    return top_searches

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from threading import local
from hashlib import md5
import cPickle as pickle
from copy import copy
from curses.ascii import isgraph
import logging
from time import sleep

from pylons import app_globals as g

import pylibmc
from _pylibmc import MemcachedError

import random

from pycassa import ColumnFamily
from pycassa.cassandra.ttypes import ConsistencyLevel

from r2.lib.utils import in_chunks, prefix_keys, trace, tup
from r2.lib.hardcachebackend import HardCacheBackend

# This is for use in the health controller
_CACHE_SERVERS = set()

class NoneResult(object): pass

class CacheUtils(object):
    # Caches that never expire entries should set this to true, so that
    # CacheChain can properly count hits and misses.
    permanent = False

    def incr_multi(self, keys, delta=1, prefix=''):
        for k in keys:
            try:
                self.incr(prefix + k, delta)
            except ValueError:
                pass

    def add_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            self.add(prefix+str(k), v, time = time)

    def get_multi(self, keys, prefix='', **kw):
        return prefix_keys(keys, prefix, lambda k: self.simple_get_multi(k, **kw))


class CMemcache(CacheUtils):
    def __init__(self,
                 name,
                 servers,
                 debug=False,
                 noreply=False,
                 no_block=False,
                 min_compress_len=512 * 1024,
                 num_clients=10,
                 binary=False):
        self.name = name
        self.servers = servers
        self.clients = pylibmc.ClientPool(n_slots = num_clients)

        for x in xrange(num_clients):
            client = pylibmc.Client(servers, binary=binary)
            behaviors = {
                'no_block': no_block, # use async I/O
                'tcp_nodelay': True, # no nagle
                '_noreply': int(noreply),
                'ketama': True, # consistent hashing
            }
            if not binary:
                behaviors['verify_keys'] = True

            client.behaviors.update(behaviors)
            self.clients.put(client)

        self.min_compress_len = min_compress_len

        _CACHE_SERVERS.update(servers)

    def get(self, key, default = None):
        with self.clients.reserve() as mc:
            ret = mc.get(str(key))
            if ret is None:
                return default
            return ret

    def get_multi(self, keys, prefix = ''):
        str_keys = [str(key) for key in keys]
        with self.clients.reserve() as mc:
            return mc.get_multi(str_keys, key_prefix=prefix)

    # simple_get_multi exists so that a cache chain can
    # single-instance the handling of prefixes for performance, but
    # pylibmc does this in C which is faster anyway, so CMemcache
    # implements get_multi itself. But the CacheChain still wants
    # simple_get_multi to be available for when it's already prefixed
    # them, so here it is
    simple_get_multi = get_multi

    def set(self, key, val, time=0):
        # pylibmc converts this number to an unsigned integer without warning
        if time < 0:
            raise ValueError("Rejecting negative TTL for key %s" % key)

        with self.clients.reserve() as mc:
            return mc.set(str(key), val, time=time,
                            min_compress_len = self.min_compress_len)

    def set_multi(self, keys, prefix='', time=0):
        if time < 0:
            raise ValueError("Rejecting negative TTL for key %s" % key)

        str_keys = {str(k): v for k, v in keys.iteritems()}
        with self.clients.reserve() as mc:
            return mc.set_multi(str_keys, key_prefix=prefix, time=time,
                                min_compress_len=self.min_compress_len)

    def add_multi(self, keys, prefix='', time=0):
        # pylibmc converts this number to an unsigned integer without warning
        if time < 0:
            raise ValueError("Rejecting negative TTL for key %s" % key)

        str_keys = {str(k): v for k, v in keys.iteritems()}
        with self.clients.reserve() as mc:
            return mc.add_multi(str_keys, key_prefix=prefix, time=time)

    def incr_multi(self, keys, prefix='', delta=1):
        str_keys = [str(key) for key in keys]
        with self.clients.reserve() as mc:
            return mc.incr_multi(str_keys, key_prefix=prefix, delta=delta)

    def append(self, key, val, time=0):
        # pylibmc converts this number to an unsigned integer without warning
        if time < 0:
            raise ValueError("Rejecting negative TTL for key %s" % key)

        with self.clients.reserve() as mc:
            return mc.append(str(key), val, time=time)

    def incr(self, key, delta=1, time=0):
        # ignore the time on these
        with self.clients.reserve() as mc:
            return mc.incr(str(key), delta)

    def add(self, key, val, time=0):
        # pylibmc converts this number to an unsigned integer without warning
        if time < 0:
            raise ValueError("Rejecting negative TTL for key %s" % key)

        try:
            with self.clients.reserve() as mc:
                return mc.add(str(key), val, time=time)
        except pylibmc.DataExists:
            return None

    def delete(self, key, time=0):
        with self.clients.reserve() as mc:
            return mc.delete(str(key))

    def delete_multi(self, keys, prefix=''):
        str_keys = [str(key) for key in keys]
        with self.clients.reserve() as mc:
            return mc.delete_multi(str_keys, key_prefix=prefix)

    def __repr__(self):
        return '<%s(%r)>' % (self.__class__.__name__,
                             self.servers)


class Mcrouter(CMemcache):
    """Wrapper class to make mcrouter appear like a regular memcached client.

    Expected behavior (benefits of mcrouter):
    * get() with a cache unresponsive will return `None` to be interpreted as a
      cache miss rather than raising MemcachedError.
    * get_multi() with a cache unresponsive returns only the values that were
      retrieved.

    Error cases:
    * set() with a cache unresponsive will raise a ServerError.
    * set_multi() with a cache unresponsive will raise a ServerError. Some of
      the writes may have succeeded, which is the same behavior in mcrouter and
      memcached.
    * add() same as set()
    * add_multi() same as set_multi()

    In all cases where mcrouter raises a ServerError memcached would raise a
    MemcachedError. This behavior is acceptable because ServerError inherits
    from MemcachedError.

    Special cases:
    * set() if we are using prefix routing and the key doesn't match any routes
      mcrouter will return `False`. This is converted to a MemcachedError but
      it's possibly more correct to depend on the client checking the return
      value and deciding how to proceed.

    Unhandled cases:
    * delete() with a cache unresponsive will return `False`, but memcached will
      raise a MemcachedError. This can't be simply interpreted as the error case
      because `False` is the correct return when deleting a key that doesn't
      exist. The caller must check the return value.
    * delete_multi() with a cache unresponsive will return `False`, but
      memcached will raise a MemcachedError. Same logic follows as delete().
    * incr() with a cache unresponsive will raise a NotFound exception, which is
      the same error as attempting to incr an un-set key.
    * incr_multi() with a cache unresponsive will raise a NotFound exception,
      but memcached will raise a MemcachedError. This can't be interpreted as
      being the error case and replaced with a MemcachedError because NotFound
      is a valid exception when attempting to incr keys that don't exist.

    """

    def set(self, key, val, time=0):
        success = CMemcache.set(self, key, val, time)

        if not success:
            # If we are using prefix routing and the key doesn't match any
            # routes mcrouter will return `False`.
            raise MemcachedError("set failed")
        else:
            return True


class HardCache(CacheUtils):
    backend = None
    permanent = True

    def __init__(self, gc):
        self.backend = HardCacheBackend(gc)

    def _split_key(self, key):
        tokens = key.split("-", 1)
        if len(tokens) != 2:
            raise ValueError("key %s has no dash" % key)

        category, ids = tokens
        return category, ids

    def set(self, key, val, time=0):
        if val == NoneResult:
            # NoneResult caching is for other parts of the chain
            return

        category, ids = self._split_key(key)
        self.backend.set(category, ids, val, time)

    def simple_get_multi(self, keys):
        results = {}
        category_bundles = {}
        for key in keys:
            category, ids = self._split_key(key)
            category_bundles.setdefault(category, []).append(ids)

        for category in category_bundles:
            idses = category_bundles[category]
            chunks = in_chunks(idses, size=50)
            for chunk in chunks:
                new_results = self.backend.get_multi(category, chunk)
                results.update(new_results)

        return results

    def set_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            if v != NoneResult:
                self.set(prefix+str(k), v, time=time)

    def get(self, key, default=None):
        category, ids = self._split_key(key)
        r = self.backend.get(category, ids)
        if r is None: return default
        return r

    def delete(self, key, time=0):
        # Potential optimization: When on a negative-result caching chain,
        # shove NoneResult throughout the chain when a key is deleted.
        category, ids = self._split_key(key)
        self.backend.delete(category, ids)

    def add(self, key, value, time=0):
        category, ids = self._split_key(key)
        return self.backend.add(category, ids, value, time=time)

    def incr(self, key, delta=1, time=0):
        category, ids = self._split_key(key)
        return self.backend.incr(category, ids, delta=delta, time=time)


class LocalCache(dict, CacheUtils):
    def __init__(self, *a, **kw):
        return dict.__init__(self, *a, **kw)

    def _check_key(self, key):
        if isinstance(key, unicode):
            key = str(key) # try to convert it first
        if not isinstance(key, str):
            raise TypeError('Key is not a string: %r' % (key,))

    def get(self, key, default=None):
        r = dict.get(self, key)
        if r is None: return default
        return r

    def simple_get_multi(self, keys):
        out = {}
        for k in keys:
            try:
                out[k] = self[k]
            except KeyError:
                pass
        return out

    def set(self, key, val, time = 0):
        # time is ignored on localcache
        self._check_key(key)
        self[key] = val

    def set_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            self.set(prefix+str(k), v, time=time)

    def add(self, key, val, time = 0):
        self._check_key(key)
        was = key in self
        self.setdefault(key, val)
        return not was

    def delete(self, key):
        if self.has_key(key):
            del self[key]

    def delete_multi(self, keys):
        for key in keys:
            if self.has_key(key):
                del self[key]

    def incr(self, key, delta=1, time=0):
        if self.has_key(key):
            self[key] = int(self[key]) + delta

    def decr(self, key, amt=1):
        if self.has_key(key):
            self[key] = int(self[key]) - amt

    def append(self, key, val, time = 0):
        if self.has_key(key):
            self[key] = str(self[key]) + val

    def prepend(self, key, val, time = 0):
        if self.has_key(key):
            self[key] = val + str(self[key])

    def replace(self, key, val, time = 0):
        if self.has_key(key):
            self[key] = val

    def flush_all(self):
        self.clear()

    def reset(self):
        self.clear()

    def __repr__(self):
        return "<LocalCache(%d)>" % (len(self),)


class TransitionalCache(CacheUtils):
    """A cache "chain" for moving keys to a new cluster live.

    `original_cache` is the cache chain previously in use
    `replacement_cache` is the new place for the keys using this chain to live.
    `key_transform` is an optional function to translate the key names into
    different names on the `replacement_cache`

    To use this cache chain, do three separate deployments as follows:

        * start dual-writing to the new pool by putting this chain in place
          with `read_original=True`.
        * cut reads over to the new pool after it is sufficiently heated up by
          deploying `read_original=False`.
        * remove this cache chain entirely and replace it with
          `replacement_cache`.

    This ensures that at any point, all apps regardless of their position in
    the push order will have a consistent view of the data in the cache pool as
    much as is possible.

    """

    def __init__(
            self, original_cache, replacement_cache, read_original,
            key_transform=None):
        self.original = original_cache
        self.replacement = replacement_cache
        self.read_original = read_original
        self.key_transform = key_transform

    @property
    def stats(self):
        if self.read_original:
            return self.original.stats
        else:
            return self.replacement.stats

    @property
    def read_chain(self):
        if self.read_original:
            return self.original
        else:
            return self.replacement

    @property
    def caches(self):
        if self.read_original:
            return self.original.caches
        else:
            return self.replacement.caches

    def transform_memcache_key(self, args, kwargs):
        """Use key_transform to transform keys and prefix.

        key_transform() returns (new_prefix, new_key)

        If "prefix" is specified in kwargs, the transformation will look like:
        key_transform("key", "old_prefix_") --> "new_prefix_", "key"

        If "prefix" is not specified in kwargs, it must already be part of the
        key, and the transformation looks like:
        key_transform("old_prefix_key") --> "", "new_prefix_key"

        We don't currently handle multiple gets or sets where the prefix is
        already prepended to the keys because the return values are different:
        get(["old_prefix_A", "old_prefix_B"])
        old:
            {"old_prefix_A": val, "old_prefix_B": val}
        new:
            {"new_prefix_A": val, "new_prefix_B": val}

        They must be looked up with a prefix:
        get(["A", "B"], prefix="old_prefix_")
        old:
            {"A": val, "B": val}
        new (translated to get(["A", "B"], prefix="new_prefix_"):
            {"A": val, "B": val}

        The special case of the above is for a single item lookup, where the
        return value does not include the key.

        We could handle the general multiple key case by maintaining a mapping
        of {old_key: new_key} and using that to transform the return value.

        """

        if self.key_transform:
            prefix = kwargs.get("prefix", "")
            new_kwargs = copy(kwargs)

            if isinstance(args[0], dict):
                assert prefix, "must include prefix"
                new_prefixes = []
                old_key_dict = args[0]
                new_key_dict = {}

                for old_key, val in old_key_dict.iteritems():
                    new_prefix, new_key = self.key_transform(old_key, prefix)
                    new_key_dict[new_key] = val
                    new_prefixes.append(new_prefix)

                assert all(p == new_prefixes[0] for p in new_prefixes[1:])
                new_kwargs["prefix"] = new_prefixes[0]
                new_args = (new_key_dict,) + args[1:]
            elif isinstance(args[0], (list, set, tuple)):
                assert prefix, "must include prefix"
                new_prefixes = []
                old_key_list = args[0]
                new_key_list = []

                for old_key in old_key_list:
                    new_prefix, new_key = self.key_transform(old_key, prefix)
                    new_key_list.append(new_key)
                    new_prefixes.append(new_prefix)

                assert all(p == new_prefixes[0] for p in new_prefixes[1:])
                new_kwargs["prefix"] = new_prefixes[0]
                new_args = (new_key_list,) + args[1:]
            else:
                # single keys can't specify a prefix
                _, new_key = self.key_transform(args[0])
                new_args = (new_key,) + args[1:]

            return new_args, new_kwargs
        else:
            return args, kwargs

    def make_get_fn(fn_name):
        def transitional_cache_get_fn(self, *args, **kwargs):
            if self.read_original:
                return getattr(self.original, fn_name)(*args, **kwargs)
            else:
                new_args, new_kwargs = self.transform_memcache_key(args, kwargs)
                return getattr(self.replacement, fn_name)(*new_args, **new_kwargs)
        return transitional_cache_get_fn

    get = make_get_fn("get")
    get_multi = make_get_fn("get_multi")
    simple_get_multi = make_get_fn("simple_get_multi")

    def make_set_fn(fn_name):
        def transitional_cache_set_fn(self, *args, **kwargs):
            ret_original = getattr(self.original, fn_name)(*args, **kwargs)

            new_args, new_kwargs = self.transform_memcache_key(args, kwargs)
            ret_replacement = getattr(self.replacement, fn_name)(*new_args, **new_kwargs)

            if self.read_original:
                return ret_original
            else:
                return ret_replacement
        return transitional_cache_set_fn

    add = make_set_fn("add")
    set = make_set_fn("set")
    append = make_set_fn("append")
    prepend = make_set_fn("prepend")
    replace = make_set_fn("replace")
    set_multi = make_set_fn("set_multi")
    add = make_set_fn("add")
    add_multi = make_set_fn("add_multi")
    incr = make_set_fn("incr")
    incr_multi = make_set_fn("incr_multi")
    decr = make_set_fn("decr")
    delete = make_set_fn("delete")
    delete_multi = make_set_fn("delete_multi")
    flush_all = make_set_fn("flush_all")


def cache_timer_decorator(fn_name):
    """Use to decorate CacheChain operations so timings will be recorded."""
    def wrap(fn):
        def timed_fn(self, *a, **kw):
            use_timer = kw.pop("use_timer", True)

            try:
                getattr(g, "log")
            except TypeError:
                # don't have access to g, maybe in a thread?
                return fn(self, *a, **kw)

            if use_timer and self.stats:
                publish = random.random() < g.stats.CACHE_SAMPLE_RATE
                cache_name = self.stats.cache_name
                timer_name = "cache.%s.%s" % (cache_name, fn_name)
                timer = g.stats.get_timer(timer_name, publish)
                timer.start()
            else:
                timer = None

            result = fn(self, *a, **kw)
            if timer:
                timer.stop()

            return result
        return timed_fn
    return wrap


class CacheChain(CacheUtils, local):
    def __init__(self, caches, cache_negative_results=False):
        self.caches = caches
        self.cache_negative_results = cache_negative_results
        self.stats = None

    def make_set_fn(fn_name):
        @cache_timer_decorator(fn_name)
        def fn(self, *a, **kw):
            ret = None
            for c in self.caches:
                ret = getattr(c, fn_name)(*a, **kw)
            return ret
        return fn

    # note that because of the naive nature of `add' when used on a
    # cache chain, its return value isn't reliable. if you need to
    # verify its return value you'll either need to make it smarter or
    # use the underlying cache directly
    add = make_set_fn('add')

    set = make_set_fn('set')
    append = make_set_fn('append')
    prepend = make_set_fn('prepend')
    replace = make_set_fn('replace')
    set_multi = make_set_fn('set_multi')
    add = make_set_fn('add')
    add_multi = make_set_fn('add_multi')
    incr = make_set_fn('incr')
    incr_multi = make_set_fn('incr_multi')
    decr = make_set_fn('decr')
    delete = make_set_fn('delete')
    delete_multi = make_set_fn('delete_multi')
    flush_all = make_set_fn('flush_all')
    cache_negative_results = False

    @cache_timer_decorator("get")
    def get(self, key, default = None, allow_local = True, stale=None):
        stat_outcome = False  # assume a miss until a result is found
        is_localcache = False
        try:
            for c in self.caches:
                is_localcache = isinstance(c, LocalCache)
                if not allow_local and is_localcache:
                    continue

                val = c.get(key)

                if val is not None:
                    if not c.permanent:
                        stat_outcome = True

                    #update other caches
                    for d in self.caches:
                        if c is d:
                            break # so we don't set caches later in the chain
                        d.set(key, val)

                    if val == NoneResult:
                        return default
                    else:
                        return val

            if self.cache_negative_results:
                for c in self.caches[:-1]:
                    c.set(key, NoneResult)

            return default
        finally:
            if self.stats:
                if stat_outcome:
                    if not is_localcache:
                        self.stats.cache_hit()
                else:
                    self.stats.cache_miss()

    def get_multi(self, keys, prefix='', allow_local = True, **kw):
        l = lambda ks: self.simple_get_multi(ks, allow_local = allow_local, **kw)
        return prefix_keys(keys, prefix, l)

    @cache_timer_decorator("get_multi")
    def simple_get_multi(self, keys, allow_local = True, stale=None,
                         stat_subname=None):
        out = {}
        need = set(keys)
        hits = 0
        local_hits = 0
        misses = 0
        for c in self.caches:
            is_localcache = isinstance(c, LocalCache)
            if not allow_local and is_localcache:
                continue

            if c.permanent and not misses:
                # Once we reach a "permanent" cache, we count any outstanding
                # items as misses.
                misses = len(need)

            if len(out) == len(keys):
                # we've found them all
                break

            r = c.simple_get_multi(need)
            #update other caches
            if r:
                if is_localcache:
                    local_hits += len(r)
                elif not c.permanent:
                    hits += len(r)

                for d in self.caches:
                    if c is d:
                        break # so we don't set caches later in the chain
                    d.set_multi(r)
                r.update(out)
                out = r
                need = need - set(r.keys())

        if need and self.cache_negative_results:
            d = dict((key, NoneResult) for key in need)
            for c in self.caches[:-1]:
                c.set_multi(d)

        out = dict((k, v)
                   for (k, v) in out.iteritems()
                   if v != NoneResult)

        if self.stats:
            if not misses:
                # If this chain contains no permanent caches, then we need to
                # count the misses here.
                misses = len(need)
            self.stats.cache_hit(hits, subname=stat_subname)
            self.stats.cache_miss(misses, subname=stat_subname)

        return out

    def __repr__(self):
        return '<%s %r>' % (self.__class__.__name__,
                            self.caches)

    def debug(self, key):
        print "Looking up [%r]" % key
        for i, c in enumerate(self.caches):
            print "[%d] %10s has value [%r]" % (i, c.__class__.__name__,
                                                c.get(key))

    def reset(self):
        # the first item in a cache chain is a LocalCache
        self.caches = (self.caches[0].__class__(),) +  self.caches[1:]

class MemcacheChain(CacheChain):
    pass

class HardcacheChain(CacheChain):
    def add(self, key, val, time=0):
        authority = self.caches[-1] # the authority is the hardcache
                                    # itself
        added_val = authority.add(key, val, time=time)
        for cache in self.caches[:-1]:
            # Calling set() rather than add() to ensure that all caches are
            # in sync and that de-syncs repair themselves
            cache.set(key, added_val, time=time)

        return added_val

    def accrue(self, key, time=0, delta=1):
        auth_value = self.caches[-1].get(key)

        if auth_value is None:
            auth_value = 0

        try:
            auth_value = int(auth_value) + delta
        except ValueError:
            raise ValueError("Can't accrue %s; it's a %s (%r)" %
                             (key, auth_value.__class__.__name__, auth_value))

        for c in self.caches:
            c.set(key, auth_value, time=time)

        return auth_value

    @property
    def backend(self):
        # the hardcache is always the last item in a HardCacheChain
        return self.caches[-1].backend

class StaleCacheChain(CacheChain):
    """A cache chain of two cache chains. When allowed by `stale`,
       answers may be returned by a "closer" but potentially older
       cache. Probably doesn't play well with NoneResult cacheing"""
    staleness = 30

    def __init__(self, localcache, stalecache, realcache):
        self.localcache = localcache
        self.stalecache = stalecache
        self.realcache = realcache
        self.caches = (localcache, realcache) # for the other
                                              # CacheChain machinery
        self.stats = None

    @cache_timer_decorator("get")
    def get(self, key, default=None, stale = False, **kw):
        if kw.get('allow_local', True) and key in self.localcache:
            return self.localcache[key]

        if stale:
            stale_value = self._getstale([key]).get(key, None)
            if stale_value is not None:
                if self.stats:
                    self.stats.cache_hit()
                    self.stats.stale_hit()
                return stale_value # never return stale data into the
                                   # LocalCache, or people that didn't
                                   # say they'll take stale data may
                                   # get it
            else:
                self.stats.stale_miss()

        value = self.realcache.get(key)
        if value is None:
            if self.stats:
                self.stats.cache_miss()
            return default

        if stale:
            self.stalecache.set(key, value, time=self.staleness)

        self.localcache.set(key, value)

        if self.stats:
            self.stats.cache_hit()

        return value

    @cache_timer_decorator("get_multi")
    def simple_get_multi(self, keys, stale=False, stat_subname=None, **kw):
        if not isinstance(keys, set):
            keys = set(keys)

        ret = {}
        local_hits = 0

        if kw.get('allow_local'):
            for k in list(keys):
                if k in self.localcache:
                    ret[k] = self.localcache[k]
                    keys.remove(k)
                    local_hits += 1

        if keys and stale:
            stale_values = self._getstale(keys)
            # never put stale data into the localcache
            for k, v in stale_values.iteritems():
                ret[k] = v
                keys.remove(k)

            stale_hits = len(stale_values)
            stale_misses = len(keys)
            if self.stats:
                self.stats.stale_hit(stale_hits, subname=stat_subname)
                self.stats.stale_miss(stale_misses, subname=stat_subname)

        if keys:
            values = self.realcache.simple_get_multi(keys)
            if values and stale:
                self.stalecache.set_multi(values, time=self.staleness)
            self.localcache.update(values)
            ret.update(values)

        if self.stats:
            misses = len(keys - set(ret.keys()))
            hits = len(ret) - local_hits
            self.stats.cache_hit(hits, subname=stat_subname)
            self.stats.cache_miss(misses, subname=stat_subname)

        return ret

    def _getstale(self, keys):
        # this is only in its own function to make tapping it for
        # debugging easier
        return self.stalecache.simple_get_multi(keys)

    def reset(self):
        newcache = self.localcache.__class__()
        self.localcache = newcache
        self.caches = (newcache,) +  self.caches[1:]
        if isinstance(self.realcache, CacheChain):
            assert isinstance(self.realcache.caches[0], LocalCache)
            self.realcache.caches = (newcache,) + self.realcache.caches[1:]

    def __repr__(self):
        return '<%s %r>' % (self.__class__.__name__,
                            (self.localcache, self.stalecache, self.realcache))

CL_ONE = ConsistencyLevel.ONE
CL_QUORUM = ConsistencyLevel.QUORUM


class Permacache(object):
    """Cassandra key/value column family backend with a cachechain in front.
    
    Probably best to not think of this as a cache but rather as a key/value
    datastore that's faster to access than cassandra because of the cache.

    """

    COLUMN_NAME = 'value'

    def __init__(self, cache_chain, column_family, lock_factory):
        self.cache_chain = cache_chain
        self.make_lock = lock_factory
        self.cf = column_family

    @classmethod
    def _setup_column_family(cls, column_family_name, client):
        cf = ColumnFamily(client, column_family_name,
                          read_consistency_level=CL_QUORUM,
                          write_consistency_level=CL_QUORUM)
        return cf

    def _backend_get(self, keys):
        keys, is_single = tup(keys, ret_is_single=True)
        rows = self.cf.multiget(keys, columns=[self.COLUMN_NAME])
        ret = {
            key: pickle.loads(columns[self.COLUMN_NAME])
            for key, columns in rows.iteritems()
        }
        if is_single:
            if ret:
                return ret.values()[0]
            else:
                return None
        else:
            return ret

    def _backend_set(self, key, val):
        keys = {key: val}
        ret = self._backend_set_multi(keys)
        return ret.get(key)

    def _backend_set_multi(self, keys, prefix=''):
        ret = {}
        with self.cf.batch():
            for key, val in keys.iteritems():
                rowkey = "%s%s" % (prefix, key)
                column = {self.COLUMN_NAME: pickle.dumps(val, protocol=2)}
                ret[key] = self.cf.insert(rowkey, column)
        return ret

    def _backend_delete(self, key):
        self.cf.remove(key)

    def get(self, key, default=None, allow_local=True, stale=False):
        val = self.cache_chain.get(
            key, default=None, allow_local=allow_local, stale=stale)

        if val is None:
            val = self._backend_get(key)
            if val:
                self.cache_chain.set(key, val)
        return val

    def set(self, key, val):
        self._backend_set(key, val)
        self.cache_chain.set(key, val)

    def set_multi(self, keys, prefix='', time=None):
        # time is sent by sgm but will be ignored
        self._backend_set_multi(keys, prefix=prefix)
        self.cache_chain.set_multi(keys, prefix=prefix)

    def pessimistically_set(self, key, value):
        """
        Sets a value in Cassandra but instead of setting it in memcached,
        deletes it from there instead. This is useful for the mr_top job which
        sets thousands of keys but almost all of them will never be read out of
        """
        self._backend_set(key, value)
        self.cache_chain.delete(key)

    def get_multi(self, keys, prefix='', allow_local=True, stale=False):
        call_fn = lambda k: self.simple_get_multi(k, allow_local=allow_local,
                                                  stale=stale)
        return prefix_keys(keys, prefix, call_fn)

    def simple_get_multi(self, keys, allow_local=True, stale=False):
        ret = self.cache_chain.simple_get_multi(
            keys, allow_local=allow_local, stale=stale)
        still_need = {key for key in keys if key not in ret}
        if still_need:
            from_cass = self._backend_get(keys)
            self.cache_chain.set_multi(from_cass)
            ret.update(from_cass)
        return ret

    def delete(self, key):
        self._backend_delete(key)
        self.cache_chain.delete(key)

    def mutate(self, key, mutation_fn, default=None, willread=True):
        """Mutate a Cassandra key as atomically as possible"""
        with self.make_lock("permacache_mutate", "mutate_%s" % key):
            # This has an edge-case where the cache chain was populated by a ONE
            # read rather than a QUORUM one just before running this. All reads
            # should use consistency level QUORUM.
            if willread:
                value = self.cache_chain.get(key, allow_local=False)
                if value is None:
                    value = self._backend_get(key)
            else:
                value = None

            # send in a copy in case they mutate it in-place
            new_value = mutation_fn(copy(value))

            if not willread or value != new_value:
                self._backend_set(key, new_value)
            self.cache_chain.set(key, new_value, use_timer=False)
        return new_value

    def __repr__(self):
        return '<%s %r %r>' % (self.__class__.__name__,
                            self.cache_chain, self.cf.column_family)


def test_cache(cache, prefix=''):
    #basic set/get
    cache.set('%s1' % prefix, 1)
    assert cache.get('%s1' % prefix) == 1

    #python data
    cache.set('%s2' % prefix, [1,2,3])
    assert cache.get('%s2' % prefix) == [1,2,3]

    #set multi, no prefix
    cache.set_multi({'%s3' % prefix:3, '%s4' % prefix: 4})
    assert cache.get_multi(('%s3' % prefix, '%s4' % prefix)) == {'%s3' % prefix: 3, 
                                                                 '%s4' % prefix: 4}

    #set multi, prefix
    cache.set_multi({'3':3, '4': 4}, prefix='%sp_' % prefix)
    assert cache.get_multi(('3', 4), prefix='%sp_' % prefix) == {'3':3, 4: 4}
    assert cache.get_multi(('%sp_3' % prefix, '%sp_4' % prefix)) == {'%sp_3'%prefix: 3,
                                                                     '%sp_4'%prefix: 4}

    # delete
    cache.set('%s1'%prefix, 1)
    assert cache.get('%s1'%prefix) == 1
    cache.delete('%s1'%prefix)
    assert cache.get('%s1'%prefix) is None

    cache.set('%s1'%prefix, 1)
    cache.set('%s2'%prefix, 2)
    cache.set('%s3'%prefix, 3)
    assert cache.get('%s1'%prefix) == 1 and cache.get('%s2'%prefix) == 2
    cache.delete_multi(['%s1'%prefix, '%s2'%prefix])
    assert (cache.get('%s1'%prefix) is None
            and cache.get('%s2'%prefix) is None
            and cache.get('%s3'%prefix) == 3)

    #incr
    cache.set('%s5'%prefix, 1)
    cache.set('%s6'%prefix, 1)
    cache.incr('%s5'%prefix)
    assert cache.get('%s5'%prefix) == 2
    cache.incr('%s5'%prefix,2)
    assert cache.get('%s5'%prefix) == 4
    cache.incr_multi(('%s5'%prefix, '%s6'%prefix), 1)
    assert cache.get('%s5'%prefix) == 5
    assert cache.get('%s6'%prefix) == 2

def test_multi(cache):
    from threading import Thread

    num_threads = 100
    num_per_thread = 1000

    threads = []
    for x in range(num_threads):
        def _fn(prefix):
            def __fn():
                for y in range(num_per_thread):
                    test_cache(cache,prefix=prefix)
            return __fn
        t = Thread(target=_fn(str(x)))
        t.start()
        threads.append(t)

    for thread in threads:
        thread.join()

# a cache that occasionally dumps itself to be used for long-running
# processes
class SelfEmptyingCache(LocalCache):
    def __init__(self, max_size=10*1000):
        self.max_size = max_size

    def maybe_reset(self):
        if len(self) > self.max_size:
            self.clear()

    def set(self, key, val, time=0):
        self.maybe_reset()
        return LocalCache.set(self,key,val,time)

    def add(self, key, val, time=0):
        self.maybe_reset()
        return LocalCache.add(self, key, val)


def _make_hashable(s):
    if isinstance(s, str):
        return s
    elif isinstance(s, unicode):
        return s.encode('utf-8')
    elif isinstance(s, (tuple, list)):
        return ','.join(_make_hashable(x) for x in s)
    elif isinstance(s, dict):
        return ','.join('%s:%s' % (_make_hashable(k), _make_hashable(v))
                        for (k, v) in sorted(s.iteritems()))
    else:
        return str(s)


def make_key_id(*a, **kw):
    h = md5()
    h.update(_make_hashable(a))
    h.update(_make_hashable(kw))
    return h.hexdigest()


def test_stale():
    from pylons import app_globals as g
    ca = g.gencache
    assert isinstance(ca, StaleCacheChain)

    ca.localcache.clear()

    ca.stalecache.set('foo', 'bar', time=ca.staleness)
    assert ca.stalecache.get('foo') == 'bar'
    ca.realcache.set('foo', 'baz')
    assert ca.realcache.get('foo') == 'baz'

    assert ca.get('foo', stale=True) == 'bar'
    ca.localcache.clear()
    assert ca.get('foo', stale=False) == 'baz'
    ca.localcache.clear()

    assert ca.get_multi(['foo'], stale=True) == {'foo': 'bar'}
    assert len(ca.localcache) == 0
    assert ca.get_multi(['foo'], stale=False) == {'foo': 'baz'}
    ca.localcache.clear()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


class CSRFPreventionException(Exception):
    pass


def csrf_exempt(fn):
    """Mark an endpoint as exempt from CSRF prevention checks"""
    fn.handles_csrf = True
    return fn


def check_controller_csrf_prevention(controller):
    """Check that the a controller and its handlers are properly protected
       from CSRF attacks"""
    if getattr(controller, 'handles_csrf', False):
        return

    # We're only interested in handlers that might mutate data
    mutating_methods = {"POST", "PUT", "PATCH", "DELETE"}

    for name, func in controller.__dict__.iteritems():
        method, sep, action = name.partition('_')
        if not action:
            continue
        if method not in mutating_methods:
            continue

        # Check if the handler has specified how it deals with CSRF
        if not getattr(func, 'handles_csrf', False):
            endpoint_name = ':'.join((controller.__name__, name))
            msg = ("Handlers that might mutate data must be "
                   "explicit about CSRF prevention: %s" % endpoint_name)
            raise CSRFPreventionException(msg)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
import os.path
import pkg_resources
from collections import OrderedDict


class Plugin(object):
    js = {}
    config = {}
    live_config = {}
    needs_static_build = False
    needs_translation = True
    errors = {}
    source_root_url = None

    def __init__(self, entry_point):
        self.entry_point = entry_point

    @property
    def name(self):
        return self.entry_point.name

    @property
    def path(self):
        module = sys.modules[type(self).__module__]
        return os.path.dirname(module.__file__)

    @property
    def template_dir(self):
        """Add module/templates/ as a template directory."""
        return os.path.join(self.path, 'templates')

    @property
    def static_dir(self):
        return os.path.join(self.path, 'public')

    def on_load(self, g):
        pass

    def add_js(self, module_registry=None):
        if not module_registry:
            from r2.lib import js
            module_registry = js.module

        for name, module in self.js.iteritems():
            if name not in module_registry:
                module_registry[name] = module
            else:
                module_registry[name].extend(module)

    def declare_queues(self, queues):
        pass

    def add_routes(self, mc):
        pass

    def load_controllers(self):
        pass

    def get_documented_controllers(self):
        return []


class PluginLoader(object):
    def __init__(self, working_set=None, plugin_names=None):
        self.working_set = working_set or pkg_resources.WorkingSet()

        if plugin_names is None:
            entry_points = self.available_plugins()
        else:
            entry_points = []
            for name in plugin_names:
                try:
                    entry_point = self.available_plugins(name).next()
                except StopIteration:
                    print >> sys.stderr, ("Unable to locate plugin "
                                          "%s. Skipping." % name)
                    continue
                else:
                    entry_points.append(entry_point)

        self.plugins = OrderedDict()
        for entry_point in entry_points:
            try:
                plugin_cls = entry_point.load()
            except Exception as e:
                if plugin_names:
                    # if this plugin was specifically requested, fail.
                    raise e
                else:
                    print >> sys.stderr, ("Error loading plugin %s (%s)."
                                          " Skipping." % (entry_point.name, e))
                    continue
            self.plugins[entry_point.name] = plugin_cls(entry_point)

    def __len__(self):
        return len(self.plugins)

    def __iter__(self):
        return self.plugins.itervalues()

    def __reversed__(self):
        return reversed(self.plugins.values())

    def __getitem__(self, key):
        return self.plugins[key]

    def available_plugins(self, name=None):
        return self.working_set.iter_entry_points('r2.plugin', name)

    def declare_queues(self, queues):
        for plugin in self:
            plugin.declare_queues(queues)

    def load_plugins(self, config):
        g = config['pylons.app_globals']
        for plugin in self:
            # Record plugin version
            entry = plugin.entry_point
            git_dir = os.path.join(entry.dist.location, '.git')
            g.record_repo_version(entry.name, git_dir)

            # Load plugin
            g.config.add_spec(plugin.config)
            config['pylons.paths']['templates'].insert(0, plugin.template_dir)
            plugin.add_js()
            plugin.on_load(g)

    def load_controllers(self):
        # this module relies on pylons.i18n._ at import time (for translating
        # messages) which isn't available 'til we're in request context.
        from r2.lib import errors

        for plugin in self:
            errors.add_error_codes(plugin.errors)
            plugin.load_controllers()

    def get_documented_controllers(self):
        for plugin in self:
            for controller, url_prefix in plugin.get_documented_controllers():
                yield controller, url_prefix
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import cStringIO
import gzip
import wsgiref.headers

from paste.util.mimeparse import parse_mime_type, desired_matches


ENCODABLE_CONTENT_TYPES = {
    "application/json",
    "application/javascript",
    "application/xml",
    "text/css",
    "text/csv",
    "text/html",
    "text/javascript",
    "text/plain",
    "text/xml",
}


class GzipMiddleware(object):
    """A middleware that transparently compresses content with gzip.

    Note: this middleware deliberately violates PEP-333 in three ways:

        - it disables the use of the "write()" callable.
        - it does content encoding which is a "hop-by-hop" feature.
        - it does not "yield at least one value each time its underlying
          application yields a value".

    None of these are an issue for the reddit application, but use at your
    own risk.

    """

    def __init__(self, app, compression_level, min_size):
        self.app = app
        self.compression_level = compression_level
        self.min_size = min_size

    def _start_response(self, status, response_headers, exc_info=None):
        self.status = status
        self.headers = response_headers
        self.exc_info = exc_info
        return self._write_not_implemented

    @staticmethod
    def _write_not_implemented(*args, **kwargs):
        """Raise an exception.

        This middleware doesn't work with the write callable.

        """
        raise NotImplementedError

    @staticmethod
    def content_length(headers, app_iter):
        """Return the content-length of this response as best as we can tell.

        If the application returned a Content-Length header we will trust it.
        If not, we are allowed by PEP-333 to attempt to determine the length of
        the app's iterable and if it's 1, use the length of the only chunk as
        the content-length.

        """
        content_length_header = headers["Content-Length"]

        if content_length_header:
            return int(content_length_header)

        try:
            app_iter_len = len(app_iter)
        except ValueError:
            return None  # streaming response; we're done here.

        if app_iter_len == 1:
            return len(app_iter[0])
        return None

    def should_gzip_response(self, headers, app_iter):
        # this middleware isn't smart enough to deal with stuff like ETags or
        # content ranges at the moment. let's just bail out. (this prevents
        # issues with pylons/paste's static file middleware)
        if "ETag" in headers:
            return False

        # here we are, violating pep-333 by looking at a hop-by-hop header
        # within the middleware chain. but this will prevent us from overriding
        # encoding done lower down in the app, if present. so it goes.
        if "Content-Encoding" in headers:
            return False

        # bail if we can't figure out how big it is or it's too small
        content_length = self.content_length(headers, app_iter)
        if not content_length or content_length < self.min_size:
            return False

        # make sure this is one of the content-types we're allowed to encode
        content_type = headers["Content-Type"]
        type, subtype, params = parse_mime_type(content_type)
        if "%s/%s" % (type, subtype) not in ENCODABLE_CONTENT_TYPES:
            return False

        return True

    @staticmethod
    def update_vary_header(headers):
        vary_headers = headers.get_all("Vary")
        del headers["Vary"]

        varies = []
        for vary_header in vary_headers:
            varies.extend(field.strip().lower()
                          for field in vary_header.split(","))

        if "*" in varies:
            varies = ["*"]
        elif "accept-encoding" not in varies:
            varies.append("accept-encoding")

        headers["Vary"] = ", ".join(varies)

    @staticmethod
    def request_accepts_gzip(environ):
        accept_encoding = environ.get("HTTP_ACCEPT_ENCODING", "identity")
        return "gzip" in desired_matches(["gzip"], accept_encoding)

    def __call__(self, environ, start_response):
        app_iter = self.app(environ, self._start_response)
        headers = wsgiref.headers.Headers(self.headers)

        response_compressible = self.should_gzip_response(headers, app_iter)
        if response_compressible:
            # this means that the sole factor left in determining whether or
            # not to gzip is the Accept-Encoding header; we should let
            # downstream caches know that this is the case with the Vary header
            self.update_vary_header(headers)

        if response_compressible and self.request_accepts_gzip(environ):
            headers["Content-Encoding"] = "gzip"

            response_buffer = cStringIO.StringIO()
            gzipper = gzip.GzipFile(fileobj=response_buffer, mode="wb",
                                    compresslevel=self.compression_level)
            try:
                for chunk in app_iter:
                    gzipper.write(chunk)
            finally:
                if hasattr(app_iter, "close"):
                    app_iter.close()

            gzipper.close()
            new_response = response_buffer.getvalue()
            encoded_app_iter = [new_response]
            response_buffer.close()

            headers["Content-Length"] = str(len(new_response))
        else:
            encoded_app_iter = app_iter

        # send the response
        start_response(self.status, self.headers, self.exc_info)
        return encoded_app_iter


def make_gzip_middleware(app, global_conf=None, compress_level=9, min_size=0):
    """Return a gzip-compressing middleware."""
    return GzipMiddleware(app, int(compress_level), int(min_size))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import copy

from pylons import app_globals as g

from r2.lib.memoize import memoize

LIVE_STATES = ['RUNNING', 'STARTING', 'WAITING', 'BOOTSTRAPPING']
COMPLETED = 'COMPLETED'
PENDING = 'PENDING'
NOTFOUND = 'NOTFOUND'


def get_live_clusters(emr_connection):
    ret = emr_connection.list_clusters(cluster_states=LIVE_STATES)
    return ret.clusters or []


@memoize('get_step_states', time=60, timeout=60)
def get_step_states(emr_connection, jobflowid):
    """Return the names and states of all steps in the jobflow.

    Memoized to prevent ratelimiting.

    """

    ret = emr_connection.list_steps(jobflowid)
    steps = []
    steps.extend(ret.steps)
    while hasattr(ret, "marker"):
        ret = emr_connection.list_steps(jobflowid, marker=ret.marker)
        steps.extend(ret.steps)

    ret = []
    for step in steps:
        start_str = step.status.timeline.creationdatetime
        ret.append((step.name, step.status.state, start_str))
    return ret


def get_step_state(emr_connection, jobflowid, step_name, update=False):
    """Return the state of a step.

    If jobflowid/step_name combination is not unique this will return the state
    of the most recent step.

    """

    g.reset_caches()
    steps = get_step_states(emr_connection, jobflowid, _update=update)

    for name, state, start in sorted(steps, key=lambda t: t[2], reverse=True):
        if name == step_name:
            return state
    else:
        return NOTFOUND


def get_jobflow_id(emr_connection, name):
    """Return id of the live cluster with specified name."""
    ret = emr_connection.list_clusters(cluster_states=LIVE_STATES)
    clusters = ret.clusters

    try:
        # clusters appear to be ordered by creation time
        return [cluster.id for cluster in clusters if cluster.name == name][0]
    except IndexError:
        return


def terminate_jobflow(emr_connection, jobflow_name):
    jobflow_id = get_jobflow_id(emr_connection, jobflow_name)
    if jobflow_id:
        emr_connection.terminate_jobflow(jobflow_id)


def modify_slave_count(emr_connection, jobflow_name, num_slaves=1):
    jobflow_id = get_jobflow_id(emr_connection, jobflow_name)
    if not jobflow_id:
        return

    ret = emr_connection.list_instance_groups(jobflow_id)

    try:
        instancegroup = [i for i in ret.instancegroups if i.name == "slave"][0]
    except IndexError:
        # no slave instance group
        return

    if instancegroup.requestedinstancecount != num_slaves:
        return

    msg = 'Modifying slave instance count of %s (%s -> %s)'
    print msg % (jobflow_name, instancegroup.requestedinstancecount, num_slaves)
    emr_connection.modify_instance_groups(instancegroup.id, num_slaves)


class EmrJob(object):
    def __init__(self, emr_connection, name, steps=[], setup_steps=[],
                 bootstrap_actions=[], log_uri=None, keep_alive=True,
                 ec2_keyname=None, hadoop_version='1.0.3',
                 ami_version='latest', master_instance_type='m1.small',
                 slave_instance_type='m1.small', num_slaves=1,
                 visible_to_all_users=True, job_flow_role=None,
                 service_role=None, tags=None):

        self.jobflowid = None
        self.conn = emr_connection
        self.name = name
        self.steps = steps
        self.setup_steps = setup_steps
        self.bootstrap_actions = bootstrap_actions
        self.log_uri = log_uri
        self.enable_debugging = bool(log_uri)
        self.keep_alive = keep_alive
        self.ec2_keyname = ec2_keyname
        self.hadoop_version = hadoop_version
        self.ami_version = ami_version
        self.master_instance_type = master_instance_type
        self.slave_instance_type = slave_instance_type
        self.num_instances = num_slaves + 1
        self.visible_to_all_users = visible_to_all_users
        self.job_flow_role = job_flow_role
        self.service_role = service_role
        self.tags = tags or {}

    def run(self):
        steps = copy(self.setup_steps)
        steps.extend(self.steps)

        job_flow_args = dict(name=self.name,
            steps=steps, bootstrap_actions=self.bootstrap_actions,
            keep_alive=self.keep_alive, ec2_keyname=self.ec2_keyname,
            hadoop_version=self.hadoop_version, ami_version=self.ami_version,
            master_instance_type=self.master_instance_type,
            slave_instance_type=self.slave_instance_type,
            num_instances=self.num_instances,
            enable_debugging=self.enable_debugging,
            log_uri=self.log_uri,
            visible_to_all_users=self.visible_to_all_users,
            job_flow_role=self.job_flow_role, service_role=self.service_role)

        self.jobflowid = self.conn.run_jobflow(**job_flow_args)
        if self.tags:
            assert isinstance(self.tags, dict)
            # The EMR Cluster name is distinct from the Name tag. The latter
            # is exportable as a cost allocation tag.
            self.tags["Name"] = self.name
            self.conn.add_tags(self.jobflowid, self.tags)

    def terminate(self):
        terminate_jobflow(self.conn, self.name)

    def modify_slave_count(self, num_slaves=1):
        modify_slave_count(self.conn, self.name, num_slaves)


class EmrException(Exception):
    def __init__(self, msg):
        self.msg = msg

    def __str__(self):
        return self.msg
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.config.extensions import get_api_subtype
from r2.lib.utils import tup
from r2.lib.captcha import get_iden
from r2.lib.wrapped import Wrapped, StringTemplate
from r2.lib.filters import websafe_json, spaceCompress
from r2.lib.base import BaseController
from r2.lib.pages.things import wrap_links
from r2.models import IDBuilder, Listing

import simplejson
from pylons import tmpl_context as c
from pylons import app_globals as g


class JsonResponse(object):
    """
    Simple Api response handler, returning a list of errors generated
    in the api func's validators, as well as blobs of data set by the
    api func.
    """

    content_type = 'application/json'

    def __init__(self):
        self._clear()

    def _clear(self):
        self._errors = set()
        self._new_captcha = False
        self._ratelimit = False
        self._data = {}

    def send_failure(self, error):
        c.errors.add(error)
        self._clear()
        self._errors.add((error, None))

    def __call__(self, *a, **kw):
        return self

    def __getattr__(self, key):
        return self

    def make_response(self):
        res = {}
        if self._data:
            res['data'] = self._data
        if self._new_captcha:
            res['captcha'] = get_iden()
        if self._ratelimit:
            res['ratelimit'] = self._ratelimit
        res['errors'] = [(e[0], c.errors[e].message, e[1]) for e in self._errors]
        return {"json": res}

    def set_error(self, error_name, field_name):
        self._errors.add((error_name, field_name))

    def has_error(self):
        return bool(self._errors)

    def has_errors(self, field_name, *errors, **kw):
        have_error = False
        field_name = tup(field_name)
        for error_name in errors:
            for fname in field_name:
                if (error_name, fname) in c.errors:
                    self.set_error(error_name, fname)
                    have_error = True
        return have_error

    def process_rendered(self, res):
        return res

    def _things(self, things, action, *a, **kw):
        """
        function for inserting/replacing things in listings.
        """
        things = tup(things)
        if not all(isinstance(t, Wrapped) for t in things):
            wrap = kw.pop('wrap', Wrapped)
            things = wrap_links(things, wrapper = wrap)
        data = [self.process_rendered(t.render()) for t in things]

        if kw:
            for d in data:
                if d.has_key('data'):
                    d['data'].update(kw)

        self._data['things'] = data
        return data

    def insert_things(self, things, append = False, **kw):
        return self._things(things, "insert_things", append, **kw)

    def replace_things(self, things, keep_children = False,
                       reveal = False, stubs = False, **kw):
        return self._things(things, "replace_things",
                            keep_children, reveal, stubs, **kw)

    def _send_data(self, **kw):
        self._data.update(kw)

    def new_captcha(self):
        self._new_captcha = True

    def ratelimit(self, seconds):
        self._ratelimit = seconds


class JQueryResponse(JsonResponse):
    """
    class which mimics the jQuery in javascript for allowing Dom
    manipulations on the client side.

    An instantiated JQueryResponse acts just like the "$" function on
    the JS layer with the exception of the ability to run arbitrary
    code on the client.  Selectors and method functions evaluate to
    new JQueryResponse objects, and the transformations are cataloged
    by the original object which can be iterated and sent across the
    wire.
    """
    def __init__(self, top_node = None):
        if top_node:
            self.top_node = top_node
        else:
            self.top_node = self
        JsonResponse.__init__(self)
        self._clear()

    def _clear(self):
        if self.top_node == self:
            self.objs = {self: 0}
            self.ops  = []
        else:
            self.objs = None
            self.ops  = None
        JsonResponse._clear(self)

    def process_rendered(self, res):
        if 'data' in res:
            if 'content' in res['data']:
                res['data']['content'] = spaceCompress(res['data']['content'])
        return res

    def send_failure(self, error):
        c.errors.add(error)
        self._clear()
        self._errors.add((self, error, None))
        self.refresh()

    def __call__(self, *a):
        return self.top_node.transform(self, "call", a)

    def __getattr__(self, key):
        if not key.startswith("__"):
            return self.top_node.transform(self, "attr", key)

    def transform(self, obj, op, args):
        new = self.__class__(self)
        newi = self.objs[new] = len(self.objs)
        self.ops.append([self.objs[obj], newi, op, args])
        return new

    def set_error(self, error_name, field_name):
        #self is the form that had the error checked, but we need to
        #add this error to the top_node of this response and give it a
        #reference to the form.
        self.top_node._errors.add((self, error_name, field_name))

    def has_error(self):
        return bool(self.top_node._errors)

    def make_response(self):
        #add the error messages
        for (form, error_name, field_name) in self._errors:
            selector = ".error." + error_name
            if field_name:
                selector += ".field-" + field_name
            message = c.errors[(error_name, field_name)].message
            form.find(selector).show().text(message).end()
        return {"jquery": self.ops,
                "success": not self.has_error()}

    # thing methods
    #--------------

    def _things(self, things, action, *a, **kw):
        data = JsonResponse._things(self, things, action, *a, **kw)
        new = self.__getattr__(action)
        return new(data, *a)

    def insert_table_rows(self, rows, index = -1):
        new = self.__getattr__("insert_table_rows")
        return new([row.render(style='html') for row in tup(rows)], index)


    # convenience methods:
    # --------------------
    #def _mark_error(self, e, field):
    #    self.find("." + e).show().html(c.errors[e].message).end()
    #
    #def _unmark_error(self, e):
    #    self.find("." + e).html("").end()

    def new_captcha(self):
        if not self._new_captcha:
            self.captcha(get_iden())
            self._new_captcha = True
        
    def get_input(self, name):
        return self.find("*[name=%s]" % name)

    def set_inputs(self, **kw):
        for k, v in kw.iteritems():
            # Using 'val' instead of setting the 'value' attribute allows this
            # To work for non-textbox inputs, like textareas
            self.get_input(k).val(v).end()
        return self

    def focus_input(self, name):
        return self.get_input(name).focus().end()

    def set_html(self, selector, value):
        if value:
            return self.find(selector).show().html(value).end()
        return self.find(selector).hide().html("").end()

    def set_text(self, selector, value):
        if value:
            return self.find(selector).show().text(value).end()
        return self.find(selector).hide().html("").end()

    def set(self, **kw):
        obj = self
        for k, v in kw.iteritems():
            obj = obj.attr(k, v)
        return obj

    def refresh(self):
        return self.top_node.transform(self, "refresh", [])

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import *
from r2.lib.normalized_hot import normalized_hot
from r2.lib import count
from r2.lib.utils import UniqueIterator, timeago

import random
from time import time

organic_max_length= 50


def cached_organic_links(*sr_ids):
    sr_count = count.get_link_counts()
    #only use links from reddits that you're subscribed to
    link_names = filter(lambda n: sr_count[n][1] in sr_ids, sr_count.keys())
    link_names.sort(key = lambda n: sr_count[n][0])

    if not link_names and g.debug:
        q = All.get_links('new', 'all')
        q._limit = 100 # this decomposes to a _query
        link_names = [x._fullname for x in q if x.promoted is None]
        g.log.debug('Used inorganic links')

    #potentially add an up and coming link
    if random.choice((True, False)) and sr_ids:
        sr_id = random.choice(sr_ids)
        fnames = normalized_hot([sr_id])
        if fnames:
            if len(fnames) == 1:
                new_item = fnames[0]
            else:
                new_item = random.choice(fnames[1:4])
            link_names.insert(0, new_item)

    return link_names

def organic_links(user):
    sr_ids = Subreddit.user_subreddits(user)
    # make sure that these are sorted so the cache keys are constant
    sr_ids.sort()

    # get the default subreddits if the user is not logged in
    user_id = None if isinstance(user, FakeAccount) else user
    sr_ids = Subreddit.user_subreddits(user, True)

    # pass the cached function a sorted list so that we can guarantee
    # cachability
    sr_ids.sort()
    return cached_organic_links(*sr_ids)[:organic_max_length]

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import subprocess
import tempfile
import difflib
from pylons.i18n import _
from pylons import app_globals as g


MAX_DIFF_LINE_LENGTH = 4000


class ConflictException(Exception):
    def __init__(self, new, your, original):
        self.your = your
        self.new = new
        self.original = original
        self.htmldiff = make_htmldiff(new, your, _("current edit"), _("your edit"))
        Exception.__init__(self)


def make_htmldiff(a, b, adesc, bdesc):
    diffcontent = difflib.HtmlDiff(wrapcolumn=60)

    def truncate(line):
        if len(line) > MAX_DIFF_LINE_LENGTH:
            line = line[:MAX_DIFF_LINE_LENGTH] + "..."
        return line
    return diffcontent.make_table([truncate(i) for i in a.splitlines()],
                                  [truncate(i) for i in b.splitlines()],
                                  fromdesc=adesc,
                                  todesc=bdesc,
                                  context=3)

def threewaymerge(original, a, b):
    temp_dir = g.diff3_temp_location if g.diff3_temp_location else None
    data = [a, original, b]
    files = []
    try:
        for d in data:
            f = tempfile.NamedTemporaryFile(dir=temp_dir)
            f.write(d.encode('utf-8'))
            f.flush()
            files.append(f)
        try:
            final = subprocess.check_output(["diff3", "-a", "--merge"] + [f.name for f in files])
        except subprocess.CalledProcessError:
            raise ConflictException(b, a, original)
    finally:
        for f in files:
            f.close()
    return final.decode('utf-8')

if __name__ == "__main__":
    class test_globals:
        diff3_temp_location = None
    
    g = test_globals()
    
    original = "Hello people of the human rance\n\nHow are you tday"
    a = "Hello people of the human rance\n\nHow are you today"
    b = "Hello people of the human race\n\nHow are you tday"
    
    print threewaymerge(original, a, b)
    
    g.diff3_temp_location = '/dev/shm'
    
    print threewaymerge(original, a, b)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib.db import queries
from r2.lib.db.tdb_sql import CreationError
from r2.lib import amqp
from r2.lib.utils import extract_user_mentions
from r2.models import query_cache, Thing, Comment, Account, Inbox, NotFound


def notify_mention(user, thing):
    try:
        inbox_rel = Inbox._add(user, thing, "mention")
    except CreationError:
        # this mention was already inserted, ignore it
        g.log.error("duplicate mention for (%s, %s)", user, thing)
        return

    with query_cache.CachedQueryMutator() as m:
        m.insert(queries.get_inbox_comment_mentions(user), [inbox_rel])
        queries.set_unread(thing, user, unread=True, mutator=m)


def remove_mention_notification(mention):
    inbox_owner = mention._thing1
    thing = mention._thing2
    with query_cache.CachedQueryMutator() as m:
        m.delete(queries.get_inbox_comment_mentions(inbox_owner), [mention])
        queries.set_unread(thing, inbox_owner, unread=False, mutator=m)


def readd_mention_notification(mention):
    """Reinsert into inbox after a comment has been unspammed"""
    inbox_owner = mention._thing1
    thing = mention._thing2
    with query_cache.CachedQueryMutator() as m:
        m.insert(queries.get_inbox_comment_mentions(inbox_owner), [mention])
        unread = getattr(mention, 'unread_preremoval', True)
        queries.set_unread(thing, inbox_owner, unread=unread, mutator=m)


def monitor_mentions(comment):
    if comment._spam or comment._deleted:
        return

    sender = comment.author_slow
    if getattr(sender, "butler_ignore", False):
        # this is an account that generates false notifications, e.g.
        # LinkFixer
        return

    if sender.in_timeout:
        return

    subreddit = comment.subreddit_slow
    usernames = extract_user_mentions(comment.body)
    inbox_class = Inbox.rel(Account, Comment)

    # If more than our allowed number of mentions were passed, don't highlight
    # any of them.
    if len(usernames) > g.butler_max_mentions:
        return

    # Subreddit.can_view stupidly requires this.
    c.user_is_loggedin = True

    for username in usernames:
        try:
            account = Account._by_name(username)
        except NotFound:
            continue

        # most people are aware of when they mention themselves.
        if account == sender:
            continue

        # bail out if that user has the feature turned off
        if not account.pref_monitor_mentions:
            continue

        # don't notify users of things they can't see
        if not subreddit.can_view(account):
            continue

        # don't notify users when a person they've blocked mentions them
        if account.is_enemy(sender):
            continue

        # ensure this comment isn't already in the user's inbox already
        rels = inbox_class._fast_query(
            account,
            comment,
            ("inbox", "selfreply", "mention"),
        )
        if filter(None, rels.values()):
            continue

        notify_mention(account, comment)


def run():
    @g.stats.amqp_processor("butler_q")
    def process_message(msg):
        fname = msg.body
        item = Thing._by_fullname(fname, data=True)
        monitor_mentions(item)

    amqp.consume_items("butler_q",
                       process_message,
                       verbose=True)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys

__all__ = ["export", "ExportError"]


class ExportError(Exception):
    def __init__(self, module):
        msg = "Missing __all__ declaration in module %s.  " \
              "@export cannot be used without declaring __all__ " \
              "in that module." % (module)
        Exception.__init__(self, msg)


def export(exported_entity):
    """Use a decorator to avoid retyping function/class names.
  
    * Based on an idea by Duncan Booth:
    http://groups.google.com/group/comp.lang.python/msg/11cbb03e09611b8a
    * Improved via a suggestion by Dave Angel:
    http://groups.google.com/group/comp.lang.python/msg/3d400fb22d8a42e1
    * Copied from Stack Overflow
    http://stackoverflow.com/questions/6206089/is-it-a-good-practice-to-add-names-to-all-using-a-decorator
    """
    all_var = sys.modules[exported_entity.__module__].__dict__.get('__all__')
    if all_var is None:
        raise ExportError(exported_entity.__module__)
    if exported_entity.__name__ not in all_var:  # Prevent duplicates if run from an IDE.
        all_var.append(exported_entity.__name__)
    return exported_entity

export(export)  # Emulate decorating ourself

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


funny_translatable_strings = {
    "500_page": ["Funny 500 page message %d" % i for i in xrange(1, 11)],
    "create_subreddit": [
        "Reason to create a reddit %d" % i for i in xrange(1, 21)],
}

def generate_strings():
    """Print out automatically generated strings for translation."""

    # used by error pages and in the sidebar for why to create a subreddit
    for category, strings in funny_translatable_strings.iteritems():
        for string in strings:
            print "# TRANSLATORS: Do not translate literally. Come up with a funny/relevant phrase (see the English version for ideas.) Accepts markdown formatting."
            print "print _('" + string + "')"

    # these are used in r2.lib.pages.trafficpages
    INTERVALS = ("hour", "day", "month")
    TYPES = ("uniques", "pageviews", "traffic", "impressions", "clicks")
    for interval in INTERVALS:
        for type in TYPES:
            print "print _('%s by %s')" % (type, interval)


if __name__ == "__main__":
    generate_strings()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import cProfile
import pstats
from functools import wraps, partial


def profile(fn):
    @wraps(fn)
    def _fn(*a, **kw):
        currently_profiling = cProfile.Profile()
        currently_profiling.enable()

        ret = fn(*a, **kw)

        currently_profiling.disable()
        stats = pstats.Stats(currently_profiling)
        stats.sort_stats('cumtime')
        stats.print_stats(.1)

        return ret
    return _fn
<EOF>
<BOF>
import hashlib
import hmac
from pylons import app_globals as g

# A map of cache policies to their respective cache headers
# loggedout omitted because loggedout responses are intentionally cacheable
CACHE_POLICY_DIRECTIVES = {
    "loggedin_www": {
        "cache-control": {"private", "no-cache"},
        "pragma": {"no-cache"},
        "expires": set(),
    },
    "loggedin_www_new": {
        "cache-control": {"private", "max-age=0", "must-revalidate"},
        "pragma": set(),
        "expires": {"-1"},
    },
    "loggedin_mweb": {
        "cache-control": {"private", "no-cache"},
        "pragma": set(),
        "expires": set(),
    },
}


def make_poisoning_report_mac(
        poisoner_canary,
        poisoner_name,
        poisoner_id,
        cache_policy,
        source,
        # Can't MAC based on URL, some caches don't care about the
        # order of query params and suchlike.
        route_name,
):
    """
    Make a MAC to send with cache poisoning reports for this page
    """
    mac_key = g.secrets["cache_poisoning"]
    mac_data = (
        poisoner_canary,
        poisoner_name,
        str(poisoner_id),
        cache_policy,
        source,
        route_name,
    )
    return hmac.new(mac_key, "|".join(mac_data), hashlib.sha1).hexdigest()


def cache_headers_valid(policy_name, headers):
    """Check if a response's headers make sense given a cache policy"""

    policy_headers = CACHE_POLICY_DIRECTIVES[policy_name]

    for header_name, expected_vals in policy_headers.items():
        # Cache-Control is a little special, you can have multiple directives
        # in multiple headers
        found_vals = set(headers.get(header_name, []))
        if header_name == "cache-control":
            parsed_cache_control = set()
            for cache_header in found_vals:
                for split_header in cache_header.split(","):
                    cache_directive = split_header.strip().lower()
                    parsed_cache_control.add(cache_directive)
            if parsed_cache_control != expected_vals:
                return False
        elif found_vals != expected_vals:
            return False
    return True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import cgi
import json
import re

from collections import Counter

import snudown

from BeautifulSoup import BeautifulSoup, Tag
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib.souptest import (
    souptest_fragment,
    SoupError,
    SoupUnsupportedEntityError,
)
from r2.lib.unicode import _force_utf8, _force_unicode

SC_OFF = "<!-- SC_OFF -->"
SC_ON = "<!-- SC_ON -->"

MD_START = '<div class="md">'
MD_END = '</div>'

WIKI_MD_START = '<div class="md wiki">'
WIKI_MD_END = '</div>'

custom_img_url = re.compile(r'\A%%([a-zA-Z0-9\-]+)%%$')

def python_websafe(text):
    return text.replace('&', "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")

def python_websafe_json(text):
    return text.replace('&', "&amp;").replace("<", "&lt;").replace(">", "&gt;")

try:
    from Cfilters import uwebsafe as c_websafe, uspace_compress, \
        uwebsafe_json as c_websafe_json
    def spaceCompress(text):
        try:
            text = unicode(text, 'utf-8')
        except TypeError:
            text = unicode(text)
        return uspace_compress(text)
except ImportError:
    c_websafe      = python_websafe
    c_websafe_json = python_websafe_json
    _between_tags1 = re.compile('> +')
    _between_tags2 = re.compile(' +<')
    _spaces = re.compile('[\s]+')
    _ignore = re.compile('(' + SC_OFF + '|' + SC_ON + ')', re.S | re.I)
    def spaceCompress(content):
        res = ''
        sc = True
        for p in _ignore.split(content):
            if p == SC_ON:
                sc = True
            elif p == SC_OFF:
                sc = False
            elif sc:
                p = _spaces.sub(' ', p)
                p = _between_tags1.sub('>', p)
                p = _between_tags2.sub('<', p)
                res += p
            else:
                res += p

        return res


class _Unsafe(unicode):
    # Necessary so Wrapped instances with these can get cached
    def cache_key(self, style):
        return unicode(self)


def unsafe(text=''):
    return _Unsafe(_force_unicode(text))

def websafe_json(text=""):
    return c_websafe_json(_force_unicode(text))

def double_websafe(text=""):
    # RSS requires double escaping on fields that could be interpreted as HTML
    return unsafe(python_websafe(python_websafe(text)))

def conditional_websafe(text = ''):
    from wrapped import Templated, CacheStub

    if text.__class__ == _Unsafe:
        return text
    elif isinstance(text, Templated):
        return _Unsafe(text.render())
    elif isinstance(text, CacheStub):
        return _Unsafe(text)
    elif text is None:
        return ""
    elif text.__class__ != unicode:
        text = _force_unicode(text)
    return c_websafe(text)


def mako_websafe(text=''):
    """Wrapper for conditional_websafe so cached templates don't explode"""
    return conditional_websafe(text)


def websafe(text=''):
    if text.__class__ != unicode:
        text = _force_unicode(text)
    #wrap the response in _Unsafe so make_websafe doesn't unescape it
    return _Unsafe(c_websafe(text))


# From https://github.com/django/django/blob/master/django/utils/html.py
_js_escapes = {
    ord('\\'): u'\\u005C',
    ord('\''): u'\\u0027',
    ord('"'): u'\\u0022',
    ord('>'): u'\\u003E',
    ord('<'): u'\\u003C',
    ord('&'): u'\\u0026',
    ord('='): u'\\u003D',
    ord('-'): u'\\u002D',
    ord(';'): u'\\u003B',
    ord(u'\u2028'): u'\\u2028',
    ord(u'\u2029'): u'\\u2029',
}
# Escape every ASCII character with a value less than 32.
_js_escapes.update((ord('%c' % z), u'\\u%04X' % z) for z in range(32))


def jssafe(text=u''):
    """Prevents text from breaking outside of string literals in JS"""
    if text.__class__ != unicode:
        text = _force_unicode(text)
    # wrap the response in _Unsafe so conditional_websafe doesn't touch it
    return _Unsafe(text.translate(_js_escapes))


_json_escapes = {
    ord('>'): u'\\u003E',
    ord('<'): u'\\u003C',
    ord('&'): u'\\u0026',
}


def scriptsafe_dumps(obj, **kwargs):
    """
    Like `json.dumps()`, but safe for use in `<script>` blocks.

    Also nice for response bodies that might be consumed by terrible browsers!

    You should avoid using this to template data into inline event handlers.
    When possible, you should do something like this instead:
    ```
    <button
      onclick="console.log($(this).data('json-thing'))"
      data-json-thing="${json_thing}">
    </button>
    ```
    """
    text = _force_unicode(json.dumps(obj, **kwargs))
    # wrap the response in _Unsafe so conditional_websafe doesn't touch it
    # TODO: this might be a hot path soon, C-ify it?
    return _Unsafe(text.translate(_json_escapes))


def markdown_souptest(text, nofollow=False, target=None, renderer='reddit'):
    if not text:
        return text
    
    if renderer == 'reddit':
        smd = safemarkdown(text, nofollow=nofollow, target=target)
    elif renderer == 'wiki':
        smd = wikimarkdown(text)

    souptest_fragment(smd)

    return smd

def safemarkdown(text, nofollow=False, wrap=True, **kwargs):
    if not text:
        return None

    target = kwargs.get("target", None)
    text = snudown.markdown(_force_utf8(text), nofollow, target)

    if wrap:
        return SC_OFF + MD_START + text + MD_END + SC_ON
    else:
        return SC_OFF + text + SC_ON

def wikimarkdown(text, include_toc=True, target=None):
    from r2.lib.template_helpers import make_url_protocol_relative

    # this hard codes the stylesheet page for now, but should be parameterized
    # in the future to allow per-page images.
    from r2.models.wiki import ImagesByWikiPage
    from r2.lib.utils import UrlParser
    from r2.lib.template_helpers import add_sr
    page_images = ImagesByWikiPage.get_images(c.site, "config/stylesheet")
    
    def img_swap(tag):
        name = tag.get('src')
        name = custom_img_url.search(name)
        name = name and name.group(1)
        if name and name in page_images:
            url = page_images[name]
            url = make_url_protocol_relative(url)
            tag['src'] = url
        else:
            tag.extract()
    
    nofollow = True
    
    text = snudown.markdown(_force_utf8(text), nofollow, target,
                            renderer=snudown.RENDERER_WIKI)
    
    # TODO: We should test how much of a load this adds to the app
    soup = BeautifulSoup(text.decode('utf-8'))
    images = soup.findAll('img')
    
    if images:
        [img_swap(image) for image in images]

    def add_ext_to_link(link):
        url = UrlParser(link.get('href'))
        if url.is_reddit_url():
            link['href'] = add_sr(link.get('href'), sr_path=False)

    if c.render_style == 'compact':
        links = soup.findAll('a')
        [add_ext_to_link(a) for a in links]

    if include_toc:
        tocdiv = generate_table_of_contents(soup, prefix="wiki")
        if tocdiv:
            soup.insert(0, tocdiv)
    
    text = str(soup)
    
    return SC_OFF + WIKI_MD_START + text + WIKI_MD_END + SC_ON

title_re = re.compile('[^\w.-]')
header_re = re.compile('^h[1-6]$')
def generate_table_of_contents(soup, prefix):
    header_ids = Counter()
    headers = soup.findAll(header_re)
    if not headers:
        return
    tocdiv = Tag(soup, "div", [("class", "toc")])
    parent = Tag(soup, "ul")
    parent.level = 0
    tocdiv.append(parent)
    level = 0
    previous = 0
    for header in headers:
        contents = u''.join(header.findAll(text=True))
        
        # In the event of an empty header, skip
        if not contents:
            continue
        
        # Convert html entities to avoid ugly header ids
        aid = unicode(BeautifulSoup(contents, convertEntities=BeautifulSoup.XML_ENTITIES))
        # Prefix with PREFIX_ to avoid ID conflict with the rest of the page
        aid = u'%s_%s' % (prefix, aid.replace(" ", "_").lower())
        # Convert down to ascii replacing special characters with hex
        aid = str(title_re.sub(lambda c: '.%X' % ord(c.group()), aid))
        
        # Check to see if a tag with the same ID exists
        id_num = header_ids[aid] + 1
        header_ids[aid] += 1
        # Only start numbering ids with the second instance of an id
        if id_num > 1:
            aid = '%s%d' % (aid, id_num)
        
        header['id'] = aid
        
        li = Tag(soup, "li", [("class", aid)])
        a = Tag(soup, "a", [("href", "#%s" % aid)])
        a.string = contents
        li.append(a)
        
        thislevel = int(header.name[-1])
        
        if previous and thislevel > previous:
            newul = Tag(soup, "ul")
            newul.level = thislevel
            newli = Tag(soup, "li", [("class", "toc_child")])
            newli.append(newul)
            parent.append(newli)
            parent = newul
            level += 1
        elif level and thislevel < previous:
            while level and parent.level > thislevel:
                parent = parent.findParent("ul")
                level -= 1
        
        previous = thislevel
        parent.append(li)
    
    return tocdiv


def keep_space(text):
    text = websafe(text)
    for i in " \n\r\t":
        text=text.replace(i,'&#%02d;' % ord(i))
    return unsafe(text)


def unkeep_space(text):
    return text.replace('&#32;', ' ').replace('&#10;', '\n').replace('&#09;', '\t')
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Methods and classes for inserting/removing from reddit's queues

There are three main ways of interacting with this module:

add_item: Adds a single item to a queue*
handle_items: For processing multiple items from a queue
consume_items: For processing a queue one item at a time


* _add_item (the internal function for adding items to amqp that are
  added using add_item) might block for an arbitrary amount of time
  while trying to get a connection to amqp.

"""
from Queue import Queue
from threading import local, Thread
from datetime import datetime
import os
import sys
import time
import errno
import socket
import itertools
import cPickle as pickle

from amqplib import client_0_8 as amqp

cfg = None
worker = None
connection_manager = None


def initialize(app_globals):
    global cfg
    cfg = Config(app_globals)
    global worker
    worker = Worker()
    global connection_manager
    connection_manager = ConnectionManager()


class Config(object):
    def __init__(self, g):
        self.amqp_host = g.amqp_host
        self.amqp_user = g.amqp_user
        self.amqp_pass = g.amqp_pass
        self.amqp_exchange = 'reddit_exchange'
        self.log = g.log
        self.amqp_virtual_host = g.amqp_virtual_host
        self.amqp_logging = g.amqp_logging
        self.stats = g.stats
        self.queues = g.queues
        self.reset_caches = g.reset_caches


class Worker:
    def __init__(self):
        self.q = Queue()
        self.t = Thread(target=self._handle)
        self.t.setDaemon(True)
        self.t.start()

    def _handle(self):
        while True:
            cfg.reset_caches()

            fn = self.q.get()
            try:
                fn()
                self.q.task_done()
            except:
                import traceback
                print traceback.format_exc()

    def do(self, fn, *a, **kw):
        fn1 = lambda: fn(*a, **kw)
        self.q.put(fn1)

    def join(self):
        self.q.join()


class ConnectionManager(local):
    # There should be only two threads that ever talk to AMQP: the
    # worker thread and the foreground thread (whether consuming queue
    # items or a shell). This class is just a wrapper to make sure
    # that they get separate connections
    def __init__(self):
        self.connection = None
        self.channel = None
        self.have_init = False

    def get_connection(self):
        while not self.connection:
            try:
                self.connection = amqp.Connection(
                    host=cfg.amqp_host,
                    userid=cfg.amqp_user,
                    password=cfg.amqp_pass,
                    virtual_host=cfg.amqp_virtual_host,
                    insist=False,
                )
            except (socket.error, IOError), e:
                print ('error connecting to amqp %s @ %s (%r)' %
                       (cfg.amqp_user, cfg.amqp_host, e))
                time.sleep(1)

        # don't run init_queue until someone actually needs it. this
        # allows the app server to start and serve most pages if amqp
        # isn't running
        if not self.have_init:
            self.init_queue()
            self.have_init = True

        return self.connection

    def get_channel(self, reconnect = False):
        # Periodic (and increasing with uptime) errors appearing when
        # connection object is still present, but appears to have been
        # closed.  This checks that the the connection is still open.
        if self.connection and self.connection.channels is None:
            cfg.log.error(
                "Error: amqp.py, connection object with no available channels."
                "  Reconnecting...")
            self.connection = None

        if not self.connection or reconnect:
            self.connection = None
            self.channel = None
            self.get_connection()

        if not self.channel:
            self.channel = self.connection.channel()

        return self.channel

    def init_queue(self):
        chan = self.get_channel()
        chan.exchange_declare(exchange=cfg.amqp_exchange,
                              type="direct",
                              durable=True,
                              auto_delete=False)

        for queue in cfg.queues:
            chan.queue_declare(queue=queue.name,
                               durable=queue.durable,
                               exclusive=queue.exclusive,
                               auto_delete=queue.auto_delete)

        for queue, key in cfg.queues.bindings:
            chan.queue_bind(routing_key=key,
                            queue=queue,
                            exchange=cfg.amqp_exchange)



DELIVERY_TRANSIENT = 1
DELIVERY_DURABLE = 2

def _add_item(routing_key, body, message_id = None,
              delivery_mode=DELIVERY_DURABLE, headers=None,
              exchange=None, send_stats=True):
    """adds an item onto a queue. If the connection to amqp is lost it
    will try to reconnect and then call itself again."""
    if not cfg.amqp_host:
        cfg.log.error("Ignoring amqp message %r to %r" % (body, routing_key))
        return
    if not exchange:
        exchange = cfg.amqp_exchange

    chan = connection_manager.get_channel()
    msg = amqp.Message(body,
                       timestamp = datetime.now(),
                       delivery_mode = delivery_mode)
    if message_id:
        msg.properties['message_id'] = message_id

    if headers:
        msg.properties["application_headers"] = headers

    event_name = 'amqp.%s' % routing_key
    try:
        chan.basic_publish(msg,
                           exchange=exchange,
                           routing_key = routing_key)
    except Exception as e:
        if send_stats:
            cfg.stats.event_count(event_name, 'enqueue_failed')

        if e.errno == errno.EPIPE:
            connection_manager.get_channel(True)
            add_item(routing_key, body, message_id)
        else:
            raise
    else:
        if send_stats:
            cfg.stats.event_count(event_name, 'enqueue')

def add_item(routing_key, body, message_id=None,
             delivery_mode=DELIVERY_DURABLE, headers=None,
             exchange=None, send_stats=True):
    if cfg.amqp_host and cfg.amqp_logging:
        cfg.log.debug("amqp: adding item %r to %r", body, routing_key)
    if exchange is None:
        exchange = cfg.amqp_exchange

    worker.do(_add_item, routing_key, body, message_id = message_id,
              delivery_mode=delivery_mode, headers=headers, exchange=exchange,
              send_stats=send_stats)

def add_kw(routing_key, **kw):
    add_item(routing_key, pickle.dumps(kw))

def consume_items(queue, callback, verbose=True):
    """A lighter-weight version of handle_items that uses AMQP's
       basic.consume instead of basic.get. Callback is only passed a
       single items at a time. This is more efficient than
       handle_items when the queue is likely to be occasionally empty
       or if batching the received messages is not necessary."""
    from pylons import tmpl_context as c

    chan = connection_manager.get_channel()

    # configure the amount of data rabbit will send down to our buffer before
    # we're ready for it (to reduce network latency). by default, it will send
    # as much as our buffers will allow.
    chan.basic_qos(
        # size in bytes of prefetch window. zero indicates no preference.
        prefetch_size=0,
        # maximum number of prefetched messages.
        prefetch_count=10,
        # if global, applies to the whole connection, else just this channel.
        a_global=False
    )

    def _callback(msg):
        if verbose:
            count_str = ''
            if 'message_count' in msg.delivery_info:
                # the count from the last message, if the count is
                # available
                count_str = '(%d remaining)' % msg.delivery_info['message_count']

            print "%s: 1 item %s" % (queue, count_str)

        cfg.reset_caches()
        c.use_write_db = {}

        ret = callback(msg)
        msg.channel.basic_ack(msg.delivery_tag)
        sys.stdout.flush()
        return ret

    chan.basic_consume(queue=queue, callback=_callback)

    try:
        while chan.callbacks:
            try:
                chan.wait()
            except KeyboardInterrupt:
                break
    finally:
        worker.join()
        if chan.is_open:
            chan.close()

def handle_items(queue, callback, ack=True, limit=1, min_size=0,
                 drain=False, verbose=True, sleep_time=1):
    """Call callback() on every item in a particular queue. If the
    connection to the queue is lost, it will die. Intended to be
    used as a long-running process."""
    if limit < min_size:
        raise ValueError("min_size must be less than limit")
    from pylons import tmpl_context as c

    chan = connection_manager.get_channel()
    countdown = None

    while True:
        # NB: None != 0, so we don't need an "is not None" check here
        if countdown == 0:
            break

        msg = chan.basic_get(queue)
        if not msg and drain:
            return
        elif not msg:
            time.sleep(sleep_time)
            continue

        if countdown is None and drain and 'message_count' in msg.delivery_info:
            countdown = 1 + msg.delivery_info['message_count']

        cfg.reset_caches()
        c.use_write_db = {}

        items = [msg]

        while countdown != 0:
            if countdown is not None:
                countdown -= 1
            if len(items) >= limit:
                break # the innermost loop only
            msg = chan.basic_get(queue)
            if msg is None:
                if len(items) < min_size:
                    time.sleep(sleep_time)
                else:
                    break
            else:
                items.append(msg)

        try:
            count_str = ''
            if 'message_count' in items[-1].delivery_info:
                # the count from the last message, if the count is
                # available
                count_str = '(%d remaining)' % items[-1].delivery_info['message_count']
            if verbose:
                print "%s: %d items %s" % (queue, len(items), count_str)
            callback(items, chan)

            if ack:
                # ack *all* outstanding messages
                chan.basic_ack(0, multiple=True)

            # flush any log messages printed by the callback
            sys.stdout.flush()
        except:
            for item in items:
                # explicitly reject the items that we've not processed
                chan.basic_reject(item.delivery_tag, requeue = True)
            raise


def empty_queue(queue):
    """debug function to completely erase the contents of a queue"""
    chan = connection_manager.get_channel()
    chan.queue_purge(queue)


def black_hole(queue):
    """continually empty out a queue as new items are created"""
    def _ignore(msg):
        print 'Ignoring msg: %r' % msg.body

    consume_items(queue, _ignore)

def dedup_queue(queue, rk = None, limit=None,
                delivery_mode = DELIVERY_DURABLE):
    """Hackily try to reduce the size of a queue by removing duplicate
       messages. The consumers of the target queue must consider
       identical messages to be idempotent. Preserves only message
       bodies"""
    chan = connection_manager.get_channel()

    if rk is None:
        rk = queue

    bodies = set()

    while True:
        msg = chan.basic_get(queue)

        if msg is None:
            break

        if msg.body not in bodies:
            bodies.add(msg.body)

        if limit is None:
            limit = msg.delivery_info.get('message_count')
            if limit is None:
                default_max = 100*1000
                print ("Message count was unavailable, defaulting to %d"
                       % (default_max,))
                limit = default_max
            else:
                print "Grabbing %d messages" % (limit,)
        else:
            limit -= 1
            if limit <= 0:
                break
            elif limit % 1000 == 0:
                print limit

    print "Grabbed %d unique bodies" % (len(bodies),)

    if bodies:
        for body in bodies:
            _add_item(rk, body, delivery_mode = delivery_mode)

        worker.join()

        chan.basic_ack(0, multiple=True)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class RequirementException(Exception):
    pass

def require(val):
    """A safe version of assert

    Assert can be stripped out if python is run in an optimized
    mode. This function implements assertions in a way that is
    guaranteed to execute.
    """
    if not val:
        raise RequirementException
    return val

def require_split(s, length, sep=None):
    require(s)
    res = s.split(sep)
    require(len(res) == length)
    return res
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib import count
from r2.models import Subreddit


def set_downs():
    sr_counts = count.get_sr_counts()
    names = [k for k, v in sr_counts.iteritems() if v != 0]
    srs = Subreddit._by_fullname(names)
    for name in names:
        sr,c = srs[name], sr_counts[name]
        if c != sr._downs and c > 0:
            sr._downs = max(c, 0)
            sr._commit()


def run():
    set_downs()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hmac
import hashlib
import urllib

from r2.config import feature
from r2.models import *
from filters import (
    _force_unicode,
    _force_utf8,
    conditional_websafe,
    keep_space,
    unsafe,
    double_websafe,
    websafe,
)
from r2.lib.cache_poisoning import make_poisoning_report_mac
from r2.lib.utils import UrlParser, timeago, timesince, is_subdomain

from r2.lib import hooks
from r2.lib.static import static_mtime
from r2.lib import js, tracking

import babel.numbers
import simplejson
import os.path
from copy import copy
import random
import urlparse
import calendar
import math
import time
import pytz

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, ungettext

static_text_extensions = {
    '.js': 'js',
    '.css': 'css',
    '.less': 'css'
}
def static(path, absolute=False, mangle_name=True):
    """
    Simple static file maintainer which automatically paths and
    versions files being served out of static.

    In the case of JS and CSS where g.uncompressedJS is set, the
    version of the file is set to be random to prevent caching and it
    mangles the path to point to the uncompressed versions.
    """
    dirname, filename = os.path.split(path)
    extension = os.path.splitext(filename)[1]
    is_text = extension in static_text_extensions
    should_cache_bust = False

    path_components = []
    actual_filename = None if mangle_name else filename

    # If building an absolute url, default to https because we like it and the
    # static server should support it.
    scheme = 'https' if absolute else None

    if g.static_domain:
        domain = g.static_domain
    else:
        path_components.append(c.site.static_path)

        if g.uncompressedJS:
            # unminified static files are in type-specific subdirectories
            if not dirname and is_text:
                path_components.append(static_text_extensions[extension])

            should_cache_bust = True
            actual_filename = filename

        domain = g.domain if absolute else None

    path_components.append(dirname)
    if not actual_filename:
        actual_filename = g.static_names.get(filename, filename)
    path_components.append(actual_filename)

    actual_path = os.path.join(*path_components)

    query = None
    if path and should_cache_bust:
        file_id = static_mtime(actual_path) or random.randint(0, 1000000)
        query = 'v=' + str(file_id)

    return urlparse.urlunsplit((
        scheme,
        domain,
        actual_path,
        query,
        None
    ))


def make_url_protocol_relative(url):
    if not url or url.startswith("//"):
        return url

    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)
    return urlparse.urlunsplit((None, netloc, path, query, fragment))


def make_url_https(url):
    if not url or url.startswith("https://"):
        return url

    scheme, netloc, path, query, fragment = urlparse.urlsplit(url)
    return urlparse.urlunsplit(("https", netloc, path, query, fragment))


def header_url(url, absolute=False):
    if url == g.default_header_url:
        return static(url, absolute=absolute)
    elif absolute:
        return make_url_https(url)
    else:
        return make_url_protocol_relative(url)


def js_config(extra_config=None):
    logged = c.user_is_loggedin and c.user.name
    user_id = c.user_is_loggedin and c.user._id
    user_in_timeout = c.user_is_loggedin and c.user.in_timeout
    gold = bool(logged and c.user.gold)
    controller_name = request.environ['pylons.routes_dict']['controller']
    action_name = request.environ['pylons.routes_dict']['action']
    route_name = controller_name + '.' + action_name

    cache_policy = "loggedout_www"
    if c.user_is_loggedin:
        cache_policy = "loggedin_www_new"

    # Canary for detecting cache poisoning
    poisoning_canary = None
    poisoning_report_mac = None
    if logged:
        if "pc" in c.cookies and len(c.cookies["pc"].value) == 2:
            poisoning_canary = c.cookies["pc"].value
            poisoning_report_mac = make_poisoning_report_mac(
                poisoner_canary=poisoning_canary,
                poisoner_name=logged,
                poisoner_id=user_id,
                cache_policy=cache_policy,
                source="web",
                route_name=route_name,
            )

    mac = hmac.new(g.secrets["action_name"], route_name, hashlib.sha1)
    verification = mac.hexdigest()
    cur_subreddit = ""
    cur_sr_fullname = ""
    cur_listing = ""
    listing_over_18 = False
    pref_no_profanity = not logged or c.user.pref_no_profanity
    pref_media_preview = c.user.pref_media_preview

    if not feature.is_enabled("autoexpand_media_previews"):
        expando_preference = None
    elif pref_media_preview == "subreddit":
        expando_preference = "subreddit_default"
    elif pref_media_preview == "on":
        expando_preference = "auto_expand"
    else:
        expando_preference = "do_not_expand"

    pref_beta = c.user.pref_beta
    nsfw_media_acknowledged = logged and c.user.nsfw_media_acknowledged

    if isinstance(c.site, Subreddit) and not c.default_sr:
        cur_subreddit = c.site.name
        cur_sr_fullname = c.site._fullname
        cur_listing = cur_subreddit
        listing_over_18 = c.site.over_18
    elif isinstance(c.site, DefaultSR):
        cur_listing = "frontpage"
    elif isinstance(c.site, FakeSubreddit):
        cur_listing = c.site.name

    if g.debug:
        events_collector_url = g.events_collector_test_url
        events_collector_key = g.secrets['events_collector_test_js_key']
        events_collector_secret = g.secrets['events_collector_test_js_secret']
    else:
        events_collector_url = g.events_collector_url
        events_collector_key = g.secrets['events_collector_js_key']
        events_collector_secret = g.secrets['events_collector_js_secret']

    config = {
        # is the user logged in?
        "logged": logged,
        # logged in user's id
        "user_id": user_id,
        # is user in timeout?
        "user_in_timeout": user_in_timeout,
        # the subreddit's name (for posts)
        "post_site": cur_subreddit,
        "cur_site": cur_sr_fullname,
        "cur_listing": cur_listing,
        # the user's voting hash
        "modhash": c.modhash or False,
        # the current rendering style
        "renderstyle": c.render_style,

        # they're welcome to try to override this in the DOM because we just
        # disable the features server-side if applicable
        'store_visits': gold and c.user.pref_store_visits,

        # current domain
        "cur_domain": get_domain(subreddit=False, no_www=True),
        # where do ajax requests go?
        "ajax_domain": get_domain(subreddit=False),
        "stats_domain": g.stats_domain or '',
        "stats_sample_rate": g.stats_sample_rate or 0,
        "extension": c.extension,
        "https_endpoint": is_subdomain(request.host, g.domain) and g.https_endpoint,
        "media_domain": g.media_domain,
        # does the client only want to communicate over HTTPS?
        "https_forced": feature.is_enabled("force_https"),
        # debugging?
        "debug": g.debug,
        "poisoning_canary": poisoning_canary,
        "poisoning_report_mac": poisoning_report_mac,
        "cache_policy": cache_policy,
        "send_logs": g.live_config["frontend_logging"],
        "server_time": math.floor(time.time()),
        "status_msg": {
          "fetching": _("fetching title..."),
          "submitting": _("submitting..."),
          "loading": _("loading...")
        },
        "is_fake": isinstance(c.site, FakeSubreddit),
        "tracker_url": "",  # overridden below if configured
        "adtracker_url": g.adtracker_url,
        "clicktracker_url": g.clicktracker_url,
        "uitracker_url": g.uitracker_url,
        "eventtracker_url": g.eventtracker_url,
        "anon_eventtracker_url": g.anon_eventtracker_url,
        "events_collector_url": events_collector_url,
        "events_collector_key": events_collector_key,
        "events_collector_secret": events_collector_secret,
        "feature_screenview_events": feature.is_enabled('screenview_events'),
        "static_root": static(''),
        "over_18": bool(c.over18),
        "listing_over_18": listing_over_18,
        "expando_preference": expando_preference,
        "pref_no_profanity": pref_no_profanity,
        "pref_beta": pref_beta,
        "nsfw_media_acknowledged": nsfw_media_acknowledged,
        "new_window": logged and bool(c.user.pref_newwindow),
        "mweb_blacklist_expressions": g.live_config['mweb_blacklist_expressions'],
        "gold": gold,
        "has_subscribed": logged and c.user.has_subscribed,
        "is_sponsor": logged and c.user_is_sponsor,
        "pageInfo": {
          "verification": verification,
          "actionName": route_name,
        },
        "facebook_app_id": g.live_config["facebook_app_id"],
        "feature_new_report_dialog": feature.is_enabled('new_report_dialog'),
        "email_verified": logged and c.user.email and c.user.email_verified,
    }

    if g.tracker_url:
        config["tracker_url"] = tracking.get_pageview_pixel_url()

    if g.uncompressedJS:
        config["uncompressedJS"] = True

    if extra_config:
        config.update(extra_config)

    hooks.get_hook("js_config").call(config=config)

    return config


class JSPreload(js.DataSource):
    def __init__(self, data=None):
        if data is None:
            data = {}
        js.DataSource.__init__(self, "r.preload.set({content})", data)

    def set(self, url, data):
        self.data[url] = data

    def set_wrapped(self, url, wrapped):
        from r2.lib.pages.things import wrap_things
        if not isinstance(wrapped, Wrapped):
            wrapped = wrap_things(wrapped)[0]
        self.data[url] = wrapped.render_nocache('api').finalize()

    def use(self):
        hooks.get_hook("js_preload.use").call(js_preload=self)

        if self.data:
            return js.DataSource.use(self)
        else:
            return ''


def class_dict():
    t_cls = [Link, Comment, Message, Subreddit]
    l_cls = [Listing, OrganicListing]

    classes  = [('%s: %s') % ('t'+ str(cl._type_id), cl.__name__ ) for cl in t_cls] \
             + [('%s: %s') % (cl.__name__, cl._js_cls) for cl in l_cls]

    res = ', '.join(classes)
    return unsafe('{ %s }' % res)


def comment_label(num_comments=None):
    if not num_comments:
        # generates "comment" the imperative verb
        com_label = _("comment {verb}")
        com_cls = 'comments empty may-blank'
    else:
        # generates "XX comments" as a noun
        com_label = ungettext("comment", "comments", num_comments)
        com_label = strings.number_label % dict(num=num_comments,
                                                thing=com_label)
        com_cls = 'comments may-blank'
    return com_label, com_cls

def replace_render(listing, item, render_func):
    def _replace_render(style = None, display = True):
        """
        A helper function for listings to set uncachable attributes on a
        rendered thing (item) to its proper display values for the current
        context.
        """
        style = style or c.render_style or 'html'
        replacements = {}

        if hasattr(item, 'child'):
            if item.child:
                replacements['childlisting'] = item.child.render(style=style)
            else:
                # Special case for when the comment tree wasn't built which
                # occurs both in the inbox and spam page view of comments.
                replacements['childlisting'] = None
        else:
            replacements['childlisting'] = ''

        #only LinkListing has a show_nums attribute
        if listing and hasattr(listing, "show_nums"):
            if listing.show_nums and item.num > 0:
                num = str(item.num)
            else:
                num = ""
            replacements["num"] = num

        if getattr(item, "rowstyle_cls", None):
            replacements["rowstyle"] = item.rowstyle_cls

        if hasattr(item, "num_comments"):
            com_label, com_cls = comment_label(item.num_comments)
            if style == "compact":
                com_label = unicode(item.num_comments)
            replacements['numcomments'] = com_label
            replacements['commentcls'] = com_cls

        if hasattr(item, "num_children"):
            label = ungettext("child", "children", item.num_children)
            numchildren_text = strings.number_label % {'num': item.num_children,
                                                       'thing': label}
            replacements['numchildren_text'] = numchildren_text

        replacements['display'] =  "" if display else "style='display:none'"

        if hasattr(item, "render_score"):
            # replace the score stub
            (replacements['scoredislikes'],
             replacements['scoreunvoted'],
             replacements['scorelikes'])  = item.render_score

        # compute the timesince here so we don't end up caching it
        if hasattr(item, "_date"):
            if hasattr(item, "promoted") and item.promoted is not None:
                from r2.lib import promote
                # promoted links are special in their date handling
                replacements['timesince'] = \
                    simplified_timesince(item._date - promote.timezone_offset)
            else:
                replacements['timesince'] = simplified_timesince(item._date)

        # compute the last edited time here so we don't end up caching it
        if hasattr(item, "editted") and not isinstance(item.editted, bool):
            replacements['lastedited'] = simplified_timesince(item.editted)

        renderer = render_func or item.render
        res = renderer(style = style, **replacements)

        if isinstance(res, (str, unicode)):
            rv = unsafe(res)
            if g.debug:
                for leftover in re.findall('<\$>(.+?)(?:<|$)', rv):
                    print "replace_render didn't replace %s" % leftover

            return rv

        return res

    return _replace_render

def get_domain(cname=False, subreddit=True, no_www=False):
    """
    returns the domain on the current subreddit, possibly including
    the subreddit part of the path, suitable for insertion after an
    "http://" and before a fullpath (i.e., something including the
    first '/') in a template.  The domain is updated to include the
    current port (request.port).  The effect of the arguments is:

     * no_www: if the domain ends up being g.domain, the default
       behavior is to prepend "www." to the front of it (for akamai).
       This flag will optionally disable it.

     * cname: deprecated.

     * subreddit: flags whether or not to append to the domain the
       subreddit path (without the trailing path).

    """
    # locally cache these lookups as this gets run in a loop in add_props
    domain = g.domain

    # c.domain_prefix is only set to non '' values, so we're safe to
    # override if it is falsy. we need to check this here because this method
    # might be getting called out of request, but c.domain_prefix is set in
    # request in MinimalController.pre.
    domain_prefix = c.domain_prefix or g.domain_prefix

    site = c.site

    if not no_www and domain_prefix:
        domain = domain_prefix + "." + domain

    if hasattr(request, "port") and request.port:
        domain += ":" + str(request.port)

    if subreddit:
        domain += site.path.rstrip('/')

    return domain


def add_sr(
        path, sr_path=True, nocname=False, force_hostname=False,
        retain_extension=True, force_https=False,
        force_extension=None):
    """
    Given a path (which may be a full-fledged url or a relative path),
    parses the path and updates it to include the subreddit path
    according to the rules set by its arguments:

     * sr_path: if a cname is not used for the domain, updates the
       path to include c.site.path.

     * nocname: deprecated.

     * force_hostname: if True, force the url's hostname to be updated
       even if it is already set in the path. If false, the path will still
       have its domain updated if no hostname is specified in the url.

     * retain_extension: if True, sets the extention according to
       c.render_style.

     * force_https: force the URL scheme to https

    For caching purposes: note that this function uses:
      c.render_style, c.site.name

    """
    # don't do anything if it is just an anchor
    if path.startswith(('#', 'javascript:')):
        return path

    u = UrlParser(path)
    if sr_path:
        u.path_add_subreddit(c.site)

    if not u.hostname or force_hostname:
        u.hostname = get_domain(subreddit=False)

    if (c.secure and u.is_reddit_url()) or force_https:
        u.scheme = "https"

    if force_extension is not None:
        u.set_extension(force_extension)
    elif retain_extension:
        if c.render_style == 'mobile':
            u.set_extension('mobile')

        elif c.render_style == 'compact':
            u.set_extension('compact')

    return u.unparse()

def join_urls(*urls):
    """joins a series of urls together without doubles slashes"""
    if not urls:
        return

    url = urls[0]
    for u in urls[1:]:
        if not url.endswith('/'):
            url += '/'
        while u.startswith('/'):
            u = utils.lstrips(u, '/')
        url += u
    return url

def style_line(button_width = None, bgcolor = "", bordercolor = ""):
    style_line = ''
    bordercolor = c.bordercolor or bordercolor
    bgcolor     = c.bgcolor or bgcolor
    if bgcolor:
        style_line += "background-color: #%s;" % bgcolor
    if bordercolor:
        style_line += "border: 1px solid #%s;" % bordercolor
    if button_width:
        style_line += "width: %spx;" % button_width
    return style_line

def choose_width(link, width):
    if width:
        return width - 5
    else:
        if hasattr(link, "_ups"):
            return 100 + (10 * (len(str(link._ups - link._downs))))
        else:
            return 110

# Appends to the list "attrs" a tuple of:
# <priority (higher trumps lower), letter,
#  css class, i18n'ed mouseover label, hyperlink (opt)>
def add_attr(attrs, kind, label=None, link=None, cssclass=None, symbol=None):
    from r2.lib.template_helpers import static

    symbol = symbol or kind

    if kind == 'F':
        priority = 1
        cssclass = 'friend'
        if not label:
            label = _('friend')
        if not link:
            link = '/prefs/friends'
    elif kind == 'S':
        priority = 2
        cssclass = 'submitter'
        if not label:
            label = _('submitter')
        if not link:
            raise ValueError ("Need a link")
    elif kind == 'M':
        priority = 3
        cssclass = 'moderator'
        if not label:
            raise ValueError ("Need a label")
        if not link:
            raise ValueError ("Need a link")
    elif kind == 'A':
        priority = 4
        cssclass = 'admin'
        if not label:
            label = _('reddit admin, speaking officially')
    elif kind in ('X', '@'):
        priority = 5
        cssclass = 'gray'
        if not label:
            raise ValueError ("Need a label")
    elif kind == 'V':
        priority = 6
        cssclass = 'green'
        if not label:
            raise ValueError ("Need a label")
    elif kind == 'B':
        priority = 7
        cssclass = 'wrong'
        if not label:
            raise ValueError ("Need a label")
    elif kind == 'special':
        priority = 98
    elif kind == "cake":
        priority = 99
        cssclass = "cakeday"
        symbol = "&#x1F370;"
        if not label:
            raise ValueError ("Need a label")
        if not link:
            raise ValueError ("Need a link")
    else:
        raise ValueError ("Got weird kind [%s]" % kind)

    attrs.append( (priority, symbol, cssclass, label, link) )


def add_admin_distinguish(distinguish_attribs_list):
    add_attr(distinguish_attribs_list, 'A')


def add_moderator_distinguish(distinguish_attribs_list, subreddit):
    link = '/r/%s/about/moderators' % subreddit.name
    label = _('moderator of /r/%(reddit)s, speaking officially')
    label %= {'reddit': subreddit.name}
    add_attr(distinguish_attribs_list, 'M', label=label, link=link)


def add_friend_distinguish(distinguish_attribs_list, note=None):
    if note:
        label = u"%s (%s)" % (_("friend"), _force_unicode(note))
    else:
        label = None
    add_attr(distinguish_attribs_list, 'F', label)


def add_cakeday_distinguish(distinguish_attribs_list, user):
    label = _("%(user)s just celebrated a reddit birthday!")
    label %= {"user": user.name}
    link = "/user/%s" % user.name
    add_attr(distinguish_attribs_list, kind="cake", label=label, link=link)


def add_special_distinguish(distinguish_attribs_list, user):
    args = user.special_distinguish()
    args.pop('name')
    if not args.get('kind'):
        args['kind'] = 'special'
    add_attr(distinguish_attribs_list, **args)


def add_submitter_distinguish(distinguish_attribs_list, link, subreddit):
    permalink = link.make_permalink(subreddit)
    add_attr(distinguish_attribs_list, 'S', link=permalink)


def search_url(query, subreddit, restrict_sr="off", sort=None, recent=None, ref=None):
    import urllib
    query = _force_utf8(query)
    url_query = {"q": query}
    if ref:
        url_query["ref"] = ref
    if restrict_sr:
        url_query["restrict_sr"] = restrict_sr
    if sort:
        url_query["sort"] = sort
    if recent:
        url_query["t"] = recent
    path = "/r/%s/search?" % subreddit if subreddit else "/search?"
    path += urllib.urlencode(url_query)
    return path


def format_number(number, locale=None):
    if not locale:
        locale = c.locale or g.locale

    return babel.numbers.format_number(number, locale=locale)


def format_percent(ratio, locale=None):
    if not locale:
        locale = c.locale or g.locale

    return babel.numbers.format_percent(ratio, locale=locale)


def html_datetime(date):
    # Strip off the microsecond to appease the HTML5 gods, since
    # datetime.isoformat() returns too long of a microsecond value.
    # http://www.whatwg.org/specs/web-apps/current-work/multipage/common-microsyntaxes.html#times
    return date.astimezone(pytz.UTC).replace(microsecond=0).isoformat()


def js_timestamp(date):
    return '%d' % (calendar.timegm(date.timetuple()) * 1000)


def simplified_timesince(date, include_tense=True):
    if date > timeago("1 minute"):
        return _("just now")

    since = timesince(date)
    if include_tense:
        return _("%s ago") % since
    else:
        return since


def display_link_karma(karma):
    if not c.user_is_admin:
        return max(karma, g.link_karma_display_floor)
    return karma


def display_comment_karma(karma):
    if not c.user_is_admin:
        return max(karma, g.comment_karma_display_floor)
    return karma


def format_html(format_string, *args, **kwargs):
    """
    Similar to str % foo, but passes all arguments through conditional_websafe,
    and calls 'unsafe' on the result. This function should be used instead
    of str.format or % interpolation to build up small HTML fragments.

    Example:

      format_html("Are you %s? %s", name, unsafe(checkbox_html))
    """
    if args and kwargs:
        raise ValueError("Can't specify both positional and keyword args")
    args_safe = tuple(map(conditional_websafe, args))
    kwargs_gen = ((k, conditional_websafe(v)) for (k, v) in kwargs.iteritems())
    kwargs_safe = dict(kwargs_gen)

    format_args = args_safe or kwargs_safe
    return unsafe(format_string % format_args)


def _ws(text, keep_spaces=False):
    """Helper function to get HTML escaped output from gettext"""
    if keep_spaces:
        return keep_space(_(text))
    else:
        return websafe(_(text))


def _wsf(format, keep_spaces=True, *args, **kwargs):
    """
    format_html, but with an escaped, translated string as the format str

    Sometimes trusted HTML needs to be included in a translatable string,
    but we don't trust translators to write HTML themselves.

    Example:

      _wsf("Are you %(name)s? %(box)s", name=name, box=unsafe(checkbox_html))
    """
    format_trans = _ws(format, keep_spaces)
    return format_html(format_trans, *args, **kwargs)


def get_linkflair_css_classes(thing, prefix="linkflair-", on_class="has-linkflair", off_class="no-linkflair"):
    has_linkflair =  thing.flair_text or thing.flair_css_class
    show_linkflair = c.user.pref_show_link_flair
    if has_linkflair and show_linkflair:
        if thing.flair_css_class:
            flair_css_classes = thing.flair_css_class.split()
            prefixed_css_classes = ["%s%s" % (prefix, css_class) for css_class in flair_css_classes]
            on_class = "%s %s" % (on_class, ' '.join(prefixed_css_classes))
        return on_class
    else:
        return off_class


def update_query(base_url, **kw):
    parsed = UrlParser(base_url)
    parsed.update_query(**kw)
    return parsed.unparse()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
from urlparse import urlparse

import base64
import ConfigParser
import locale
import json
import logging
import os
import re
import signal
import site
import socket
import subprocess
import sys

from sqlalchemy import engine, event
from baseplate import Baseplate, config as baseplate_config
from baseplate.thrift_pool import ThriftConnectionPool
from baseplate.context.thrift import ThriftContextFactory
from baseplate.server import einhorn

import pkg_resources
import pytz

from r2.config import queues
import r2.lib.amqp
from r2.lib.baseplate_integration import R2BaseplateObserver
from r2.lib.cache import (
    CacheChain,
    CL_ONE,
    CL_QUORUM,
    CMemcache,
    HardCache,
    HardcacheChain,
    LocalCache,
    Mcrouter,
    MemcacheChain,
    Permacache,
    SelfEmptyingCache,
    StaleCacheChain,
    TransitionalCache,
)
from r2.lib.configparse import ConfigValue, ConfigValueParser
from r2.lib.contrib import ipaddress
from r2.lib.contrib.activity_thrift import ActivityService
from r2.lib.contrib.activity_thrift.ttypes import ActivityInfo
from r2.lib.eventcollector import EventQueue
from r2.lib.lock import make_lock_factory
from r2.lib.manager import db_manager
from r2.lib.plugin import PluginLoader
from r2.lib.providers import select_provider
from r2.lib.stats import (
    CacheStats,
    StaleCacheStats,
    Stats,
    StatsCollectingConnectionPool,
)
from r2.lib.translation import get_active_langs, I18N_PATH
from r2.lib.utils import config_gold_price, thread_dump
from r2.lib.zookeeper import (
    connect_to_zookeeper,
    LiveConfig,
    IPNetworkLiveList,
)


LIVE_CONFIG_NODE = "/config/live"

SEARCH_SYNTAXES = {
        'cloudsearch': ('cloudsearch', 'lucene', 'plain'),
        'solr': ('solr', 'plain'),
        }

SECRETS_NODE = "/config/secrets"


def extract_live_config(config, plugins):
    """Gets live config out of INI file and validates it according to spec."""

    # ConfigParser will include every value in DEFAULT (which paste abuses)
    # if we do this the way we're supposed to. sorry for the horribleness.
    live_config = config._sections["live_config"].copy()
    del live_config["__name__"]  # magic value used by ConfigParser

    # parse the config data including specs from plugins
    parsed = ConfigValueParser(live_config)
    parsed.add_spec(Globals.live_config_spec)
    for plugin in plugins:
        parsed.add_spec(plugin.live_config)

    return parsed


def _decode_secrets(secrets):
    return {key: base64.b64decode(value) for key, value in secrets.iteritems()}


def extract_secrets(config):
    # similarly to the live_config one above, if we just did
    # .options("secrets") we'd get back all the junk from DEFAULT too. bleh.
    secrets = config._sections["secrets"].copy()
    del secrets["__name__"]  # magic value used by ConfigParser
    return _decode_secrets(secrets)


def fetch_secrets(zk_client):
    node_data = zk_client.get(SECRETS_NODE)[0]
    secrets = json.loads(node_data)
    return _decode_secrets(secrets)


PERMISSIONS = {
    "admin": "admin",
    "sponsor": "sponsor",
    "employee": "employee",
}


class PermissionFilteredEmployeeList(object):
    def __init__(self, config, type):
        self.config = config
        self.type = type

    def __iter__(self):
        return (username
                for username, permission in self.config["employees"].iteritems()
                if permission == self.type)

    def __getitem__(self, key):
        return list(self)[key]

    def __contains__(self, item):
        # since we do these permission checks off usernames, make it case
        # insensitive to relax config file editing pains (safe because
        # Account._by_name is case-insensitive)
        return any(item.lower() == username.lower() for username in self)

    def __repr__(self):
        return "<PermissionFilteredEmployeeList %r>" % (list(self),)


SHUTDOWN_CALLBACKS = []
def on_app_shutdown(arbiter, worker):
    for callback in SHUTDOWN_CALLBACKS:
        callback()


class Globals(object):
    spec = {

        ConfigValue.int: [
            'db_pool_size',
            'db_pool_overflow_size',
            'commentpane_cache_time',
            'num_mc_clients',
            'MAX_CAMPAIGNS_PER_LINK',
            'MIN_DOWN_LINK',
            'MIN_UP_KARMA',
            'MIN_DOWN_KARMA',
            'MIN_RATE_LIMIT_KARMA',
            'MIN_RATE_LIMIT_COMMENT_KARMA',
            'HOT_PAGE_AGE',
            'ADMIN_COOKIE_TTL',
            'ADMIN_COOKIE_MAX_IDLE',
            'OTP_COOKIE_TTL',
            'hsts_max_age',
            'num_comments',
            'max_comments',
            'max_comments_gold',
            'max_comment_parent_walk',
            'max_sr_images',
            'num_serendipity',
            'comment_visits_period',
            'butler_max_mentions',
            'min_membership_create_community',
            'bcrypt_work_factor',
            'cassandra_pool_size',
            'sr_banned_quota',
            'sr_muted_quota',
            'sr_wikibanned_quota',
            'sr_wikicontributor_quota',
            'sr_moderator_invite_quota',
            'sr_contributor_quota',
            'sr_quota_time',
            'sr_invite_limit',
            'thumbnail_hidpi_scaling',
            'wiki_keep_recent_days',
            'wiki_max_page_length_bytes',
            'wiki_max_page_name_length',
            'wiki_max_page_separators',
            'RL_RESET_MINUTES',
            'RL_OAUTH_RESET_MINUTES',
            'comment_karma_display_floor',
            'link_karma_display_floor',
            'mobile_auth_gild_time',
            'default_total_budget_pennies',
            'min_total_budget_pennies',
            'max_total_budget_pennies',
            'default_bid_pennies',
            'min_bid_pennies',
            'max_bid_pennies',
            'frequency_cap_min',
            'frequency_cap_default',
            'eu_cookie_max_attempts',
        ],

        ConfigValue.float: [
            'statsd_sample_rate',
            'querycache_prune_chance',
            'RL_AVG_REQ_PER_SEC',
            'RL_OAUTH_AVG_REQ_PER_SEC',
            'RL_LOGIN_AVG_PER_SEC',
            'RL_LOGIN_IP_AVG_PER_SEC',
            'RL_SHARE_AVG_PER_SEC',
            'tracing_sample_rate',
        ],

        ConfigValue.bool: [
            'debug',
            'log_start',
            'sqlprinting',
            'template_debug',
            'reload_templates',
            'uncompressedJS',
            'css_killswitch',
            'db_create_tables',
            'disallow_db_writes',
            'disable_ratelimit',
            'amqp_logging',
            'read_only_mode',
            'disable_wiki',
            'heavy_load_mode',
            'disable_captcha',
            'disable_ads',
            'disable_require_admin_otp',
            'trust_local_proxies',
            'shard_commentstree_queues',
            'shard_author_query_queues',
            'shard_subreddit_query_queues',
            'shard_domain_query_queues',
            'authnet_validate',
            'ENFORCE_RATELIMIT',
            'RL_SITEWIDE_ENABLED',
            'RL_OAUTH_SITEWIDE_ENABLED',
            'enable_loggedout_experiments',
        ],

        ConfigValue.tuple: [
            'plugins',
            'stalecaches',
            'lockcaches',
            'permacache_memcaches',
            'cassandra_seeds',
            'automatic_reddits',
            'hardcache_categories',
            'case_sensitive_domains',
            'known_image_domains',
            'reserved_subdomains',
            'offsite_subdomains',
            'TRAFFIC_LOG_HOSTS',
            'exempt_login_user_agents',
            'autoexpand_media_types',
            'media_preview_domain_whitelist',
            'multi_icons',
            'hide_subscribers_srs',
            'mcrouter_addr',
        ],

        ConfigValue.tuple_of(ConfigValue.int): [
            'thumbnail_size',
            'preview_image_max_size',
            'preview_image_min_size',
            'mobile_ad_image_size',
        ],

        ConfigValue.tuple_of(ConfigValue.float): [
            'ios_versions',
            'android_versions',
        ],

        ConfigValue.dict(ConfigValue.str, ConfigValue.int): [
            'user_agent_ratelimit_regexes',
        ],

        ConfigValue.str: [
            'wiki_page_registration_info',
            'wiki_page_privacy_policy',
            'wiki_page_user_agreement',
            'wiki_page_gold_bottlecaps',
            'fraud_email',
            'feedback_email',
            'share_reply',
            'community_email',
            'smtp_server',
            'events_collector_url',
            'events_collector_test_url',
            'search_provider',
        ],

        ConfigValue.choice(ONE=CL_ONE, QUORUM=CL_QUORUM): [
             'cassandra_rcl',
             'cassandra_wcl',
        ],

        ConfigValue.choice(zookeeper="zookeeper", config="config"): [
            "liveconfig_source",
            "secrets_source",
        ],

        ConfigValue.timeinterval: [
            'ARCHIVE_AGE',
            "vote_queue_grace_period",
        ],

        config_gold_price: [
            'gold_month_price',
            'gold_year_price',
            'cpm_selfserve',
            'cpm_selfserve_geotarget_metro',
            'cpm_selfserve_geotarget_country',
            'cpm_selfserve_collection',
        ],

        ConfigValue.baseplate(baseplate_config.Optional(baseplate_config.Endpoint)): [
            "activity_endpoint",
            "tracing_endpoint",
        ],

        ConfigValue.dict(ConfigValue.str, ConfigValue.str): [
            'emr_traffic_tags',
        ],
    }

    live_config_spec = {
        ConfigValue.bool: [
            'frontend_logging',
            'mobile_gild_first_login',
            'precomputed_comment_suggested_sort',
        ],
        ConfigValue.int: [
            'captcha_exempt_comment_karma',
            'captcha_exempt_link_karma',
            'create_sr_account_age_days',
            'create_sr_comment_karma',
            'create_sr_link_karma',
            'cflag_min_votes',
            'ads_popularity_threshold',
            'precomputed_comment_sort_min_comments',
            'comment_vote_update_threshold',
            'comment_vote_update_period',
        ],
        ConfigValue.float: [
            'cflag_lower_bound',
            'cflag_upper_bound',
            'spotlight_interest_sub_p',
            'spotlight_interest_nosub_p',
            'gold_revenue_goal',
            'invalid_key_sample_rate',
            'events_collector_vote_sample_rate',
            'events_collector_poison_sample_rate',
            'events_collector_mod_sample_rate',
            'events_collector_quarantine_sample_rate',
            'events_collector_modmail_sample_rate',
            'events_collector_report_sample_rate',
            'events_collector_submit_sample_rate',
            'events_collector_comment_sample_rate',
            'events_collector_use_gzip_chance',
            'https_cert_testing_probability',
        ],
        ConfigValue.tuple: [
            'fastlane_links',
            'listing_chooser_sample_multis',
            'discovery_srs',
            'proxy_gilding_accounts',
            'mweb_blacklist_expressions',
            'global_loid_experiments',
            'precomputed_comment_sorts',
            'mailgun_domains',
        ],
        ConfigValue.str: [
            'listing_chooser_gold_multi',
            'listing_chooser_explore_sr',
        ],
        ConfigValue.messages: [
            'welcomebar_messages',
            'sidebar_message',
            'gold_sidebar_message',
        ],
        ConfigValue.dict(ConfigValue.str, ConfigValue.int): [
            'ticket_groups',
            'ticket_user_fields', 
        ],
        ConfigValue.dict(ConfigValue.str, ConfigValue.float): [
            'pennies_per_server_second',
        ],
        ConfigValue.dict(ConfigValue.str, ConfigValue.str): [
            'employee_approved_clients',
            'modmail_forwarding_email',
            'modmail_account_map',
        ],
        ConfigValue.dict(ConfigValue.str, ConfigValue.choice(**PERMISSIONS)): [
            'employees',
        ],
    }

    def __init__(self, config, global_conf, app_conf, paths, **extra):
        """
        Globals acts as a container for objects available throughout
        the life of the application.

        One instance of Globals is created by Pylons during
        application initialization and is available during requests
        via the 'g' variable.

        ``config``
            The PylonsConfig object passed in from ``config/environment.py``

        ``global_conf``
            The same variable used throughout ``config/middleware.py``
            namely, the variables from the ``[DEFAULT]`` section of the
            configuration file.

        ``app_conf``
            The same ``kw`` dictionary used throughout
            ``config/middleware.py`` namely, the variables from the
            section in the config file for your application.

        ``extra``
            The configuration returned from ``load_config`` in 
            ``config/middleware.py`` which may be of use in the setup of
            your global variables.

        """

        global_conf.setdefault("debug", False)

        # reloading site ensures that we have a fresh sys.path to build our
        # working set off of. this means that forked worker processes won't get
        # the sys.path that was current when the master process was spawned
        # meaning that new plugins will be picked up on regular app reload
        # rather than having to restart the master process as well.
        reload(site)
        self.pkg_resources_working_set = pkg_resources.WorkingSet()

        self.config = ConfigValueParser(global_conf)
        self.config.add_spec(self.spec)
        self.plugins = PluginLoader(self.pkg_resources_working_set,
                                    self.config.get("plugins", []))

        self.stats = Stats(self.config.get('statsd_addr'),
                           self.config.get('statsd_sample_rate'))
        self.startup_timer = self.stats.get_timer("app_startup")
        self.startup_timer.start()

        self.baseplate = Baseplate()
        self.baseplate.configure_logging()
        self.baseplate.register(R2BaseplateObserver())
        self.baseplate.configure_tracing(
            "r2",
            tracing_endpoint=self.config.get("tracing_endpoint"),
            sample_rate=self.config.get("tracing_sample_rate"),
        )

        self.paths = paths

        self.running_as_script = global_conf.get('running_as_script', False)
        
        # turn on for language support
        self.lang = getattr(self, 'site_lang', 'en')
        self.languages, self.lang_name = get_active_langs(
            config, default_lang=self.lang)

        all_languages = self.lang_name.keys()
        all_languages.sort()
        self.all_languages = all_languages
        
        # set default time zone if one is not set
        tz = global_conf.get('timezone', 'UTC')
        self.tz = pytz.timezone(tz)
        
        dtz = global_conf.get('display_timezone', tz)
        self.display_tz = pytz.timezone(dtz)

        self.startup_timer.intermediate("init")

    def __getattr__(self, name):
        if not name.startswith('_') and name in self.config:
            return self.config[name]
        else:
            raise AttributeError("g has no attr %r" % name)

    def setup(self):
        self.env = ''
        if (
            # handle direct invocation of "nosetests"
            "test" in sys.argv[0] or
            # handle "setup.py test" and all permutations thereof.
            "setup.py" in sys.argv[0] and "test" in sys.argv[1:]
        ):
            self.env = "unit_test"

        self.queues = queues.declare_queues(self)

        self.extension_subdomains = dict(
            simple="mobile",
            i="compact",
            api="api",
            rss="rss",
            xml="xml",
            json="json",
        )

        ################# PROVIDERS
        self.auth_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.auth",
            self.authentication_provider,
        )
        self.media_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.media",
            self.media_provider,
        )
        self.cdn_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.cdn",
            self.cdn_provider,
        )
        self.ticket_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.support",
            # TODO: fix this later, it refuses to pick up 
            # g.config['ticket_provider'] value, so hardcoding for now.
            # really, the next uncommented line should be:
            #self.ticket_provider,
            # instead of:
            "zendesk",
        )
        self.image_resizing_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.image_resizing",
            self.image_resizing_provider,
        )
        self.email_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.email",
            self.email_provider,
        )
        self.startup_timer.intermediate("providers")

        ################# CONFIGURATION
        # AMQP is required
        if not self.amqp_host:
            raise ValueError("amqp_host not set in the .ini")

        if not self.cassandra_seeds:
            raise ValueError("cassandra_seeds not set in the .ini")

        # heavy load mode is read only mode with a different infobar
        if self.heavy_load_mode:
            self.read_only_mode = True

        origin_prefix = self.domain_prefix + "." if self.domain_prefix else ""
        self.origin = self.default_scheme + "://" + origin_prefix + self.domain

        self.trusted_domains = set([self.domain])
        if self.https_endpoint:
            https_url = urlparse(self.https_endpoint)
            self.trusted_domains.add(https_url.hostname)

        # load the unique hashed names of files under static
        static_files = os.path.join(self.paths.get('static_files'), 'static')
        names_file_path = os.path.join(static_files, 'names.json')
        if os.path.exists(names_file_path):
            with open(names_file_path) as handle:
                self.static_names = json.load(handle)
        else:
            self.static_names = {}

        # make python warnings go through the logging system
        logging.captureWarnings(capture=True)

        log = logging.getLogger('reddit')

        # when we're a script (paster run) just set up super simple logging
        if self.running_as_script:
            log.setLevel(logging.INFO)
            log.addHandler(logging.StreamHandler())

        # if in debug mode, override the logging level to DEBUG
        if self.debug:
            log.setLevel(logging.DEBUG)

        # attempt to figure out which pool we're in and add that to the
        # LogRecords.
        try:
            with open("/etc/ec2_asg", "r") as f:
                pool = f.read().strip()
            # clean up the pool name since we're putting stuff after "-"
            pool = pool.partition("-")[0]
        except IOError:
            pool = "reddit-app"
        self.log = logging.LoggerAdapter(log, {"pool": pool})

        # set locations
        locations = pkg_resources.resource_stream(__name__,
                                                  "../data/locations.json")
        self.locations = json.loads(locations.read())

        if not self.media_domain:
            self.media_domain = self.domain
        if self.media_domain == self.domain:
            print >> sys.stderr, ("Warning: g.media_domain == g.domain. " +
                   "This may give untrusted content access to user cookies")
        if self.oauth_domain == self.domain:
            print >> sys.stderr, ("Warning: g.oauth_domain == g.domain. "
                    "CORS requests to g.domain will be allowed")

        for arg in sys.argv:
            tokens = arg.split("=")
            if len(tokens) == 2:
                k, v = tokens
                self.log.debug("Overriding g.%s to %s" % (k, v))
                setattr(self, k, v)

        self.reddit_host = socket.gethostname()
        self.reddit_pid  = os.getpid()

        if hasattr(signal, 'SIGUSR1'):
            # not all platforms have user signals
            signal.signal(signal.SIGUSR1, thread_dump)

        locale.setlocale(locale.LC_ALL, self.locale)

        # Pre-calculate ratelimit values
        self.RL_RESET_SECONDS = self.config["RL_RESET_MINUTES"] * 60
        self.RL_MAX_REQS = int(self.config["RL_AVG_REQ_PER_SEC"] *
                                      self.RL_RESET_SECONDS)

        self.RL_OAUTH_RESET_SECONDS = self.config["RL_OAUTH_RESET_MINUTES"] * 60
        self.RL_OAUTH_MAX_REQS = int(self.config["RL_OAUTH_AVG_REQ_PER_SEC"] *
                                     self.RL_OAUTH_RESET_SECONDS)

        self.RL_LOGIN_MAX_REQS = int(self.config["RL_LOGIN_AVG_PER_SEC"] *
                                     self.RL_RESET_SECONDS)
        self.RL_LOGIN_IP_MAX_REQS = int(self.config["RL_LOGIN_IP_AVG_PER_SEC"] *
                                        self.RL_RESET_SECONDS)
        self.RL_SHARE_MAX_REQS = int(self.config["RL_SHARE_AVG_PER_SEC"] *
                                     self.RL_RESET_SECONDS)

        # Compile ratelimit regexs
        user_agent_ratelimit_regexes = {}
        for agent_re, limit in self.user_agent_ratelimit_regexes.iteritems():
            user_agent_ratelimit_regexes[re.compile(agent_re)] = limit
        self.user_agent_ratelimit_regexes = user_agent_ratelimit_regexes

        self.startup_timer.intermediate("configuration")

        ################# ZOOKEEPER
        zk_hosts = self.config["zookeeper_connection_string"]
        zk_username = self.config["zookeeper_username"]
        zk_password = self.config["zookeeper_password"]
        self.zookeeper = connect_to_zookeeper(zk_hosts, (zk_username,
                                                         zk_password))

        self.throttles = IPNetworkLiveList(
            self.zookeeper,
            root="/throttles",
            reduced_data_node="/throttles_reduced",
        )

        parser = ConfigParser.RawConfigParser()
        parser.optionxform = str
        parser.read([self.config["__file__"]])

        if self.config["liveconfig_source"] == "zookeeper":
            self.live_config = LiveConfig(self.zookeeper, LIVE_CONFIG_NODE)
        else:
            self.live_config = extract_live_config(parser, self.plugins)

        if self.config["secrets_source"] == "zookeeper":
            self.secrets = fetch_secrets(self.zookeeper)
        else:
            self.secrets = extract_secrets(parser)

        ################# PRIVILEGED USERS
        self.admins = PermissionFilteredEmployeeList(
            self.live_config, type="admin")
        self.sponsors = PermissionFilteredEmployeeList(
            self.live_config, type="sponsor")
        self.employees = PermissionFilteredEmployeeList(
            self.live_config, type="employee")

        # Store which OAuth clients employees may use, the keys are just for
        # readability.
        self.employee_approved_clients = \
            self.live_config["employee_approved_clients"].values()

        self.startup_timer.intermediate("zookeeper")

        ################# MEMCACHE
        num_mc_clients = self.num_mc_clients

        # a smaller pool of caches used only for distributed locks.
        self.lock_cache = CMemcache(
            "lock",
            self.lockcaches,
            num_clients=num_mc_clients,
        )
        self.make_lock = make_lock_factory(self.lock_cache, self.stats)

        # memcaches used in front of the permacache CF in cassandra.
        # XXX: this is a legacy thing; permacache was made when C* didn't have
        # a row cache.
        permacache_memcaches = CMemcache(
            "perma",
            self.permacache_memcaches,
            min_compress_len=1400,
            num_clients=num_mc_clients,
        )

        # the stalecache is a memcached local to the current app server used
        # for data that's frequently fetched but doesn't need to be fresh.
        if self.stalecaches:
            stalecaches = CMemcache(
                "stale",
                self.stalecaches,
                num_clients=num_mc_clients,
            )
        else:
            stalecaches = None

        self.startup_timer.intermediate("memcache")

        ################# MCROUTER
        self.mcrouter = Mcrouter(
            "mcrouter",
            self.mcrouter_addr,
            min_compress_len=1400,
            num_clients=num_mc_clients,
        )

        ################# THRIFT-BASED SERVICES
        activity_endpoint = self.config.get("activity_endpoint")
        if activity_endpoint:
            # make ActivityInfo objects rendercache-key friendly
            # TODO: figure out a more general solution for this if
            # we need to do this for other thrift-generated objects
            ActivityInfo.cache_key = lambda self, style: repr(self)

            activity_pool = ThriftConnectionPool(activity_endpoint, timeout=0.1)
            self.baseplate.add_to_context("activity_service",
                ThriftContextFactory(activity_pool, ActivityService.Client))

        self.startup_timer.intermediate("thrift")

        ################# CASSANDRA
        keyspace = "reddit"
        self.cassandra_pools = {
            "main":
                StatsCollectingConnectionPool(
                    keyspace,
                    stats=self.stats,
                    logging_name="main",
                    server_list=self.cassandra_seeds,
                    pool_size=self.cassandra_pool_size,
                    timeout=4,
                    max_retries=3,
                    prefill=False
                ),
        }

        permacache_cf = Permacache._setup_column_family(
            'permacache',
            self.cassandra_pools[self.cassandra_default_pool],
        )

        self.startup_timer.intermediate("cassandra")

        ################# POSTGRES
        self.dbm = self.load_db_params()
        self.startup_timer.intermediate("postgres")

        ################# CHAINS
        # initialize caches. Any cache-chains built here must be added
        # to cache_chains (closed around by reset_caches) so that they
        # can properly reset their local components
        cache_chains = {}
        localcache_cls = (SelfEmptyingCache if self.running_as_script
                          else LocalCache)

        if stalecaches:
            self.gencache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                self.mcrouter,
            )
        else:
            self.gencache = CacheChain((localcache_cls(), self.mcrouter))
        cache_chains.update(gencache=self.gencache)

        if stalecaches:
            self.thingcache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                self.mcrouter,
            )
        else:
            self.thingcache = CacheChain((localcache_cls(), self.mcrouter))
        cache_chains.update(thingcache=self.thingcache)

        if stalecaches:
            self.memoizecache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                self.mcrouter,
            )
        else:
            self.memoizecache = MemcacheChain(
                (localcache_cls(), self.mcrouter))
        cache_chains.update(memoizecache=self.memoizecache)

        if stalecaches:
            self.srmembercache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                self.mcrouter,
            )
        else:
            self.srmembercache = MemcacheChain(
                (localcache_cls(), self.mcrouter))
        cache_chains.update(srmembercache=self.srmembercache)

        if stalecaches:
            self.relcache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                self.mcrouter,
            )
        else:
            self.relcache = MemcacheChain(
                (localcache_cls(), self.mcrouter))
        cache_chains.update(relcache=self.relcache)

        self.ratelimitcache = MemcacheChain(
                (localcache_cls(), self.mcrouter))
        cache_chains.update(ratelimitcache=self.ratelimitcache)

        # rendercache holds rendered partial templates.
        self.rendercache = MemcacheChain((
            localcache_cls(),
            self.mcrouter,
        ))
        cache_chains.update(rendercache=self.rendercache)

        # commentpanecaches hold fully rendered comment panes
        self.commentpanecache = MemcacheChain((
            localcache_cls(),
            self.mcrouter,
        ))
        cache_chains.update(commentpanecache=self.commentpanecache)

        # cassandra_local_cache is used for request-local caching in tdb_cassandra
        self.cassandra_local_cache = localcache_cls()
        cache_chains.update(cassandra_local_cache=self.cassandra_local_cache)

        if stalecaches:
            permacache_cache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                permacache_memcaches,
            )
        else:
            permacache_cache = CacheChain(
                (localcache_cls(), permacache_memcaches),
            )
        cache_chains.update(permacache=permacache_cache)

        self.permacache = Permacache(
            permacache_cache,
            permacache_cf,
            lock_factory=self.make_lock,
        )

        # hardcache is used for various things that tend to expire
        # TODO: replace hardcache w/ cassandra stuff
        self.hardcache = HardcacheChain(
            (localcache_cls(), HardCache(self)),
            cache_negative_results=True,
        )
        cache_chains.update(hardcache=self.hardcache)

        # I know this sucks, but we need non-request-threads to be
        # able to reset the caches, so we need them be able to close
        # around 'cache_chains' without being able to call getattr on
        # 'g'
        def reset_caches():
            for name, chain in cache_chains.iteritems():
                if isinstance(chain, TransitionalCache):
                    chain = chain.read_chain

                chain.reset()
                if isinstance(chain, LocalCache):
                    continue
                elif isinstance(chain, StaleCacheChain):
                    chain.stats = StaleCacheStats(self.stats, name)
                else:
                    chain.stats = CacheStats(self.stats, name)
        self.cache_chains = cache_chains

        self.reset_caches = reset_caches
        self.reset_caches()

        self.startup_timer.intermediate("cache_chains")

        # try to set the source control revision numbers
        self.versions = {}
        r2_root = os.path.dirname(os.path.dirname(self.paths["root"]))
        r2_gitdir = os.path.join(r2_root, ".git")
        self.short_version = self.record_repo_version("r2", r2_gitdir)

        if I18N_PATH:
            i18n_git_path = os.path.join(os.path.dirname(I18N_PATH), ".git")
            self.record_repo_version("i18n", i18n_git_path)

        # Initialize the amqp module globals, start the worker, etc.
        r2.lib.amqp.initialize(self)

        self.events = EventQueue()

        self.startup_timer.intermediate("revisions")

    def setup_complete(self):
        self.startup_timer.stop()
        self.stats.flush()

        if self.log_start:
            self.log.error(
                "%s:%s started %s at %s (took %.02fs)",
                self.reddit_host,
                self.reddit_pid,
                self.short_version,
                datetime.now().strftime("%H:%M:%S"),
                self.startup_timer.elapsed_seconds()
            )

        if einhorn.is_worker():
            einhorn.ack_startup()

    def record_repo_version(self, repo_name, git_dir):
        """Get the currently checked out git revision for a given repository,
        record it in g.versions, and return the short version of the hash."""
        try:
            subprocess.check_output
        except AttributeError:
            # python 2.6 compat
            pass
        else:
            try:
                revision = subprocess.check_output(["git",
                                                    "--git-dir", git_dir,
                                                    "rev-parse", "HEAD"])
            except subprocess.CalledProcessError, e:
                self.log.warning("Unable to fetch git revision: %r", e)
            else:
                self.versions[repo_name] = revision.rstrip()
                return revision[:7]

        return "(unknown)"

    def load_db_params(self):
        self.databases = tuple(ConfigValue.to_iter(self.config.raw_data['databases']))
        self.db_params = {}
        self.predefined_type_ids = {}
        if not self.databases:
            return

        if self.env == 'unit_test':
            from mock import MagicMock
            return MagicMock()

        dbm = db_manager.db_manager()
        db_param_names = ('name', 'db_host', 'db_user', 'db_pass', 'db_port',
                          'pool_size', 'max_overflow')
        for db_name in self.databases:
            conf_params = ConfigValue.to_iter(self.config.raw_data[db_name + '_db'])
            params = dict(zip(db_param_names, conf_params))
            if params['db_user'] == "*":
                params['db_user'] = self.db_user
            if params['db_pass'] == "*":
                params['db_pass'] = self.db_pass
            if params['db_port'] == "*":
                params['db_port'] = self.db_port

            if params['pool_size'] == "*":
                params['pool_size'] = self.db_pool_size
            if params['max_overflow'] == "*":
                params['max_overflow'] = self.db_pool_overflow_size

            dbm.setup_db(db_name, g_override=self, **params)
            self.db_params[db_name] = params

        dbm.type_db = dbm.get_engine(self.config.raw_data['type_db'])
        dbm.relation_type_db = dbm.get_engine(self.config.raw_data['rel_type_db'])

        def split_flags(raw_params):
            params = []
            flags = {}

            for param in raw_params:
                if not param.startswith("!"):
                    params.append(param)
                else:
                    key, sep, value = param[1:].partition("=")
                    if sep:
                        flags[key] = value
                    else:
                        flags[key] = True

            return params, flags

        prefix = 'db_table_'
        for k, v in self.config.raw_data.iteritems():
            if not k.startswith(prefix):
                continue

            params, table_flags = split_flags(ConfigValue.to_iter(v))
            name = k[len(prefix):]
            kind = params[0]
            server_list = self.config.raw_data["db_servers_" + name]
            engines, flags = split_flags(ConfigValue.to_iter(server_list))

            typeid = table_flags.get("typeid")
            if typeid:
                self.predefined_type_ids[name] = int(typeid)

            if kind == 'thing':
                dbm.add_thing(name, dbm.get_engines(engines),
                              **flags)
            elif kind == 'relation':
                dbm.add_relation(name, params[1], params[2],
                                 dbm.get_engines(engines),
                                 **flags)
        return dbm

    def __del__(self):
        """
        Put any cleanup code to be run when the application finally exits 
        here.
        """
        pass

    @property
    def search(self):
        if getattr(self, 'search_provider', None):
            if type(self.search_provider) == str:
                self.search_provider = select_provider(self.config,
                                       self.pkg_resources_working_set,
                                       "r2.provider.search",
                                       self.search_provider,
                                       )
            return  self.search_provider
        return None

    @property
    def search_syntaxes(self):
        return SEARCH_SYNTAXES[self.config.get('search_provider')]
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Parse and validate a safe subset of CSS.

The goal of this validation is not to ensure functionally correct stylesheets
but rather that the stylesheet is safe to show to downstream users.  This
includes:

    * not generating requests to third party hosts (information leak)
    * xss via strange syntax in buggy browsers

Beyond that, every effort is made to allow the full gamut of modern CSS.

"""

import itertools
import re
import unicodedata

import tinycss2

from pylons.i18n import N_

from r2.lib.contrib import rcssmin
from r2.lib.utils import tup


__all__ = ["validate_css"]


SIMPLE_TOKEN_TYPES = {
    "dimension",
    "hash",
    "ident",
    "literal",
    "number",
    "percentage",
    "string",
    "whitespace",
}


VENDOR_PREFIXES = {
    "-apple-",
    "-khtml-",
    "-moz-",
    "-ms-",
    "-o-",
    "-webkit-",
}
assert all(prefix == prefix.lower() for prefix in VENDOR_PREFIXES)


SAFE_PROPERTIES = {
    "align-content",
    "align-items",
    "align-self",
    "animation",
    "animation-delay",
    "animation-direction",
    "animation-duration",
    "animation-fill-mode",
    "animation-iteration-count",
    "animation-name",
    "animation-play-state",
    "animation-timing-function",
    "appearance",
    "backface-visibility",
    "background",
    "background-attachment",
    "background-blend-mode",
    "background-clip",
    "background-color",
    "background-image",
    "background-origin",
    "background-position",
    "background-position-x",
    "background-position-y",
    "background-repeat",
    "background-size",
    "border",
    "border-bottom",
    "border-bottom-color",
    "border-bottom-left-radius",
    "border-bottom-right-radius",
    "border-bottom-style",
    "border-bottom-width",
    "border-collapse",
    "border-color",
    "border-image",
    "border-image-outset",
    "border-image-repeat",
    "border-image-slice",
    "border-image-source",
    "border-image-width",
    "border-left",
    "border-left-color",
    "border-left-style",
    "border-left-width",
    "border-radius",
    "border-radius-bottomleft",
    "border-radius-bottomright",
    "border-radius-topleft",
    "border-radius-topright",
    "border-right",
    "border-right-color",
    "border-right-style",
    "border-right-width",
    "border-spacing",
    "border-style",
    "border-top",
    "border-top-color",
    "border-top-left-radius",
    "border-top-right-radius",
    "border-top-style",
    "border-top-width",
    "border-width",
    "bottom",
    "box-shadow",
    "box-sizing",
    "caption-side",
    "clear",
    "clip",
    "clip-path",
    "color",
    "content",
    "counter-increment",
    "counter-reset",
    "cue",
    "cue-after",
    "cue-before",
    "cursor",
    "direction",
    "display",
    "elevation",
    "empty-cells",
    # the "filter" property cannot be safely added while IE9 is allowed to
    # use subreddit stylesheets. see explanation here:
    # https://github.com/reddit/reddit/pull/1058#issuecomment-76466180
    # "filter",
    "flex",
    "flex-align",
    "flex-basis",
    "flex-direction",
    "flex-flow",
    "flex-grow",
    "flex-item-align",
    "flex-line-pack",
    "flex-order",
    "flex-pack",
    "flex-shrink",
    "flex-wrap",
    "float",
    "font",
    "font-family",
    "font-size",
    "font-style",
    "font-variant",
    "font-weight",
    "grid",
    "grid-area",
    "grid-auto-columns",
    "grid-auto-flow",
    "grid-auto-position",
    "grid-auto-rows",
    "grid-column",
    "grid-column-start",
    "grid-column-end",
    "grid-row",
    "grid-row-start",
    "grid-row-end",
    "grid-template",
    "grid-template-areas",
    "grid-template-rows",
    "grid-template-columns",
    "hanging-punctuation",
    "height",
    "hyphens",
    "image-orientation",
    "image-rendering",
    "image-resolution",
    "justify-content",
    "left",
    "letter-spacing",
    "line-break",
    "line-height",
    "list-style",
    "list-style-image",
    "list-style-position",
    "list-style-type",
    "margin",
    "margin-bottom",
    "margin-left",
    "margin-right",
    "margin-top",
    "max-height",
    "max-width",
    "mask",
    "mask-border",
    "mask-border-mode",
    "mask-border-outset",
    "mask-border-repeat",
    "mask-border-source",
    "mask-border-slice",
    "mask-border-width",
    "mask-clip",
    "mask-composite",
    "mask-image",
    "mask-mode",
    "mask-origin",
    "mask-position",
    "mask-repeat",
    "mask-size",
    "min-height",
    "min-width",
    "mix-blend-mode",
    "opacity",
    "order",
    "orphans",
    "outline",
    "outline-color",
    "outline-offset",
    "outline-style",
    "outline-width",
    "overflow",
    "overflow-wrap",
    "overflow-x",
    "overflow-y",
    "padding",
    "padding-bottom",
    "padding-left",
    "padding-right",
    "padding-top",
    "page-break-after",
    "page-break-before",
    "page-break-inside",
    "pause",
    "pause-after",
    "pause-before",
    "perspective",
    "perspective-origin",
    "pitch",
    "pitch-range",
    "play-during",
    "pointer-events",
    "position",
    "quotes",
    "resize",
    "richness",
    "right",
    "speak",
    "speak-header",
    "speak-numeral",
    "speak-punctuation",
    "speech-rate",
    "stress",
    "table-layout",
    "tab-size",
    "text-align",
    "text-align-last",
    "text-decoration",
    "text-decoration-color",
    "text-decoration-line",
    "text-decoration-skip",
    "text-decoration-style",
    "text-indent",
    "text-justify",
    "text-overflow",
    "text-rendering",
    "text-shadow",
    "text-size-adjust",
    "text-space-collapse",
    "text-transform",
    "text-underline-position",
    "text-wrap",
    "top",
    "transform",
    "transform-origin",
    "transform-style",
    "transition",
    "transition-delay",
    "transition-duration",
    "transition-property",
    "transition-timing-function",
    "unicode-bidi",
    "vertical-align",
    "visibility",
    "voice-family",
    "volume",
    "white-space",
    "widows",
    "width",
    "will-change",
    "word-break",
    "word-spacing",
    "z-index",
}
assert all(property == property.lower() for property in SAFE_PROPERTIES)


SAFE_FUNCTIONS = {
    "attr",
    "calc",
    "circle",
    "counter",
    "counters",
    "cubic-bezier",
    "ellipse",
    "hsl",
    "hsla",
    "lang",
    "line",
    "linear-gradient",
    "matrix",
    "matrix3d",
    "not",
    "nth-child",
    "nth-last-child",
    "nth-last-of-type",
    "nth-of-type",
    "perspective",
    "polygon",
    "polyline",
    "radial-gradient",
    "rect",
    "repeating-linear-gradient",
    "repeating-radial-gradient",
    "rgb",
    "rgba",
    "rotate",
    "rotate3d",
    "rotatex",
    "rotatey",
    "rotatez",
    "scale",
    "scale3d",
    "scalex",
    "scaley",
    "scalez",
    "skewx",
    "skewy",
    "steps",
    "translate",
    "translate3d",
    "translatex",
    "translatey",
    "translatez",
}
assert all(function == function.lower() for function in SAFE_FUNCTIONS)


ERROR_MESSAGES = {
    "IMAGE_NOT_FOUND": N_('no image found with name "%(name)s"'),
    "NON_PLACEHOLDER_URL": N_("only uploaded images are allowed; reference "
                              "them with the %%%%imagename%%%% system"),
    "SYNTAX_ERROR": N_("syntax error: %(message)s"),
    "UNKNOWN_AT_RULE": N_("@%(keyword)s is not allowed"),
    "UNKNOWN_PROPERTY": N_('unknown property "%(name)s"'),
    "UNKNOWN_FUNCTION": N_('unknown function "%(function)s"'),
    "UNEXPECTED_TOKEN": N_('unexpected token "%(token)s"'),
    "BACKSLASH": N_("backslashes are not allowed"),
    "CONTROL_CHARACTER": N_("control characters are not allowed"),
    "TOO_BIG": N_("the stylesheet is too big. maximum size: %(size)d KiB"),
}


MAX_SIZE_KIB = 100
SUBREDDIT_IMAGE_URL_PLACEHOLDER = re.compile(r"\A%%([a-zA-Z0-9\-]+)%%\Z")


def strip_vendor_prefix(identifier):
    for prefix in VENDOR_PREFIXES:
        if identifier.startswith(prefix):
            return identifier[len(prefix):]
    return identifier


class ValidationError(object):
    def __init__(self, line_number, error_code, message_params=None):
        self.line = line_number
        self.error_code = error_code
        self.message_params = message_params or {}
        # note: _source_lines is added to these objects by the parser

    @property
    def offending_line(self):
        return self._source_lines[self.line - 1]

    @property
    def message_key(self):
        return ERROR_MESSAGES[self.error_code]


class StylesheetValidator(object):
    def __init__(self, images):
        self.images = images

    def validate_url(self, url_node):
        m = SUBREDDIT_IMAGE_URL_PLACEHOLDER.match(url_node.value)
        if not m:
            return ValidationError(url_node.source_line, "NON_PLACEHOLDER_URL")

        image_name = m.group(1)
        if image_name not in self.images:
            return ValidationError(url_node.source_line, "IMAGE_NOT_FOUND",
                                   {"name": image_name})

        # rewrite the url value to the actual url of the image
        url_node.value = self.images[image_name]

    def validate_function(self, function_node):
        function_name = strip_vendor_prefix(function_node.lower_name)

        if function_name not in SAFE_FUNCTIONS:
            return ValidationError(function_node.source_line,
                                   "UNKNOWN_FUNCTION",
                                   {"function": function_node.name})
        # property: attr(something url)
        # https://developer.mozilla.org/en-US/docs/Web/CSS/attr
        elif function_name == "attr":
            for argument in function_node.arguments:
                if argument.type == "ident" and argument.lower_value == "url":
                    return ValidationError(argument.source_line,
                                           "NON_PLACEHOLDER_URL")

        return self.validate_component_values(function_node.arguments)

    def validate_block(self, block):
        return self.validate_component_values(block.content)

    def validate_component_values(self, component_values):
        return self.validate_list(component_values, {
            # {} blocks are technically part of component values but i don't
            # know of any actual valid uses for them in selectors etc. and they
            # can cause issues with e.g.
            # Safari 5: p[foo=bar{}*{background:green}]{background:red}
            "[] block": self.validate_block,
            "() block": self.validate_block,
            "url": self.validate_url,
            "function": self.validate_function,
        }, ignored_types=SIMPLE_TOKEN_TYPES)

    def validate_declaration(self, declaration):
        if strip_vendor_prefix(declaration.lower_name) not in SAFE_PROPERTIES:
            return ValidationError(declaration.source_line, "UNKNOWN_PROPERTY",
                                   {"name": declaration.name})
        return self.validate_component_values(declaration.value)

    def validate_declaration_list(self, declarations):
        return self.validate_list(declarations, {
            "at-rule": self.validate_at_rule,
            "declaration": self.validate_declaration,
        })

    def validate_qualified_rule(self, rule):
        prelude_errors = self.validate_component_values(rule.prelude)
        declarations = tinycss2.parse_declaration_list(rule.content)
        declaration_errors = self.validate_declaration_list(declarations)
        return itertools.chain(prelude_errors, declaration_errors)

    def validate_at_rule(self, rule):
        prelude_errors = self.validate_component_values(rule.prelude)

        keyword = strip_vendor_prefix(rule.lower_at_keyword)

        if keyword in ("media", "keyframes"):
            rules = tinycss2.parse_rule_list(rule.content)
            rule_errors = self.validate_rule_list(rules)
        elif keyword == "page":
            rule_errors = self.validate_qualified_rule(rule)
        else:
            return ValidationError(rule.source_line, "UNKNOWN_AT_RULE",
                                   {"keyword": rule.at_keyword})

        return itertools.chain(prelude_errors, rule_errors)

    def validate_rule_list(self, rules):
        return self.validate_list(rules, {
            "qualified-rule": self.validate_qualified_rule,
            "at-rule": self.validate_at_rule,
        })

    def validate_list(self, nodes, validators_by_type, ignored_types=None):
        for node in nodes:
            if node.type == "error":
                yield ValidationError(node.source_line, "SYNTAX_ERROR",
                                      {"message": node.message})
                continue
            elif node.type == "literal":
                if node.value == ";":
                    # if we're seeing a semicolon as a literal, it's in a place
                    # that doesn't fit naturally in the syntax.
                    # Safari 5 will treat this as two color properties:
                    # color: calc(;color:red;);
                    message = "semicolons are not allowed in this context"
                    yield ValidationError(node.source_line, "SYNTAX_ERROR",
                                          {"message": message})
                    continue

            validator = validators_by_type.get(node.type)

            if validator:
                for error in tup(validator(node)):
                    if error:
                        yield error
            else:
                if not ignored_types or node.type not in ignored_types:
                    yield ValidationError(node.source_line,
                                          "UNEXPECTED_TOKEN",
                                          {"token": node.type})

    def check_for_evil_codepoints(self, source_lines):
        for line_number, line_text in enumerate(source_lines, start=1):
            for codepoint in line_text:
                # IE<8: *{color: expression\28 alert\28 1 \29 \29 }
                if codepoint == "\\":
                    yield ValidationError(line_number, "BACKSLASH")
                    break
                # accept these characters that get classified as control
                elif codepoint in ("\t", "\n", "\r"):
                    continue
                # Safari: *{font-family:'foobar\x03;background:url(evil);';}
                elif unicodedata.category(codepoint).startswith("C"):
                    yield ValidationError(line_number, "CONTROL_CHARACTER")
                    break

    def parse_and_validate(self, stylesheet_source):
        if len(stylesheet_source) > (MAX_SIZE_KIB * 1024):
            return "", [ValidationError(0, "TOO_BIG", {"size": MAX_SIZE_KIB})]

        nodes = tinycss2.parse_stylesheet(stylesheet_source)

        source_lines = stylesheet_source.splitlines()

        backslash_errors = self.check_for_evil_codepoints(source_lines)
        validation_errors = self.validate_rule_list(nodes)

        errors = []
        for error in itertools.chain(backslash_errors, validation_errors):
            error._source_lines = source_lines
            errors.append(error)
        errors.sort(key=lambda e: e.line)

        if not errors:
            serialized = rcssmin.cssmin(tinycss2.serialize(nodes))
        else:
            serialized = ""

        return serialized.encode("utf-8"), errors


def validate_css(stylesheet, images):
    """Validate and re-serialize the user submitted stylesheet.

    images is a mapping of subreddit image names to their URLs.  The
    re-serialized stylesheet will have %%name%% tokens replaced with their
    appropriate URLs.

    The return value is a two-tuple of the re-serialized (and minified)
    stylesheet and a list of errors.  If the list is empty, the stylesheet is
    valid.

    """
    assert isinstance(stylesheet, unicode)
    validator = StylesheetValidator(images)
    return validator.parse_and_validate(stylesheet)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from hashlib import md5

from r2.lib.filters import _force_utf8
from r2.lib.cache import NoneResult, make_key_id
from r2.lib.lock import make_lock_factory
from pylons import app_globals as g


def memoize(iden, time = 0, stale=False, timeout=30):
    def memoize_fn(fn):
        from r2.lib.memoize import NoneResult
        def new_fn(*a, **kw):

            #if the keyword param _update == True, the cache will be
            #overwritten no matter what
            update = kw.pop('_update', False)

            key = "memo:%s:%s" % (iden, make_key_id(*a, **kw))

            res = None if update else g.memoizecache.get(key, stale=stale)

            if res is None:
                # not cached, we should calculate it.
                with g.make_lock("memoize", 'memoize_lock(%s)' % key,
                                 time=timeout, timeout=timeout):

                    # see if it was completed while we were waiting
                    # for the lock
                    stored = None if update else g.memoizecache.get(key)
                    if stored is not None:
                        # it was calculated while we were waiting
                        res = stored
                    else:
                        # okay now go and actually calculate it
                        res = fn(*a, **kw)
                        if res is None:
                            res = NoneResult
                        g.memoizecache.set(key, res, time=time)

            if res == NoneResult:
                res = None

            return res

        new_fn.memoized_fn = fn
        return new_fn
    return memoize_fn

@memoize('test')
def test(x, y):
    import time
    time.sleep(1)
    print 'calculating %d + %d' % (x, y)
    if x + y == 10:
        return None
    else:
        return x + y
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import app_globals as g
from datetime import timedelta as timedelta
from datetime import datetime
import sqlalchemy as sa
from r2.lib.db.tdb_lite import tdb_lite
import pytz
import random

COUNT_CATEGORY = 'hc_count'
ELAPSED_CATEGORY = 'hc_elapsed'
TZ = pytz.timezone("MST")

def expiration_from_time(time):
    if time <= 0:
        raise ValueError ("HardCache items *must* have an expiration time")
    return datetime.now(TZ) + timedelta(0, time)

class HardCacheBackend(object):
    def __init__(self, gc):
        self.tdb = tdb_lite(gc)
        self.profile_categories = {}
        TZ = gc.display_tz

        def _table(metadata):
            return sa.Table(gc.db_app_name + '_hardcache', metadata,
                            sa.Column('category', sa.String, nullable = False,
                                      primary_key = True),
                            sa.Column('ids', sa.String, nullable = False,
                                      primary_key = True),
                            sa.Column('value', sa.String, nullable = False),
                            sa.Column('kind', sa.String, nullable = False),
                            sa.Column('expiration',
                                      sa.DateTime(timezone = True),
                                      nullable = False)
                            )
        enginenames_by_category = {}
        all_enginenames = set()
        for item in gc.hardcache_categories:
            chunks = item.split(":")
            if len(chunks) < 2:
                raise ValueError("Invalid hardcache_overrides")
            category = chunks.pop(0)
            enginenames_by_category[category] = []
            for c in chunks:
                if c == '!profile':
                    self.profile_categories[category] = True
                elif c.startswith("!"):
                    raise ValueError("WTF is [%s] in hardcache_overrides?" % c)
                else:
                    all_enginenames.add(c)
                    enginenames_by_category[category].append(c)

        assert('*' in enginenames_by_category.keys())

        engines_by_enginename = {}
        for enginename in all_enginenames:
            engine = gc.dbm.get_engine(enginename)
            md = self.tdb.make_metadata(engine)
            table = _table(md)
            indstr = self.tdb.index_str(table, 'expiration', 'expiration')
            self.tdb.create_table(table, [ indstr ])
            engines_by_enginename[enginename] = table

        self.mapping = {}
        for category, enginenames in enginenames_by_category.iteritems():
            self.mapping[category] = [ engines_by_enginename[e]
                                       for e in enginenames]

    def engine_by_category(self, category, type="master"):
        if category not in self.mapping:
            category = '*'
        engines = self.mapping[category]
        if type == 'master':
            return engines[0]
        elif type == 'readslave':
            return random.choice(engines[1:])
        else:
            raise ValueError("invalid type %s" % type)

    def profile_start(self, operation, category):
        if category == COUNT_CATEGORY:
            return None

        if category == ELAPSED_CATEGORY:
            return None

        if category in self.mapping:
            effective_category = category
        else:
            effective_category = '*'

        if effective_category not in self.profile_categories:
            return None

        return (datetime.now(TZ), operation, category)

    def profile_stop(self, t):
        if t is None:
            return

        start_time, operation, category = t

        end_time = datetime.now(TZ)

        period = end_time.strftime("%Y/%m/%d_%H:%M")[:-1] + 'x'

        elapsed = end_time - start_time
        msec = elapsed.seconds * 1000 + elapsed.microseconds / 1000

        ids = "-".join((operation, category, period))

        self.add(COUNT_CATEGORY, ids, 0, time=86400)
        self.add(ELAPSED_CATEGORY, ids, 0, time=86400)

        self.incr(COUNT_CATEGORY, ids, time=86400)
        self.incr(ELAPSED_CATEGORY, ids, time=86400, delta=msec)


    def set(self, category, ids, val, time):

        self.delete(category, ids) # delete it if it already exists

        value, kind = self.tdb.py2db(val, True)

        expiration = expiration_from_time(time)

        prof = self.profile_start('set', category)

        engine = self.engine_by_category(category, "master")

        engine.insert().execute(
            category=category,
            ids=ids,
            value=value,
            kind=kind,
            expiration=expiration
            )

        self.profile_stop(prof)

    def add(self, category, ids, val, time=0):
        self.delete_if_expired(category, ids)

        expiration = expiration_from_time(time)

        value, kind = self.tdb.py2db(val, True)

        prof = self.profile_start('add', category)

        engine = self.engine_by_category(category, "master")

        try:
            rp = engine.insert().execute(
                category=category,
                ids=ids,
                value=value,
                kind=kind,
                expiration=expiration
                )
            self.profile_stop(prof)
            return value

        except sa.exc.IntegrityError, e:
            self.profile_stop(prof)
            return self.get(category, ids, force_write_table=True)

    def incr(self, category, ids, time=0, delta=1):
        self.delete_if_expired(category, ids)

        expiration = expiration_from_time(time)

        prof = self.profile_start('incr', category)

        engine = self.engine_by_category(category, "master")

        rp = engine.update(sa.and_(engine.c.category==category,
                                   engine.c.ids==ids,
                                   engine.c.kind=='num'),
                           values = {
                                   engine.c.value:
                                           sa.cast(
                                           sa.cast(engine.c.value, sa.Integer)
                                           + delta, sa.String),
                                   engine.c.expiration: expiration
                                   }
                           ).execute()

        self.profile_stop(prof)

        if rp.rowcount == 1:
            return self.get(category, ids, force_write_table=True)
        elif rp.rowcount == 0:
            existing_value = self.get(category, ids, force_write_table=True)
            if existing_value is None:
                raise ValueError("[%s][%s] can't be incr()ed -- it's not set" %
                                 (category, ids))
            else:
                raise ValueError("[%s][%s] has non-integer value %r" %
                                 (category, ids, existing_value))
        else:
            raise ValueError("Somehow %d rows got updated" % rp.rowcount)

    def get(self, category, ids, force_write_table=False):
        if force_write_table:
            type = "master"
        else:
            type = "readslave"

        engine = self.engine_by_category(category, type)

        prof = self.profile_start('get', category)

        s = sa.select([engine.c.value,
                       engine.c.kind,
                       engine.c.expiration],
                      sa.and_(engine.c.category==category,
                              engine.c.ids==ids),
                      limit = 1)
        rows = s.execute().fetchall()

        self.profile_stop(prof)

        if len(rows) < 1:
            return None
        elif rows[0].expiration < datetime.now(TZ):
            return None
        else:
            return self.tdb.db2py(rows[0].value, rows[0].kind)

    def get_multi(self, category, idses):
        prof = self.profile_start('get_multi', category)

        engine = self.engine_by_category(category, "readslave")

        s = sa.select([engine.c.ids,
                       engine.c.value,
                       engine.c.kind,
                       engine.c.expiration],
                      sa.and_(engine.c.category==category,
                              sa.or_(*[engine.c.ids==ids
                                       for ids in idses])))
        rows = s.execute().fetchall()

        self.profile_stop(prof)

        results = {}

        for row in rows:
          if row.expiration >= datetime.now(TZ):
              k = "%s-%s" % (category, row.ids)
              results[k] = self.tdb.db2py(row.value, row.kind)

        return results

    def delete(self, category, ids):
        prof = self.profile_start('delete', category)
        engine = self.engine_by_category(category, "master")
        engine.delete(
            sa.and_(engine.c.category==category,
                    engine.c.ids==ids)).execute()
        self.profile_stop(prof)

    def ids_by_category(self, category, limit=1000):
        prof = self.profile_start('ids_by_category', category)
        engine = self.engine_by_category(category, "readslave")
        s = sa.select([engine.c.ids],
                      sa.and_(engine.c.category==category,
                              engine.c.expiration > datetime.now(TZ)),
                      limit = limit)
        rows = s.execute().fetchall()
        self.profile_stop(prof)
        return [ r.ids for r in rows ]

    def clause_from_expiration(self, engine, expiration):
        if expiration is None:
            return True
        elif expiration == "now":
            return engine.c.expiration < datetime.now(TZ)
        else:
            return engine.c.expiration < expiration

    def expired(self, engine, expiration_clause, limit=1000):
        s = sa.select([engine.c.category,
                       engine.c.ids,
                       engine.c.expiration],
                      expiration_clause,
                      limit = limit,
                      order_by = engine.c.expiration
                      )
        rows = s.execute().fetchall()
        return [ (r.expiration, r.category, r.ids) for r in rows ]

    def delete_if_expired(self, category, ids, expiration="now"):
        prof = self.profile_start('delete_if_expired', category)
        engine = self.engine_by_category(category, "master")
        expiration_clause = self.clause_from_expiration(engine, expiration)
        engine.delete(sa.and_(engine.c.category==category,
                              engine.c.ids==ids,
                              expiration_clause)).execute()
        self.profile_stop(prof)


def delete_expired(expiration="now", limit=5000):
    # the following depends on the structure of g.hardcache not changing
    backend = g.hardcache.caches[1].backend
    # localcache = g.hardcache.caches[0]

    masters = set()

    for engines in backend.mapping.values():
        masters.add(engines[0])

    for engine in masters:
        expiration_clause = backend.clause_from_expiration(engine, expiration)

        # Get all the expired keys
        rows = backend.expired(engine, expiration_clause, limit)

        if len(rows) == 0:
            continue

        # Delete from the backend.
        engine.delete(expiration_clause).execute()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Transitional integration with Baseplate.

This module provides basic transitional integration with Baseplate. Its intent
is to integrate baseplate-provided functionality (like thrift clients) into
r2's existing diagnostics infrastructure. It is not meant to be the last word
on r2+baseplate; ideally r2 will move towards using more of baseplate rather
than its own implementations.

"""

import functools
import sys

from baseplate.core import BaseplateObserver, ServerSpanObserver, SpanObserver
from pylons import app_globals as g, tmpl_context as c


def make_server_span(span_name):
    c.trace = g.baseplate.make_server_span(context=c, name=span_name)
    return c.trace


def finish_server_span():
    c.trace.finish()


def with_server_span(name):
    """A decorator for functions that run outside request context.

    This will add a server span which starts just before invocation of the
    function and ends immediately after. The context (`c`) will have all
    appropriate baseplate stuff added to it, and metrics will be flushed when
    the function returns.

    This is useful for functions run in cron jobs or from the shell. Note that
    you cannot call a function wrapped with this decorator from within an
    existing server span.

    """
    def with_server_span_decorator(fn):
        @functools.wraps(fn)
        def with_server_span_wrapper(*args, **kwargs):
            assert not c.trace, "called while already in a server span"

            try:
                with make_server_span(name):
                    return fn(*args, **kwargs)
            finally:
                g.stats.flush()
        return with_server_span_wrapper
    return with_server_span_decorator


# this is just for backwards compatibility
with_root_span = with_server_span


class R2BaseplateObserver(BaseplateObserver):
    def on_server_span_created(self, context, server_span):
        observer = R2ServerSpanObserver()
        server_span.register(observer)


class R2ServerSpanObserver(ServerSpanObserver):
    def on_child_span_created(self, span):
        observer = R2SpanObserver(span.name)
        span.register(observer)


class R2SpanObserver(SpanObserver):
    def __init__(self, span_name):
        self.metric_name = "providers.{}".format(span_name)
        self.timer = g.stats.get_timer(self.metric_name)

    def on_start(self):
        self.timer.start()

    def on_finish(self, exc_info):
        self.timer.stop()

        if exc_info:
            error = exc_info[1]
            g.log.warning("%s: error: %s", self.metric_name, error)
            g.stats.simple_event("{}.error".format(self.metric_name))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from email import encoders
from email.MIMEBase import MIMEBase
from email.MIMEText import MIMEText
from email.MIMEMultipart import MIMEMultipart
from email.errors import HeaderParseError
import datetime
import traceback, sys, smtplib

from pylons import tmpl_context as c
from pylons import app_globals as g
import simplejson as json

from r2.config import feature
from r2.lib import hooks
from r2.lib.ratelimit import SimpleRateLimit
from r2.lib.utils import timeago
from r2.models import Comment, Email, DefaultSR, Account, Award
from r2.models.token import EmailVerificationToken, PasswordResetToken


trylater_hooks = hooks.HookRegistrar()

def _system_email(email, plaintext_body, kind, reply_to="",
        thing=None, from_address=g.feedback_email,
        html_body="", list_unsubscribe_header="", user=None,
        suppress_username=False):
    """
    For sending email from the system to a user (reply address will be
    feedback and the name will be reddit.com)
    """
    if suppress_username:
        user = None
    elif user is None and c.user_is_loggedin:
        user = c.user

    Email.handler.add_to_queue(user,
        email, g.domain, from_address, kind,
        body=plaintext_body, reply_to=reply_to, thing=thing,
    )


def _ads_email(body, from_name, kind):
    """
    For sending email to ads
    """
    Email.handler.add_to_queue(None, g.ads_email, from_name, g.ads_email,
                               kind, body=body)

def _fraud_email(body, kind):
    """
    For sending email to the fraud mailbox
    """
    Email.handler.add_to_queue(None, g.fraud_email, g.domain, g.fraud_email,
                               kind, body=body)

def _community_email(body, kind):
    """
    For sending email to the community mailbox
    """
    Email.handler.add_to_queue(c.user, g.community_email, g.domain, g.community_email,
                               kind, body=body)

def verify_email(user, dest=None):
    """
    For verifying an email address
    """
    from r2.lib.pages import VerifyEmail
    user.email_verified = False
    user._commit()
    Award.take_away("verified_email", user)

    token = EmailVerificationToken._new(user)
    base = g.https_endpoint or g.origin
    emaillink = base + '/verification/' + token._id
    if dest:
        emaillink += '?dest=%s' % dest
    g.log.debug("Generated email verification link: " + emaillink)

    _system_email(user.email,
                  VerifyEmail(user=user,
                              emaillink = emaillink).render(style='email'),
                  Email.Kind.VERIFY_EMAIL)

def password_email(user):
    """
    For resetting a user's password.
    """
    from r2.lib.pages import PasswordReset

    user_reset_ratelimit = SimpleRateLimit(
        name="email_reset_count_%s" % user._id36,
        seconds=int(datetime.timedelta(hours=12).total_seconds()),
        limit=3,
    )
    if not user_reset_ratelimit.record_and_check():
        return False

    global_reset_ratelimit = SimpleRateLimit(
        name="email_reset_count_global",
        seconds=int(datetime.timedelta(hours=1).total_seconds()),
        limit=1000,
    )
    if not global_reset_ratelimit.record_and_check():
        raise ValueError("password reset ratelimit exceeded")

    token = PasswordResetToken._new(user)
    base = g.https_endpoint or g.origin
    passlink = base + '/resetpassword/' + token._id
    g.log.info("Generated password reset link: " + passlink)
    _system_email(user.email,
                  PasswordReset(user=user,
                                passlink=passlink).render(style='email'),
                  Email.Kind.RESET_PASSWORD,
                  user=user,
                  )
    return True

@trylater_hooks.on('trylater.message_notification_email')
def message_notification_email(data):
    """Queues a system email for a new message notification."""
    from r2.lib.pages import MessageNotificationEmail

    MAX_EMAILS_PER_DAY = 1000
    MESSAGE_THROTTLE_KEY = 'message_notification_emails'

    # If our counter's expired, initialize it again.
    g.cache.add(MESSAGE_THROTTLE_KEY, 0, time=24*60*60)

    for datum in data.itervalues():
        datum = json.loads(datum)
        user = Account._byID36(datum['to'], data=True)
        comment = Comment._by_fullname(datum['comment'], data=True)

        # In case a user has enabled the preference while it was enabled for
        # them, but we've since turned it off.  We need to explicitly state the
        # user because we're not in the context of an HTTP request from them.
        if not feature.is_enabled('orangereds_as_emails', user=user):
            continue

        if g.cache.get(MESSAGE_THROTTLE_KEY) > MAX_EMAILS_PER_DAY:
            raise Exception(
                    'Message notification emails: safety limit exceeded!')

        mac = generate_notification_email_unsubscribe_token(
                datum['to'], user_email=user.email,
                user_password_hash=user.password)
        base = g.https_endpoint or g.origin
        unsubscribe_link = base + '/mail/unsubscribe/%s/%s' % (datum['to'], mac)

        templateData = {
            'sender_username': datum.get('from', ''),
            'comment': comment,
            'permalink': datum['permalink'],
            'unsubscribe_link': unsubscribe_link,
        }
        _system_email(user.email,
                      MessageNotificationEmail(**templateData).render(style='email'),
                      Email.Kind.MESSAGE_NOTIFICATION,
                      from_address=g.notification_email)

        g.stats.simple_event('email.message_notification.queued')
        g.cache.incr(MESSAGE_THROTTLE_KEY)

def generate_notification_email_unsubscribe_token(user_id36, user_email=None,
                                                  user_password_hash=None):
    """Generate a token used for one-click unsubscribe links for notification
    emails.

    user_id36: A base36-encoded user id.
    user_email: The user's email.  Looked up if not provided.
    user_password_hash: The hash of the user's password.  Looked up if not
                        provided.
    """
    import hashlib
    import hmac

    if (not user_email) or (not user_password_hash):
        user = Account._byID36(user_id36, data=True)
        if not user_email:
            user_email = user.email
        if not user_password_hash:
            user_password_hash = user.password

    return hmac.new(
        g.secrets['email_notifications'],
        user_id36 + user_email + user_password_hash,
        hashlib.sha256).hexdigest()

def password_change_email(user):
    """Queues a system email for a password change notification."""
    from r2.lib.pages import PasswordChangeEmail

    return _system_email(user.email,
                         PasswordChangeEmail(user=user).render(style='email'),
                         Email.Kind.PASSWORD_CHANGE,
                         user=user,
                         )

def email_change_email(user):
    """Queues a system email for a email change notification."""
    from r2.lib.pages import EmailChangeEmail

    return _system_email(user.email,
                         EmailChangeEmail(user=user).render(style='email'),
                         Email.Kind.EMAIL_CHANGE)

def community_email(body, kind):
    return _community_email(body, kind)


def ads_email(body, from_name=g.domain):
    """Queues an email to the Sales team."""
    return _ads_email(body, from_name, Email.Kind.ADS_ALERT)

def share(link, emails, from_name = "", reply_to = "", body = ""):
    """Queues a 'share link' email."""
    now = datetime.datetime.now(g.tz)
    ival = now - timeago(g.new_link_share_delay)
    date = max(now,link._date + ival)
    Email.handler.add_to_queue(c.user, emails, from_name, g.share_reply,
                               Email.Kind.SHARE, date = date,
                               body = body, reply_to = reply_to,
                               thing = link)

def send_queued_mail(test = False):
    """sends mail from the mail queue to smtplib for delivery.  Also,
    on successes, empties the mail queue and adds all emails to the
    sent_mail list."""
    from r2.lib.pages import Share, Mail_Opt
    now = datetime.datetime.now(g.tz)
    if not c.site:
        c.site = DefaultSR()

    clear = False
    if not test:
        session = smtplib.SMTP(g.smtp_server)
    def sendmail(email):
        try:
            mimetext = email.to_MIMEText()
            if mimetext is None:
                print ("Got None mimetext for email from %r and to %r"
                       % (email.fr_addr, email.to_addr))
            if test:
                print mimetext.as_string()
            else:
                session.sendmail(email.fr_addr, email.to_addr,
                                 mimetext.as_string())
                email.set_sent(rejected = False)
        # exception happens only for local recipient that doesn't exist
        except (smtplib.SMTPRecipientsRefused, smtplib.SMTPSenderRefused,
                UnicodeDecodeError, AttributeError, HeaderParseError):
            # handle error and print, but don't stall the rest of the queue
            print "Handled error sending mail (traceback to follow)"
            traceback.print_exc(file = sys.stdout)
            email.set_sent(rejected = True)


    try:
        for email in Email.get_unsent(now):
            clear = True

            should_queue = email.should_queue()
            # check only on sharing that the mail is invalid
            if email.kind == Email.Kind.SHARE:
                if should_queue:
                    email.body = Share(username = email.from_name(),
                                       msg_hash = email.msg_hash,
                                       link = email.thing,
                                       body =email.body).render(style = "email")
                else:
                    email.set_sent(rejected = True)
                    continue
            elif email.kind == Email.Kind.OPTOUT:
                email.body = Mail_Opt(msg_hash = email.msg_hash,
                                      leave = True).render(style = "email")
            elif email.kind == Email.Kind.OPTIN:
                email.body = Mail_Opt(msg_hash = email.msg_hash,
                                      leave = False).render(style = "email")
            # handle unknown types here
            elif not email.body:
                print ("Rejecting email with an empty body from %r and to %r"
                       % (email.fr_addr, email.to_addr))
                email.set_sent(rejected = True)
                continue
            sendmail(email)

    finally:
        if not test:
            session.quit()

    # clear is true if anything was found and processed above
    if clear:
        Email.handler.clear_queue(now)



def opt_out(msg_hash):
    """Queues an opt-out email (i.e., a confirmation that the email
    address has been opted out of receiving any future mail)"""
    email, added =  Email.handler.opt_out(msg_hash)
    if email and added:
        _system_email(email, "", Email.Kind.OPTOUT)
    return email, added

def opt_in(msg_hash):
    """Queues an opt-in email (i.e., that the email has been removed
    from our opt out list)"""
    email, removed =  Email.handler.opt_in(msg_hash)
    if email and removed:
        _system_email(email, "", Email.Kind.OPTIN)
    return email, removed


def _promo_email(thing, kind, body = "", **kw):
    from r2.lib.pages import Promo_Email
    a = Account._byID(thing.author_id, True)

    if not a.email:
        return

    body = Promo_Email(link = thing, kind = kind,
                       body = body, **kw).render(style = "email")
    return _system_email(a.email, body, kind, thing = thing,
                         reply_to = g.selfserve_support_email,
                         suppress_username=True)


def new_promo(thing):
    return _promo_email(thing, Email.Kind.NEW_PROMO)

def promo_total_budget(thing, total_budget_dollars, start_date):
    return _promo_email(thing, Email.Kind.BID_PROMO,
        total_budget_dollars = total_budget_dollars, start_date = start_date)

def accept_promo(thing):
    return _promo_email(thing, Email.Kind.ACCEPT_PROMO)

def reject_promo(thing, reason = ""):
    return _promo_email(thing, Email.Kind.REJECT_PROMO, reason)

def edited_live_promo(thing):
    return _promo_email(thing, Email.Kind.EDITED_LIVE_PROMO)

def queue_promo(thing, total_budget_dollars, trans_id):
    return _promo_email(thing, Email.Kind.QUEUED_PROMO,
        total_budget_dollars=total_budget_dollars, trans_id = trans_id)

def live_promo(thing):
    return _promo_email(thing, Email.Kind.LIVE_PROMO)

def finished_promo(thing):
    return _promo_email(thing, Email.Kind.FINISHED_PROMO)


def refunded_promo(thing):
    return _promo_email(thing, Email.Kind.REFUNDED_PROMO)


def void_payment(thing, campaign, total_budget_dollars, reason):
    return _promo_email(thing, Email.Kind.VOID_PAYMENT, campaign=campaign,
                        total_budget_dollars=total_budget_dollars,
                        reason=reason)


def fraud_alert(body):
    return _fraud_email(body, Email.Kind.FRAUD_ALERT)

def suspicious_payment(user, link):
    from r2.lib.pages import SuspiciousPaymentEmail

    body = SuspiciousPaymentEmail(user, link).render(style="email")
    kind = Email.Kind.SUSPICIOUS_PAYMENT

    return _fraud_email(body, kind)


def send_html_email(to_addr, from_addr, subject, html,
        subtype="html", attachments=None):
    from r2.lib.filters import _force_utf8
    if not attachments:
        attachments = []

    msg = MIMEMultipart()
    msg.attach(MIMEText(_force_utf8(html), subtype))
    msg["Subject"] = subject
    msg["From"] = from_addr
    msg["To"] = to_addr

    for attachment in attachments:
        part = MIMEBase('application', "octet-stream")
        part.set_payload(attachment['contents'])
        encoders.encode_base64(part)
        part.add_header('Content-Disposition', 'attachment',
            filename=attachment['name'])
        msg.attach(part)

    session = smtplib.SMTP(g.smtp_server)
    session.sendmail(from_addr, to_addr, msg.as_string())
    session.quit()

trylater_hooks.register_all()
<EOF>
<BOF>
from collections import Counter

from r2.lib.geoip import location_by_ips, organization_by_ips
from r2.lib.utils import tup
from r2.models.ip import IPsByAccount, AccountsByIP


def ips_by_account_id(account_id, limit=None):
    ips = IPsByAccount.get(account_id, column_count=limit or 1000)
    flattened_ips = [j for i in ips for j in i.iteritems()]
    locations = location_by_ips(set(ip for _, ip in flattened_ips))
    orgs = organization_by_ips(set(ip for _, ip in flattened_ips))

    # Deduplicate and summarize total usage time
    counts = Counter((ip for _, ip in flattened_ips))
    seen = set()
    results = []
    for visit_time, ip in flattened_ips:
        if ip in seen:
            continue
        results.append(
            (ip, visit_time, locations.get(ip) or {},
                orgs.get(ip), counts.get(ip))
        )
        seen.add(ip)
    return results


def account_ids_by_ip(ip, after=None, before=None, limit=1000):
    """Get a list of account IDs that an IP has accessed.

    Parameters:
    after -- a `datetime.datetime` from which results should start
    before -- a `datetime.datetime` from which results should end.  If `after`
        is specified, this will be ignored.
    limit -- number of results to return
    """
    ips = tup(ip)
    results = []
    flattened_accounts = {}
    for ip in ips:
        if before and not after:
            # One less result will be returned for `before` queries, so we
            # increase the limit by one.
            account_ip = AccountsByIP.get(
                ip, column_start=before, column_count=limit + 1,
                column_reversed=False)
            account_ip = sorted(account_ip, reverse=True)
        else:
            account_ip = AccountsByIP.get(
                ip, column_start=after, column_count=limit)
        flattened_account_ip = [j for i in account_ip
                                for j in i.iteritems()]
        flattened_accounts[ip] = flattened_account_ip

    for ip, flattened_account_ip in flattened_accounts.iteritems():
        for last_visit, account in flattened_account_ip:
            results.append((account, last_visit, [ip]))
    return results
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""A very simple system for event hooks for plugins etc.

In general, you will probably want to use a ``HookRegistrar`` to manage your
hooks.  The file that contains the code you want to hook into will look
something like this::

    from r2.lib import hooks
    
    def foo(spam):
        # Do a little bit of this and a little bit of that.
        eggs = this(spam)
        baked_beans = that(eggs)
    
        hooks.get_hook('foo').call(ingredient=baked_beans)

Then, any place you want to hook into it, just throw on a decorator::

    from r2.lib.hooks import HookRegistrar
    hooks = HookRegistrar()
    
    @hooks.on('foo')
    def bar(ingredient):
        print ingredient

    hooks.register_all()
"""


_HOOKS = {}


def all_hooks():
    """Return all registered hooks."""
    return _HOOKS


class Hook(object):
    """A single hook that can be listened for."""
    def __init__(self):
        self.handlers = []

    def register_handler(self, handler):
        """Register a handler to call from this hook."""
        self.handlers.append(handler)

    def call(self, **kwargs):
        """Call handlers and return their results.

        Handlers will be called in the same order they were registered and
        their results will be returned in the same order as well.

        """
        return [handler(**kwargs) for handler in self.handlers]

    def call_until_return(self, **kwargs):
        """Call handlers until one returns a non-None value.

        As with call, handlers are called in the same order they are
        registered.  Only the return value of the first non-None handler is
        returned.

        """
        for handler in self.handlers:
            ret = handler(**kwargs)
            if ret is not None:
                return ret


def get_hook(name):
    """Return the named hook `name` creating it if necessary."""
    # this should be atomic as long as `name`'s __hash__ isn't python code
    # or for all types after the fixes in python#13521 are merged into 2.7.
    return _HOOKS.setdefault(name, Hook())


class HookRegistrar(object):
    """A registry for deferring module-scope hook registrations.

    This registry allows us to use module-level decorators but not actually
    register with global hooks unless we're told to.

    """
    def __init__(self):
        self.registered = False
        self.connections = []

    def on(self, name):
        """Return a decorator that registers the wrapped function."""

        hook = get_hook(name)

        def hook_decorator(fn):
            if self.registered:
                hook.register_handler(fn)
            else:
                self.connections.append((hook, fn))
            return fn
        return hook_decorator

    def register_all(self):
        """Complete all deferred registrations."""
        for hook, handler in self.connections:
            hook.register_handler(handler)
        self.registered = True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from Crypto.Cipher import AES
from Crypto.Random import get_random_bytes
import base64
import hashlib
import urllib

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib.filters import _force_utf8


KEY_SIZE = 16  # AES-128
SALT_SIZE = KEY_SIZE * 2  # backwards compatibility


def _pad_message(text):
    """Return `text` padded out to a multiple of block_size bytes.

    This uses the PKCS7 padding algorithm. The pad-bytes have a value of N
    where N is the number of bytes of padding added. If the input string is
    already a multiple of the block size, it will be padded with one full extra
    block to make an unambiguous output string.

    """
    block_size = AES.block_size
    padding_size = (block_size - len(text) % block_size) or block_size
    padding = chr(padding_size) * padding_size
    return text + padding


def _unpad_message(text):
    """Return `text` with padding removed. The inverse of _pad_message."""
    if not text:
        return ""

    padding_size = ord(text[-1])
    if padding_size > AES.block_size:
        return ""

    unpadded, padding = text[:-padding_size], text[-padding_size:]
    if any(ord(x) != padding_size for x in padding):
        return ""

    return unpadded


def _make_cipher(initialization_vector, secret):
    """Return a block cipher object for use in `encrypt` and `decrypt`."""
    return AES.new(secret[:KEY_SIZE], AES.MODE_CBC,
                   initialization_vector[:AES.block_size])


def encrypt(plaintext):
    """Return the message `plaintext` encrypted.

    The encrypted message will have its salt prepended and will be URL encoded
    to make it suitable for use in URLs and Cookies.

    NOTE: this function is here for backwards compatibility. Please do not
    use it for new code.

    """

    salt = _make_salt()
    return _encrypt(salt, plaintext, g.tracking_secret)


def _make_salt():
    # we want SALT_SIZE letters of salt text, but we're generating random bytes
    # so we'll calculate how many bytes we need to get SALT_SIZE characters of
    # base64 output. because of padding, this only works for SALT_SIZE % 4 == 0
    assert SALT_SIZE % 4 == 0
    salt_byte_count = (SALT_SIZE / 4) * 3
    salt_bytes = get_random_bytes(salt_byte_count)
    return base64.b64encode(salt_bytes)


def _encrypt(salt, plaintext, secret):
    cipher = _make_cipher(salt, secret)

    padded = _pad_message(plaintext)
    ciphertext = cipher.encrypt(padded)
    encoded = base64.b64encode(ciphertext)

    return urllib.quote_plus(salt + encoded, safe="")


def decrypt(encrypted):
    """Decrypt `encrypted` and return the plaintext.

    NOTE: like `encrypt` above, please do not use this function for new code.

    """

    return _decrypt(encrypted, g.tracking_secret)


def _decrypt(encrypted, secret):
    encrypted = urllib.unquote_plus(encrypted)
    salt, encoded = encrypted[:SALT_SIZE], encrypted[SALT_SIZE:]
    ciphertext = base64.b64decode(encoded)
    cipher = _make_cipher(salt, secret)
    padded = cipher.decrypt(ciphertext)
    return _unpad_message(padded)


def get_site():
    """Return the name of the current "site" (subreddit)."""
    return c.site.analytics_name if c.site else ""


def get_srpath():
    """Return the srpath of the current request.

    The srpath is Subredditname-Action. e.g. sophiepotamus-GET_listing.

    """
    name = get_site()
    action = None
    if c.render_style in ("mobile", "compact"):
        action = c.render_style
    else:
        action = request.environ['pylons.routes_dict'].get('action')

    if not action:
        return name
    return '-'.join((name, action))


def _get_encrypted_user_slug():
    """Return an encrypted string containing context info."""
    # The cname value (formerly c.cname) is expected by The traffic system.
    cname = False
    data = [
        c.user._id36 if c.user_is_loggedin else "",
        get_srpath(),
        c.lang or "",
        cname,
    ]
    return encrypt("|".join(_force_utf8(s) for s in data))


def get_pageview_pixel_url():
    """Return a URL to use for tracking pageviews for the current request."""
    return g.tracker_url + "?v=" + _get_encrypted_user_slug()


def get_impression_pixel_url(codename):
    """Return a URL to use for tracking impressions of the given advert."""
    # TODO: use HMAC here
    mac = codename + hashlib.sha1(codename + g.tracking_secret).hexdigest()
    v_param = "?v=%s&" % _get_encrypted_user_slug()
    hash_and_id_params = urllib.urlencode({"hash": mac,
                                           "id": codename,})
    return g.adframetracker_url + v_param + hash_and_id_params
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Module for request signing.

"""
import hmac
import hashlib
import re
import pytz

from datetime import datetime
from collections import namedtuple
from pylons import app_globals as g

from r2.lib.utils import Storage, epoch_timestamp, constant_time_compare, tup

GLOBAL_TOKEN_VERSION = 1
SIGNATURE_UA_HEADER = "X-hmac-signed-result"
SIGNATURE_BODY_HEADER = "X-hmac-signed-body"

SIG_HEADER_RE = re.compile(r"^(?P<global_version>\d+?):(?P<payload>.*)$")
SIG_CONTENT_V1_RE = re.compile(
    r"^(?P<platform>.+?):(?P<version>\d+?):(?P<epoch>\d+?):(?P<mac>.*)$"
)


ERRORS = Storage()
SignatureError = namedtuple("SignatureError", "code msg")
for code, msg in (
    ("UNKNOWN", "default signature failure mode (shouldn't happen!)"),
    ("INVALID_FORMAT", "no signature header or completely unparsable"),
    ("UNKOWN_GLOBAL_VERSION", "token global version is from the future"),
    ("UNPARSEABLE", "couldn't parse signature for this global version"),
    ("INVALIDATED_TOKEN", "platform/version combination is invalid."),
    ("EXPIRED_TOKEN", "epoch provided is too old."),
    ("SIGNATURE_MISMATCH", "the payload's signature doesn't match the header"),
    ("MULTISIG_MISMATCH", "more than one version on multiple signatures!")
):
    code = code.upper()
    ERRORS[code] = SignatureError(code, msg)


class SigningResult(object):
    """
    """
    __slots__ = ["global_version", "platform", "version",
                 "mac", "valid_hmac", "epoch", "ignored_errors", "errors"]

    def __init__(
        self,
        global_version=-1,
        platform=None,
        version=-1,
        mac=None,
        valid_hmac=False,
        epoch=None,
    ):
        self.global_version = global_version
        self.platform = platform
        self.version = version
        self.mac = mac
        self.valid_hmac = valid_hmac
        self.epoch = epoch
        self.ignored_errors = []
        self.errors = {}

    def __repr__(self):
        return "<%s (%s)>" % (
            self.__class__.__name__,
            ", ".join("%s=%r" % (k, getattr(self, k)) for k in self.__slots__)
        )

    def add_error(self, error, field=None, details=None):
        """Add an error.

        Duplicate errors (those with the same code and field) will have the
        last `details` stored.

        :param error: The error to be set.
        :param field: where the error came from (generally "body" or "ua")
        :param details: additional error info (for the event)

        :type error: :py:class:`SignatureError`
        :type field: str or None
        :type details: object
        """
        self.errors[(error.code, field)] = details

    def add_ignore(self, ignored_error):
        """Add error to list of ignored errors.

        :param ignored_error: error to be ignored.
        :type ignored_error: :py:class:`SignatureError`
        """
        self.ignored_errors.append(ignored_error)

    def has_errors(self):
        """Determines if the signature has any errors.

        :returns: whether or not there are non-ignored errors
        :rtype: bool
        """
        if self.ignored_errors:
            igcodes = {err.code for err in tup(self.ignored_errors)}
            error_codes = {code for code, _ in self.errors}
            return not error_codes.issubset(igcodes)
        else:
            return bool(self.errors)

    def is_valid(self):
        """Returns if the hmac is valid and the signature has no errors.

        :returns: whether or not this is valid
        :rtype: bool
        """
        return self.valid_hmac and not self.has_errors()

    def update(self, other):
        """Destructively merge this result with another.

        the signatures are combined as needed to generate a final signature
        that is generally the combination of the two as follows:

         - `errors` are combined
         - `global_version`, `platform`, and `version` are compared. In the
            case of a mismatch, "MULTISIG_MISMATCH" error is set.
         - signature validity is independently checked with :py:meth:`is_valid`

        :param other: other result to be merged from
        :type other: :py:class:`SigningResult`

        """
        assert isinstance(other, SigningResult)

        # copy errors onto self
        self.errors.update(other.errors)

        # verify both signatures share versioning info (if they don't,
        # something is _very weird_)
        for attr in ("global_version", "platform", "version"):
            if getattr(self, attr) != getattr(other, attr):
                self.add_error(ERRORS.MULTISIG_MISMATCH, details=attr)
                break

        # also the signature is valid only if both hmacs are valid
        self.valid_hmac = self.valid_hmac and other.valid_hmac


def current_epoch():
    return int(epoch_timestamp(datetime.now(pytz.UTC)))


def valid_epoch(platform, epoch, max_age=5 * 60):
    now = current_epoch()
    dt = abs(now - epoch)
    g.stats.simple_timing("signing.%s.skew" % platform, dt * 1000)
    return dt < max_age


def epoch_wrap(epoch, payload):
    return "Epoch:{}|{}".format(epoch, payload)


def versioned_hmac(secret, body, global_version=GLOBAL_TOKEN_VERSION):
    """Provide an hex hmac for the provided global_version.

    This provides future compatibility if we want to bump the token version
    and change our hashing algorithm.
    """
    # If we want to change the hashing algo or anything else about this hmac,
    # this is the place to make that change based on global_version.
    assert global_version <= GLOBAL_TOKEN_VERSION, (
        "Invalid version signing version '%s'!" % global_version
    )
    return hmac.new(secret, body, hashlib.sha256).hexdigest()


def get_secret_token(platform, version, global_version=GLOBAL_TOKEN_VERSION):
    """For a given platform and version, provide the signing token.

    The signing token for a given platform ("ios", "android", etc.) and version
    is derived by hashing the platform and versions with a server secret. This
    ensures that we can issue new tokens to new clients as need be without
    needing to keep them in a database.  It also means we can invalidate
    old versions of tokens in `is_invalid_token` and trust that the client
    isn't lying to us.
    """
    token_identifier = "{global_version}:{platform}:{version}".format(
        global_version=global_version,
        platform=platform,
        version=version,
    )
    # NOTE: this is the only place in this file where we reference g.secrets.
    # If we wanted to rotate the global secret, this is the place to do it.
    global_secret = g.secrets["request_signature_secret"]
    return versioned_hmac(global_secret, token_identifier, global_version)


def is_invalid_token(platform, version):
    """Conditionally reject a token based on platform and version."""
    return False


def valid_post_signature(request, signature_header=SIGNATURE_BODY_HEADER):
    "Validate that the request has a properly signed body."
    return valid_signature(
        "Body:{}".format(request.body),
        request.headers.get(signature_header),
        field="body",
    )


def valid_ua_signature(
    request,
    signed_headers=("User-Agent", "Client-Vendor-ID"),
    signature_header=SIGNATURE_UA_HEADER,
):
    "Validate that the request has a properly signed user-agent."
    payload = "|".join(
        "{}:{}".format(h, request.headers.get(h) or "")
        for h in signed_headers
    )
    return valid_signature(
        payload,
        request.headers.get(signature_header),
        field="ua",
    )


def valid_signature(payload, signature, field=None):
    """Checks if `signature` matches `payload`.

    `Signature` (at least as of version 1) be of the form:

       {global_version}:{platform}:{version}:{signature}

    where:

      * global_version (currently hard-coded to be "1") can be used to change
            this header's underlying schema later if needs be.  As such, can
            be treated as a protocol version.
      * platform is the client platform type (generally "ios" or "android")
      * version is the client's token version (can be updated and incremented
            per app build as needs be.
      * signature is the hmac of the request's POST body with the token derived
            from the above three parameters via `get_secret_token`

    :param str payload: the signed data
    :param str signature: the signature of the payload
    :param str field: error field to set (one of "ua", "body")
    :returns: object with signature validity and any errors
    :rtype: :py:class:`SigningResult`
    """
    result = SigningResult()

    # if the signature is unparseable, there's not much to do
    sig_match = SIG_HEADER_RE.match(signature or "")
    if not sig_match:
        result.add_error(ERRORS.INVALID_FORMAT, field=field)
        return result

    sig_header_dict = sig_match.groupdict()
    # we're matching \d so this shouldn't throw a TypeError
    result.global_version = int(sig_header_dict['global_version'])

    # incrementing this value is drastic.  We can't validate a token protocol
    # we don't understand.
    if result.global_version > GLOBAL_TOKEN_VERSION:
        result.add_error(ERRORS.UNKOWN_GLOBAL_VERSION, field=field)
        return result

    # currently there's only one version, but here's where we'll eventually
    # patch in more.
    sig_match = SIG_CONTENT_V1_RE.match(sig_header_dict['payload'])
    if not sig_match:
        result.add_error(ERRORS.UNPARSEABLE, field=field)
        return result

    # slop the matched data over to the SigningResult
    sig_match_dict = sig_match.groupdict()
    result.platform = sig_match_dict['platform']
    result.version = int(sig_match_dict['version'])
    result.epoch = int(sig_match_dict['epoch'])
    result.mac = sig_match_dict['mac']

    # verify that the token provided hasn't been invalidated
    if is_invalid_token(result.platform, result.version):
        result.add_error(ERRORS.INVALIDATED_TOKEN, field=field)
        return result

    # check the epoch validity, but don't fail -- leave that up to the
    # validator!
    if not valid_epoch(result.platform, result.epoch):
        result.add_error(
            ERRORS.EXPIRED_TOKEN,
            field=field,
            details=result.epoch,
        )

    # get the expected secret used to verify this request.
    secret_token = get_secret_token(
        result.platform,
        result.version,
        global_version=result.global_version,
    )

    result.valid_hmac = constant_time_compare(
        result.mac,
        versioned_hmac(
            secret_token,
            epoch_wrap(result.epoch, payload),
            result.global_version
        ),
    )

    if not result.valid_hmac:
        result.add_error(ERRORS.SIGNATURE_MISMATCH, field=field)

    return result


def sign_v1_message(body, platform, version, epoch=None):
    """Reference implementation of the v1 mobile body signing."""
    token = get_secret_token(platform, version, global_version=1)
    epoch = int(epoch or current_epoch())
    payload = epoch_wrap(epoch, body)
    signature = versioned_hmac(token, payload, global_version=1)
    return "{global_version}:{platform}:{version}:{epoch}:{signature}".format(
        global_version=1,
        platform=platform,
        version=version,
        epoch=epoch,
        signature=signature,
    )
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime, timedelta

from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons import request

from r2.lib import utils
from r2.models import COOKIE_TIMESTAMP_FORMAT

NEVER = datetime(2037, 12, 31, 23, 59, 59)
DELETE = datetime(1970, 01, 01, 0, 0, 1)


class Cookies(dict):
    def add(self, name, value, *k, **kw):
        name = name.encode('utf-8')
        self[name] = Cookie(value, *k, **kw)


class Cookie(object):
    def __init__(self, value, expires=None, domain=None,
                 dirty=True, secure=None, httponly=False):
        self.value = value
        self.expires = expires
        self.dirty = dirty
        self.secure = secure
        self.httponly = httponly
        if domain:
            self.domain = domain
        else:
            self.domain = g.domain

    @staticmethod
    def classify(cookie_name):
        if cookie_name == g.login_cookie:
            return "session"
        elif cookie_name == g.admin_cookie:
            return "admin"
        elif cookie_name == "reddit_first":
            return "first"
        elif cookie_name == "over18":
            return "over18"
        elif cookie_name == "secure_session":
            return "secure_session"
        elif cookie_name.endswith("_last_thing"):
            return "last_thing"
        elif cookie_name.endswith("_options"):
            return "options"
        elif cookie_name.endswith("_recentclicks2"):
            return "clicks"
        elif cookie_name.startswith("__utm"):
            return "ga"
        elif cookie_name.startswith("beta_"):
            return "beta"
        else:
            return "other"

    def __repr__(self):
        return ("Cookie(value=%r, expires=%r, domain=%r, dirty=%r)"
                % (self.value, self.expires, self.domain, self.dirty))


# Cookies that might need the secure flag toggled
PRIVATE_USER_COOKIES = ["recentclicks2"]
PRIVATE_SESSION_COOKIES = [g.login_cookie, g.admin_cookie, "_options"]


def have_secure_session_cookie():
    cookie = c.cookies.get("secure_session", None)
    return cookie and cookie.value == "1"


def change_user_cookie_security(secure, remember):
    """Mark a user's cookies as either secure or insecure.

    (Un)set the secure flag on sensitive cookies, and add / remove
    the cookie marking the session as HTTPS-only
    """
    if secure:
        set_secure_session_cookie(remember)
    else:
        delete_secure_session_cookie()

    if not c.user_is_loggedin:
        return

    user_name = c.user.name
    securable = (PRIVATE_SESSION_COOKIES +
                 [user_name + "_" + c_name for c_name in PRIVATE_USER_COOKIES])
    for name, cookie in c.cookies.iteritems():
        if name in securable:
            cookie.secure = secure
            if name in PRIVATE_SESSION_COOKIES:
                if name != "_options":
                    cookie.httponly = True
                # TODO: need a way to tell if a session is supposed to last
                # forever. We don't get to see the expiry date of a cookie
                if remember and name == g.login_cookie:
                    cookie.expires = NEVER
            cookie.dirty = True


def set_secure_session_cookie(remember=False):
    expires = NEVER if remember else None
    c.cookies["secure_session"] = Cookie(
        value="1",
        httponly=True,
        expires=expires,
        secure=False,
    )


def delete_secure_session_cookie():
    c.cookies["secure_session"] = Cookie(
        value="",
        httponly=True,
        expires=DELETE,
    )


def upgrade_cookie_security():
    # We only upgrade on POSTs over HTTPS to prevent cookies from being cached
    # by bad proxies
    if not c.secure or request.method != "POST":
        return

    # There's likely not any cookies we need to upgrade
    if not c.user_is_loggedin or c.oauth_user or have_secure_session_cookie():
        return

    # If they authed using a feedhash they might not even have this cookie
    if g.login_cookie not in c.cookies:
        return

    sess_split = c.cookies[g.login_cookie].value.split(",")
    if len(sess_split) != 3:
        return

    # If the cookie's old enough, just pretend we know they had "remember me"
    # ticked.
    sess_start_time = datetime.strptime(sess_split[1], COOKIE_TIMESTAMP_FORMAT)
    rem = (datetime.now() - sess_start_time > timedelta(days=30))
    change_user_cookie_security(secure=True, remember=rem)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import os
import token
import tokenize

from babel.messages.extract import extract_javascript
from cStringIO import StringIO

import babel.messages.frontend
import babel.messages.pofile
import pylons

from pylons.i18n.translation import translation, LanguageError, NullTranslations

try:
    import reddit_i18n
except ImportError:
    I18N_PATH = ''
else:
    I18N_PATH = os.path.dirname(reddit_i18n.__file__)

# Different from the default lang (as defined in the ini file)
# Source language is what is in the source code
SOURCE_LANG = 'en'


def _get_translator(lang, graceful_fail=False, **kwargs):
    """Utility method to get a valid translator object from a language name"""
    from pylons import config

    if not isinstance(lang, list):
        lang = [lang]
    try:
        translator = translation(config['pylons.package'], I18N_PATH,
                                 languages=lang, **kwargs)
    except IOError, ioe:
        if graceful_fail:
            translator = NullTranslations()
        else:
            raise LanguageError('IOError: %s' % ioe)
    translator.pylons_lang = lang
    return translator


def set_lang(lang, graceful_fail=False, fallback_lang=None, **kwargs):
    """Set the i18n language used"""
    registry = pylons.request.environ['paste.registry']
    if not lang:
        registry.replace(pylons.translator, NullTranslations())
    else:
        translator = _get_translator(lang, graceful_fail = graceful_fail, **kwargs)
        base_lang, is_dialect, dialect = lang.partition("-")
        if is_dialect:
            try:
                base_translator = _get_translator(base_lang)
            except LanguageError:
                pass
            else:
                translator.add_fallback(base_translator)
        if fallback_lang:
            fallback_translator = _get_translator(fallback_lang,
                                                  graceful_fail=True)
            translator.add_fallback(fallback_translator)
        registry.replace(pylons.translator, translator)


def load_data(lang_path, domain, extension='data'):
    filename = os.path.join(lang_path, domain + '.' + extension)
    with open(filename) as datafile:
        data = json.load(datafile)
    return data


def iter_langs(base_path=I18N_PATH):
    if base_path:
        # sorted() so that get_active_langs can check completion
        # data on "base" languages of a dialect
        for lang in sorted(os.listdir(base_path)):
            full_path = os.path.join(base_path, lang, 'LC_MESSAGES')
            if os.path.isdir(full_path):
                yield lang, full_path


def get_active_langs(config, path=I18N_PATH, default_lang='en'):
    trans = []
    trans_name = {}
    completions = {}
    domain = config['pylons.package']

    for lang, lang_path in iter_langs(path):
        data = load_data(lang_path, domain)
        name = [data['name'], '']
        if data['_is_enabled'] and lang != default_lang:
            trans.append(lang)
            completion = float(data['num_completed']) / float(data['num_total'])
            completions[lang] = completion
            # This relies on iter_langs hitting the base_lang first
            base_lang, is_dialect, dialect = lang.partition("-")
            if is_dialect:
                if base_lang == SOURCE_LANG:
                    # Source language has to be 100% complete
                    base_completion = 1.0
                else:
                    base_completion = completions.get(base_lang, 0)
                completion = max(completion, base_completion)
            if completion < .5:
                name[1] = ' (*)'
        trans_name[lang] = name
    trans.sort()
    # insert the default language at the top of the list
    trans.insert(0, default_lang)
    if default_lang not in trans_name:
        trans_name[default_lang] = default_lang
    return trans, trans_name


def get_catalog(lang):
    """Return a Catalog object given the language code."""
    path = os.path.join(I18N_PATH, lang, "LC_MESSAGES", "r2.po")
    with open(path, "r") as f:
        return babel.messages.pofile.read_po(f)


def validate_plural_forms(plural_forms_str):
    """Ensure the gettext plural forms expression supplied is valid."""

    # this code is taken from the python stdlib; gettext.py:c2py
    tokens = tokenize.generate_tokens(StringIO(plural_forms_str).readline)

    try:
        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']
    except tokenize.TokenError:
        raise ValueError, \
              'plural forms expression error, maybe unbalanced parenthesis'
    else:
        if danger:
            raise ValueError, 'plural forms expression could be dangerous'


def extract_javascript_msgids(source):
    """Return message ids of translateable strings in JS source."""

    extracted = extract_javascript(
        fileobj=StringIO(source),
        keywords={
            "_": None,
            "P_": (1, 2),
            "N_": None,
            "NP_": (1, 2),
        },
        comment_tags={},
        options={},
    )

    return [msg_id for line, func, msg_id, comments in extracted]
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Module for maintaining long or commonly used translatable strings,
removing the need to pollute the code with lots of extra _ and
ungettext calls.
"""

from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, ungettext, get_lang
import random
import babel.numbers

from r2.lib.filters import websafe
from r2.lib.generate_strings import funny_translatable_strings
from r2.lib.translation import set_lang


__all__ = ['StringHandler', 'strings', 'PluralManager', 'plurals',
           'Score', 'get_funny_translated_string']

# here's where all of the really long site strings (that need to be
# translated) live so as not to clutter up the rest of the code.  This
# dictionary is not used directly but rather is managed by the single
# StringHandler instance strings
string_dict = dict(

    banned_by = "removed by %s",
    banned    = "removed",
    times_banned="removed %d times",
    time_banned="removed at %s",
    time_approved="approved at %s",
    reports   = "reports: %d",

    submitting = _("submitting..."),

    # this accomodates asian languages which don't use spaces
    number_label = _("%(num)d %(thing)s"),

    # this accomodates asian languages which don't use spaces
    points_label = _("%(num)d %(point)s"),

    # this accomodates asian languages which don't use spaces
    time_label = _("%(num)d %(time)s"),

    # this accomodates asian languages which don't use spaces
    float_label = _("%(num)5.3f %(thing)s"),

    already_submitted = _("that link has already been submitted, but you can try to [submit it again](%s)."),

    multiple_submitted = _("that link has been submitted to multiple subreddits. you can try to [submit it again](%s)."),

    user_deactivated = _("your account has been deactivated, but we won't judge you for it."),

    oauth_login_msg = _(
        "Log in or sign up to connect your reddit account with %(app)s."),

    legal = _("I understand and agree that registration on or use of this site constitutes agreement to its %(user_agreement)s and %(privacy_policy)s."),

    friends = _('to view reddit with only submissions from your friends, use [reddit.com/r/friends](%s)'),

    sr_created = _('your subreddit has been created'),

    more_info_link = _("visit [%(link)s](%(link)s) for more information"),

    sr_messages = dict(
        empty =  _('you have not subscribed to any subreddits.'),
        subscriber =  _('below are the subreddits you have subscribed to.'),
        contributor =  _('below are the subreddits that you are an approved submitter on.'),
        moderator = _('below are the subreddits that you have moderator access to.')
        ),

    sr_subscribe =  _('click the `subscribe` or `unsubscribe` buttons to choose which subreddits appear on your front page.'),

    searching_a_reddit = _('you\'re searching within the [%(reddit_name)s](%(reddit_link)s) subreddit. '+
                           'you can also search within [all subreddits](%(all_reddits_link)s)'),

    permalink_title = _("%(author)s comments on %(title)s"),
    link_info_title = _("%(title)s : %(site)s"),
    link_info_og_description = _("%(score)s points and %(num_comments)s comments so far on reddit"),

    submit_link = _("""You are submitting a link. The key to a successful submission is interesting content and a descriptive title."""),
    submit_text = _("""You are submitting a text-based post. Speak your mind. A title is required, but expanding further in the text field is not. Beginning your title with "vote up if" is violation of intergalactic law."""),
    submit_link_label = _("Submit a new link"),
    submit_text_label = _("Submit a new text post"),
    verify_email = _("we're going to need to verify your email address for you to proceed."),
    verify_email_submit = _("you'll be able to submit more frequently once you verify your email address"),
    email_verified =  _("your email address has been verified"),
    email_verify_failed = _("Verification failed.  Please try that again"),
    email_verify_wrong_user = _("The email verification link you've followed is for a different user. Please log out and switch to that user or try again below."),
    search_failed = _("Our search machines are under too much load to handle your request right now. :( Sorry for the inconvenience. Try again in a little bit -- but please don't mash reload; that only makes the problem worse."),
    invalid_search_query = _("I couldn't understand your query, so I simplified it and searched for \"%(clean_query)s\" instead."),
    completely_invalid_search_query = _("I couldn't understand your search query. Please try again."),
    search_help = _("You may also want to check the [search help page](%(search_help)s) for more information."),
    formatting_help_info = _('reddit uses a slightly-customized version of [Markdown](http://daringfireball.net/projects/markdown/syntax) for formatting. See below for some basics, or check [the commenting wiki page](/wiki/commenting) for more detailed help and solutions to common issues.'),
    read_only_msg = _("Reddit is in \"emergency read-only mode\" right now. :( You won't be able to log in. We're sorry and are working frantically to fix the problem."),
    heavy_load_msg = _("this page is temporarily in read-only mode due to heavy traffic."),
    in_perma_timeout_msg = _("Your account has been permanently [suspended](https://reddit.zendesk.com/hc/en-us/articles/205687686) from Reddit."),
    in_temp_timeout_msg = _("Your account has been [suspended](https://reddit.zendesk.com/hc/en-us/articles/205687686) from Reddit for %(days)s."),
    gold_benefits_msg = "reddit gold is our premium membership program. It grants you access to [extra features](https://www.reddit.com/gold/about) to improve your reddit experience. It also makes you really quite dapper. If you have questions about your gold, please visit /r/goldbenefits.",
    lounge_msg = "Grab a drink and join us in /r/lounge, the super-secret members-only community that may or may not exist.",
    postcard_msg = _("You sent us a postcard! (Or something similar.) When we run out of room on our refrigerator, we might one day auction off the stuff that people sent in. Is it okay if we include your thing?"),
    over_comment_limit = _("Sorry, the maximum number of comments is %(max)d. (However, if you subscribe to reddit gold, it goes up to %(goldmax)d.)"),
    over_comment_limit_gold = _("Sorry, the maximum number of comments is %d."),
    youve_got_gold = "%(sender)s just gifted you %(amount)s of reddit gold!",
    giftgold_note = "Here's a note that was included:\n\n----\n\n",
    youve_been_gilded_comment = "%(sender)s liked [your comment](%(url)s) so much that they gilded it, giving you reddit gold.\n\n",
    youve_been_gilded_link = "%(sender)s liked [your submission](%(url)s) so much that they gilded it, giving you reddit gold.\n\n",
    respond_to_anonymous_gilder = "Want to say thanks to your mysterious benefactor? Reply to this message. You will find out their username if they choose to reply back.",
    unsupported_respond_to_gilder = "Sorry, replying directly to your mysterious benefactor is not yet supported for this gilding.",
    anonymous_gilder_warning = _("***WARNING: Responding to this message will reveal your username to the gildee.***\n\n"),
    gold_claimed_code = _("Thanks for claiming a reddit gold code.\n\n"),
    gold_summary_autorenew_monthly = _("You're about to set up an ongoing, autorenewing subscription to reddit gold for yourself (%(user)s). \n\nYou'll pay **%(price)s** for this, **monthly**. \n\n>This subscription will renew automatically each month until you cancel. You may cancel at any time. If you cancel, you will not be billed for any additional months of service, and service will continue until the end of the billing period. If you cancel, you will not receive a refund for any service already paid for. Receipts will be delivered via private message in your account."),
    gold_summary_autorenew_yearly = _("You're about to set up an ongoing, autorenewing subscription to reddit gold for yourself (%(user)s). \n\nYou'll pay **%(price)s** for this, **yearly**. \n\n>This subscription will renew automatically each year until you cancel. You may cancel at any time. If you cancel, you will not be billed for any additional years of service, and service will continue until the end of the billing period. If you cancel, you will not receive a refund for any service already paid for. Receipts will be delivered via private message in your account."),
    gold_summary_onetime = _("You're about to make a one-time purchase of %(amount)s of reddit gold for yourself (%(user)s). You'll pay a total of %(price)s for this."),
    gold_summary_creddits = _("You're about to purchase %(amount)s. They work like gift certificates: each creddit you have will allow you to give one month of reddit gold to someone else. You'll pay a total of %(price)s for this."),
    gold_summary_gift_code = _("You're about to purchase %(amount)s of reddit gold in the form of a gift code. The recipient (or you) will be able to claim the code to redeem that gold to their account. You'll pay a total of %(price)s for this."),
    gold_summary_signed_gift = _("You're about to give %(amount)s of reddit gold to %(recipient)s, who will be told that it came from you. You'll pay a total of %(price)s for this."),
    gold_summary_anonymous_gift = _("You're about to give %(amount)s of reddit gold to %(recipient)s. It will be an anonymous gift. You'll pay a total of %(price)s for this."),
    gold_summary_gilding_comment = _("Want to say thanks to *%(recipient)s* for this comment? Give them a month of [reddit gold](/gold/about)."),
    gold_summary_gilding_link = _("Want to say thanks to *%(recipient)s* for this submission? Give them a month of [reddit gold](/gold/about)."),
    gold_summary_gilding_page_comment = _("You're about to give *%(recipient)s* a month of [reddit gold](/gold/about) for this comment:"),
    gold_summary_gilding_page_link = _("You're about to give *%(recipient)s* a month of [reddit gold](/gold/about) for this submission:"),
    gold_summary_gilding_page_footer = _("You'll pay a total of %(price)s for this."),
    archived_post_message = _("This is an archived post. You won't be able to vote or comment."),
    locked_post_message = _("This post is locked. You won't be able to comment."),
    account_activity_blurb = _("This page shows a history of recent activity on your account. If you notice unusual activity, you should change your password immediately. Location information is guessed from your computer's IP address and may be wildly wrong, especially for visits from mobile devices."),
    your_current_ip_is = _("You are currently accessing reddit from this IP address: %(address)s."),
    account_activity_apps_blurb = _("""
These apps are authorized to access your account. Signing out of all sessions
will revoke access from all apps. You may also revoke access from individual
apps below.
"""),

    traffic_promoted_link_explanation = _("Below you will see your promotion's impression and click traffic per hour of promotion.  Please note that these traffic totals will lag behind by two to three hours, and that daily totals will be preliminary until 24 hours after the link has finished its run."),
    traffic_processing_slow = _("Traffic processing is currently running slow. The latest data available is from %(date)s. This page will be updated as new data becomes available."),
    traffic_processing_normal = _("Traffic processing occurs on an hourly basis. The latest data available is from %(date)s. This page will be updated as new data becomes available."),
    traffic_help_email = _("Questions? Email self serve support: %(email)s"),

    traffic_subreddit_explanation = _("""
Below are the traffic statistics for your subreddit. Each graph represents one of the following over the interval specified.

* **pageviews** are all hits to %(subreddit)s, including both listing pages and comment pages.
* **uniques** are the total number of unique visitors (determined by a combination of their IP address and User Agent string) that generate the above pageviews. This is independent of whether or not they are signed in.
* **subscriptions** is the number of new subscriptions that have been generated in a given day. This number is less accurate than the first two metrics, as, though we can track new subscriptions, we have no way to track unsubscriptions.

Note: there are a couple of places outside of your subreddit where someone can click "subscribe", so it is possible (though unlikely) that the subscription count can exceed the unique count on a given day.
"""),

    subscribed_multi = _("multireddit of your subscriptions"),
    mod_multi = _("multireddit of subreddits you moderate"),

    r_all_description = _("/r/all displays content from all of reddit, including subreddits you aren't subscribed to. Some subreddits have chosen to exclude themselves from /r/all."),
    r_all_minus_description = _("Displaying content from /r/all of reddit, except the following subreddits:"),
    all_minus_gold_only = _('Filtering /r/all is a feature only available to [reddit gold](/gold/about) subscribers. Displaying unfiltered results from /r/all.'),
)

class StringHandler(object):
    """Class for managing long translatable strings.  Allows accessing
    of strings via both getitem and getattr.  In both cases, the
    string is passed through the gettext _ function before being
    returned."""
    def __init__(self, **sdict):
        self.string_dict = sdict

    def get(self, attr, default=None):
        try:
            return self[attr]
        except KeyError:
            return default

    def __getitem__(self, attr):
        try:
            return self.__getattr__(attr)
        except AttributeError:
            raise KeyError

    def __getattr__(self, attr):
        rval = self.string_dict[attr]
        if isinstance(rval, (str, unicode)):
            return _(rval)
        elif isinstance(rval, dict):
            return StringHandler(**rval)
        else:
            raise AttributeError

    def __iter__(self):
        return iter(self.string_dict)

    def keys(self):
        return self.string_dict.keys()

strings = StringHandler(**string_dict)


def P_(x, y):
    """Convenience method for handling pluralizations.  This identity
    function has been added to the list of keyword functions for babel
    in setup.cfg so that the arguments are translated without having
    to resort to ungettext and _ trickery."""
    return (x, y)

class PluralManager(object):
    """String handler for dealing with pluralizable forms.  plurals
    are passed in in pairs (sing, pl) and can be accessed via
    self.sing and self.pl.

    Additionally, calling self.N_sing(n) (or self.N_pl(n)) (where
    'sing' and 'pl' are placeholders for a (sing, pl) pairing) is
    equivalent to ungettext(sing, pl, n)
    """
    def __init__(self, plurals):
        self.string_dict = {}
        for s, p in plurals:
            self.string_dict[s] = self.string_dict[p] = (s, p)

    def __getattr__(self, attr):
        to_func = False
        if attr.startswith("N_"):
            attr = attr[2:]
            to_func = True

        attr = attr.replace("_", " ")
        if to_func:
            rval = self.string_dict[attr]
            return lambda x: ungettext(rval[0], rval[1], x)
        else:
            rval = self.string_dict[attr]
            n = 1 if attr == rval[0] else 5
            return ungettext(rval[0], rval[1], n)

plurals = PluralManager([P_("comment",     "comments"),
                         P_("point",       "points"),

                         # things
                         P_("link",        "links"),
                         P_("comment",     "comments"),
                         P_("message",     "messages"),
                         P_("subreddit",   "subreddits"),
                         P_("creddit",     "creddits"),

                         # people
                         P_("reader",  "readers"),
                         P_("subscriber",  "subscribers"),
                         P_("approved submitter", "approved submitters"),
                         P_("moderator",   "moderators"),
                         P_("user here now",   "users here now"),

                         # time words
                         P_("milliseconds","milliseconds"),
                         P_("second",      "seconds"),
                         P_("minute",      "minutes"),
                         P_("hour",        "hours"),
                         P_("day",         "days"),
                         P_("month",       "months"),
                         P_("year",        "years"),
])


class Score(object):
    """Convienience class for populating '10 points' in a traslatible
    fasion, used primarily by the score() method in printable.html"""

    # This used to pass through _() because allegedly Japanese needed different
    # markup, but that doesn't appear to be the case anymore
    PERSON_LABEL = ('<span class="number">%(num)s</span>&#32;'
                    '<span class="word">%(persons)s</span>')

    @staticmethod
    def number_only(x):
        return str(max(x, 0))

    @staticmethod
    def points(x):
        return strings.points_label % dict(num=x,
                                           point=plurals.N_points(x))

    @staticmethod
    def safepoints(x):
        return Score.points(max(x, 0))

    @staticmethod
    def _people(x, label, prepend=''):
        num = prepend + babel.numbers.format_number(x, c.locale)
        return Score.PERSON_LABEL % \
            dict(num=num, persons=websafe(label(x)))

    @staticmethod
    def subscribers(x):
        return Score._people(x, plurals.N_subscribers)

    @staticmethod
    def readers(x):
        return Score._people(x, plurals.N_readers)

    @staticmethod
    def somethings(x, word):
        p = plurals.string_dict[word]
        f = lambda x: ungettext(p[0], p[1], x)
        return strings.number_label % dict(num=x, thing=f(x))

    @staticmethod
    def users_here_now(x, prepend=''):
        return Score._people(x, plurals.N_users_here_now, prepend=prepend)

    @staticmethod
    def none(x):
        return ""


def fallback_trans(x):
    """For translating placeholder strings the user should never see
    in raw form, such as 'funny 500 message'.  If the string does not
    translate in the current language, falls back on the g.lang
    translation that we've hopefully already provided"""
    t = _(x)
    if t == x:
        l = get_lang()
        set_lang(g.lang, graceful_fail = True)
        t = _(x)
        if l and l[0] != g.lang:
            set_lang(l[0])
    return t


def get_funny_translated_string(category, num=1):
    strings = random.sample(funny_translatable_strings[category], num)
    ret = [fallback_trans(string) for string in strings]
    if len(ret) == 1:
        return ret[0]
    else:
        return ret
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Try to regenerate the permacache items devoted to listings after a
storage failure in Cassandra
"""

"""
cat > mr_permacache <<HERE
#!/bin/sh
cd ~/reddit/r2
paster run staging.ini ./mr_permacache.py -c "\$1"
HERE
chmod u+x mr_permacache

LINKDBHOST=prec01
COMMENTDBHOST=db02s1
VOTEDBHOST=db03s1
SAVEHIDEDBHOST=db01s1

## links
time psql -F"\t" -A -t -d newreddit -U ri -h $LINKDBHOST \
     -c "\\copy (select t.thing_id, 'thing', 'link',
                        t.ups, t.downs, t.deleted, t.spam, extract(epoch from t.date)
                   from reddit_thing_link t) to 'reddit_thing_link.dump'"
time psql -F"\t" -A -t -d newreddit -U ri -h $LINKDBHOST \
     -c "\\copy (select d.thing_id, 'data', 'link',
                        d.key, d.value
                   from reddit_data_link d
                  where d.key = 'author_id' or d.key = 'sr_id') to 'reddit_data_link.dump'"
pv reddit_data_link.dump reddit_thing_link.dump | sort -T. -S200m | ./mr_permacache "join_links()" > links.joined
pv links.joined | ./mr_permacache "link_listings()" | sort -T. -S200m > links.listings

## comments
psql -F"\t" -A -t -d newreddit -U ri -h $COMMENTDBHOST \
     -c "\\copy (select t.thing_id, 'thing', 'comment',
                        t.ups, t.downs, t.deleted, t.spam, extract(epoch from t.date)
                   from reddit_thing_comment t) to 'reddit_thing_comment.dump'"
psql -F"\t" -A -t -d newreddit -U ri -h $COMMENTDBHOST \
     -c "\\copy (select d.thing_id, 'data', 'comment',
                        d.key, d.value
                   from reddit_data_comment d
                  where d.key = 'author_id') to 'reddit_data_comment.dump'"
cat reddit_data_comment.dump reddit_thing_comment.dump | sort -T. -S200m | ./mr_permacache "join_comments()" > comments.joined
cat links.joined | ./mr_permacache "comment_listings()" | sort -T. -S200m > comments.listings

## linkvotes
psql -F"\t" -A -t -d newreddit -U ri -h $VOTEDBHOST \
     -c "\\copy (select r.rel_id, 'vote_account_link',
                        r.thing1_id, r.thing2_id, r.name, extract(epoch from r.date)
                   from reddit_rel_vote_account_link r) to 'reddit_linkvote.dump'"
pv reddit_linkvote.dump | ./mr_permacache "linkvote_listings()" | sort -T. -S200m > linkvotes.listings

#savehide
psql -F"\t" -A -t -d newreddit -U ri -h $SAVEHIDEDBHOST \
     -c "\\copy (select r.rel_id, 'savehide',
                        r.thing1_id, r.thing2_id, r.name, extract(epoch from r.date)
                   from reddit_rel_savehide r) to 'reddit_savehide.dump'"
pv reddit_savehide.dump | ./mr_permacache "savehide_listings()" | sort -T. -S200m > savehide.listings

## load them up
# the individual .listings files are sorted so even if it's not sorted
# overall we don't need to re-sort them
mkdir listings
pv *.listings | ./mr_permacache "top1k_writefiles('listings')"
./mr_permacache "write_permacache_from_dir('$PWD/listings')"

"""

import os, os.path, errno
import sys
import itertools
from hashlib import md5

from r2.lib import mr_tools
from r2.lib.mr_tools import dataspec_m_thing, dataspec_m_rel, join_things


from dateutil.parser import parse as parse_timestamp

from r2.models import *
from r2.lib.db.sorts import epoch_seconds, score, controversy, _hot
from r2.lib.utils import fetch_things2, in_chunks, progress, UniqueIterator, tup
from r2.lib import comment_tree
from r2.lib.db import queries

from r2.lib.jsontemplates import make_fullname # what a strange place
                                               # for this function

def join_links():
    join_things(('author_id', 'sr_id'))

def link_listings():
    @dataspec_m_thing(('author_id', int),
                      ('sr_id', int))
    def process(link):
        assert link.thing_type == 'link'

        author_id = link.author_id
        timestamp = link.timestamp
        fname = make_fullname(Link, link.thing_id)

        yield 'user-submitted-%d' % author_id, timestamp, fname
        if not link.spam:
            sr_id = link.sr_id
            ups, downs = link.ups, link.downs

            yield ('sr-hot-all-%d' % sr_id, _hot(ups, downs, timestamp),
                   timestamp, fname)
            yield 'sr-new-all-%d' % sr_id, timestamp, fname
            yield 'sr-top-all-%d' % sr_id, score(ups, downs), timestamp, fname
            yield ('sr-controversial-all-%d' % sr_id,
                   controversy(ups, downs), timestamp, fname)
            for time in '1 year', '1 month', '1 week', '1 day', '1 hour':
                if timestamp > epoch_seconds(timeago(time)):
                    tkey = time.split(' ')[1]
                    yield ('sr-top-%s-%d' % (tkey, sr_id),
                           score(ups, downs), timestamp, fname)
                    yield ('sr-controversial-%s-%d' % (tkey, sr_id),
                           controversy(ups, downs),
                           timestamp, fname)

    mr_tools.mr_map(process)

def join_comments():
    join_things(('author_id',))

def comment_listings():
    @dataspec_m_thing(('author_id', int),)
    def process(comment):
        assert comment.thing_type == 'comment'

        yield ('user-commented-%d' % comment.author_id,
               comment.timestamp, make_fullname(Comment, comment.thing_id))

    mr_tools.mr_map(process)

def rel_listings(names, thing2_cls = Link):
    # names examples: {'1': 'liked',
    #                  '-1': 'disliked'}
    @dataspec_m_rel()
    def process(rel):
        if rel.name in names:
            yield ('%s-%s' % (names[rel.name], rel.thing1_id), rel.timestamp,
                   make_fullname(thing2_cls, rel.thing2_id))
    mr_tools.mr_map(process)

def linkvote_listings():
    rel_listings({'1': 'liked',
                  '-1': 'disliked'})

def savehide_listings():
    rel_listings({'save': 'saved',
                  'hide': 'hidden'})

def insert_to_query(q, items):
    q._insert_tuples(items)

def store_keys(key, maxes):
    # we're building queries from queries.py, but we could avoid this
    # by making the queries ourselves if we wanted to avoid the
    # individual lookups for accounts and subreddits
    userrel_fns = dict(liked = queries.get_liked,
                       disliked = queries.get_disliked,
                       saved = queries.get_saved,
                       hidden = queries.get_hidden)
    if key.startswith('user-'):
        acc_str, keytype, account_id = key.split('-')
        account_id = int(account_id)
        fn = queries.get_submitted if keytype == 'submitted' else queries.get_comments
        q = fn(Account._byID(account_id), 'new', 'all')
        insert_to_query(q, [(fname, float(timestamp))
                            for (timestamp, fname)
                            in maxes ])
    elif key.startswith('sr-'):
        sr_str, sort, time, sr_id = key.split('-')
        sr_id = int(sr_id)

        if sort == 'controversy':
            # I screwed this up in the mapper and it's too late to fix
            # it
            sort = 'controversial'

        q = queries.get_links(Subreddit._byID(sr_id), sort, time)
        insert_to_query(q, [tuple([item[-1]] + map(float, item[:-1]))
                            for item in maxes])

    elif key.split('-')[0] in userrel_fns:
        key_type, account_id = key.split('-')
        account_id = int(account_id)
        fn = userrel_fns[key_type]
        q = fn(Account._byID(account_id))
        insert_to_query(q, [tuple([item[-1]] + map(float, item[:-1]))
                            for item in maxes])

def top1k_writefiles(dirname):
    """Divide up the top 1k of each key into its own file to make
       restarting after a failure much easier. Pairs with
       write_permacache_from_dir"""
    def hashdir(name, levels = [3]):
        # levels is a list of how long each stage if the hashdirname
        # should be. So [2,2] would make dirs like
        # 'ab/cd/thelisting.txt' (and this function would just return
        # the string 'ab/cd', so that you have the dirname that you
        # can create before os.path.joining to the filename)
        h = md5(name).hexdigest()

        last = 0
        dirs = []
        for l in levels:
            dirs.append(h[last:last+l])
            last += l

        return os.path.join(*dirs)

    def post(key, maxes):
        # we're taking a hash like 12345678901234567890123456789012
        # and making a directory name two deep out of the first half
        # of the characters. We may want to tweak this as the number
        # of listings

        hd = os.path.join(dirname, hashdir(key))
        try:
            os.makedirs(hd)
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise
        filename = os.path.join(hd, key)

        with open(filename, 'w') as f:
            for item in maxes:
                f.write('%s\t' % key)
                f.write('\t'.join(item))
                f.write('\n')
        
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=post)

def top1k_writepermacache(fd = sys.stdin):
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=store_keys,
                                   fd = fd)

def write_permacache_from_dir(dirname):
    # we want the whole list so that we can display accurate progress
    # information. If we're operating on more than tens of millions of
    # files, we should either bail out or tweak this to not need the
    # whole list at once
    allfiles = []
    for root, dirs, files in os.walk(dirname):
        for f in files:
            allfiles.append(os.path.join(root, f))

    for fname in progress(allfiles, persec=True):
        try:
            write_permacache_from_file(fname)
            os.unlink(fname)
        except:
            mr_tools.status('failed on %r' % fname)
            raise

    mr_tools.status('Removing empty directories')
    for root, dirs, files in os.walk(dirname, topdown=False):
        for d in dirs:
            dname = os.path.join(root, d)
            try:
                os.rmdir(dname)
            except OSError as e:
                if e.errno == errno.ENOTEMPTY:
                    mr_tools.status('%s not empty' % (dname,))
                else:
                    raise

def write_permacache_from_file(fname):
    with open(fname) as fd:
        top1k_writepermacache(fd = fd)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Generate the data for the listings for the time-based Subreddit
queries. The format is eventually that of the CachedResults objects
used by r2.lib.db.queries (with some intermediate steps), so changes
there may warrant changes here
"""

# to run:
"""
export LINKDBHOST=prec01
export USER=ri
export INI=production.ini
cd ~/reddit/r2
time psql -F"\t" -A -t -d newreddit -U $USER -h $LINKDBHOST \
     -c "\\copy (select t.thing_id, 'thing', 'link',
                        t.ups, t.downs, t.deleted, t.spam, extract(epoch from t.date)
                   from reddit_thing_link t
                  where not t.spam and not t.deleted
                  )
                  to 'reddit_thing_link.dump'"
time psql -F"\t" -A -t -d newreddit -U $USER -h $LINKDBHOST \
     -c "\\copy (select d.thing_id, 'data', 'link',
                        d.key, d.value
                   from reddit_data_link d
                  where d.key = 'url' ) to 'reddit_data_link.dump'"
cat reddit_data_link.dump reddit_thing_link.dump | sort -T. -S200m | paster --plugin=r2 run $INI r2/lib/migrate/mr_domains.py -c "join_links()" > links.joined
cat links.joined | paster --plugin=r2 run $INI r2/lib/migrate/mr_domains.py -c "time_listings()" | sort -T. -S200m | paster --plugin=r2 run $INI r2/lib/migrate/mr_domains.py -c "write_permacache()"
"""

import sys

from r2.models import Account, Subreddit, Link
from r2.lib.db.sorts import epoch_seconds, score, controversy, _hot
from r2.lib.db import queries
from r2.lib import mr_tools
from r2.lib.utils import timeago, UrlParser
from r2.lib.jsontemplates import make_fullname # what a strange place
                                               # for this function
def join_links():
    mr_tools.join_things(('url',))


def time_listings(times = ('all',)):
    oldests = dict((t, epoch_seconds(timeago('1 %s' % t)))
                   for t in times if t != "all")
    oldests['all'] = epoch_seconds(timeago('10 years'))

    @mr_tools.dataspec_m_thing(("url", str),)
    def process(link):
        assert link.thing_type == 'link'

        timestamp = link.timestamp
        fname = make_fullname(Link, link.thing_id)

        if not link.spam and not link.deleted:
            if link.url:
                domains = UrlParser(link.url).domain_permutations()
            else:
                domains = []
            ups, downs = link.ups, link.downs

            for tkey, oldest in oldests.iteritems():
                if timestamp > oldest:
                    sc = score(ups, downs)
                    contr = controversy(ups, downs)
                    h = _hot(ups, downs, timestamp)
                    for domain in domains:
                        yield ('domain/top/%s/%s' % (tkey, domain),
                               sc, timestamp, fname)
                        yield ('domain/controversial/%s/%s' % (tkey, domain),
                               contr, timestamp, fname)
                        if tkey == "all":
                            yield ('domain/hot/%s/%s' % (tkey, domain),
                                   h, timestamp, fname)
                            yield ('domain/new/%s/%s' % (tkey, domain),
                                   timestamp, timestamp, fname)

    mr_tools.mr_map(process)

def store_keys(key, maxes):
    # we're building queries using queries.py, but we could make the
    # queries ourselves if we wanted to avoid the individual lookups
    # for accounts and subreddits.

    # Note that we're only generating the 'sr-' type queries here, but
    # we're also able to process the other listings generated by the
    # old migrate.mr_permacache for convenience

    userrel_fns = dict(liked = queries.get_liked,
                       disliked = queries.get_disliked,
                       saved = queries.get_saved,
                       hidden = queries.get_hidden)

    if key.startswith('user-'):
        acc_str, keytype, account_id = key.split('-')
        account_id = int(account_id)
        fn = queries.get_submitted if keytype == 'submitted' else queries.get_comments
        q = fn(Account._byID(account_id), 'new', 'all')
        q._insert_tuples([(fname, float(timestamp))
                    for (timestamp, fname)
                    in maxes])

    elif key.startswith('sr-'):
        sr_str, sort, time, sr_id = key.split('-')
        sr_id = int(sr_id)

        if sort == 'controversy':
            # I screwed this up in the mapper and it's too late to fix
            # it
            sort = 'controversial'

        q = queries.get_links(Subreddit._byID(sr_id), sort, time)
        q._insert_tuples([tuple([item[-1]] + map(float, item[:-1]))
                    for item in maxes])
    elif key.startswith('domain/'):
        d_str, sort, time, domain = key.split('/')
        q = queries.get_domain_links(domain, sort, time)
        q._insert_tuples([tuple([item[-1]] + map(float, item[:-1]))
                    for item in maxes])


    elif key.split('-')[0] in userrel_fns:
        key_type, account_id = key.split('-')
        account_id = int(account_id)
        fn = userrel_fns[key_type]
        q = fn(Account._byID(account_id))
        q._insert_tuples([tuple([item[-1]] + map(float, item[:-1]))
                    for item in maxes])


def write_permacache(fd = sys.stdin):
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=store_keys,
                                   fd = fd)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
One-time use functions to migrate from one reddit-version to another
"""
from r2.lib.promote import *

def add_allow_top_to_srs():
    "Add the allow_top property to all stored subreddits"
    from r2.models import Subreddit
    from r2.lib.db.operators import desc
    from r2.lib.utils import fetch_things2

    q = Subreddit._query(Subreddit.c._spam == (True,False),
                         sort = desc('_date'))
    for sr in fetch_things2(q):
        sr.allow_top = True; sr._commit()

def subscribe_to_blog_and_annoucements(filename):
    import re
    from time import sleep
    from r2.models import Account, Subreddit

    r_blog = Subreddit._by_name("blog")
    r_announcements = Subreddit._by_name("announcements")

    contents = file(filename).read()
    numbers = [ int(s) for s in re.findall("\d+", contents) ]

#    d = Account._byID(numbers, data=True)

#   for i, account in enumerate(d.values()):
    for i, account_id in enumerate(numbers):
        account = Account._byID(account_id, data=True)

        for sr in r_blog, r_announcements:
            if sr.add_subscriber(account):
                sr._incr("_ups", 1)
                print ("%d: subscribed %s to %s" % (i, account.name, sr.name))
            else:
                print ("%d: didn't subscribe %s to %s" % (i, account.name, sr.name))


def recompute_unread(min_date = None):
    from r2.models import Inbox, Account, Comment, Message
    from r2.lib.db import queries

    def load_accounts(inbox_rel):
        accounts = set()
        q = inbox_rel._query(eager_load = False, data = False,
                             sort = desc("_date"))
        if min_date:
            q._filter(inbox_rel.c._date > min_date)

        for i in fetch_things2(q):
            accounts.add(i._thing1_id)

        return accounts

    accounts_m = load_accounts(Inbox.rel(Account, Message))
    for i, a in enumerate(accounts_m):
        a = Account._byID(a)
        print "%s / %s : %s" % (i, len(accounts_m), a)
        queries.get_unread_messages(a).update()
        queries.get_unread_comments(a).update()
        queries.get_unread_selfreply(a).update()

    accounts = load_accounts(Inbox.rel(Account, Comment)) - accounts_m
    for i, a in enumerate(accounts):
        a = Account._byID(a)
        print "%s / %s : %s" % (i, len(accounts), a)
        queries.get_unread_comments(a).update()
        queries.get_unread_selfreply(a).update()



def pushup_permacache(verbosity=1000):
    """When putting cassandra into the permacache chain, we need to
       push everything up into the rest of the chain, so this is
       everything that uses the permacache, as of that check-in."""
    from pylons import app_globals as g
    from r2.models import Link, Subreddit, Account
    from r2.lib.db.operators import desc
    from r2.lib.comment_tree import comments_key, messages_key
    from r2.lib.utils import fetch_things2, in_chunks
    from r2.lib.utils import last_modified_key
    from r2.lib.promote import promoted_memo_key
    from r2.lib.subreddit_search import load_all_reddits
    from r2.lib.db import queries
    from r2.lib.cache import CassandraCacheChain

    authority = g.permacache.caches[-1]
    nonauthority = CassandraCacheChain(g.permacache.caches[1:-1])

    def populate(keys):
        vals = authority.simple_get_multi(keys)
        if vals:
            nonauthority.set_multi(vals)

    def gen_keys():
        yield promoted_memo_key

        # just let this one do its own writing
        load_all_reddits()

        yield queries.get_all_comments().iden

        l_q = Link._query(Link.c._spam == (True, False),
                          Link.c._deleted == (True, False),
                          sort=desc('_date'),
                          data=True,
                          )
        for link in fetch_things2(l_q, verbosity):
            yield comments_key(link._id)
            yield last_modified_key(link, 'comments')

        a_q = Account._query(Account.c._spam == (True, False),
                             sort=desc('_date'),
                             )
        for account in fetch_things2(a_q, verbosity):
            yield messages_key(account._id)
            yield last_modified_key(account, 'overview')
            yield last_modified_key(account, 'commented')
            yield last_modified_key(account, 'submitted')
            yield last_modified_key(account, 'liked')
            yield last_modified_key(account, 'disliked')
            yield queries.get_comments(account, 'new', 'all').iden
            yield queries.get_submitted(account, 'new', 'all').iden
            yield queries.get_liked(account).iden
            yield queries.get_disliked(account).iden
            yield queries.get_hidden(account).iden
            yield queries.get_saved(account).iden
            yield queries.get_inbox_messages(account).iden
            yield queries.get_unread_messages(account).iden
            yield queries.get_inbox_comments(account).iden
            yield queries.get_unread_comments(account).iden
            yield queries.get_inbox_selfreply(account).iden
            yield queries.get_unread_selfreply(account).iden
            yield queries.get_sent(account).iden

        sr_q = Subreddit._query(Subreddit.c._spam == (True, False),
                                sort=desc('_date'),
                                )
        for sr in fetch_things2(sr_q, verbosity):
            yield last_modified_key(sr, 'stylesheet_contents')
            yield queries.get_links(sr, 'hot', 'all').iden
            yield queries.get_links(sr, 'new', 'all').iden

            for sort in 'top', 'controversial':
                for time in 'hour', 'day', 'week', 'month', 'year', 'all':
                    yield queries.get_links(sr, sort, time,
                                            merge_batched=False).iden
            yield queries.get_spam_links(sr).iden
            yield queries.get_spam_comments(sr).iden
            yield queries.get_reported_links(sr).iden
            yield queries.get_reported_comments(sr).iden
            yield queries.get_subreddit_messages(sr).iden
            yield queries.get_unread_subreddit_messages(sr).iden

    done = 0
    for keys in in_chunks(gen_keys(), verbosity):
        g.reset_caches()
        done += len(keys)
        print 'Done %d: %r' % (done, keys[-1])
        populate(keys)


def port_cassaurls(after_id=None, estimate=15231317):
    from r2.models import Link, LinksByUrlAndSubreddit
    from r2.lib.db import tdb_cassandra
    from r2.lib.db.operators import desc
    from r2.lib.db.tdb_cassandra import CL
    from r2.lib.utils import fetch_things2, in_chunks, progress

    q = Link._query(Link.c._spam == (True, False),
                    sort=desc('_date'), data=True)
    if after_id:
        q._after(Link._byID(after_id,data=True))
    q = fetch_things2(q, chunk_size=500)
    q = progress(q, estimate=estimate)
    q = (l for l in q
         if getattr(l, 'url', 'self') != 'self'
         and not getattr(l, 'is_self', False))
    chunks = in_chunks(q, 500)

    for chunk in chunks:
        for l in chunk:
            LinksByUrlAndSubreddit.add_link(l)

def port_deleted_links(after_id=None):
    from r2.models import Link
    from r2.lib.db.operators import desc
    from r2.models.query_cache import CachedQueryMutator
    from r2.lib.db.queries import get_deleted_links
    from r2.lib.utils import fetch_things2, in_chunks, progress

    q = Link._query(Link.c._deleted == True,
                    Link.c._spam == (True, False),
                    sort=desc('_date'), data=True)
    q = fetch_things2(q, chunk_size=500)
    q = progress(q, verbosity=1000)

    for chunk in in_chunks(q):
        with CachedQueryMutator() as m:
            for link in chunk:
                query = get_deleted_links(link.author_id)
                m.insert(query, [link])

def convert_query_cache_to_json():
    import cPickle
    from r2.models.query_cache import json, UserQueryCache

    with UserQueryCache._cf.batch() as m:
        for key, columns in UserQueryCache._cf.get_range():
            out = {}
            for ckey, cvalue in columns.iteritems():
                try:
                    raw = cPickle.loads(cvalue)
                except cPickle.UnpicklingError:
                    continue
                out[ckey] = json.dumps(raw)
            m.insert(key, out)

def populate_spam_filtered():
    from r2.lib.db.queries import get_spam_links, get_spam_comments
    from r2.lib.db.queries import get_spam_filtered_links, get_spam_filtered_comments
    from r2.models.query_cache import CachedQueryMutator

    def was_filtered(thing):
        if thing._spam and not thing._deleted and \
           getattr(thing, 'verdict', None) != 'mod-removed':
            return True
        else:
            return False

    q = Subreddit._query(sort = asc('_date'))
    for sr in fetch_things2(q):
        print 'Processing %s' % sr.name
        links = Thing._by_fullname(get_spam_links(sr), data=True,
                                   return_dict=False)
        comments = Thing._by_fullname(get_spam_comments(sr), data=True,
                                      return_dict=False)
        insert_links = [l for l in links if was_filtered(l)]
        insert_comments = [c for c in comments if was_filtered(c)]
        with CachedQueryMutator() as m:
            m.insert(get_spam_filtered_links(sr), insert_links)
            m.insert(get_spam_filtered_comments(sr), insert_comments)
<EOF>
<BOF>
import json
from collections import defaultdict
from datetime import datetime, timedelta

from pylons import app_globals as g

from r2.lib.db.sorts import epoch_seconds
from r2.lib.db.tdb_cassandra import write_consistency_level
from r2.lib.utils import in_chunks
from r2.models.vote import VoteDetailsByComment, VoteDetailsByLink, VoterIPByThing


def backfill_vote_details(cls):
    ninety_days = timedelta(days=90).total_seconds()
    for chunk in in_chunks(cls._all(), size=100):
        detail_chunk = defaultdict(dict)
        try:
            with VoterIPByThing._cf.batch(write_consistency_level=cls._write_consistency_level) as b:
                for vote_list in chunk:
                    thing_id36 = vote_list._id
                    thing_fullname = vote_list.votee_fullname
                    details = vote_list.decode_details()
                    for detail in details:
                        voter_id36 = detail["voter_id"]
                        if "ip" in detail and detail["ip"]:
                            ip = detail["ip"]
                            redacted = dict(detail)
                            del redacted["ip"]
                            cast = detail["date"]
                            now = epoch_seconds(datetime.utcnow().replace(tzinfo=g.tz))
                            ttl = ninety_days - (now - cast)
                            oneweek = ""
                            if ttl < 3600 * 24 * 7:
                                oneweek = "(<= one week left)"
                            print "Inserting %s with IP ttl %d %s" % (redacted, ttl, oneweek)
                            detail_chunk[thing_id36][voter_id36] = json.dumps(redacted)
                            if ttl <= 0:
                                print "Skipping bogus ttl for %s: %d" % (redacted, ttl)
                                continue
                            b.insert(thing_fullname, {voter_id36: ip}, ttl=ttl)
        except Exception:
            # Getting some really weird spurious errors here; complaints about negative
            # TTLs even though they can't possibly be negative, errors from cass
            # that have an explanation of "(why=')"
            # Just going to brute-force this through.  We might lose 100 here and there
            # but mostly it'll be intact.
            pass
        for votee_id36, valuedict in detail_chunk.iteritems():
            cls._set_values(votee_id36, valuedict)


def main():
    cfs = [VoteDetailsByComment, VoteDetailsByLink]
    for cf in cfs:
        backfill_vote_details(cf)

if __name__ == '__builtin__':
    main()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
from collections import defaultdict
from r2.models import *

def fix_trans_id():
    bad_campaigns = list(PromoCampaign._query(PromoCampaign.c.trans_id == 1, data=True))
    num_bad_campaigns = len(bad_campaigns)

    if not num_bad_campaigns:
        print "No campaigns with trans_id == 1"
        return

    # print some info and prompt user to continue
    print ("Found %d campaigns with trans_id == 1. \n"
           "Campaigns ids: %s \n" 
           "Press 'c' to fix them or any other key to abort." %
           (num_bad_campaigns, [pc._id for pc in bad_campaigns]))
    input_char = sys.stdin.read(1)
    if input_char != 'c' and input_char != 'C':
        print "aborting..."
        return

    # log the ids for reference
    print ("Fixing %d campaigns with bad freebie trans_id: %s" % 
           (num_bad_campaigns, [pc._id for pc in bad_campaigns]))

    # get corresponding links and copy trans_id from link data to campaign thing
    link_ids = set([campaign.link_id for campaign in bad_campaigns])
    print "Fetching associated links: %s" % link_ids
    try:
        links = Link._byID(link_ids, data=True, return_dict=False)
    except NotFound, e:
        print("Invalid data: Some promocampaigns have invalid link_ids. "
              "Please delete these campaigns or fix the data before "
              "continuing. Exception: %s" % e)

    # organize bad campaigns by link_id
    bad_campaigns_by_link = defaultdict(list)
    for c in bad_campaigns:
        bad_campaigns_by_link[c.link_id].append(c)

    # iterate through links and copy trans_id from pickled list on the link to 
    # the campaign thing
    failed = []
    for link in links:
        link_campaigns = getattr(link, "campaigns")
        thing_campaigns = bad_campaigns_by_link[link._id]
        for campaign in thing_campaigns:
            try:
                sd, ed, bid, sr_name, trans_id = link_campaigns[campaign._id]
                if trans_id != campaign.trans_id:
                    campaign.trans_id = trans_id
                    campaign._commit()
            except:
                failed.append({
                    'link_id': link._id,
                    'campaign_id': campaign._id,
                    'exc type': sys.exc_info()[0],
                    'exc msg': sys.exc_info()[1]
                })

    # log the actions for future reference
    msg = ("%d of %d campaigns updated successfully. %d updates failed: %s" %
           (num_bad_campaigns, num_bad_campaigns - len(failed), len(failed), failed))
    print msg

        
<EOF>
<BOF>
#!/usr/bin/env python
# -*- coding: ascii -*-
#
# Copyright 2011, 2012
# Andr\xe9 Malo or his licensors, as applicable
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r"""
==============
 CSS Minifier
==============

CSS Minifier.

The minifier is based on the semantics of the `YUI compressor`_\, which itself
is based on `the rule list by Isaac Schlueter`_\.

This module is a re-implementation aiming for speed instead of maximum
compression, so it can be used at runtime (rather than during a preprocessing
step). RCSSmin does syntactical compression only (removing spaces, comments
and possibly semicolons). It does not provide semantic compression (like
removing empty blocks, collapsing redundant properties etc). It does, however,
support various CSS hacks (by keeping them working as intended).

Here's a feature list:

- Strings are kept, except that escaped newlines are stripped
- Space/Comments before the very end or before various characters are
  stripped: ``:{});=>+],!`` (The colon (``:``) is a special case, a single
  space is kept if it's outside a ruleset.)
- Space/Comments at the very beginning or after various characters are
  stripped: ``{}(=:>+[,!``
- Optional space after unicode escapes is kept, resp. replaced by a simple
  space
- whitespaces inside ``url()`` definitions are stripped
- Comments starting with an exclamation mark (``!``) can be kept optionally.
- All other comments and/or whitespace characters are replaced by a single
  space.
- Multiple consecutive semicolons are reduced to one
- The last semicolon within a ruleset is stripped
- CSS Hacks supported:

  - IE7 hack (``>/**/``)
  - Mac-IE5 hack (``/*\*/.../**/``)
  - The boxmodelhack is supported naturally because it relies on valid CSS2
    strings
  - Between ``:first-line`` and the following comma or curly brace a space is
    inserted. (apparently it's needed for IE6)
  - Same for ``:first-letter``

rcssmin.c is a reimplementation of rcssmin.py in C and improves runtime up to
factor 50 or so (depending on the input).

Both python 2 (>= 2.4) and python 3 are supported.

.. _YUI compressor: https://github.com/yui/yuicompressor/

.. _the rule list by Isaac Schlueter: https://github.com/isaacs/cssmin/tree/
"""
__author__ = "Andr\xe9 Malo"
__author__ = getattr(__author__, 'decode', lambda x: __author__)('latin-1')
__docformat__ = "restructuredtext en"
__license__ = "Apache License, Version 2.0"
__version__ = '1.0.1'
__all__ = ['cssmin']

import re as _re


def _make_cssmin(python_only=False):
    """
    Generate CSS minifier.

    :Parameters:
      `python_only` : ``bool``
        Use only the python variant. If true, the c extension is not even
        tried to be loaded.

    :Return: Minifier
    :Rtype: ``callable``
    """
    # pylint: disable = W0612
    # ("unused" variables)

    # pylint: disable = R0911, R0912, R0914, R0915
    # (too many anything)

    if not python_only:
        try:
            import _rcssmin
        except ImportError:
            pass
        else:
            return _rcssmin.cssmin

    nl = r'(?:[\n\f]|\r\n?)' # pylint: disable = C0103
    spacechar = r'[\r\n\f\040\t]'

    unicoded = r'[0-9a-fA-F]{1,6}(?:[\040\n\t\f]|\r\n?)?'
    escaped = r'[^\n\r\f0-9a-fA-F]'
    escape = r'(?:\\(?:%(unicoded)s|%(escaped)s))' % locals()

    nmchar = r'[^\000-\054\056\057\072-\100\133-\136\140\173-\177]'
    #nmstart = r'[^\000-\100\133-\136\140\173-\177]'
    #ident = (r'(?:'
    #    r'-?(?:%(nmstart)s|%(escape)s)%(nmchar)s*(?:%(escape)s%(nmchar)s*)*'
    #r')') % locals()

    comment = r'(?:/\*[^*]*\*+(?:[^/*][^*]*\*+)*/)'

    # only for specific purposes. The bang is grouped:
    _bang_comment = r'(?:/\*(!?)[^*]*\*+(?:[^/*][^*]*\*+)*/)'

    string1 = \
        r'(?:\047[^\047\\\r\n\f]*(?:\\[^\r\n\f][^\047\\\r\n\f]*)*\047)'
    string2 = r'(?:"[^"\\\r\n\f]*(?:\\[^\r\n\f][^"\\\r\n\f]*)*")'
    strings = r'(?:%s|%s)' % (string1, string2)

    nl_string1 = \
        r'(?:\047[^\047\\\r\n\f]*(?:\\(?:[^\r]|\r\n?)[^\047\\\r\n\f]*)*\047)'
    nl_string2 = r'(?:"[^"\\\r\n\f]*(?:\\(?:[^\r]|\r\n?)[^"\\\r\n\f]*)*")'
    nl_strings = r'(?:%s|%s)' % (nl_string1, nl_string2)

    uri_nl_string1 = r'(?:\047[^\047\\]*(?:\\(?:[^\r]|\r\n?)[^\047\\]*)*\047)'
    uri_nl_string2 = r'(?:"[^"\\]*(?:\\(?:[^\r]|\r\n?)[^"\\]*)*")'
    uri_nl_strings = r'(?:%s|%s)' % (uri_nl_string1, uri_nl_string2)

    nl_escaped = r'(?:\\%(nl)s)' % locals()

    space = r'(?:%(spacechar)s|%(comment)s)' % locals()

    ie7hack = r'(?:>/\*\*/)'

    uri = (r'(?:'
        r'(?:[^\000-\040"\047()\\\177]*'
            r'(?:%(escape)s[^\000-\040"\047()\\\177]*)*)'
        r'(?:'
            r'(?:%(spacechar)s+|%(nl_escaped)s+)'
            r'(?:'
                r'(?:[^\000-\040"\047()\\\177]|%(escape)s|%(nl_escaped)s)'
                r'[^\000-\040"\047()\\\177]*'
                r'(?:%(escape)s[^\000-\040"\047()\\\177]*)*'
            r')+'
        r')*'
    r')') % locals()

    nl_unesc_sub = _re.compile(nl_escaped).sub

    uri_space_sub = _re.compile((
        r'(%(escape)s+)|%(spacechar)s+|%(nl_escaped)s+'
    ) % locals()).sub
    uri_space_subber = lambda m: m.groups()[0] or ''

    space_sub_simple = _re.compile((
        r'[\r\n\f\040\t;]+|(%(comment)s+)'
    ) % locals()).sub
    space_sub_banged = _re.compile((
        r'[\r\n\f\040\t;]+|(%(_bang_comment)s+)'
    ) % locals()).sub

    post_esc_sub = _re.compile(r'[\r\n\f\t]+').sub

    main_sub = _re.compile((
        r'([^\\"\047u>@\r\n\f\040\t/;:{}]+)'
        r'|(?<=[{}(=:>+[,!])(%(space)s+)'
        r'|^(%(space)s+)'
        r'|(%(space)s+)(?=(([:{});=>+\],!])|$)?)'
        r'|;(%(space)s*(?:;%(space)s*)*)(?=(\})?)'
        r'|(\{)'
        r'|(\})'
        r'|(%(strings)s)'
        r'|(?<!%(nmchar)s)url\(%(spacechar)s*('
                r'%(uri_nl_strings)s'
                r'|%(uri)s'
            r')%(spacechar)s*\)'
        r'|(@[mM][eE][dD][iI][aA])(?!%(nmchar)s)'
        r'|(%(ie7hack)s)(%(space)s*)'
        r'|(:[fF][iI][rR][sS][tT]-[lL]'
            r'(?:[iI][nN][eE]|[eE][tT][tT][eE][rR]))'
            r'(%(space)s*)(?=[{,])'
        r'|(%(nl_strings)s)'
        r'|(%(escape)s[^\\"\047u>@\r\n\f\040\t/;:{}]*)'
    ) % locals()).sub

    #print main_sub.__self__.pattern

    def main_subber(keep_bang_comments):
        """ Make main subber """
        in_macie5, in_rule, at_media = [0], [0], [0]

        if keep_bang_comments:
            space_sub = space_sub_banged
            def space_subber(match):
                """ Space|Comment subber """
                if match.lastindex:
                    group1, group2 = match.group(1, 2)
                    if group2:
                        if group1.endswith(r'\*/'):
                            in_macie5[0] = 1
                        else:
                            in_macie5[0] = 0
                        return group1
                    elif group1:
                        if group1.endswith(r'\*/'):
                            if in_macie5[0]:
                                return ''
                            in_macie5[0] = 1
                            return r'/*\*/'
                        elif in_macie5[0]:
                            in_macie5[0] = 0
                            return '/**/'
                return ''
        else:
            space_sub = space_sub_simple
            def space_subber(match):
                """ Space|Comment subber """
                if match.lastindex:
                    if match.group(1).endswith(r'\*/'):
                        if in_macie5[0]:
                            return ''
                        in_macie5[0] = 1
                        return r'/*\*/'
                    elif in_macie5[0]:
                        in_macie5[0] = 0
                        return '/**/'
                return ''

        def fn_space_post(group):
            """ space with token after """
            if group(5) is None or (
                    group(6) == ':' and not in_rule[0] and not at_media[0]):
                return ' ' + space_sub(space_subber, group(4))
            return space_sub(space_subber, group(4))

        def fn_semicolon(group):
            """ ; handler """
            return ';' + space_sub(space_subber, group(7))

        def fn_semicolon2(group):
            """ ; handler """
            if in_rule[0]:
                return space_sub(space_subber, group(7))
            return ';' + space_sub(space_subber, group(7))

        def fn_open(group):
            """ { handler """
            # pylint: disable = W0613
            if at_media[0]:
                at_media[0] -= 1
            else:
                in_rule[0] = 1
            return '{'

        def fn_close(group):
            """ } handler """
            # pylint: disable = W0613
            in_rule[0] = 0
            return '}'

        def fn_media(group):
            """ @media handler """
            at_media[0] += 1
            return group(13)

        def fn_ie7hack(group):
            """ IE7 Hack handler """
            if not in_rule[0] and not at_media[0]:
                in_macie5[0] = 0
                return group(14) + space_sub(space_subber, group(15))
            return '>' + space_sub(space_subber, group(15))

        table = (
            None,
            None,
            None,
            None,
            fn_space_post,                      # space with token after
            fn_space_post,                      # space with token after
            fn_space_post,                      # space with token after
            fn_semicolon,                       # semicolon
            fn_semicolon2,                      # semicolon
            fn_open,                            # {
            fn_close,                           # }
            lambda g: g(11),                    # string
            lambda g: 'url(%s)' % uri_space_sub(uri_space_subber, g(12)),
                                                # url(...)
            fn_media,                           # @media
            None,
            fn_ie7hack,                         # ie7hack
            None,
            lambda g: g(16) + ' ' + space_sub(space_subber, g(17)),
                                                # :first-line|letter followed
                                                # by [{,] (apparently space
                                                # needed for IE6)
            lambda g: nl_unesc_sub('', g(18)),  # nl_string
            lambda g: post_esc_sub(' ', g(19)), # escape
        )

        def func(match):
            """ Main subber """
            idx, group = match.lastindex, match.group
            if idx > 3:
                return table[idx](group)

            # shortcuts for frequent operations below:
            elif idx == 1:     # not interesting
                return group(1)
            #else: # space with token before or at the beginning
            return space_sub(space_subber, group(idx))

        return func

    def cssmin(style, keep_bang_comments=False): # pylint: disable = W0621
        """
        Minify CSS.

        :Parameters:
          `style` : ``str``
            CSS to minify

          `keep_bang_comments` : ``bool``
            Keep comments starting with an exclamation mark? (``/*!...*/``)

        :Return: Minified style
        :Rtype: ``str``
        """
        return main_sub(main_subber(keep_bang_comments), style)

    return cssmin

cssmin = _make_cssmin()


if __name__ == '__main__':
    def main():
        """ Main """
        import sys as _sys
        keep_bang_comments = (
            '-b' in _sys.argv[1:]
            or '-bp' in _sys.argv[1:]
            or '-pb' in _sys.argv[1:]
        )
        if '-p' in _sys.argv[1:] or '-bp' in _sys.argv[1:] \
                or '-pb' in _sys.argv[1:]:
            global cssmin # pylint: disable = W0603
            cssmin = _make_cssmin(python_only=True)
        _sys.stdout.write(cssmin(
            _sys.stdin.read(), keep_bang_comments=keep_bang_comments
        ))
    main()
<EOF>
<BOF>
#!/usr/bin/python3
#
# Copyright 2007 Google Inc.
#  Licensed to PSF under a Contributor Agreement.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License.

"""A fast, lightweight IPv4/IPv6 manipulation library in Python.

This library is used to create/poke/manipulate IPv4 and IPv6 addresses
and networks.

"""

__version__ = '1.0'

import struct

IPV4LENGTH = 32
IPV6LENGTH = 128


class AddressValueError(ValueError):
    """A Value Error related to the address."""


class NetmaskValueError(ValueError):
    """A Value Error related to the netmask."""


def ip_address(address, version=None):
    """Take an IP string/int and return an object of the correct type.

    Args:
        address: A string or integer, the IP address.  Either IPv4 or
          IPv6 addresses may be supplied; integers less than 2**32 will
          be considered to be IPv4 by default.
        version: An Integer, 4 or 6. If set, don't try to automatically
          determine what the IP address type is. important for things
          like ip_address(1), which could be IPv4, '192.0.2.1',  or IPv6,
          '2001:db8::1'.

    Returns:
        An IPv4Address or IPv6Address object.

    Raises:
        ValueError: if the string passed isn't either a v4 or a v6
          address.

    """
    if version:
        if version == 4:
            return IPv4Address(address)
        elif version == 6:
            return IPv6Address(address)

    try:
        return IPv4Address(address)
    except (AddressValueError, NetmaskValueError):
        pass

    try:
        return IPv6Address(address)
    except (AddressValueError, NetmaskValueError):
        pass

    raise ValueError('%r does not appear to be an IPv4 or IPv6 address' %
                     address)


def ip_network(address, version=None, strict=True):
    """Take an IP string/int and return an object of the correct type.

    Args:
        address: A string or integer, the IP network.  Either IPv4 or
          IPv6 networks may be supplied; integers less than 2**32 will
          be considered to be IPv4 by default.
        version: An Integer, if set, don't try to automatically
          determine what the IP address type is. important for things
          like ip_network(1), which could be IPv4, '192.0.2.1/32', or IPv6,
          '2001:db8::1/128'.

    Returns:
        An IPv4Network or IPv6Network object.

    Raises:
        ValueError: if the string passed isn't either a v4 or a v6
          address. Or if the network has host bits set.

    """
    if version:
        if version == 4:
            return IPv4Network(address, strict)
        elif version == 6:
            return IPv6Network(address, strict)

    try:
        return IPv4Network(address, strict)
    except (AddressValueError, NetmaskValueError):
        pass

    try:
        return IPv6Network(address, strict)
    except (AddressValueError, NetmaskValueError):
        pass

    raise ValueError('%r does not appear to be an IPv4 or IPv6 network' %
                     address)


def ip_interface(address, version=None):
    """Take an IP string/int and return an object of the correct type.

    Args:
        address: A string or integer, the IP address.  Either IPv4 or
          IPv6 addresses may be supplied; integers less than 2**32 will
          be considered to be IPv4 by default.
        version: An Integer, if set, don't try to automatically
          determine what the IP address type is. important for things
          like ip_network(1), which could be IPv4, '192.0.2.1/32', or IPv6,
          '2001:db8::1/128'.

    Returns:
        An IPv4Network or IPv6Network object.

    Raises:
        ValueError: if the string passed isn't either a v4 or a v6
          address.

    Notes:
        The IPv?Interface classes describe an Address on a particular
        Network, so they're basically a combination of both the Address
        and Network classes.
    """
    if version:
        if version == 4:
            return IPv4Interface(address)
        elif version == 6:
            return IPv6Interface(address)

    try:
        return IPv4Interface(address)
    except (AddressValueError, NetmaskValueError):
        pass

    try:
        return IPv6Interface(address)
    except (AddressValueError, NetmaskValueError):
        pass

    raise ValueError('%r does not appear to be an IPv4 or IPv6 network' %
                     address)


def v4_int_to_packed(address):
    """The binary representation of this address.

    Args:
        address: An integer representation of an IPv4 IP address.

    Returns:
        The binary representation of this address.

    Raises:
        ValueError: If the integer is too large to be an IPv4 IP
          address.
    """
    if address > _BaseV4._ALL_ONES:
        raise ValueError('Address too large for IPv4')
    return struct.pack('!I', address)


def v6_int_to_packed(address):
    """The binary representation of this address.

    Args:
        address: An integer representation of an IPv6 IP address.

    Returns:
        The binary representation of this address.
    """
    return struct.pack('!QQ', address >> 64, address & (2**64 - 1))


def _find_address_range(addresses):
    """Find a sequence of addresses.

    Args:
        addresses: a list of IPv4 or IPv6 addresses.

    Returns:
        A tuple containing the first and last IP addresses in the sequence.

    """
    first = last = addresses[0]
    for ip in addresses[1:]:
        if ip._ip == last._ip + 1:
            last = ip
        else:
            break
    return (first, last)

def _get_prefix_length(number1, number2, bits):
    """Get the number of leading bits that are same for two numbers.

    Args:
        number1: an integer.
        number2: another integer.
        bits: the maximum number of bits to compare.

    Returns:
        The number of leading bits that are the same for two numbers.

    """
    for i in range(bits):
        if number1 >> i == number2 >> i:
            return bits - i
    return 0

def _count_righthand_zero_bits(number, bits):
    """Count the number of zero bits on the right hand side.

    Args:
        number: an integer.
        bits: maximum number of bits to count.

    Returns:
        The number of zero bits on the right hand side of the number.

    """
    if number == 0:
        return bits
    for i in range(bits):
        if (number >> i) % 2:
            return i


def summarize_address_range(first, last):
    """Summarize a network range given the first and last IP addresses.

    Example:
        >>> summarize_address_range(IPv4Address('192.0.2.0'),
            IPv4Address('192.0.2.130'))
        [IPv4Network('192.0.2.0/25'), IPv4Network('192.0.2.128/31'),
        IPv4Network('192.0.2.130/32')]

    Args:
        first: the first IPv4Address or IPv6Address in the range.
        last: the last IPv4Address or IPv6Address in the range.

    Returns:
        An iterator of the summarized IPv(4|6) network objects.

    Raise:
        TypeError:
            If the first and last objects are not IP addresses.
            If the first and last objects are not the same version.
        ValueError:
            If the last object is not greater than the first.
            If the version is not 4 or 6.

    """
    if not (isinstance(first, _BaseAddress) and isinstance(last, _BaseAddress)):
        raise TypeError('first and last must be IP addresses, not networks')
    if first.version != last.version:
        raise TypeError("%s and %s are not of the same version" % (
                str(first), str(last)))
    if first > last:
        raise ValueError('last IP address must be greater than first')

    networks = []

    if first.version == 4:
        ip = IPv4Network
    elif first.version == 6:
        ip = IPv6Network
    else:
        raise ValueError('unknown IP version')

    ip_bits = first._max_prefixlen
    first_int = first._ip
    last_int = last._ip
    while first_int <= last_int:
        nbits = _count_righthand_zero_bits(first_int, ip_bits)
        current = None
        while nbits >= 0:
            addend = 2**nbits - 1
            current = first_int + addend
            nbits -= 1
            if current <= last_int:
                break
        prefix = _get_prefix_length(first_int, current, ip_bits)
        net = ip('%s/%d' % (str(first), prefix))
        yield net
        #networks.append(net)
        if current == ip._ALL_ONES:
            break
        first_int = current + 1
        first = ip_address(first_int, version=first._version)

def _collapse_addresses_recursive(addresses):
    """Loops through the addresses, collapsing concurrent netblocks.

    Example:

        ip1 = IPv4Network('192.0.2.0/26')
        ip2 = IPv4Network('192.0.2.64/26')
        ip3 = IPv4Network('192.0.2.128/26')
        ip4 = IPv4Network('192.0.2.192/26')

        _collapse_addresses_recursive([ip1, ip2, ip3, ip4]) ->
          [IPv4Network('192.0.2.0/24')]

        This shouldn't be called directly; it is called via
          collapse_addresses([]).

    Args:
        addresses: A list of IPv4Network's or IPv6Network's

    Returns:
        A list of IPv4Network's or IPv6Network's depending on what we were
        passed.

    """
    ret_array = []
    optimized = False

    for cur_addr in addresses:
        if not ret_array:
            ret_array.append(cur_addr)
            continue
        if (cur_addr.network_address >= ret_array[-1].network_address and
            cur_addr.broadcast_address <= ret_array[-1].broadcast_address):
            optimized = True
        elif cur_addr == list(ret_array[-1].supernet().subnets())[1]:
            ret_array.append(ret_array.pop().supernet())
            optimized = True
        else:
            ret_array.append(cur_addr)

    if optimized:
        return _collapse_addresses_recursive(ret_array)

    return ret_array


def collapse_addresses(addresses):
    """Collapse a list of IP objects.

    Example:
        collapse_addresses([IPv4Network('192.0.2.0/25'),
                            IPv4Network('192.0.2.128/25')]) ->
                           [IPv4Network('192.0.2.0/24')]

    Args:
        addresses: An iterator of IPv4Network or IPv6Network objects.

    Returns:
        An iterator of the collapsed IPv(4|6)Network objects.

    Raises:
        TypeError: If passed a list of mixed version objects.

    """
    i = 0
    addrs = []
    ips = []
    nets = []

    # split IP addresses and networks
    for ip in addresses:
        if isinstance(ip, _BaseAddress):
            if ips and ips[-1]._version != ip._version:
                raise TypeError("%s and %s are not of the same version" % (
                        str(ip), str(ips[-1])))
            ips.append(ip)
        elif ip._prefixlen == ip._max_prefixlen:
            if ips and ips[-1]._version != ip._version:
                raise TypeError("%s and %s are not of the same version" % (
                        str(ip), str(ips[-1])))
            try:
                ips.append(ip.ip)
            except AttributeError:
                ips.append(ip.network_address)
        else:
            if nets and nets[-1]._version != ip._version:
                raise TypeError("%s and %s are not of the same version" % (
                        str(ip), str(nets[-1])))
            nets.append(ip)

    # sort and dedup
    ips = sorted(set(ips))
    nets = sorted(set(nets))

    while i < len(ips):
        (first, last) = _find_address_range(ips[i:])
        i = ips.index(last) + 1
        addrs.extend(summarize_address_range(first, last))

    return iter(_collapse_addresses_recursive(sorted(
        addrs + nets, key=_BaseNetwork._get_networks_key)))


def get_mixed_type_key(obj):
    """Return a key suitable for sorting between networks and addresses.

    Address and Network objects are not sortable by default; they're
    fundamentally different so the expression

        IPv4Address('192.0.2.0') <= IPv4Network('192.0.2.0/24')

    doesn't make any sense.  There are some times however, where you may wish
    to have ipaddress sort these for you anyway. If you need to do this, you
    can use this function as the key= argument to sorted().

    Args:
      obj: either a Network or Address object.
    Returns:
      appropriate key.

    """
    if isinstance(obj, _BaseNetwork):
        return obj._get_networks_key()
    elif isinstance(obj, _BaseAddress):
        return obj._get_address_key()
    return NotImplemented


class _IPAddressBase(object):

    """The mother class."""

    @property
    def exploded(self):
        """Return the longhand version of the IP address as a string."""
        return self._explode_shorthand_ip_string()

    @property
    def compressed(self):
        """Return the shorthand version of the IP address as a string."""
        return str(self)

    def _ip_int_from_prefix(self, prefixlen=None):
        """Turn the prefix length netmask into a int for comparison.

        Args:
            prefixlen: An integer, the prefix length.

        Returns:
            An integer.

        """
        if not prefixlen and prefixlen != 0:
            prefixlen = self._prefixlen
        return self._ALL_ONES ^ (self._ALL_ONES >> prefixlen)

    def _prefix_from_ip_int(self, ip_int, mask=32):
        """Return prefix length from the decimal netmask.

        Args:
            ip_int: An integer, the IP address.
            mask: The netmask.  Defaults to 32.

        Returns:
            An integer, the prefix length.

        """
        while mask:
            if ip_int & 1 == 1:
                break
            ip_int >>= 1
            mask -= 1

        return mask

    def _ip_string_from_prefix(self, prefixlen=None):
        """Turn a prefix length into a dotted decimal string.

        Args:
            prefixlen: An integer, the netmask prefix length.

        Returns:
            A string, the dotted decimal netmask string.

        """
        if not prefixlen:
            prefixlen = self._prefixlen
        return self._string_from_ip_int(self._ip_int_from_prefix(prefixlen))


class _BaseAddress(_IPAddressBase):

    """A generic IP object.

    This IP class contains the version independent methods which are
    used by single IP addresses.

    """

    def __init__(self, address):
        if (not isinstance(address, bytes)
            and '/' in str(address)):
            raise AddressValueError(address)

    def __index__(self):
        return self._ip

    def __int__(self):
        return self._ip

    def __hex__(self):
        return hex(self._ip)

    def __eq__(self, other):
        try:
            return (self._ip == other._ip
                    and self._version == other._version)
        except AttributeError:
            return NotImplemented

    def __ne__(self, other):
        eq = self.__eq__(other)
        if eq is NotImplemented:
            return NotImplemented
        return not eq

    def __le__(self, other):
        gt = self.__gt__(other)
        if gt is NotImplemented:
            return NotImplemented
        return not gt

    def __ge__(self, other):
        lt = self.__lt__(other)
        if lt is NotImplemented:
            return NotImplemented
        return not lt

    def __lt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseAddress):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self._ip != other._ip:
            return self._ip < other._ip
        return False

    def __gt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseAddress):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self._ip != other._ip:
            return self._ip > other._ip
        return False

    # Shorthand for Integer addition and subtraction. This is not
    # meant to ever support addition/subtraction of addresses.
    def __add__(self, other):
        if not isinstance(other, int):
            return NotImplemented
        return ip_address(int(self) + other, version=self._version)

    def __sub__(self, other):
        if not isinstance(other, int):
            return NotImplemented
        return ip_address(int(self) - other, version=self._version)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, str(self))

    def __str__(self):
        return  '%s' % self._string_from_ip_int(self._ip)

    def __hash__(self):
        return hash(hex(int(self._ip)))

    def _get_address_key(self):
        return (self._version, self)

    @property
    def version(self):
        raise NotImplementedError('BaseIP has no version')


class _BaseNetwork(_IPAddressBase):

    """A generic IP object.

    This IP class contains the version independent methods which are
    used by networks.

    """

    def __init__(self, address):
        self._cache = {}

    def __index__(self):
        return int(self.network_address) ^ self.prefixlen

    def __int__(self):
        return int(self.network_address)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, str(self))

    def hosts(self):
        """Generate Iterator over usable hosts in a network.

           This is like __iter__ except it doesn't return the network
           or broadcast addresses.

        """
        cur = int(self.network_address) + 1
        bcast = int(self.broadcast_address) - 1
        while cur <= bcast:
            cur += 1
            yield ip_address(cur - 1, version=self._version)

    def __iter__(self):
        cur = int(self.network_address)
        bcast = int(self.broadcast_address)
        while cur <= bcast:
            cur += 1
            yield ip_address(cur - 1, version=self._version)

    def __getitem__(self, n):
        network = int(self.network_address)
        broadcast = int(self.broadcast_address)
        if n >= 0:
            if network + n > broadcast:
                raise IndexError
            return ip_address(network + n, version=self._version)
        else:
            n += 1
            if broadcast + n < network:
                raise IndexError
            return ip_address(broadcast + n, version=self._version)

    def __lt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseNetwork):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self.network_address != other.network_address:
            return self.network_address < other.network_address
        if self.netmask != other.netmask:
            return self.netmask < other.netmask
        return False

    def __gt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseNetwork):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self.network_address != other.network_address:
            return self.network_address > other.network_address
        if self.netmask != other.netmask:
            return self.netmask > other.netmask
        return False

    def __le__(self, other):
        gt = self.__gt__(other)
        if gt is NotImplemented:
            return NotImplemented
        return not gt

    def __ge__(self, other):
        lt = self.__lt__(other)
        if lt is NotImplemented:
            return NotImplemented
        return not lt

    def __eq__(self, other):
        if not isinstance(other, _BaseNetwork):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        return (self._version == other._version and
                self.network_address == other.network_address and
                int(self.netmask) == int(other.netmask))

    def __ne__(self, other):
        eq = self.__eq__(other)
        if eq is NotImplemented:
            return NotImplemented
        return not eq

    def __str__(self):
        return  '%s/%s' % (str(self.ip),
                           str(self._prefixlen))

    def __hash__(self):
        return hash(int(self.network_address) ^ int(self.netmask))

    def __contains__(self, other):
        # always false if one is v4 and the other is v6.
        if self._version != other._version:
          return False
        # dealing with another network.
        if isinstance(other, _BaseNetwork):
            return False
        # dealing with another address
        else:
            # address
            return (int(self.network_address) <= int(other._ip) <=
                    int(self.broadcast_address))

    def overlaps(self, other):
        """Tell if self is partly contained in other."""
        return self.network_address in other or (
            self.broadcast_address in other or (
                other.network_address in self or (
                    other.broadcast_address in self)))

    @property
    def broadcast_address(self):
        x = self._cache.get('broadcast_address')
        if x is None:
            x = ip_address(int(self.network_address) | int(self.hostmask),
                           version=self._version)
            self._cache['broadcast_address'] = x
        return x

    @property
    def hostmask(self):
        x = self._cache.get('hostmask')
        if x is None:
            x = ip_address(int(self.netmask) ^ self._ALL_ONES,
                          version=self._version)
            self._cache['hostmask'] = x
        return x

    @property
    def network(self):
        return ip_network('%s/%d' % (str(self.network_address),
                                     self.prefixlen))

    @property
    def with_prefixlen(self):
        return '%s/%d' % (str(self.ip), self._prefixlen)

    @property
    def with_netmask(self):
        return '%s/%s' % (str(self.ip), str(self.netmask))

    @property
    def with_hostmask(self):
        return '%s/%s' % (str(self.ip), str(self.hostmask))

    @property
    def num_addresses(self):
        """Number of hosts in the current subnet."""
        return int(self.broadcast_address) - int(self.network_address) + 1

    @property
    def version(self):
        raise NotImplementedError('BaseNet has no version')

    @property
    def prefixlen(self):
        return self._prefixlen

    def address_exclude(self, other):
        """Remove an address from a larger block.

        For example:

            addr1 = ip_network('192.0.2.0/28')
            addr2 = ip_network('192.0.2.1/32')
            addr1.address_exclude(addr2) =
                [IPv4Network('192.0.2.0/32'), IPv4Network('192.0.2.2/31'),
                IPv4Network('192.0.2.4/30'), IPv4Network('192.0.2.8/29')]

        or IPv6:

            addr1 = ip_network('2001:db8::1/32')
            addr2 = ip_network('2001:db8::1/128')
            addr1.address_exclude(addr2) =
                [ip_network('2001:db8::1/128'),
                ip_network('2001:db8::2/127'),
                ip_network('2001:db8::4/126'),
                ip_network('2001:db8::8/125'),
                ...
                ip_network('2001:db8:8000::/33')]

        Args:
            other: An IPv4Network or IPv6Network object of the same type.

        Returns:
            An iterator of the the IPv(4|6)Network objects which is self
            minus other.

        Raises:
            TypeError: If self and other are of difffering address
              versions, or if other is not a network object.
            ValueError: If other is not completely contained by self.

        """
        if not self._version == other._version:
            raise TypeError("%s and %s are not of the same version" % (
                str(self), str(other)))

        if not isinstance(other, _BaseNetwork):
            raise TypeError("%s is not a network object" % str(other))

        if not (other.network_address >= self.network_address and
                other.broadcast_address <= self.broadcast_address):
            raise ValueError('%s not contained in %s' % (str(other), str(self)))

        if other == self:
            raise StopIteration

        ret_addrs = []

        # Make sure we're comparing the network of other.
        other = ip_network('%s/%s' % (str(other.network_address),
                                      str(other.prefixlen)),
                           version=other._version)

        s1, s2 = self.subnets()
        while s1 != other and s2 != other:
            if (other.network_address >= s1.network_address and
                other.broadcast_address <= s1.broadcast_address):
                yield s2
                s1, s2 = s1.subnets()
            elif (other.network_address >= s2.network_address and
                  other.broadcast_address <= s2.broadcast_address):
                yield s1
                s1, s2 = s2.subnets()
            else:
                # If we got here, there's a bug somewhere.
                raise AssertionError('Error performing exclusion: '
                                     's1: %s s2: %s other: %s' %
                                     (str(s1), str(s2), str(other)))
        if s1 == other:
            yield s2
        elif s2 == other:
            yield s1
        else:
            # If we got here, there's a bug somewhere.
            raise AssertionError('Error performing exclusion: '
                                 's1: %s s2: %s other: %s' %
                                 (str(s1), str(s2), str(other)))

    def compare_networks(self, other):
        """Compare two IP objects.

        This is only concerned about the comparison of the integer
        representation of the network addresses.  This means that the
        host bits aren't considered at all in this method.  If you want
        to compare host bits, you can easily enough do a
        'HostA._ip < HostB._ip'

        Args:
            other: An IP object.

        Returns:
            If the IP versions of self and other are the same, returns:

            -1 if self < other:
              eg: IPv4Network('192.0.2.0/25') < IPv4Network('192.0.2.128/25')
              IPv6Network('2001:db8::1000/124') <
                  IPv6Network('2001:db8::2000/124')
            0 if self == other
              eg: IPv4Network('192.0.2.0/24') == IPv4Network('192.0.2.0/24')
              IPv6Network('2001:db8::1000/124') ==
                  IPv6Network('2001:db8::1000/124')
            1 if self > other
              eg: IPv4Network('192.0.2.128/25') > IPv4Network('192.0.2.0/25')
                  IPv6Network('2001:db8::2000/124') >
                      IPv6Network('2001:db8::1000/124')

          Raises:
              TypeError if the IP versions are different.

        """
        # does this need to raise a ValueError?
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        # self._version == other._version below here:
        if self.network_address < other.network_address:
            return -1
        if self.network_address > other.network_address:
            return 1
        # self.network_address == other.network_address below here:
        if self.netmask < other.netmask:
            return -1
        if self.netmask > other.netmask:
            return 1
        return 0

    def _get_networks_key(self):
        """Network-only key function.

        Returns an object that identifies this address' network and
        netmask. This function is a suitable "key" argument for sorted()
        and list.sort().

        """
        return (self._version, self.network_address, self.netmask)

    def subnets(self, prefixlen_diff=1, new_prefix=None):
        """The subnets which join to make the current subnet.

        In the case that self contains only one IP
        (self._prefixlen == 32 for IPv4 or self._prefixlen == 128
        for IPv6), yield an iterator with just ourself.

        Args:
            prefixlen_diff: An integer, the amount the prefix length
              should be increased by. This should not be set if
              new_prefix is also set.
            new_prefix: The desired new prefix length. This must be a
              larger number (smaller prefix) than the existing prefix.
              This should not be set if prefixlen_diff is also set.

        Returns:
            An iterator of IPv(4|6) objects.

        Raises:
            ValueError: The prefixlen_diff is too small or too large.
                OR
            prefixlen_diff and new_prefix are both set or new_prefix
              is a smaller number than the current prefix (smaller
              number means a larger network)

        """
        if self._prefixlen == self._max_prefixlen:
            yield self
            return

        if new_prefix is not None:
            if new_prefix < self._prefixlen:
                raise ValueError('new prefix must be longer')
            if prefixlen_diff != 1:
                raise ValueError('cannot set prefixlen_diff and new_prefix')
            prefixlen_diff = new_prefix - self._prefixlen

        if prefixlen_diff < 0:
            raise ValueError('prefix length diff must be > 0')
        new_prefixlen = self._prefixlen + prefixlen_diff

        if not self._is_valid_netmask(str(new_prefixlen)):
            raise ValueError(
                'prefix length diff %d is invalid for netblock %s' % (
                    new_prefixlen, str(self)))

        first = ip_network('%s/%s' % (str(self.network_address),
                                     str(self._prefixlen + prefixlen_diff)),
                         version=self._version)

        yield first
        current = first
        while True:
            broadcast = current.broadcast_address
            if broadcast == self.broadcast_address:
                return
            new_addr = ip_address(int(broadcast) + 1, version=self._version)
            current = ip_network('%s/%s' % (str(new_addr), str(new_prefixlen)),
                                version=self._version)

            yield current

    def masked(self):
        """Return the network object with the host bits masked out."""
        return ip_network('%s/%d' % (self.network_address, self._prefixlen),
                         version=self._version)

    def supernet(self, prefixlen_diff=1, new_prefix=None):
        """The supernet containing the current network.

        Args:
            prefixlen_diff: An integer, the amount the prefix length of
              the network should be decreased by.  For example, given a
              /24 network and a prefixlen_diff of 3, a supernet with a
              /21 netmask is returned.

        Returns:
            An IPv4 network object.

        Raises:
            ValueError: If self.prefixlen - prefixlen_diff < 0. I.e., you have a
              negative prefix length.
                OR
            If prefixlen_diff and new_prefix are both set or new_prefix is a
              larger number than the current prefix (larger number means a
              smaller network)

        """
        if self._prefixlen == 0:
            return self

        if new_prefix is not None:
            if new_prefix > self._prefixlen:
                raise ValueError('new prefix must be shorter')
            if prefixlen_diff != 1:
                raise ValueError('cannot set prefixlen_diff and new_prefix')
            prefixlen_diff = self._prefixlen - new_prefix


        if self.prefixlen - prefixlen_diff < 0:
            raise ValueError(
                'current prefixlen is %d, cannot have a prefixlen_diff of %d' %
                (self.prefixlen, prefixlen_diff))
        # TODO (pmoody): optimize this.
        t = ip_network('%s/%d' % (str(self.network_address),
                                    self.prefixlen - prefixlen_diff),
                         version=self._version, strict=False)
        return ip_network('%s/%d' % (str(t.network_address), t.prefixlen),
                          version=t._version)


class _BaseV4(object):

    """Base IPv4 object.

    The following methods are used by IPv4 objects in both single IP
    addresses and networks.

    """

    # Equivalent to 255.255.255.255 or 32 bits of 1's.
    _ALL_ONES = (2**IPV4LENGTH) - 1
    _DECIMAL_DIGITS = frozenset('0123456789')

    def __init__(self, address):
        self._version = 4
        self._max_prefixlen = IPV4LENGTH

    def _explode_shorthand_ip_string(self):
        return str(self)

    def _ip_int_from_string(self, ip_str):
        """Turn the given IP string into an integer for comparison.

        Args:
            ip_str: A string, the IP ip_str.

        Returns:
            The IP ip_str as an integer.

        Raises:
            AddressValueError: if ip_str isn't a valid IPv4 Address.

        """
        octets = ip_str.split('.')
        if len(octets) != 4:
            raise AddressValueError(ip_str)

        packed_ip = 0
        for oc in octets:
            try:
                packed_ip = (packed_ip << 8) | self._parse_octet(oc)
            except ValueError:
                raise AddressValueError(ip_str)
        return packed_ip

    def _parse_octet(self, octet_str):
        """Convert a decimal octet into an integer.

        Args:
            octet_str: A string, the number to parse.

        Returns:
            The octet as an integer.

        Raises:
            ValueError: if the octet isn't strictly a decimal from [0..255].

        """
        # Whitelist the characters, since int() allows a lot of bizarre stuff.
        if not self._DECIMAL_DIGITS.issuperset(octet_str):
            raise ValueError
        octet_int = int(octet_str, 10)
        # Disallow leading zeroes, because no clear standard exists on
        # whether these should be interpreted as decimal or octal.
        if octet_int > 255 or (octet_str[0] == '0' and len(octet_str) > 1):
            raise ValueError
        return octet_int

    def _string_from_ip_int(self, ip_int):
        """Turns a 32-bit integer into dotted decimal notation.

        Args:
            ip_int: An integer, the IP address.

        Returns:
            The IP address as a string in dotted decimal notation.

        """
        octets = []
        for _ in range(4):
            octets.insert(0, str(ip_int & 0xFF))
            ip_int >>= 8
        return '.'.join(octets)

    @property
    def max_prefixlen(self):
        return self._max_prefixlen

    @property
    def version(self):
        return self._version

    @property
    def is_reserved(self):
       """Test if the address is otherwise IETF reserved.

        Returns:
            A boolean, True if the address is within the
            reserved IPv4 Network range.

       """
       reserved_network = IPv4Network('240.0.0.0/4')
       if isinstance(self, _BaseAddress):
           return self in reserved_network
       return (self.network_address in reserved_network and
               self.broadcast_address in reserved_network)

    @property
    def is_private(self):
        """Test if this address is allocated for private networks.

        Returns:
            A boolean, True if the address is reserved per RFC 1918.

        """
        private_10 = IPv4Network('10.0.0.0/8')
        private_172 = IPv4Network('172.16.0.0/12')
        private_192 = IPv4Network('192.168.0.0/16')
        if isinstance(self, _BaseAddress):
            return (self in private_10 or self in private_172 or
                    self in private_192)
        else:
            return ((self.network_address in private_10 and
                     self.broadcast_address in private_10) or
                    (self.network_address in private_172 and
                     self.broadcast_address in private_172) or
                    (self.network_address in private_192 and
                     self.broadcast_address in private_192))

    @property
    def is_multicast(self):
        """Test if the address is reserved for multicast use.

        Returns:
            A boolean, True if the address is multicast.
            See RFC 3171 for details.

        """
        multicast_network = IPv4Network('224.0.0.0/4')
        if isinstance(self, _BaseAddress):
            return self in IPv4Network('224.0.0.0/4')
        return (self.network_address in multicast_network and
                self.broadcast_address in multicast_network)

    @property
    def is_unspecified(self):
        """Test if the address is unspecified.

        Returns:
            A boolean, True if this is the unspecified address as defined in
            RFC 5735 3.

        """
        unspecified_address = IPv4Address('0.0.0.0')
        if isinstance(self, _BaseAddress):
            return self in unspecified_address
        return (self.network_address == self.broadcast_address ==
                unspecified_address)

    @property
    def is_loopback(self):
        """Test if the address is a loopback address.

        Returns:
            A boolean, True if the address is a loopback per RFC 3330.

        """
        loopback_address = IPv4Network('127.0.0.0/8')
        if isinstance(self, _BaseAddress):
            return self in loopback_address

        return (self.network_address in loopback_address and
                self.broadcast_address in loopback_address)

    @property
    def is_link_local(self):
        """Test if the address is reserved for link-local.

        Returns:
            A boolean, True if the address is link-local per RFC 3927.

        """
        linklocal_network = IPv4Network('169.254.0.0/16')
        if isinstance(self, _BaseAddress):
            return self in linklocal_network
        return (self.network_address in linklocal_network and
                self.broadcast_address in linklocal_network)


class IPv4Address(_BaseV4, _BaseAddress):

    """Represent and manipulate single IPv4 Addresses."""

    def __init__(self, address):

        """
        Args:
            address: A string or integer representing the IP

              Additionally, an integer can be passed, so
              IPv4Address('192.0.2.1') == IPv4Address(3221225985).
              or, more generally
              IPv4Address(int(IPv4Address('192.0.2.1'))) ==
                IPv4Address('192.0.2.1')

        Raises:
            AddressValueError: If ipaddressisn't a valid IPv4 address.

        """
        _BaseAddress.__init__(self, address)
        _BaseV4.__init__(self, address)

        # Efficient constructor from integer.
        if isinstance(address, int):
            self._ip = address
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            return

        # Constructing from a packed address
        if (not isinstance(address, str) and
            isinstance(address, bytes) and len(address) == 4):
            self._ip = struct.unpack('!I', address)[0]
            return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP string.
        addr_str = str(address)
        self._ip = self._ip_int_from_string(addr_str)

    @property
    def packed(self):
        """The binary representation of this address."""
        return v4_int_to_packed(self._ip)


class IPv4Interface(IPv4Address):

    # the valid octets for host and netmasks. only useful for IPv4.
    _valid_mask_octets = set((255, 254, 252, 248, 240, 224, 192, 128, 0))

    def __init__(self, address):
        if isinstance(address, (bytes, int)):
            IPv4Address.__init__(self, address)
            self.network = IPv4Network(self._ip)
            self._prefixlen = self._max_prefixlen
            return

        addr = str(address).split('/')
        if len(addr) > 2:
            raise AddressValueError(address)
        IPv4Address.__init__(self, addr[0])

        self.network = IPv4Network(address, strict=False)
        self._prefixlen = self.network._prefixlen

        self.netmask = self.network.netmask
        self.hostmask = self.network.hostmask


    def __str__(self):
        return '%s/%d' % (self._string_from_ip_int(self._ip),
                          self.network.prefixlen)

    def __eq__(self, other):
        try:
            return (IPv4Address.__eq__(self, other) and
                    self.network == other.network)
        except AttributeError:
            return NotImplemented

    def __hash__(self):
        return self._ip ^ self._prefixlen ^ int(self.network.network_address)

    def _is_valid_netmask(self, netmask):
        """Verify that the netmask is valid.

        Args:
            netmask: A string, either a prefix or dotted decimal
              netmask.

        Returns:
            A boolean, True if the prefix represents a valid IPv4
            netmask.

        """
        mask = netmask.split('.')
        if len(mask) == 4:
            if [x for x in mask if int(x) not in self._valid_mask_octets]:
                return False
            if [y for idx, y in enumerate(mask) if idx > 0 and
                y > mask[idx - 1]]:
                return False
            return True
        try:
            netmask = int(netmask)
        except ValueError:
            return False
        return 0 <= netmask <= self._max_prefixlen

    def _is_hostmask(self, ip_str):
        """Test if the IP string is a hostmask (rather than a netmask).

        Args:
            ip_str: A string, the potential hostmask.

        Returns:
            A boolean, True if the IP string is a hostmask.

        """
        bits = ip_str.split('.')
        try:
            parts = [int(x) for x in bits if int(x) in self._valid_mask_octets]
        except ValueError:
            return False
        if len(parts) != len(bits):
            return False
        if parts[0] < parts[-1]:
            return True
        return False


    @property
    def prefixlen(self):
        return self._prefixlen

    @property
    def ip(self):
        return IPv4Address(self._ip)

    @property
    def with_prefixlen(self):
        return self

    @property
    def with_netmask(self):
        return '%s/%s' % (self._string_from_ip_int(self._ip),
                          self.netmask)
    @property
    def with_hostmask(self):
        return '%s/%s' % (self._string_from_ip_int(self._ip),
                          self.hostmask)


class IPv4Network(_BaseV4, _BaseNetwork):

    """This class represents and manipulates 32-bit IPv4 network + addresses..

    Attributes: [examples for IPv4Network('192.0.2.0/27')]
        .network_address: IPv4Address('192.0.2.0')
        .hostmask: IPv4Address('0.0.0.31')
        .broadcast_address: IPv4Address('192.0.2.32')
        .netmask: IPv4Address('255.255.255.224')
        .prefixlen: 27

    """

    # the valid octets for host and netmasks. only useful for IPv4.
    _valid_mask_octets = set((255, 254, 252, 248, 240, 224, 192, 128, 0))

    def __init__(self, address, strict=True):

        """Instantiate a new IPv4 network object.

        Args:
            address: A string or integer representing the IP [& network].
              '192.0.2.0/24'
              '192.0.2.0/255.255.255.0'
              '192.0.0.2/0.0.0.255'
              are all functionally the same in IPv4. Similarly,
              '192.0.2.1'
              '192.0.2.1/255.255.255.255'
              '192.0.2.1/32'
              are also functionaly equivalent. That is to say, failing to
              provide a subnetmask will create an object with a mask of /32.

              If the mask (portion after the / in the argument) is given in
              dotted quad form, it is treated as a netmask if it starts with a
              non-zero field (e.g. /255.0.0.0 == /8) and as a hostmask if it
              starts with a zero field (e.g. 0.255.255.255 == /8), with the
              single exception of an all-zero mask which is treated as a
              netmask == /0. If no mask is given, a default of /32 is used.

              Additionally, an integer can be passed, so
              IPv4Network('192.0.2.1') == IPv4Network(3221225985)
              or, more generally
              IPv4Interface(int(IPv4Interface('192.0.2.1'))) ==
                IPv4Interface('192.0.2.1')

        Raises:
            AddressValueError: If ipaddressisn't a valid IPv4 address.
            NetmaskValueError: If the netmask isn't valid for
              an IPv4 address.
            ValueError: If strict was True and a network address was not
              supplied.

        """

        _BaseV4.__init__(self, address)
        _BaseNetwork.__init__(self, address)

        # Constructing from a packed address
        if isinstance(address, bytes) and len(address) == 4:
            self.network_address = IPv4Address(
                struct.unpack('!I', address)[0])
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv4Address(self._ALL_ONES)
            #fixme: address/network test here
            return

        # Efficient constructor from integer.
        if isinstance(address, int):
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv4Address(self._ALL_ONES)
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            self.network_address = IPv4Address(address)
            #fixme: address/network test here.
            return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP prefix string.
        addr = str(address).split('/')
        self.network_address = IPv4Address(self._ip_int_from_string(addr[0]))

        if len(addr) > 2:
            raise AddressValueError(address)

        if len(addr) == 2:
            mask = addr[1].split('.')

            if len(mask) == 4:
                # We have dotted decimal netmask.
                if self._is_valid_netmask(addr[1]):
                    self.netmask = IPv4Address(self._ip_int_from_string(
                            addr[1]))
                elif self._is_hostmask(addr[1]):
                    self.netmask = IPv4Address(
                        self._ip_int_from_string(addr[1]) ^ self._ALL_ONES)
                else:
                    raise NetmaskValueError('%s is not a valid netmask'
                                                     % addr[1])

                self._prefixlen = self._prefix_from_ip_int(int(self.netmask))
            else:
                # We have a netmask in prefix length form.
                if not self._is_valid_netmask(addr[1]):
                    raise NetmaskValueError(addr[1])
                self._prefixlen = int(addr[1])
                self.netmask = IPv4Address(self._ip_int_from_prefix(
                    self._prefixlen))
        else:
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv4Address(self._ip_int_from_prefix(
                self._prefixlen))

        if strict:
            if (IPv4Address(int(self.network_address) & int(self.netmask)) !=
                self.network_address):
                raise ValueError('%s has host bits set' % self)
        self.network_address = IPv4Address(int(self.network_address) &
                                           int(self.netmask))

        if self._prefixlen == (self._max_prefixlen - 1):
            self.hosts = self.__iter__

    @property
    def packed(self):
        """The binary representation of this address."""
        return v4_int_to_packed(self.network_address)

    def __str__(self):
        return '%s/%d' % (str(self.network_address),
                          self.prefixlen)

    def _is_valid_netmask(self, netmask):
        """Verify that the netmask is valid.

        Args:
            netmask: A string, either a prefix or dotted decimal
              netmask.

        Returns:
            A boolean, True if the prefix represents a valid IPv4
            netmask.

        """
        mask = netmask.split('.')
        if len(mask) == 4:
            if [x for x in mask if int(x) not in self._valid_mask_octets]:
                return False
            if [y for idx, y in enumerate(mask) if idx > 0 and
                y > mask[idx - 1]]:
                return False
            return True
        try:
            netmask = int(netmask)
        except ValueError:
            return False
        return 0 <= netmask <= self._max_prefixlen

    def _is_hostmask(self, ip_str):
        """Test if the IP string is a hostmask (rather than a netmask).

        Args:
            ip_str: A string, the potential hostmask.

        Returns:
            A boolean, True if the IP string is a hostmask.

        """
        bits = ip_str.split('.')
        try:
            parts = [int(x) for x in bits if int(x) in self._valid_mask_octets]
        except ValueError:
            return False
        if len(parts) != len(bits):
            return False
        if parts[0] < parts[-1]:
            return True
        return False

    @property
    def with_prefixlen(self):
        return '%s/%d' % (str(self.network_address), self._prefixlen)

    @property
    def with_netmask(self):
        return '%s/%s' % (str(self.network_address), str(self.netmask))

    @property
    def with_hostmask(self):
        return '%s/%s' % (str(self.network_address), str(self.hostmask))


class _BaseV6(object):

    """Base IPv6 object.

    The following methods are used by IPv6 objects in both single IP
    addresses and networks.

    """

    _ALL_ONES = (2**IPV6LENGTH) - 1
    _HEXTET_COUNT = 8
    _HEX_DIGITS = frozenset('0123456789ABCDEFabcdef')

    def __init__(self, address):
        self._version = 6
        self._max_prefixlen = IPV6LENGTH

    def _ip_int_from_string(self, ip_str):
        """Turn an IPv6 ip_str into an integer.

        Args:
            ip_str: A string, the IPv6 ip_str.

        Returns:
            An int, the IPv6 address

        Raises:
            AddressValueError: if ip_str isn't a valid IPv6 Address.

        """
        parts = ip_str.split(':')

        # An IPv6 address needs at least 2 colons (3 parts).
        if len(parts) < 3:
            raise AddressValueError(ip_str)

        # If the address has an IPv4-style suffix, convert it to hexadecimal.
        if '.' in parts[-1]:
            ipv4_int = IPv4Address(parts.pop())._ip
            parts.append('%x' % ((ipv4_int >> 16) & 0xFFFF))
            parts.append('%x' % (ipv4_int & 0xFFFF))

        # An IPv6 address can't have more than 8 colons (9 parts).
        if len(parts) > self._HEXTET_COUNT + 1:
            raise AddressValueError(ip_str)

        # Disregarding the endpoints, find '::' with nothing in between.
        # This indicates that a run of zeroes has been skipped.
        try:
            skip_index, = (
                [i for i in range(1, len(parts) - 1) if not parts[i]] or
                [None])
        except ValueError:
            # Can't have more than one '::'
            raise AddressValueError(ip_str)

        # parts_hi is the number of parts to copy from above/before the '::'
        # parts_lo is the number of parts to copy from below/after the '::'
        if skip_index is not None:
            # If we found a '::', then check if it also covers the endpoints.
            parts_hi = skip_index
            parts_lo = len(parts) - skip_index - 1
            if not parts[0]:
                parts_hi -= 1
                if parts_hi:
                    raise AddressValueError(ip_str)  # ^: requires ^::
            if not parts[-1]:
                parts_lo -= 1
                if parts_lo:
                    raise AddressValueError(ip_str)  # :$ requires ::$
            parts_skipped = self._HEXTET_COUNT - (parts_hi + parts_lo)
            if parts_skipped < 1:
                raise AddressValueError(ip_str)
        else:
            # Otherwise, allocate the entire address to parts_hi.  The endpoints
            # could still be empty, but _parse_hextet() will check for that.
            if len(parts) != self._HEXTET_COUNT:
                raise AddressValueError(ip_str)
            parts_hi = len(parts)
            parts_lo = 0
            parts_skipped = 0

        try:
            # Now, parse the hextets into a 128-bit integer.
            ip_int = 0
            for i in range(parts_hi):
                ip_int <<= 16
                ip_int |= self._parse_hextet(parts[i])
            ip_int <<= 16 * parts_skipped
            for i in range(-parts_lo, 0):
                ip_int <<= 16
                ip_int |= self._parse_hextet(parts[i])
            return ip_int
        except ValueError:
            raise AddressValueError(ip_str)

    def _parse_hextet(self, hextet_str):
        """Convert an IPv6 hextet string into an integer.

        Args:
            hextet_str: A string, the number to parse.

        Returns:
            The hextet as an integer.

        Raises:
            ValueError: if the input isn't strictly a hex number from [0..FFFF].

        """
        # Whitelist the characters, since int() allows a lot of bizarre stuff.
        if not self._HEX_DIGITS.issuperset(hextet_str):
            raise ValueError
        if len(hextet_str) > 4:
            raise ValueError
        hextet_int = int(hextet_str, 16)
        if hextet_int > 0xFFFF:
            raise ValueError
        return hextet_int

    def _compress_hextets(self, hextets):
        """Compresses a list of hextets.

        Compresses a list of strings, replacing the longest continuous
        sequence of "0" in the list with "" and adding empty strings at
        the beginning or at the end of the string such that subsequently
        calling ":".join(hextets) will produce the compressed version of
        the IPv6 address.

        Args:
            hextets: A list of strings, the hextets to compress.

        Returns:
            A list of strings.

        """
        best_doublecolon_start = -1
        best_doublecolon_len = 0
        doublecolon_start = -1
        doublecolon_len = 0
        for index in range(len(hextets)):
            if hextets[index] == '0':
                doublecolon_len += 1
                if doublecolon_start == -1:
                    # Start of a sequence of zeros.
                    doublecolon_start = index
                if doublecolon_len > best_doublecolon_len:
                    # This is the longest sequence of zeros so far.
                    best_doublecolon_len = doublecolon_len
                    best_doublecolon_start = doublecolon_start
            else:
                doublecolon_len = 0
                doublecolon_start = -1

        if best_doublecolon_len > 1:
            best_doublecolon_end = (best_doublecolon_start +
                                    best_doublecolon_len)
            # For zeros at the end of the address.
            if best_doublecolon_end == len(hextets):
                hextets += ['']
            hextets[best_doublecolon_start:best_doublecolon_end] = ['']
            # For zeros at the beginning of the address.
            if best_doublecolon_start == 0:
                hextets = [''] + hextets

        return hextets

    def _string_from_ip_int(self, ip_int=None):
        """Turns a 128-bit integer into hexadecimal notation.

        Args:
            ip_int: An integer, the IP address.

        Returns:
            A string, the hexadecimal representation of the address.

        Raises:
            ValueError: The address is bigger than 128 bits of all ones.

        """
        if not ip_int and ip_int != 0:
            ip_int = int(self._ip)

        if ip_int > self._ALL_ONES:
            raise ValueError('IPv6 address is too large')

        hex_str = '%032x' % ip_int
        hextets = []
        for x in range(0, 32, 4):
            hextets.append('%x' % int(hex_str[x:x+4], 16))

        hextets = self._compress_hextets(hextets)
        return ':'.join(hextets)

    def _explode_shorthand_ip_string(self):
        """Expand a shortened IPv6 address.

        Args:
            ip_str: A string, the IPv6 address.

        Returns:
            A string, the expanded IPv6 address.

        """
        if isinstance(self, IPv6Network):
            ip_str = str(self.network_address)
        elif isinstance(self, IPv6Interface):
            ip_str = str(self.ip)
        else:
            ip_str = str(self)

        ip_int = self._ip_int_from_string(ip_str)
        parts = []
        for i in range(self._HEXTET_COUNT):
            parts.append('%04x' % (ip_int & 0xFFFF))
            ip_int >>= 16
        parts.reverse()
        if isinstance(self, (_BaseNetwork, IPv6Interface)):
            return '%s/%d' % (':'.join(parts), self.prefixlen)
        return ':'.join(parts)

    @property
    def max_prefixlen(self):
        return self._max_prefixlen

    @property
    def packed(self):
        """The binary representation of this address."""
        return v6_int_to_packed(self._ip)

    @property
    def version(self):
        return self._version

    @property
    def is_multicast(self):
        """Test if the address is reserved for multicast use.

        Returns:
            A boolean, True if the address is a multicast address.
            See RFC 2373 2.7 for details.

        """
        multicast_network = IPv6Network('ff00::/8')
        if isinstance(self, _BaseAddress):
            return self in multicast_network
        return (self.network_address in multicast_network and
                self.broadcast_address in multicast_network)

    @property
    def is_reserved(self):
        """Test if the address is otherwise IETF reserved.

        Returns:
            A boolean, True if the address is within one of the
            reserved IPv6 Network ranges.

        """
        reserved_networks = [IPv6Network('::/8'), IPv6Network('100::/8'),
                             IPv6Network('200::/7'), IPv6Network('400::/6'),
                             IPv6Network('800::/5'), IPv6Network('1000::/4'),
                             IPv6Network('4000::/3'), IPv6Network('6000::/3'),
                             IPv6Network('8000::/3'), IPv6Network('A000::/3'),
                             IPv6Network('C000::/3'), IPv6Network('E000::/4'),
                             IPv6Network('F000::/5'), IPv6Network('F800::/6'),
                             IPv6Network('FE00::/9')]

        if isinstance(self, _BaseAddress):
            return len([x for x in reserved_networks if self in x]) > 0
        return len([x for x in reserved_networks if self.network_address in x
                    and self.broadcast_address in x]) > 0

    @property
    def is_link_local(self):
        """Test if the address is reserved for link-local.

        Returns:
            A boolean, True if the address is reserved per RFC 4291.

        """
        linklocal_network = IPv6Network('fe80::/10')
        if isinstance(self, _BaseAddress):
            return self in linklocal_network
        return (self.network_address in linklocal_network and
                self.broadcast_address in linklocal_network)

    @property
    def is_site_local(self):
        """Test if the address is reserved for site-local.

        Note that the site-local address space has been deprecated by RFC 3879.
        Use is_private to test if this address is in the space of unique local
        addresses as defined by RFC 4193.

        Returns:
            A boolean, True if the address is reserved per RFC 3513 2.5.6.

        """
        sitelocal_network = IPv6Network('fec0::/10')
        if isinstance(self, _BaseAddress):
            return self in sitelocal_network
        return (self.network_address in sitelocal_network and
                self.broadcast_address in sitelocal_network)

    @property
    def is_private(self):
        """Test if this address is allocated for private networks.

        Returns:
            A boolean, True if the address is reserved per RFC 4193.

        """
        private_network = IPv6Network('fc00::/7')
        if isinstance(self, _BaseAddress):
            return self in private_network
        return (self.network_address in private_network and
                self.broadcast_address in private_network)


    @property
    def ipv4_mapped(self):
        """Return the IPv4 mapped address.

        Returns:
            If the IPv6 address is a v4 mapped address, return the
            IPv4 mapped address. Return None otherwise.

        """
        if (self._ip >> 32) != 0xFFFF:
            return None
        return IPv4Address(self._ip & 0xFFFFFFFF)

    @property
    def teredo(self):
        """Tuple of embedded teredo IPs.

        Returns:
            Tuple of the (server, client) IPs or None if the address
            doesn't appear to be a teredo address (doesn't start with
            2001::/32)

        """
        if (self._ip >> 96) != 0x20010000:
            return None
        return (IPv4Address((self._ip >> 64) & 0xFFFFFFFF),
                IPv4Address(~self._ip & 0xFFFFFFFF))

    @property
    def sixtofour(self):
        """Return the IPv4 6to4 embedded address.

        Returns:
            The IPv4 6to4-embedded address if present or None if the
            address doesn't appear to contain a 6to4 embedded address.

        """
        if (self._ip >> 112) != 0x2002:
            return None
        return IPv4Address((self._ip >> 80) & 0xFFFFFFFF)

    @property
    def is_unspecified(self):
        """Test if the address is unspecified.

        Returns:
            A boolean, True if this is the unspecified address as defined in
            RFC 2373 2.5.2.

        """
        if isinstance(self, (IPv6Network, IPv6Interface)):
            return int(self.network_address) == 0 and getattr(
                self, '_prefixlen', 128) == 128
        return self._ip == 0

    @property
    def is_loopback(self):
        """Test if the address is a loopback address.

        Returns:
            A boolean, True if the address is a loopback address as defined in
            RFC 2373 2.5.3.

        """
        if isinstance(self, IPv6Network):
            return int(self.network) == 1 and getattr(
                self, '_prefixlen', 128) == 128
        elif isinstance(self, IPv6Interface):
            return int(self.network.network_address) == 1 and getattr(
                self, '_prefixlen', 128) == 128
        return self._ip == 1


class IPv6Address(_BaseV6, _BaseAddress):

    """Represent and manipulate single IPv6 Addresses.
    """

    def __init__(self, address):
        """Instantiate a new IPv6 address object.

        Args:
            address: A string or integer representing the IP

              Additionally, an integer can be passed, so
              IPv6Address('2001:db8::') ==
                IPv6Address(42540766411282592856903984951653826560)
              or, more generally
              IPv6Address(int(IPv6Address('2001:db8::'))) ==
                IPv6Address('2001:db8::')

        Raises:
            AddressValueError: If address isn't a valid IPv6 address.

        """
        _BaseAddress.__init__(self, address)
        _BaseV6.__init__(self, address)

        # Efficient constructor from integer.
        if isinstance(address, int):
            self._ip = address
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            return

        # Constructing from a packed address
        if (not isinstance(address, str) and
            isinstance(address, bytes) and len(address) == 16):
            tmp = struct.unpack('!QQ', address)
            self._ip = (tmp[0] << 64) | tmp[1]
            return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP string.
        addr_str = str(address)
        if not addr_str:
            raise AddressValueError('')

        self._ip = self._ip_int_from_string(addr_str)


class IPv6Interface(IPv6Address):

    def __init__(self, address):
        if isinstance(address, (bytes, int)):
            IPv6Address.__init__(self, address)
            self.network = IPv6Network(self._ip)
            self._prefixlen = self._max_prefixlen
            return

        addr = str(address).split('/')
        IPv6Address.__init__(self, addr[0])
        self.network = IPv6Network(address, strict=False)
        self.netmask = self.network.netmask
        self._prefixlen = self.network._prefixlen
        self.hostmask = self.network.hostmask


    def __str__(self):
        return '%s/%d' % (self._string_from_ip_int(self._ip),
                          self.network.prefixlen)

    def __eq__(self, other):
        try:
            return (IPv6Address.__eq__(self, other) and
                    self.network == other.network)
        except AttributeError:
            return NotImplemented

    def __hash__(self):
        return self._ip ^ self._prefixlen ^ int(self.network.network_address)

    @property
    def prefixlen(self):
        return self._prefixlen
    @property
    def ip(self):
        return IPv6Address(self._ip)

    @property
    def with_prefixlen(self):
        return self

    @property
    def with_netmask(self):
        return self.with_prefixlen
    @property
    def with_hostmask(self):
        return '%s/%s' % (self._string_from_ip_int(self._ip),
                          self.hostmask)


class IPv6Network(_BaseV6, _BaseNetwork):

    """This class represents and manipulates 128-bit IPv6 networks.

    Attributes: [examples for IPv6('2001:db8::1000/124')]
        .network_address: IPv6Address('2001:db8::1000')
        .hostmask: IPv6Address('::f')
        .broadcast_address: IPv6Address('2001:db8::100f')
        .netmask: IPv6Address('ffff:ffff:ffff:ffff:ffff:ffff:ffff:fff0')
        .prefixlen: 124

    """

    def __init__(self, address, strict=True):
        """Instantiate a new IPv6 Network object.

        Args:
            address: A string or integer representing the IPv6 network or the IP
              and prefix/netmask.
              '2001:db8::/128'
              '2001:db8:0000:0000:0000:0000:0000:0000/128'
              '2001:db8::'
              are all functionally the same in IPv6.  That is to say,
              failing to provide a subnetmask will create an object with
              a mask of /128.

              Additionally, an integer can be passed, so
              IPv6Network('2001:db8::') ==
                IPv6Network(42540766411282592856903984951653826560)
              or, more generally
              IPv6Network(int(IPv6Network('2001:db8::'))) ==
                IPv6Network('2001:db8::')

            strict: A boolean. If true, ensure that we have been passed
              A true network address, eg, 2001:db8::1000/124 and not an
              IP address on a network, eg, 2001:db8::1/124.

        Raises:
            AddressValueError: If address isn't a valid IPv6 address.
            NetmaskValueError: If the netmask isn't valid for
              an IPv6 address.
            ValueError: If strict was True and a network address was not
              supplied.

        """
        _BaseV6.__init__(self, address)
        _BaseNetwork.__init__(self, address)

        # Efficient constructor from integer.
        if isinstance(address, int):
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            self.network_address = IPv6Address(address)
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv6Address(self._ALL_ONES)
            if strict:
                if (IPv6Address(int(self.network_address) &
                                int(self.netmask)) != self.network_address):
                    raise ValueError('%s has host bits set' % str(self))
            self.network_address = IPv6Address(int(self.network_address) &
                                               int(self.netmask))
            return

        # Constructing from a packed address
        if isinstance(address, bytes) and len(address) == 16:
            tmp = struct.unpack('!QQ', address)
            self.network_address = IPv6Address((tmp[0] << 64) | tmp[1])
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv6Address(self._ALL_ONES)
            if strict:
                if (IPv6Address(int(self.network_address) &
                                int(self.netmask)) != self.network_address):
                    raise ValueError('%s has host bits set' % str(self))
                self.network_address = IPv6Address(int(self.network_address) &
                                                   int(self.netmask))
                return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP prefix string.
        addr = str(address).split('/')

        if len(addr) > 2:
            raise AddressValueError(address)

        self.network_address = IPv6Address(self._ip_int_from_string(addr[0]))

        if len(addr) == 2:
            if self._is_valid_netmask(addr[1]):
                self._prefixlen = int(addr[1])
            else:
                raise NetmaskValueError(addr[1])
        else:
            self._prefixlen = self._max_prefixlen

        self.netmask = IPv6Address(self._ip_int_from_prefix(self._prefixlen))
        if strict:
            if (IPv6Address(int(self.network_address) & int(self.netmask)) !=
                self.network_address):
                raise ValueError('%s has host bits set' % str(self))
        self.network_address = IPv6Address(int(self.network_address) &
                                           int(self.netmask))

        if self._prefixlen == (self._max_prefixlen - 1):
            self.hosts = self.__iter__

    def __str__(self):
        return '%s/%d' % (str(self.network_address),
                          self.prefixlen)

    def _is_valid_netmask(self, prefixlen):
        """Verify that the netmask/prefixlen is valid.

        Args:
            prefixlen: A string, the netmask in prefix length format.

        Returns:
            A boolean, True if the prefix represents a valid IPv6
            netmask.

        """
        try:
            prefixlen = int(prefixlen)
        except ValueError:
            return False
        return 0 <= prefixlen <= self._max_prefixlen

    @property
    def with_netmask(self):
        return self.with_prefixlen

    @property
    def with_prefixlen(self):
        return '%s/%d' % (str(self.network_address), self._prefixlen)

    @property
    def with_netmask(self):
        return '%s/%s' % (str(self.network_address), str(self.netmask))

    @property
    def with_hostmask(self):
        return '%s/%s' % (str(self.network_address), str(self.hostmask))
<EOF>
<BOF>
# ========================================================================
# simple-flake - https://github.com/SawdustSoftware/simple-flake
# ========================================================================

# Copyright (c) 2013 CustomMade Ventures

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

import time
import random
import collections

#: Epoch for simpleflake timestamps, starts at the year 2000
SIMPLEFLAKE_EPOCH = 946702800

#field lengths in bits
SIMPLEFLAKE_TIMESTAMP_LENGTH = 41
SIMPLEFLAKE_RANDOM_LENGTH = 23

#left shift amounts
SIMPLEFLAKE_RANDOM_SHIFT = 0
SIMPLEFLAKE_TIMESTAMP_SHIFT = 23

simpleflake_struct = collections.namedtuple("SimpleFlake",
                                            ["timestamp", "random_bits"])

# ===================== Utility ====================


def pad_bytes_to_64(string):
    return format(string, "064b")


def binary(num, padding=True):
    """Show binary digits of a number, pads to 64 bits unless specified."""
    binary_digits = "{0:b}".format(int(num))
    if not padding:
        return binary_digits
    return pad_bytes_to_64(int(num))


def extract_bits(data, shift, length):
    """Extract a portion of a bit string. Similar to substr()."""
    bitmask = ((1 << length) - 1) << shift
    return ((data & bitmask) >> shift)

# ==================================================


def simpleflake(timestamp=None, random_bits=None, epoch=SIMPLEFLAKE_EPOCH):
    """Generate a 64 bit, roughly-ordered, globally-unique ID."""
    second_time = timestamp if timestamp is not None else time.time()
    second_time -= epoch
    millisecond_time = int(second_time * 1000)

    randomness = random.SystemRandom().getrandbits(SIMPLEFLAKE_RANDOM_LENGTH)
    randomness = random_bits if random_bits is not None else randomness

    flake = (millisecond_time << SIMPLEFLAKE_TIMESTAMP_SHIFT) + randomness

    return flake


def parse_simpleflake(flake):
    """Parses a simpleflake and returns a named tuple with the parts."""
    timestamp = SIMPLEFLAKE_EPOCH\
        + extract_bits(flake,
                       SIMPLEFLAKE_TIMESTAMP_SHIFT,
                       SIMPLEFLAKE_TIMESTAMP_LENGTH) / 1000.0
    random = extract_bits(flake,
                          SIMPLEFLAKE_RANDOM_SHIFT,
                          SIMPLEFLAKE_RANDOM_LENGTH)
    return simpleflake_struct(timestamp, random)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
import multiprocessing

from r2.lib.mr_tools._mr_tools import mr_map, mr_reduce, format_dataspec
from r2.lib.mr_tools._mr_tools import stdin, emit

def join_things(fields, deleted=False, spam=True):
    """A reducer that joins thing table dumps and data table dumps"""
    # Because of how Python handles scope, if we want to modify these outside
    # the closure function below, they need to be inside a mutable object.
    # http://stackoverflow.com/a/23558809/120999
    counters = {
        'processed': 0,
        'skipped': 0,
    }
    def process(thing_id, vals):
        data = {}
        thing = None

        for val in vals:
            if val[0] == 'thing':
                thing = format_dataspec(val,
                                        ['data_type', # e.g. 'thing'
                                         'thing_type', # e.g. 'link'
                                         'ups',
                                         'downs',
                                         'deleted',
                                         'spam',
                                         'timestamp'])
            elif val[0] == 'data':
                val = format_dataspec(val,
                                      ['data_type', # e.g. 'data'
                                       'thing_type', # e.g. 'link'
                                       'key', # e.g. 'sr_id'
                                       'value'])
                if val.key in fields:
                    data[val.key] = val.value

        if (
            # silently ignore if we didn't see the 'thing' row
            thing is not None

            # remove spam and deleted as appriopriate
            and (deleted or thing.deleted == 'f')
            and (spam or thing.spam == 'f')

            # and silently ignore items that don't have all of the
            # data that we need
            and all(field in data for field in fields)):

            counters['processed'] += 1
            yield ((thing_id, thing.thing_type, thing.ups, thing.downs,
                    thing.deleted, thing.spam, thing.timestamp)
                   + tuple(data[field] for field in fields))
        else:
            counters['skipped'] += 1

    mr_reduce(process)
    # Print to stderr to avoid getting this caught up in the pipe of
    # compute_time_listings.
    print >> sys.stderr, '%s items processed, %s skipped' % (
                         counters['processed'], counters['skipped'])

class Mapper(object):
    def __init__(self):
        pass

    def process(self, values):
        raise NotImplemented

    def __call__(self, line):
        line = line.strip('\n')
        vals = line.split('\t')
        return list(self.process(vals)) # a list of tuples

def mr_map_parallel(processor, fd = stdin,
                    workers = multiprocessing.cpu_count(),
                    chunk_size = 1000):
    # `process` must be an instance of Mapper and promise that it is
    # safe to execute in a fork()d process.  Also note that we fuck
    # up the result ordering, but relying on result ordering breaks
    # the mapreduce contract anyway. Note also that like many of the
    # mr_tools functions, we break on newlines in the emitted output

    if workers == 1:
        return mr_map(process, fd=fd)

    pool = multiprocessing.Pool(workers)

    for res in pool.imap_unordered(processor, fd, chunk_size):
        for subres in res:
            emit(subres)

def test():
    from r2.lib.mr_tools._mr_tools import keyiter

    for key, vals in keyiter():
        print key, vals
        for val in vals:
            print '\t', val

class UpperMapper(Mapper):
    def process(self, values):
        yield map(str.upper, values)

def test_parallel():
    return mr_map_parallel(UpperMapper())
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.mr_tools._mr_tools import *
from r2.lib.mr_tools.mr_tools import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import copy
from pylons import app_globals as g
import os
from time import time, sleep

from boto.emr.step import InstallPigStep, PigStep
from boto.emr.bootstrap_action import BootstrapAction

from r2.lib.emr_helpers import (
    EmrException,
    EmrJob,
    get_live_clusters,
    get_step_state,
    LIVE_STATES,
    COMPLETED,
    PENDING,
    NOTFOUND,
)


class TrafficBase(EmrJob):

    """Base class for all traffic jobs.

    Includes required bootstrap actions and setup steps.

    """

    BOOTSTRAP_NAME = 'traffic binaries'
    BOOTSTRAP_SCRIPT = os.path.join(g.TRAFFIC_SRC_DIR, 'traffic_bootstrap.sh')
    _defaults = dict(master_instance_type='m1.small',
                     slave_instance_type='c3.2xlarge', num_slaves=1,
                     job_flow_role=g.emr_trafic_job_flow_role,
                     service_role=g.emr_traffic_service_role,
                     tags=g.emr_traffic_tags,
                )

    def __init__(self, emr_connection, jobflow_name, steps=None, **kw):
        combined_kw = copy(self._defaults)
        combined_kw.update(kw)
        bootstrap_actions = self._bootstrap_actions()
        setup_steps = self._setup_steps()
        steps = steps or []
        EmrJob.__init__(self, emr_connection, jobflow_name,
                        bootstrap_actions=bootstrap_actions,
                        setup_steps=setup_steps,
                        steps=steps,
                        **combined_kw)

    @classmethod
    def _bootstrap_actions(cls):
        name = cls.BOOTSTRAP_NAME
        path = cls.BOOTSTRAP_SCRIPT
        bootstrap_action_args = [g.TRAFFIC_SRC_DIR, g.tracking_secret]
        bootstrap = BootstrapAction(name, path, bootstrap_action_args)
        return [bootstrap]

    @classmethod
    def _setup_steps(self):
        return [InstallPigStep()]


class PigProcessHour(PigStep):
    STEP_NAME = 'pig process hour'
    PIG_FILE = os.path.join(g.TRAFFIC_SRC_DIR, 'mr_process_hour.pig')

    def __init__(self, log_path, output_path):
        self.log_path = log_path
        self.output_path = output_path
        self.name = '%s (%s)' % (self.STEP_NAME, self.log_path)
        pig_args = ['-p', 'OUTPUT=%s' % self.output_path,
                    '-p', 'LOGFILE=%s' % self.log_path]
        PigStep.__init__(self, self.name, self.PIG_FILE, pig_args=pig_args)


class PigAggregate(PigStep):
    STEP_NAME = 'pig aggregate'
    PIG_FILE = os.path.join(g.TRAFFIC_SRC_DIR, 'mr_aggregate.pig')

    def __init__(self, input_path, output_path):
        self.input_path = input_path
        self.output_path = output_path
        self.name = '%s (%s)' % (self.STEP_NAME, self.input_path)
        pig_args = ['-p', 'INPUT=%s' % self.input_path,
                    '-p', 'OUTPUT=%s' % self.output_path]
        PigStep.__init__(self, self.name, self.PIG_FILE, pig_args=pig_args)


class PigCoalesce(PigStep):
    STEP_NAME = 'pig coalesce'
    PIG_FILE = os.path.join(g.TRAFFIC_SRC_DIR, 'mr_coalesce.pig')

    def __init__(self, input_path, output_path):
        self.input_path = input_path
        self.output_path = output_path
        self.name = '%s (%s)' % (self.STEP_NAME, self.input_path)
        pig_args = ['-p', 'INPUT=%s' % self.input_path,
                    '-p', 'OUTPUT=%s' % self.output_path]
        PigStep.__init__(self, self.name, self.PIG_FILE, pig_args=pig_args)


def _add_step(emr_connection, step, jobflow_name, **jobflow_kw):
    """Add step to a running jobflow.

    Append the step onto a jobflow with the specified name if one exists,
    otherwise create a new jobflow and run it. Returns the jobflowid.
    NOTE: jobflow_kw will be used to configure the jobflow ONLY if a new
    jobflow is created.

    """

    running = get_live_clusters(emr_connection)

    for cluster in running:
        # NOTE: the existing cluster's bootstrap actions aren't checked so we
        # are assuming that any cluster with the correct name is compatible
        # with our new step
        if cluster.name == jobflow_name:
            jobflowid = cluster.id
            emr_connection.add_jobflow_steps(jobflowid, step)
            print 'Added %s to jobflow %s' % (step.name, jobflowid)
            break
    else:
        base = TrafficBase(emr_connection, jobflow_name, steps=[step],
                           **jobflow_kw)
        base.run()
        jobflowid = base.jobflowid
        print 'Added %s to new jobflow %s' % (step.name, jobflowid)

    return jobflowid


def _wait_for_step(emr_connection, step, jobflowid, sleeptime):
    """Poll EMR and wait for a step to finish."""
    sleep(180)
    start = time()
    step_state = get_step_state(emr_connection, jobflowid, step.name,
                                update=True)
    while step_state in LIVE_STATES + [PENDING]:
        sleep(sleeptime)
        step_state = get_step_state(emr_connection, jobflowid, step.name)
    end = time()
    print '%s took %0.2fs (exit: %s)' % (step.name, end - start, step_state)
    return step_state


def run_traffic_step(emr_connection, step, jobflow_name,
                     wait=True, sleeptime=60, retries=1, **jobflow_kw):
    """Run a traffic processing step.

    Helper function to force all steps to be executed by the same jobflow
    (jobflow_name). Also can hold until complete (wait) and retry on
    failure (retries).

    """

    jobflowid = _add_step(emr_connection, step, jobflow_name, **jobflow_kw)

    if not wait:
        return

    attempts = 1
    exit_state = _wait_for_step(emr_connection, step, jobflowid, sleeptime)
    while attempts <= retries and exit_state != COMPLETED:
        jobflowid = _add_step(emr_connection, step, jobflow_name, **jobflow_kw)
        exit_state = _wait_for_step(emr_connection, step, jobflowid, sleeptime)
        attempts += 1

    if exit_state != COMPLETED:
        msg = '%s failed (exit: %s)' % (step.name, exit_state)
        if retries:
            msg += 'retried %s times' % retries
        raise EmrException(msg)


def extract_hour(emr_connection, jobflow_name, log_path, output_path,
                 **jobflow_kw):
    step = PigProcessHour(log_path, output_path)
    run_traffic_step(emr_connection, step, jobflow_name, **jobflow_kw)


def aggregate_interval(emr_connection, jobflow_name, input_path, output_path,
                       **jobflow_kw):
    step = PigAggregate(input_path, output_path)
    run_traffic_step(emr_connection, step, jobflow_name, **jobflow_kw)


def coalesce_interval(emr_connection, jobflow_name, input_path, output_path,
                      **jobflow_kw):
    step = PigCoalesce(input_path, output_path)
    run_traffic_step(emr_connection, step, jobflow_name, **jobflow_kw)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from traffic import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import calendar
import os
from time import sleep
import urllib

from boto.s3.connection import S3Connection
from boto.emr.connection import EmrConnection
from boto.exception import S3ResponseError
from pylons import app_globals as g
from sqlalchemy.exc import DataError

from r2.lib.emr_helpers import (EmrException, terminate_jobflow,
    modify_slave_count)
from r2.lib.s3_helpers import get_text_from_s3, s3_key_exists, copy_to_s3
from r2.lib.traffic.emr_traffic import (extract_hour, aggregate_interval,
        coalesce_interval)
from r2.lib.utils import tup
from r2.models.traffic import (SitewidePageviews, PageviewsBySubreddit,
        PageviewsBySubredditAndPath, PageviewsByLanguage,
        ClickthroughsByCodename, TargetedClickthroughsByCodename,
        AdImpressionsByCodename, TargetedImpressionsByCodename)


RAW_LOG_DIR = g.RAW_LOG_DIR
PROCESSED_DIR = g.PROCESSED_DIR
AGGREGATE_DIR = g.AGGREGATE_DIR
AWS_LOG_DIR = g.AWS_LOG_DIR

# the "or None" business is so that a blank string becomes None to cause boto
# to look for credentials in other places.
s3_connection = S3Connection(g.TRAFFIC_ACCESS_KEY or None,
                             g.TRAFFIC_SECRET_KEY or None)
emr_connection = EmrConnection(g.TRAFFIC_ACCESS_KEY or None,
                               g.TRAFFIC_SECRET_KEY or None)

traffic_categories = (SitewidePageviews, PageviewsBySubreddit,
                      PageviewsBySubredditAndPath, PageviewsByLanguage,
                      ClickthroughsByCodename, TargetedClickthroughsByCodename,
                      AdImpressionsByCodename, TargetedImpressionsByCodename)

traffic_subdirectories = {
    SitewidePageviews: 'sitewide',
    PageviewsBySubreddit: 'subreddit',
    PageviewsBySubredditAndPath: 'srpath',
    PageviewsByLanguage: 'lang',
    ClickthroughsByCodename: 'clicks',
    TargetedClickthroughsByCodename: 'clicks_targeted',
    AdImpressionsByCodename: 'thing',
    TargetedImpressionsByCodename: 'thingtarget',
}


def _get_processed_path(basedir, interval, category_cls, filename):
    return os.path.join(basedir, interval,
                        traffic_subdirectories[category_cls], filename)


def get_aggregate(interval, category_cls):
    """Return the aggregate output file from S3."""
    part = 0
    data = {}

    while True:
        path = _get_processed_path(AGGREGATE_DIR, interval, category_cls,
                                   'part-r-%05d' % part)
        if not s3_key_exists(s3_connection, path):
            break

        # Sometimes S3 doesn't let us read immediately after key is written
        for i in xrange(5):
            try:
                txt = get_text_from_s3(s3_connection, path)
            except S3ResponseError as e:
                print 'S3ResponseError on %s, retrying' % path
                sleep(300)
            else:
                break
        else:
            print 'Could not retrieve %s' % path
            raise e

        for line in txt.splitlines():
            tuples = line.rstrip('\n').split('\t')
            group, uniques, pageviews = tuples[:-2], tuples[-2], tuples[-1]
            if len(group) > 1:
                group = tuple(group)
            else:
                group = group[0]
            data[group] = (int(uniques), int(pageviews))

        part += 1

    if not data:
        raise ValueError("No data for %s/%s" % (interval,
                                                category_cls.__name__))

    return data


def report_interval(interval, background=True):
    if background:
        from multiprocessing import Process
        p = Process(target=_report_interval, args=(interval,))
        p.start()
    else:
        _report_interval(interval)


def _name_to_kw(category_cls, name):
    """Get the keywords needed to build an instance of traffic data."""
    def target_split(name):
        """Split a name that contains multiple words.

        Name is (link,campaign-subreddit) where link and campaign are
        thing fullnames. campaign and subreddit are each optional, so
        the string could look like any of these:
        (t3_bh,t8_ab-pics), (t3_bh,t8_ab), (t3_bh,-pics), (t3_bh,)
        Also check for the old format (t3_by, pics)

        """

        link_codename, target_info = name
        campaign_codename = None
        if not target_info:
            subreddit = ''
        elif target_info.find('-') != -1:
            campaign_codename, subreddit = target_info.split('-', 1)
        elif target_info.find('_') != -1:
            campaign_codename = target_info
            subreddit = ''
        else:
            subreddit = target_info
        return {'codename': campaign_codename or link_codename,
                'subreddit': subreddit}

    d = {SitewidePageviews: lambda n: {},
         PageviewsBySubreddit: lambda n: {'subreddit': n},
         PageviewsBySubredditAndPath: lambda n: {'srpath': n},
         PageviewsByLanguage: lambda n: {'lang': n},
         ClickthroughsByCodename: lambda n: {'codename': name},
         AdImpressionsByCodename: lambda n: {'codename': name},
         TargetedClickthroughsByCodename: target_split,
         TargetedImpressionsByCodename: target_split}
    return d[category_cls](name)


def _report_interval(interval):
    """Read aggregated traffic from S3 and write to postgres."""
    from sqlalchemy.orm import scoped_session, sessionmaker
    from r2.models.traffic import engine
    Session = scoped_session(sessionmaker(bind=engine))

    # determine interval_type from YYYY-MM[-DD][-HH]
    pieces = interval.split('-')
    pieces = [int(i) for i in pieces]
    if len(pieces) == 4:
        interval_type = 'hour'
    elif len(pieces) == 3:
        interval_type = 'day'
        pieces.append(0)
    elif len(pieces) == 2:
        interval_type = 'month'
        pieces.append(1)
        pieces.append(0)
    else:
        raise

    pg_interval = "%04d-%02d-%02d %02d:00:00" % tuple(pieces)
    print 'reporting interval %s (%s)' % (pg_interval, interval_type)

    # Read aggregates and write to traffic db
    for category_cls in traffic_categories:
        now = datetime.datetime.now()
        print '*** %s - %s - %s' % (category_cls.__name__, interval, now)
        data = get_aggregate(interval, category_cls)
        len_data = len(data)
        step = max(len_data / 5, 100)
        for i, (name, (uniques, pageviews)) in enumerate(data.iteritems()):
            try:
                for n in tup(name):
                    unicode(n)
            except UnicodeDecodeError:
                print '%s - %s - %s - %s' % (category_cls.__name__, name,
                                             uniques, pageviews)
                continue

            if i % step == 0:
                now = datetime.datetime.now()
                print '%s - %s - %s/%s - %s' % (interval, category_cls.__name__,
                                                i, len_data, now)

            kw = {'date': pg_interval, 'interval': interval_type,
                  'unique_count': uniques, 'pageview_count': pageviews}
            kw.update(_name_to_kw(category_cls, name))
            r = category_cls(**kw)

            try:
                Session.merge(r)
                Session.commit()
            except DataError:
                Session.rollback()
                continue

    Session.remove()
    now = datetime.datetime.now()
    print 'finished reporting %s (%s) - %s' % (pg_interval, interval_type, now)


def process_pixel_log(log_path, fast=False):
    """Process an hourly pixel log file.

    Extract data from raw hourly log and aggregate it and report it. Also
    depending on the specific date and options, aggregate and report the day
    and month. Setting fast=True is appropriate for backfilling as it
    eliminates reduntant steps.

    """

    if log_path.endswith('/*'):
        log_dir = log_path[:-len('/*')]
        date_fields = os.path.basename(log_dir).split('.', 1)[0].split('-')
    else:
        date_fields = os.path.basename(log_path).split('.', 1)[0].split('-')
    year, month, day, hour = (int(i) for i in date_fields)
    hour_date = '%s-%02d-%02d-%02d' % (year, month, day, hour)
    day_date = '%s-%02d-%02d' % (year, month, day)
    month_date = '%s-%02d' % (year, month)

    # All logs from this day use the same jobflow
    jobflow_name = 'Traffic Processing %s' % day_date

    output_path = os.path.join(PROCESSED_DIR, 'hour', hour_date)
    extract_hour(emr_connection, jobflow_name, log_path, output_path,
                 log_uri=AWS_LOG_DIR)

    input_path = os.path.join(PROCESSED_DIR, 'hour', hour_date)
    output_path = os.path.join(AGGREGATE_DIR, hour_date)
    aggregate_interval(emr_connection, jobflow_name, input_path, output_path,
                       log_uri=AWS_LOG_DIR)
    if not fast:
        report_interval(hour_date)

    if hour == 23 or (not fast and (hour == 0 or hour % 4 == 3)):
        # Don't aggregate and report day on every hour
        input_path = os.path.join(PROCESSED_DIR, 'hour', '%s-*' % day_date)
        output_path = os.path.join(AGGREGATE_DIR, day_date)
        aggregate_interval(emr_connection, jobflow_name, input_path,
                           output_path, log_uri=AWS_LOG_DIR)
        if not fast:
            report_interval(day_date)

    if hour == 23:
        # Special tasks for final hour of the day
        input_path = os.path.join(PROCESSED_DIR, 'hour', '%s-*' % day_date)
        output_path = os.path.join(PROCESSED_DIR, 'day', day_date)
        coalesce_interval(emr_connection, jobflow_name, input_path,
                          output_path, log_uri=AWS_LOG_DIR)
        terminate_jobflow(emr_connection, jobflow_name)

        if not fast:
            aggregate_month(month_date)
            report_interval(month_date)


def aggregate_month(month_date):
    jobflow_name = 'Traffic Processing %s' % month_date
    input_path = os.path.join(PROCESSED_DIR, 'day', '%s-*' % month_date)
    output_path = os.path.join(AGGREGATE_DIR, month_date)
    aggregate_interval(emr_connection, jobflow_name, input_path, output_path,
                       log_uri=AWS_LOG_DIR, slave_instance_type='m2.2xlarge')
    terminate_jobflow(emr_connection, jobflow_name)


def process_month_hours(month_date, start_hour=0, days=None):
    """Process hourly logs from entire month.

    Complete monthly backfill requires running [verify_month_inputs,]
    process_month_hours, aggregate_month, [verify_month_outputs,] and
    report_entire_month.

    """

    year, month = month_date.split('-')
    year, month = int(year), int(month)

    days = days or xrange(1, calendar.monthrange(year, month)[1] + 1)
    hours = xrange(start_hour, 24)

    for day in days:
        for hour in hours:
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            log_path = os.path.join(RAW_LOG_DIR, '%s.log.gz' % hour_date)
            if not s3_key_exists(s3_connection, log_path):
                log_path = os.path.join(RAW_LOG_DIR, '%s.log.bz2' % hour_date)
                if not s3_key_exists(s3_connection, log_path):
                    print 'Missing log for %s' % hour_date
                    continue
            print 'Processing %s' % log_path
            process_pixel_log(log_path, fast=True)
        hours = xrange(24)


def report_entire_month(month_date, start_hour=0, start_day=1):
    """Report all hours and days from month."""
    year, month = month_date.split('-')
    year, month = int(year), int(month)
    hours = xrange(start_hour, 24)

    for day in xrange(start_day, calendar.monthrange(year, month)[1] + 1):
        for hour in hours:
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            try:
                report_interval(hour_date, background=False)
            except ValueError:
                print 'Failed for %s' % hour_date
                continue
        hours = xrange(24)
        day_date = '%04d-%02d-%02d' % (year, month, day)
        try:
            report_interval(day_date, background=False)
        except ValueError:
            print 'Failed for %s' % day_date
            continue
    report_interval(month_date, background=False)


def verify_month_outputs(month_date):
    """Check existance of all hour, day, month aggregates for month_date."""
    year, month = month_date.split('-')
    year, month = int(year), int(month)
    missing = []

    for day in xrange(1, calendar.monthrange(year, month)[1] + 1):
        for hour in xrange(24):
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            for category_cls in traffic_categories:
                for d in [AGGREGATE_DIR, os.path.join(PROCESSED_DIR, 'hour')]:
                    path = _get_processed_path(d, hour_date, category_cls,
                                               'part-r-00000')
                    if not s3_key_exists(s3_connection, path):
                        missing.append(hour_date)

        day_date = '%04d-%02d-%02d' % (year, month, day)
        for category_cls in traffic_categories:
            for d in [AGGREGATE_DIR, os.path.join(PROCESSED_DIR, 'day')]:
                path = _get_processed_path(d, day_date, category_cls,
                                           'part-r-00000')
                if not s3_key_exists(s3_connection, path):
                    missing.append(day_date)

    month_date = '%04d-%02d' % (year, month)
    for c in traffic_categories:
        path = _get_processed_path(AGGREGATE_DIR, month_date, category_cls,
                                   'part-r-00000')
        if not s3_key_exists(s3_connection, path):
            missing.append(month_date)

    for d in sorted(list(set(missing))):
        print d


def verify_month_inputs(month_date):
    """Check existance of all hourly traffic logs for month_date."""
    year, month = month_date.split('-')
    year, month = int(year), int(month)
    missing = []

    for day in xrange(1, calendar.monthrange(year, month)[1] + 1):
        for hour in xrange(24):
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            log_path = os.path.join(RAW_LOG_DIR, '%s.log.gz' % hour_date)
            if not s3_key_exists(s3_connection, log_path):
                log_path = os.path.join(RAW_LOG_DIR, '%s.log.bz2' % hour_date)
                if not s3_key_exists(s3_connection, log_path):
                    missing.append(hour_date)

    for d in missing:
        print d


def process_hour(hour_date):
    """Process hour_date's traffic.

    Can't fire at the very start of an hour because it takes time to bzip and
    upload the file to S3. Check the bucket for the file and sleep if it
    doesn't exist.

    """

    SLEEPTIME = 180

    log_dir = os.path.join(RAW_LOG_DIR, hour_date)
    files_missing = [os.path.join(log_dir, '%s.log.bz2' % h)
                     for h in g.TRAFFIC_LOG_HOSTS]
    files_missing = [f for f in files_missing
                       if not s3_key_exists(s3_connection, f)]

    while files_missing:
        print 'Missing log(s) %s, sleeping' % files_missing
        sleep(SLEEPTIME)
        files_missing = [f for f in files_missing
                           if not s3_key_exists(s3_connection, f)]
    process_pixel_log(os.path.join(log_dir, '*'))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""This module is used for the generation of sitemaps.

Sitemaps (http://www.sitemaps.org/protocol.html) are an xml
protocol designed to exhaustively describe websites. In lieu of
an external link or a link from a reddit, in general most reddit
links do not get indexed by search engines. This means that the vast
majority of Reddit content does not get indexed by Google, Bing, or Yahoo.
Sitemaps solve the problem by showing search engines links that they
likely don't have access to any other way.

Reddit contains tons and tons of links. Generating them on the fly is simply
impractical. The solution this module implements is a very slow batch that
goes through every subreddit and every link and creates crawlable permalinks
from them. These links are then put into sitemaps and stored in
s3. We then upload those sitemaps as static files to s3 where we host them.

The Sitemap protocol specifies a hard limit of 50000 links. Since we have
significantly more links than that, we have to define a Sitemap Index
(http://www.sitemaps.org/protocol.html#index.) The sitemap similarly has
up to 50000 links to other sitemaps. For now it suits our purposes to have
exactly one sitemap, but it may change in the future.

There are only two types of links we currently support. Subreddit links
in the form of

https://www.reddit.com/r/hiphopheads

and comment links in the form of

https://www.reddit.com/r/hiphopheads/comments/4gxk5i/fresh_album_drake_views/.


This module is split into 3 parts.

  r2.lib.sitemaps.data - Loads up the raw Subreddit and Link Things.
  r2.lib.sitemaps.generate - Transforms the Things into sitemap xml strings.
  r2.lib.sitemaps.store - Stores the sitemaps on s3.
  r2.lib.sitemaps.watcher - Reads from the SQS queue and starts a new upload


The only function that's supposed to be used outside of this module is
r2.lib.sitemaps.watcher.watcher. This is designed to be used as a constantly
running daemon.
"""
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""Generates all the data used in making sitemaps and sitemap links.

Currently only supports subreddit links but will soon support comment links.
"""

import tempfile

from boto.s3.connection import S3Connection
from pylons import app_globals as g

from r2.lib.hadoop_decompress import hadoop_decompress


def _read_subreddit_etl_from_s3(s3path):
    s3conn = S3Connection()
    bucket = s3conn.get_bucket(s3path.bucket, validate=False)
    s3keys = bucket.list(s3path.key)

    key_count = 0
    for s3key in s3keys:
        g.log.info("Importing key %r", s3key)

        with tempfile.TemporaryFile(mode='rw+b') as ntf_download:
            with tempfile.TemporaryFile(mode='rw+b') as ntf_decompress:

                # download it
                g.log.debug("Downloading %r", s3key)
                s3key.get_contents_to_file(ntf_download)

                # decompress it
                ntf_download.flush()
                ntf_download.seek(0)
                g.log.debug("Decompressing %r", s3key)
                hadoop_decompress(ntf_download, ntf_decompress)
                ntf_decompress.flush()
                ntf_decompress.seek(0)

                # import it
                g.log.debug("Starting import of %r", s3key)
                for line in ntf_decompress:
                    yield line
        key_count += 1

    if key_count == 0:
        raise ValueError('{0} contains no readable keys.'.format(s3path))


def find_all_subreddits(s3path):
    for line in _read_subreddit_etl_from_s3(s3path):
        _, subreddit, __ = line.split('\x01')
        yield subreddit
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""Store sitemaps in s3.

This module is uploads all subreddit sitemaps as well as the sitemap index
to s3. The basic idea is that amazon will be serving the static sitemaps for
us.

The binary data we send to s3 is a gzipped xml file. In addition we also
send the appropriate type and encoding headers so this is understood
correctly by the browser.

The only file expected to be used outside this module is:

store_sitemaps_in_s3(subreddits)

Even though the subreddits are expected to be generated and passed into this
function, the sitemap index is created here. The reasoning is that in order
to create the sitemap index we need to know how many sitemaps we have.
If we simply queried the subreddit iterator for it's length then we would
have to load all of the subreddits into memory, which would be ... bad.
"""


import gzip
from StringIO import StringIO

from boto.s3.connection import S3Connection
from boto.s3.key import Key
from pylons import app_globals as g

from r2.lib.sitemaps.generate import subreddit_sitemaps, sitemap_index


HEADERS = {
    'Content-Type': 'text/xml',
    'Content-Encoding': 'gzip',
}


def zip_string(string):
    zipbuffer = StringIO()
    with gzip.GzipFile(mode='w', fileobj=zipbuffer) as f:
        f.write(string)
    return zipbuffer.getvalue()


def upload_sitemap(key, sitemap):
    key.set_contents_from_string(zip_string(sitemap), headers=HEADERS)


def store_subreddit_sitemap(bucket, index, sitemap):
    key = Key(bucket)
    key.key = 'subreddit_sitemap/{0}.xml'.format(index)
    g.log.debug("Uploading %r", key)

    upload_sitemap(key, sitemap)


def store_sitemap_index(bucket, count):
    key = Key(bucket)
    key.key = g.sitemap_subreddit_keyname
    g.log.debug("Uploading %r", key)

    upload_sitemap(key, sitemap_index(count))


def store_sitemaps_in_s3(subreddits):
    s3conn = S3Connection()
    bucket = s3conn.get_bucket(g.sitemap_upload_s3_bucket, validate=False)

    sitemap_count = 0
    for i, sitemap in enumerate(subreddit_sitemaps(subreddits)):
        store_subreddit_sitemap(bucket, i, sitemap)
        sitemap_count += 1

    store_sitemap_index(bucket, sitemap_count)
<EOF>
<BOF>
import datetime
import dateutil
import json
import pytz
import time

from boto.s3.connection import S3Connection
from boto.sqs.connection import SQSConnection
from pylons import app_globals as g

from r2.lib.s3_helpers import parse_s3_path
from r2.lib.sitemaps.store import store_sitemaps_in_s3
from r2.lib.sitemaps.data import find_all_subreddits

"""Watch for SQS messages informing us to read, generate, and store sitemaps.

There is only function that should be used outside this module

watcher()

It is designed to be used in a daemon process.
"""


def watcher():
    """Poll for new sitemap data and process it as necessary."""
    while True:
        _process_message()


def _subreddit_sitemap_key():
    conn = S3Connection()
    bucket = conn.get_bucket(g.sitemap_upload_s3_bucket, validate=False)
    return bucket.get_key(g.sitemap_subreddit_keyname)


def _datetime_from_timestamp(timestamp):
    return datetime.datetime.fromtimestamp(timestamp / 1000, pytz.utc)


def _before_last_sitemap(timestamp):
    sitemap_key = _subreddit_sitemap_key()
    if sitemap_key is None:
        return False

    sitemap_datetime = dateutil.parser.parse(sitemap_key.last_modified)
    compare_datetime = _datetime_from_timestamp(timestamp)
    return compare_datetime < sitemap_datetime


def _process_message():
    if not g.sitemap_sqs_queue:
        return

    sqs = SQSConnection()
    sqs_q = sqs.get_queue(g.sitemap_sqs_queue)

    messages = sqs.receive_message(sqs_q, number_messages=1)

    if not messages:
        return

    message, = messages

    js = json.loads(message.get_body())
    s3path = parse_s3_path(js['location'])

    # There are some error cases that allow us to get messages
    # for sitemap creation that are now out of date.
    timestamp = js.get('timestamp')
    if timestamp is not None and _before_last_sitemap(timestamp):
        sqs_q.delete_message(message)
        return

    g.log.info("Got import job %r", js)

    subreddits = find_all_subreddits(s3path)
    store_sitemaps_in_s3(subreddits)

    sqs_q.delete_message(message)


def _current_timestamp():
    return time.time() * 1000


def _create_test_message():
    """A dev only function that drops a new message on the sqs queue."""
    sqs = SQSConnection()
    sqs_q = sqs.get_queue(g.sitemap_sqs_queue)

    # it returns None on failure
    assert sqs_q, "failed to connect to queue"

    message = sqs_q.new_message(body=json.dumps({
        'job_name': 'daily-sr-sitemap-reporting',
        'location': ('s3://reddit-data-analysis/big-data/r2/prod/' +
                     'daily_sr_sitemap_reporting/dt=2016-06-14'),
        'timestamp': _current_timestamp(),
    }))
    sqs_q.write(message)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


"""Create exhaustive sitemaps for Reddit.

This module exists to make fairly exhaustive sitemaps as defined by the
sitemap protocol (http://www.sitemaps.org/protocol.html)

We currently support two types of sitemaps:

The sitemap index which takes the form of:

------------------------------------------------------------------------
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <sitemap>
    <loc>http://reddit.com/r/subreddit_sitemap?index=0</loc>
  </sitemap>
  <sitemap>
    <loc>http://reddit.com/r/subreddit_sitemap?index=1</loc>
  </sitemap>
  <sitemap>
    <loc>http://reddit.com/r/permalink_sitemap?index=0</loc>
  </sitemap>
  <sitemap>
    <loc>http://reddit.com/r/permalink_sitemap?index=1</loc>
  </sitemap>
</sitemapindex>
------------------------------------------------------------------------

Next are subreddit sitemaps which take the form of:

------------------------------------------------------------------------
 <?xml version="1.0" encoding="UTF-8"?>
 <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
   <url>
     <loc>http://reddit.com/r/{some_postfix}</loc>
   </url>
 </urlset>
------------------------------------------------------------------------


Each sitemap and sitemap index will have 50000 links or fewer.
"""

from lxml import etree
from pylons import app_globals as g

from r2.lib.template_helpers import add_sr
from r2.lib.utils import in_chunks

SITEMAP_NAMESPACE = "http://www.sitemaps.org/schemas/sitemap/0.9"
LINKS_PER_SITEMAP = 50000


def _absolute_url(path):
    return add_sr(path, force_https=True, sr_path=False)


def _stringify_xml(root_element):
    return etree.tostring(
        root_element,
        pretty_print=g.debug,
        xml_declaration=True,
        encoding='UTF-8'
    )


def _subreddit_links(subreddits):
    for subreddit in subreddits:
        path = '/r/{0}/'.format(subreddit)
        yield _absolute_url(path)


def _subreddit_sitemap(subreddits):
    urlset = etree.Element('urlset', xmlns=SITEMAP_NAMESPACE)
    for link in _subreddit_links(subreddits):
        url_elem = etree.SubElement(urlset, 'url')
        loc_elem = etree.SubElement(url_elem, 'loc')
        loc_elem.text = link
    return _stringify_xml(urlset)


def subreddit_sitemaps(subreddits):
    """Create an array of sitemaps.

    Each sitemap has up to 50000 links, being the maximum allowable number of
    links according to the sitemap standard.
    """
    for subreddit_chunks in in_chunks(subreddits, LINKS_PER_SITEMAP):
        yield _subreddit_sitemap(subreddit_chunks)


def sitemap_index(count):
    sm_elem = etree.Element('sitemapindex', xmlns=SITEMAP_NAMESPACE)
    for i in xrange(count):
        sitemap_elem = etree.SubElement(sm_elem, 'sitemap')
        loc_elem = etree.SubElement(sitemap_elem, 'loc')
        url = '{0}/subreddit_sitemap/{1}.xml'.format(
            g.sitemap_s3_static_host, i)
        loc_elem.text = url
    return _stringify_xml(sm_elem)
<EOF>
<BOF>
# -*- coding: utf-8 -*-
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import Counter, OrderedDict

from r2.config import feature
from r2.lib.contrib.ipaddress import ip_address
from r2.lib.db.operators import asc
from r2.lib.wrapped import Wrapped, Templated, CachedTemplate
from r2.models import (
    Account,
    All,
    AllMinus,
    AllSR,
    Comment,
    DefaultSR,
    DomainSR,
    FakeSubreddit,
    Filtered,
    Flair,
    FlairListBuilder,
    FlairTemplate,
    FlairTemplateBySubredditIndex,
    Friends,
    Frontpage,
    LINK_FLAIR,
    LabeledMulti,
    Link,
    ReadNextLink,
    ReadNextListing,
    Mod,
    ModSR,
    MultiReddit,
    NotFound,
    OLD_SITEWIDE_RULES,
    Printable,
    PromoCampaign,
    PromotionPrices,
    IDBuilder,
    Random,
    RandomNSFW,
    RandomSubscription,
    SITEWIDE_RULES,
    StylesheetsEverywhere,
    Subreddit,
    SubredditRules,
    Target,
    Trophy,
    USER_FLAIR,
    make_feedurl,
)
from r2.models.bidding import Bid
from r2.models.gold import (
    calculate_server_seconds,
    days_to_pennies,
    paypal_subscription_url,
    gold_payments_by_user,
    gold_received_by_user,
    get_current_value_of_month,
    gold_goal_on,
    gold_revenue_steady,
    gold_revenue_volatile,
    get_subscription_details,
    TIMEZONE as GOLD_TIMEZONE,
)
from r2.models.promo import (
    NO_TRANSACTION,
    PROMOTE_COST_BASIS,
    PROMOTE_PRIORITIES,
    PromotionLog,
    Collection,
)
from r2.models.token import OAuth2Client, OAuth2AccessToken
from r2.models import traffic
from r2.models import ModAction
from r2.models import Thing
from r2.models.wiki import WikiPage, ImagesByWikiPage
from r2.lib.db import tdb_cassandra, queries
from r2.config.extensions import is_api
from r2.lib.menus import CommentSortMenu

from pylons.i18n import _, ungettext
from pylons import request, config
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.controllers.util import abort

from r2.lib import hooks, inventory, media
from r2.lib import promote, tracking
from r2.lib.captcha import get_iden
from r2.lib.filters import (
    scriptsafe_dumps,
    spaceCompress,
    _force_unicode,
    _force_utf8,
    unsafe,
    websafe,
    SC_ON,
    SC_OFF,
    websafe_json,
    wikimarkdown,
)
from r2.lib.menus import NavButton, NamedButton, NavMenu, PageNameNav, JsButton
from r2.lib.menus import SubredditButton, SubredditMenu, ModeratorMailButton
from r2.lib.menus import OffsiteButton, menu, JsNavMenu
from r2.lib.normalized_hot import normalized_hot
from r2.lib.providers import image_resizing
from r2.lib.strings import (
    get_funny_translated_string,
    plurals,
    Score,
    strings,
)
from r2.lib.utils import is_subdomain, title_to_url, UrlParser
from r2.lib.utils import url_links_builder, median, to36
from r2.lib.utils import trunc_time, timesince, timeuntil, weighted_lottery
from r2.lib.template_helpers import (
    add_sr,
    comment_label,
    format_number,
    get_domain,
    make_url_https,
    make_url_protocol_relative,
    static,
)
from r2.lib.subreddit_search import popular_searches
from r2.lib.memoize import memoize
from r2.lib.utils import trunc_string as _truncate, to_date
from r2.lib.filters import safemarkdown
from r2.lib.utils import (
    Storage,
    feature_utils,
    precise_format_timedelta,
    tup,
    url_is_embeddable_image,
)
from r2.lib.cache import make_key_id, MemcachedError

from babel.numbers import format_currency
from babel.dates import format_date
from collections import defaultdict, namedtuple
import csv
import hmac
import hashlib
import cStringIO
import sys, random, datetime, calendar, simplejson, re, time
import time
from itertools import chain, product
from urllib import quote, urlencode
from urlparse import urlparse

from r2.lib.ip_events import ips_by_account_id

from things import wrap_links, wrap_things, default_thing_wrapper

datefmt = _force_utf8(_('%d %b %Y'))

MAX_DESCRIPTION_LENGTH = 150

def get_captcha():
    if not c.user_is_loggedin or c.user.needs_captcha():
        return get_iden()

def responsive(res, space_compress=None):
    """
    Use in places where the template is returned as the result of the
    controller so that it becomes compatible with the page cache.
    """
    if space_compress is None:
        space_compress = not g.template_debug

    if is_api():
        res = res or u''
        if not c.allowed_callback and request.environ.get("WANT_RAW_JSON"):
            res = scriptsafe_dumps(res)
        else:
            res = websafe_json(simplejson.dumps(res))

        if c.allowed_callback:
            # Add a comment to the beginning to prevent the "Rosetta Flash"
            # XSS when an attacker controls the beginning of a resource
            res = "/**/%s(%s)" % (websafe_json(c.allowed_callback), res)
    elif space_compress:
        res = spaceCompress(res)
    return res


class Robots(Templated):

    def __init__(self, **context):
        Templated.__init__(self, **context)
        self.subreddit_sitemap = g.sitemap_subreddit_static_url

class CrossDomain(Templated):
    pass


class Reddit(Templated):
    '''Base class for rendering a page on reddit.  Handles toolbar creation,
    content of the footers, and content of the corner buttons.

    Constructor arguments:

        space_compress -- run r2.lib.filters.spaceCompress on render
        loginbox -- enable/disable rendering of the small login box in the right margin
          (only if no user is logged in; login box will be disabled for a logged in user)
        show_sidebar -- enable/disable content in the right margin

        infotext -- text to display in a <p class="infotext"> above the content
        nav_menus -- list of Menu objects to be shown in the area below the header
        content -- renderable object to fill the main content well in the page.

    settings determined at class-declaration time

      create_reddit_box -- enable/disable display of the "Create a reddit" box
      submit_box        -- enable/disable display of the "Submit" box
      searchbox         -- enable/disable the "search" box in the header
      extension_handling -- enable/disable rendering using non-html templates
                            (e.g. js, xml for rss, etc.)
    '''

    create_reddit_box  = True
    submit_box         = True
    header             = True
    searchbox          = True
    extension_handling = True
    enable_login_cover = True
    site_tracking      = True
    show_infobar       = True
    content_id         = None
    css_class          = None
    extra_page_classes = None
    extra_stylesheets  = []

    def __init__(self, space_compress=None, nav_menus=None, loginbox=True,
                 infotext='', infotext_class=None, infotext_show_icon=False,
                 content=None, short_description='', title='',
                 robots=None, show_sidebar=True, show_chooser=False,
                 header=True, srbar=True, page_classes=None, short_title=None,
                 show_wiki_actions=False, extra_js_config=None,
                 show_locationbar=False, auction_announcement=False,
                 show_newsletterbar=False, canonical_link=None,
                 **context):
        Templated.__init__(self, **context)
        self.title = title
        self.short_title = short_title
        self.short_description = short_description
        self.robots = robots
        self.infotext = infotext
        self.extra_js_config = extra_js_config
        self.show_wiki_actions = show_wiki_actions
        self.loginbox = loginbox
        self.show_sidebar = show_sidebar
        self.space_compress = space_compress
        self.dnt_enabled = feature.is_enabled("do_not_track")
        self.header = header
        self.footer = RedditFooter()
        self.debug_footer = DebugFooter()
        self.supplied_page_classes = page_classes or []
        self.show_newsletterbar = show_newsletterbar

        self.auction_announcement = auction_announcement

        #put the sort menus at the top
        self.nav_menu = MenuArea(menus = nav_menus) if nav_menus else None

        #add the infobar
        self.welcomebar = None
        self.newsletterbar = None
        self.locationbar = None
        self.infobar = None
        self.mobilewebredirectbar = None
        self.show_timeout_modal = False

        if feature.is_enabled("new_expando_icons"):
            self.feature_new_expando_icons = True
        if feature.is_enabled("expando_nsfw_flow"):
            self.feature_expando_nsfw_flow = True

        # generate a canonical link for google
        canonical_url = UrlParser(canonical_link or request.url)
        canonical_url.canonicalize()
        self.canonical_link = canonical_url.unparse()
        if c.render_style != "html":
            u = UrlParser(request.fullpath)
            u.set_extension("")
            u.hostname = g.domain
            u.scheme = g.default_scheme
            if g.domain_prefix:
                u.hostname = "%s.%s" % (g.domain_prefix, u.hostname)
            self.canonical_link = u.unparse()

        if self.show_infobar:
            if not infotext:
                if g.heavy_load_mode:
                    # heavy load mode message overrides read only
                    infotext = strings.heavy_load_msg
                elif g.read_only_mode:
                    infotext = strings.read_only_msg
                elif g.live_config.get("announcement_message"):
                    infotext = g.live_config["announcement_message"]
            if c.user_is_loggedin and c.user.in_timeout:
                timeout_days_remaining = c.user.days_remaining_in_timeout

                if timeout_days_remaining:
                    days = ungettext('day', 'days', timeout_days_remaining)
                    days_str = '%(num)s %(days)s' % {
                        'num': timeout_days_remaining,
                        'days': days,
                    }
                    message = strings.in_temp_timeout_msg % {'days': days_str}
                else:
                    message = strings.in_perma_timeout_msg

                self.infobar = RedditInfoBar(
                    message=message,
                    extra_class='timeout-infobar',
                    show_icon=True,
                )
            elif infotext:
                self.infobar = RedditInfoBar(
                    message=infotext,
                    extra_class=infotext_class,
                    show_icon=infotext_show_icon,
                )
            elif isinstance(c.site, AllMinus) and not c.user.gold:
                self.infobar = RedditInfoBar(message=strings.all_minus_gold_only,
                                       extra_class="gold")

            if not c.user_is_loggedin:
                if getattr(self, "show_welcomebar", True):
                    self.welcomebar = WelcomeBar()
                if self.show_newsletterbar:
                    self.newsletterbar = NewsletterBar()

            if (c.render_style == "compact" and
                    getattr(self, "show_mobilewebredirectbar", True)):
                self.mobilewebredirectbar = MobileWebRedirectBar()

            show_locationbar &= not c.user.pref_hide_locationbar
            if (show_locationbar and c.used_localized_defaults and
                    (not c.user_is_loggedin or
                     not c.user.has_subscribed)):
                self.locationbar = LocationBar()

        self.srtopbar = None
        if srbar and not is_api():
            self.srtopbar = SubredditTopBar()

        panes = [content]

        if c.user_is_loggedin and not is_api() and not self.show_wiki_actions:
            # insert some form templates for js to use
            # TODO: move these to client side templates
            gold_link = GoldPayment("gift",
                                    "monthly",
                                    months=1,
                                    signed=False,
                                    recipient="",
                                    giftmessage=None,
                                    passthrough=None,
                                    thing=None,
                                    clone_template=True,
                                    thing_type="link",
                                   )
            gold_comment = GoldPayment("gift",
                                       "monthly",
                                       months=1,
                                       signed=False,
                                       recipient="",
                                       giftmessage=None,
                                       passthrough=None,
                                       thing=None,
                                       clone_template=True,
                                       thing_type="comment",
                                      )

            report_form_templates = ReportFormTemplates()

            panes.append(report_form_templates)

            if self.show_sidebar:
                panes.extend([gold_comment, gold_link])

            if c.user_is_sponsor:
                panes.append(FraudForm())

        if c.user_is_loggedin and c.user.in_timeout:
            self.show_timeout_modal = True
            self.timeout_days_remaining = c.user.days_remaining_in_timeout

        self.popup_panes = self.build_popup_panes()
        panes.append(self.popup_panes)

        self._content = PaneStack(panes)

        self.show_chooser = (
            show_chooser and
            c.render_style == "html" and
            c.user_is_loggedin and
            (
                isinstance(c.site, (DefaultSR, AllSR, ModSR, LabeledMulti)) or
                c.site.name == g.live_config["listing_chooser_explore_sr"]
            )
        )

        self.toolbars = self.build_toolbars()

        has_style_override = (c.user_is_loggedin and
                c.user.pref_default_theme_sr and
                feature.is_enabled('stylesheets_everywhere') and
                c.user.pref_enable_default_themes)
        # if there is no style or the style is disabled for this subreddit
        self.no_sr_styles = (isinstance(c.site, DefaultSR) or
            (not self.get_subreddit_stylesheet_url(c.site) and not c.site.header) or
            (c.user and not c.user.use_subreddit_style(c.site)))

        self.default_theme_sr = DefaultSR()
        # use override stylesheet if they have custom styles disabled or
        # this subreddit has no custom stylesheet (or is the front page)
        if self.no_sr_styles:
            self.subreddit_stylesheet_url = self.get_subreddit_stylesheet_url(
                self.default_theme_sr)
        else:
            self.subreddit_stylesheet_url = self.get_subreddit_stylesheet_url(c.site)

        if has_style_override and self.no_sr_styles:
            sr = Subreddit._by_name(c.user.pref_default_theme_sr)
            # make sure they can still view their override subreddit
            if sr.can_view(c.user) and sr.stylesheet_url:
                self.subreddit_stylesheet_url = self.get_subreddit_stylesheet_url(sr)
                if c.can_apply_styles and c.allow_styles and sr.header:
                    self.default_theme_sr = sr


    @staticmethod
    def get_subreddit_stylesheet_url(sr):
        if not g.css_killswitch and c.can_apply_styles and c.allow_styles:
            if c.secure:
                if sr.stylesheet_url:
                    return make_url_https(sr.stylesheet_url)
                elif sr.stylesheet_url_https:
                    return sr.stylesheet_url_https
            else:
                if sr.stylesheet_url:
                    return sr.stylesheet_url
                elif sr.stylesheet_url_http:
                    return sr.stylesheet_url_http

    def wiki_actions_menu(self, moderator=False):
        data_attrs = lambda event: (
            {'type': 'subreddit', 'event-action': 'pageview', 'event-detail': event})

        buttons = []

        buttons.append(NamedButton(
            "wikirecentrevisions",
            css_class="wikiaction-revisions",
            dest="/wiki/revisions",
            data=data_attrs('wikirevisions')))
        buttons.append(NamedButton(
            "wikipageslist",
            css_class="wikiaction-pages",
            dest="/wiki/pages",
            data=data_attrs('wikipages')))

        if moderator:
            buttons.append(NamedButton(
                'wikibanned',
                css_class='reddit-ban access-required',
                dest='/about/wikibanned',
                data=data_attrs('wikibanned')))
            buttons.append(NamedButton(
                'wikicontributors',
                css_class='reddit-contributors access-required',
                dest='/about/wikicontributors',
                data=data_attrs('wikicontributors')))

        return SideContentBox(_('wiki tools'),
                      [NavMenu(buttons,
                               type="flat_vert",
                               css_class="icon-menu",
                               separator="")],
                      _id="wikiactions",
                      collapsible=True)

    def sr_admin_menu(self):
        buttons = []
        is_single_subreddit = not isinstance(c.site, (ModSR, MultiReddit))
        is_admin = c.user_is_loggedin and c.user_is_admin
        is_moderator_with_perms = lambda *perms: (
            is_admin or c.site.is_moderator_with_perms(c.user, *perms))
        data_attrs = lambda event: (
            {'type': 'subreddit', 'event-action': 'pageview', 'event-detail': event})

        if is_single_subreddit and is_moderator_with_perms('config'):
            buttons.append(NavButton(
                menu.community_settings,
                css_class="reddit-edit access-required",
                dest="edit",
                data=data_attrs('editsubreddit')))
            buttons.append(NavButton(
                menu.edit_stylesheet,
                css_class="edit-stylesheet access-required",
                dest="stylesheet",
                data=data_attrs('stylesheet')))
            if feature.is_enabled("subreddit_rules", subreddit=c.site.name):
                buttons.append(NavButton(
                    menu.community_rules,
                    css_class="community-rules access-required",
                    dest="rules",
                    data=data_attrs('rules')))

        if is_moderator_with_perms('mail'):
            buttons.append(NamedButton(
                "modmail",
                dest="message/inbox",
                css_class="moderator-mail access-required",
                data=data_attrs('modmail')))

        if is_single_subreddit:
            if is_moderator_with_perms('access'):
                buttons.append(NamedButton(
                    "moderators",
                    css_class="reddit-moderators",
                    data=data_attrs('moderators')))

                if not c.site.hide_contributors:
                    buttons.append(NavButton(
                        menu.contributors,
                        "contributors",
                        css_class="reddit-contributors access-required",
                        data=data_attrs('contributors')))

            buttons.append(NamedButton(
                "traffic",
                css_class="reddit-traffic access-required",
                data=data_attrs('traffic')))

        if is_moderator_with_perms('posts'):
            buttons.append(NamedButton(
                "modqueue",
                css_class="reddit-modqueue access-required",
                data=data_attrs('modqueue')))
            buttons.append(NamedButton(
                "reports",
                css_class="reddit-reported access-required",
                data=data_attrs('reports')))
            buttons.append(NamedButton(
                "spam",
                css_class="reddit-spam access-required",
                data=data_attrs('spam')))
            buttons.append(NamedButton(
                "edited",
                css_class="reddit-edited access-required",
                data=data_attrs('edited')))

        if is_single_subreddit:
            if is_moderator_with_perms('access'):
                buttons.append(NamedButton(
                    "banned",
                    css_class="reddit-ban access-required",
                    data=data_attrs('banned')))
            if is_moderator_with_perms('access', 'mail'):
                buttons.append(NamedButton(
                    "muted",
                    css_class="reddit-mute access-required",
                    data=data_attrs('muted')))
            if is_moderator_with_perms('flair'):
                buttons.append(NamedButton(
                    "flair",
                    css_class="reddit-flair access-required",
                    data=data_attrs('flair')))

        # append AutoMod button if it's enabled and they have perms to change it
        if (g.automoderator_account and
                is_single_subreddit and
                is_moderator_with_perms('config')):
            # link to their config if they have one, or the docs if not
            try:
                WikiPage.get(c.site, "config/automoderator")
                buttons.append(NamedButton(
                    "automod",
                    dest="../wiki/edit/config/automoderator",
                    css_class="reddit-automod access-required",
                    data=data_attrs('automoderator')))
            except tdb_cassandra.NotFound:
                buttons.append(NamedButton(
                    "new_automod",
                    sr_path=False,
                    dest="../wiki/automoderator",
                    css_class="reddit-automod access-required",
                ))

        buttons.append(NamedButton(
            "log",
            css_class="reddit-moderationlog access-required",
            data=data_attrs('moderationlog')))
        if is_moderator_with_perms('posts'):
            buttons.append(NamedButton(
                    "unmoderated",
                    css_class="reddit-unmoderated access-required",
                    data=data_attrs('unmoderated')))

        return SideContentBox(_('moderation tools'),
                              [NavMenu(buttons,
                                       type="flat_vert",
                                       base_path="/about/",
                                       css_class="icon-menu",
                                       separator="")],
                              _id="moderation_tools",
                              collapsible=True)

    def rightbox(self):
        """generates content in <div class="rightbox">"""

        ps = PaneStack(css_class='spacer')

        if self.searchbox:
            ps.append(SearchForm())

        sidebar_message = g.live_config.get("sidebar_message")
        if sidebar_message and isinstance(c.site, DefaultSR):
            ps.append(SidebarMessage(sidebar_message[0]))

        gold_sidebar_message = g.live_config.get("gold_sidebar_message")
        if (c.user_is_loggedin and c.user.gold and
                gold_sidebar_message and isinstance(c.site, DefaultSR)):
            ps.append(SidebarMessage(gold_sidebar_message[0],
                                     extra_class="gold"))

        if not c.user_is_loggedin and self.loginbox and not g.read_only_mode:
            ps.append(LoginFormWide())

        if isinstance(c.site, DomainSR) and c.user_is_admin:
            from r2.lib.pages.admin_pages import AdminNotesSidebar
            notebar = AdminNotesSidebar('domain', c.site.domain)
            ps.append(notebar)

        if isinstance(c.site, Subreddit) and c.user_is_admin:
            from r2.lib.pages.admin_pages import AdminNotesSidebar
            notebar = AdminNotesSidebar('subreddit', c.site.name)
            ps.append(notebar)

        if not c.user.pref_hide_ads or not c.user.gold:
            ps.append(SponsorshipBox())

        if (isinstance(c.site, Filtered) and not
            (isinstance(c.site, AllSR) and not c.user.gold)):
            ps.append(FilteredInfoBar())
        elif isinstance(c.site, AllSR):
            ps.append(AllInfoBar(c.site, c.user))
        elif isinstance(c.site, ModSR):
            ps.append(ModSRInfoBar())

        if isinstance(c.site, (MultiReddit, ModSR)):
            srs = Subreddit._byID(c.site.sr_ids, data=True,
                                  return_dict=False, stale=True)

            if (srs and c.user_is_loggedin and
                    (c.user_is_admin or c.site.is_moderator(c.user))):
                ps.append(self.sr_admin_menu())

            if isinstance(c.site, LabeledMulti):
                ps.append(MultiInfoBar(c.site, srs, c.user))
                c.js_preload.set_wrapped(
                    '/api/multi/%s' % c.site.path.lstrip('/'), c.site)
            elif srs:
                if isinstance(c.site, ModSR):
                    box = SubscriptionBox(srs, multi_text=strings.mod_multi)
                else:
                    box = SubscriptionBox(srs)
                ps.append(SideContentBox(_('these subreddits'), [box]))

        user_banned = c.user_is_loggedin and c.site.is_banned(c.user)

        if (self.submit_box
                and (c.user_is_loggedin or not g.read_only_mode)
                and not user_banned):
            if (not isinstance(c.site, FakeSubreddit)
                    and c.site.type in ("archived",
                                        "restricted",
                                        "gold_restricted")
                    and not (c.user_is_loggedin
                             and c.site.can_submit(c.user))):
                if c.site.type == "archived":
                    subtitle = _('this subreddit is archived '
                                 'and no longer accepting submissions.')
                    ps.append(SideBox(title=_('Submissions disabled'),
                                      css_class="submit",
                                      disabled=True,
                                      subtitles=[subtitle],
                                      show_icon=False))
                else:
                    if c.site.type == 'restricted':
                        subtitle = _('Only approved users may post in this '
                                     'community.')
                    elif c.site.type == 'gold_restricted':
                        subtitle = _('Anyone can view or comment, but only '
                                     'Reddit Gold members can post in this '
                                     'community.')
                    ps.append(SideBox(title=_('Submissions restricted'),
                                      css_class="submit",
                                      disabled=True,
                                      subtitles=[subtitle],
                                      show_icon=False))
            else:
                fake_sub = isinstance(c.site, FakeSubreddit)
                is_multi = isinstance(c.site, MultiReddit)
                mod_link_override = mod_self_override = False

                if isinstance(c.site, FakeSubreddit):
                    submit_buttons = set(("link", "self"))
                else:
                    # we want to show submit buttons for logged-out users too
                    # so we can't just use can_submit_link/text
                    submit_buttons = c.site.allowed_types

                    if c.user_is_loggedin:
                        if ("link" not in submit_buttons and
                                c.site.can_submit_link(c.user)):
                            submit_buttons.add("link")
                            mod_link_override = True
                        if ("self" not in submit_buttons and
                                c.site.can_submit_text(c.user)):
                            submit_buttons.add("self")
                            mod_self_override = True

                if "link" in submit_buttons:
                    css_class = "submit submit-link"
                    if mod_link_override:
                        css_class += " mod-override"
                    data_attrs = {
                        'type': 'subreddit',
                        'event-action': 'submit',
                        'event-detail': 'link',
                    }
                    ps.append(SideBox(title=c.site.submit_link_label or
                                            strings.submit_link_label,
                                      css_class=css_class,
                                      link="/submit",
                                      sr_path=not fake_sub or is_multi,
                                      data_attrs=data_attrs,
                                      show_cover=True))
                if "self" in submit_buttons:
                    css_class = "submit submit-text"
                    if mod_self_override:
                        css_class += " mod-override"
                    data_attrs = {
                        'type': 'subreddit',
                        'event-action': 'submit',
                        'event-detail': 'self',
                    }
                    ps.append(SideBox(title=c.site.submit_text_label or
                                            strings.submit_text_label,
                                      css_class=css_class,
                                      link="/submit?selftext=true",
                                      sr_path=not fake_sub or is_multi,
                                      data_attrs=data_attrs,
                                      show_cover=True))

        no_ads_yet = True
        user_disabled_ads = c.user.gold and c.user.pref_hide_ads
        show_adbox = c.site.allow_ads and not (user_disabled_ads or g.disable_ads)

        # don't show the subreddit info bar on cnames unless the option is set
        if not isinstance(c.site, FakeSubreddit):
            ps.append(SubredditInfoBar())
            moderator = c.user_is_loggedin and (c.user_is_admin or
                                          c.site.is_moderator(c.user))
            wiki_moderator = c.user_is_loggedin and (
                c.user_is_admin
                or c.site.is_moderator_with_perms(c.user, 'wiki'))
            if self.show_wiki_actions:
                menu = self.wiki_actions_menu(moderator=wiki_moderator)
                ps.append(menu)
            if moderator:
                ps.append(self.sr_admin_menu())
            if show_adbox:
                ps.append(Ads())
            no_ads_yet = False
        elif self.show_wiki_actions:
            ps.append(self.wiki_actions_menu())

        if self.create_reddit_box and c.user_is_loggedin:
            if (c.user._age.days >= g.min_membership_create_community and
                    c.user.can_create_subreddit):
                subtitles = get_funny_translated_string("create_subreddit", 2)
                data_attrs = {'event-action': 'createsubreddit'}
                ps.append(SideBox(_('Create your own subreddit'),
                           '/subreddits/create', 'create',
                           subtitles=subtitles,
                           data_attrs=data_attrs,
                           show_cover = True))

        if c.default_sr:
            hook = hooks.get_hook('home.add_sidebox')
            extra_sidebox = hook.call_until_return()
            if extra_sidebox:
                ps.append(extra_sidebox)

        if not isinstance(c.site, FakeSubreddit):
            moderator_ids = c.site.moderator_ids()
            if moderator_ids:
                sidebar_list_length = 10
                allow_stale = (not c.user_is_loggedin or
                    c.user._id not in moderator_ids)
                moderators = Account._byID(
                    moderator_ids[:sidebar_list_length], data=True,
                    return_dict=False, stale=allow_stale)
                num_not_shown = len(moderator_ids) - sidebar_list_length

                if num_not_shown > 0:
                    more_text = _("...and %d more") % (num_not_shown)
                else:
                    more_text = _("about moderation team")

                is_admin_sr = '/r/%s' % c.site.name == g.admin_message_acct

                if is_admin_sr:
                    label = _('message the admins')
                else:
                    label = _('message the moderators')

                wrapped_moderators = [WrappedUser(mod) for mod in moderators
                    if not mod._deleted]
                helplink = HelpLink(
                    "/message/compose?to=%%2Fr%%2F%s" % c.site.name,
                    label,
                    access_required=not is_admin_sr,
                    data_attrs={
                        'type': 'subreddit',
                        'fullname': c.site._fullname,
                        'event-action': 'compose',
                    })

                mod_href = c.site.path + 'about/moderators'
                ps.append(SideContentBox(_('moderators'),
                                         wrapped_moderators,
                                         helplink = helplink,
                                         more_href = mod_href,
                                         more_text = more_text))

        if no_ads_yet and show_adbox:
            ps.append(Ads())
            if g.live_config["gold_revenue_goal"]:
                ps.append(Goldvertisement())

        if c.user.pref_clickgadget and c.recent_clicks:
            ps.append(SideContentBox(_("Recently viewed links"),
                                     [ClickGadget(c.recent_clicks)]))

        if c.user_is_loggedin:
            activity_link = AccountActivityBox()
            ps.append(activity_link)

        return ps

    def render(self, *a, **kw):
        """Overrides default Templated.render with two additions
           * support for rendering API requests with proper wrapping
           * support for space compression of the result
        In adition, unlike Templated.render, the result is in the form of a pylons
        Response object with it's content set.
        """
        if c.bare_content:
            res = self.content().render()
        else:
            res = Templated.render(self, *a, **kw)

        return responsive(res, self.space_compress)

    def corner_buttons(self):
        """set up for buttons in upper right corner of main page."""
        buttons = []
        if c.user_is_loggedin:
            if c.user.name in g.admins:
                if c.user_is_admin:
                    buttons += [OffsiteButton(
                        _("turn admin off"),
                        dest="%s/adminoff?dest=%s" %
                            (g.https_endpoint, quote(request.fullpath)),
                        target = "_self",
                    )]
                else:
                    buttons += [OffsiteButton(
                        _("turn admin on"),
                        dest="%s/adminon?dest=%s" %
                            (g.https_endpoint, quote(request.fullpath)),
                        target = "_self",
                    )]
            buttons += [NamedButton("prefs", False,
                                  css_class = "pref-lang")]
        else:
            lang = c.lang.split('-')[0] if c.lang else ''
            lang_name = g.lang_name.get(lang) or [lang, '']
            lang_name = "".join(lang_name)
            buttons += [JsButton(lang_name,
                                 onclick = "return showlang();",
                                 css_class = "pref-lang")]
        return NavMenu(buttons, base_path = "/", type = "flatlist")

    def build_toolbars(self):
        """Sets the layout of the navigation topbar on a Reddit.  The result
        is a list of menus which will be rendered in order and
        displayed at the top of the Reddit."""
        if c.site == Friends:
            main_buttons = [NamedButton('new', dest='', aliases=['/hot']),
                            NamedButton('comments'),
                            NamedButton('gilded'),
                            ]
        else:
            main_buttons = [NamedButton('hot', dest='', aliases=['/hot']),
                            NamedButton('new'),
                            NamedButton('rising'),
                            NamedButton('controversial'),
                            NamedButton('top'),
                            ]

            if c.site.allow_gilding:
                main_buttons.append(NamedButton('gilded',
                                                aliases=['/comments/gilded']))

            mod = False
            if c.user_is_loggedin:
                mod = bool(c.user_is_admin
                           or c.site.is_moderator_with_perms(c.user, 'wiki'))
            if c.site._should_wiki and (c.site.wikimode != 'disabled' or mod):
                if not g.disable_wiki:
                    main_buttons.append(NavButton('wiki', 'wiki'))

            if (isinstance(c.site, (Subreddit, DefaultSR, MultiReddit)) and
                    c.site.allow_ads):
                main_buttons.append(NavButton(menu.promoted, 'ads'))

        more_buttons = []

        if c.user_is_loggedin and c.site.allow_ads:
            if c.user_is_sponsor:
                sponsor_button = NavButton(
                    menu.sponsor, dest='/sponsor', sr_path=False)
                more_buttons.append(sponsor_button)
            elif c.user.pref_show_promote:
                more_buttons.append(NavButton(menu.promote, 'promoted', False))

        #if there's only one button in the dropdown, get rid of the dropdown
        if len(more_buttons) == 1:
            main_buttons.append(more_buttons[0])
            more_buttons = []

        toolbar = [NavMenu(main_buttons, type='tabmenu')]
        if more_buttons:
            toolbar.append(NavMenu(more_buttons, title=menu.more, type='tabdrop'))

        if not isinstance(c.site, DefaultSR):
            func = 'subreddit'
            if isinstance(c.site, DomainSR):
                func = 'domain'
            toolbar.insert(0, PageNameNav(func))

        return toolbar

    def __repr__(self):
        return "<Reddit>"

    @staticmethod
    def content_stack(panes, css_class = None):
        """Helper method for reordering the content stack."""
        return PaneStack(filter(None, panes), css_class = css_class)

    def content(self):
        """returns a Wrapped (or renderable) item for the main content div."""
        if self.newsletterbar:
            self.welcomebar = None

        return self.content_stack((
            self.welcomebar,
            self.newsletterbar,
            self.infobar,
            self.locationbar,
            self.mobilewebredirectbar,
            self.nav_menu,
            self._content,
        ))

    def build_popup_panes(self):
        panes = []

        panes.append(Popup('archived-popup', ArchivedInterstitial()))

        if self.show_timeout_modal:
            popup_content = InTimeoutInterstitial(
                timeout_days_remaining=self.timeout_days_remaining,
                hide_message=True,
            )
            panes.append(Popup('access-popup', popup_content))

        return HtmlPaneStack(panes)

    def is_gold_page(self):
        return "gold-page-ga-tracking" in self.supplied_page_classes

    def page_classes(self):
        classes = set()

        if c.user_is_loggedin:
            classes.add('loggedin')
            if not isinstance(c.site, FakeSubreddit):
                if c.site.is_subscriber(c.user):
                    classes.add('subscriber')
                if c.site.is_contributor(c.user):
                    classes.add('contributor')
            if c.site.is_moderator(c.user):
                classes.add('moderator')
            if c.user.gold:
                classes.add('gold')
            if c.user.pref_highlight_controversial:
                classes.add('show-controversial')

        if c.user_is_admin:
            if not isinstance(c.site, FakeSubreddit) and c.site._spam:
                classes.add("banned")

        if isinstance(c.site, MultiReddit):
            classes.add('multi-page')

        if self.show_chooser:
            classes.add('with-listing-chooser')
            if c.user.pref_collapse_left_bar:
                classes.add('listing-chooser-collapsed')

        if c.user_is_loggedin and c.user.pref_compress:
            classes.add('compressed-display')

        if getattr(c.site, 'type', None) == 'gold_only':
            classes.add('gold-only')

        if getattr(c.site, 'quarantine', False):
            classes.add('quarantine')

        if self.extra_page_classes:
            classes.update(self.extra_page_classes)
        if self.supplied_page_classes:
            classes.update(self.supplied_page_classes)

        return classes


class DebugFooter(Templated):
    def __init__(self):
        if request.via_cdn:
            cdn_geoinfo = g.cdn_provider.get_client_location(request.environ)
            if cdn_geoinfo:
                c.location_info = "country code: %s" % cdn_geoinfo
        Templated.__init__(self)


class AccountActivityBox(Templated):
    def __init__(self):
        super(AccountActivityBox, self).__init__()


class RedditFooter(CachedTemplate):
    def cachable_attrs(self):
        return [('path', request.path),
                ('buttons', [[(x.title, x.path) for x in y] for y in self.nav])]

    def __init__(self):
        self.nav = [
            NavMenu([
                    NamedButton("blog", False, dest="/blog"),
                    OffsiteButton("about", "https://about.reddit.com/"),
                    NamedButton("source_code", False, dest="/code"),
                    NamedButton("advertising", False),
                    NamedButton("jobs", False),
                ],
                title = _("about"),
                type = "flat_vert",
                separator = ""),

            NavMenu([
                    NamedButton("rules", False),
                    OffsiteButton(_("FAQ"), "https://reddit.zendesk.com"),
                    NamedButton("wiki", False),
                    NamedButton("reddiquette", False, dest="/wiki/reddiquette"),
                    NamedButton("transparency", False, dest="/wiki/transparency"),
                    NamedButton("contact", False),
                ],
                title = _("help"),
                type = "flat_vert",
                separator = ""),

            NavMenu([
                    OffsiteButton(_("Reddit for iPhone"),
                        "https://itunes.apple.com/us/app/reddit-the-official-app/id1064216828?mt=8"),
                    OffsiteButton(_("Reddit for Android"),
                        "https://play.google.com/store/apps/details?id=com.reddit.frontpage"),
                    OffsiteButton(_("mobile website"), "https://m.reddit.com"),
                    NamedButton("buttons", False),
                ],
                title = _("apps & tools"),
                type = "flat_vert",
                separator = ""),

            NavMenu([
                    NamedButton("gold", False, dest="/gold/about", css_class="buygold"),
                    OffsiteButton(_("redditgifts"), "//redditgifts.com"),
                ],
                title = _("<3"),
                type = "flat_vert",
                separator = "")
        ]
        CachedTemplate.__init__(self)

class ClickGadget(Templated):
    def __init__(self, links, *a, **kw):
        self.links = links
        self.content = ''
        if c.user_is_loggedin and self.links:
            self.content = self.make_content()
        Templated.__init__(self, *a, **kw)

    def make_content(self):
        #this will disable the hardcoded widget styles
        request.GET["style"] = "off"
        wrapper = default_thing_wrapper(embed_voting_style = 'votable',
                                        style = "htmllite")
        content = wrap_links(self.links, wrapper = wrapper)

        return content.render(style = "htmllite")


class LoginFormWide(CachedTemplate):
    """generates a login form suitable for the 300px rightbox."""
    pass



class SubredditInfoBar(CachedTemplate):
    """When not on Default, renders a sidebox which gives info about
    the current reddit, including links to the moderator and
    contributor pages, as well as links to the banning page if the
    current user is a moderator."""

    def __init__(self, site = None):
        site = site or c.site

        # hackity hack. do i need to add all the others props?
        self.sr = list(wrap_links(site))[0]
        self.description_usertext = UserText(self.sr, self.sr.description)

        # we want to cache on the number of subscribers
        self.subscribers = self.sr._ups

        # so the menus cache properly
        self.path = request.path

        self.active_visitors = self.sr.count_activity()

        if c.user_is_loggedin and c.user.pref_show_flair:
            self.flair_prefs = FlairPrefs()
        else:
            self.flair_prefs = None

        self.sr_style_toggle = False
        self.use_subreddit_style = True

        self.quarantine = self.sr.quarantine

        has_custom_stylesheet = (self.sr.stylesheet_url or
            self.sr.stylesheet_url_https or self.sr.stylesheet_url_http)
        if (c.user_is_loggedin and
                (has_custom_stylesheet or self.sr.header) and
                feature.is_enabled('stylesheets_everywhere')):
            # defaults to c.user.pref_show_stylesheets if a match doesn't exist
            self.sr_style_toggle = True
            self.use_subreddit_style = c.user.use_subreddit_style(c.site)

        if c.user_is_admin and hasattr(self.sr, 'ban_info'):
            self.sr_ban_info = self.sr.ban_info

        CachedTemplate.__init__(self)

    @property
    def creator_text(self):
        if self.sr.author:
            if self.sr.is_moderator(self.sr.author) or self.sr.author._deleted:
                return WrappedUser(self.sr.author).render()
            else:
                return self.sr.author.name
        return None


class SponsorshipBox(Templated):
    pass


class HelpLink(Templated):
    def __init__(self, url, label, access_required=False, data_attrs={}):
        Templated.__init__(self, url=url, label=label,
                           access_required=access_required,
                           data_attrs=data_attrs)


class SideContentBox(Templated):
    def __init__(self, title, content, helplink=None, _id=None, extra_class=None,
                 more_href=None, more_text="more", collapsible=False):
        Templated.__init__(self, title=title, helplink = helplink,
                           content=content, _id=_id, extra_class=extra_class,
                           more_href = more_href, more_text = more_text,
                           collapsible=collapsible)

class SideBox(CachedTemplate):
    """
    Generic sidebox used to generate the 'submit' and 'create a reddit' boxes.
    """
    def __init__(self, title, link=None, css_class='', subtitles = [],
                 show_cover = False, sr_path = False,
                 disabled=False, show_icon=True, target='_top', data_attrs={}):
        CachedTemplate.__init__(self, link = link, target = target,
                                title = title, css_class = css_class,
                                sr_path = sr_path, subtitles = subtitles,
                                show_cover = show_cover,
                                disabled=disabled, show_icon=show_icon,
                                data_attrs=data_attrs)


class PrefsPage(Reddit):
    """container for pages accessible via /prefs.  No extension handling."""

    extension_handling = False

    def __init__(self, show_sidebar = False, title=None, *a, **kw):
        title = title or "%s (%s)" % (_("preferences"), c.site.name.strip(' '))
        Reddit.__init__(self, show_sidebar = show_sidebar,
                        title=title,
                        *a, **kw)

    def build_toolbars(self):
        buttons = [NavButton(menu.options, ''),
                   NamedButton('apps')]

        if c.user.pref_private_feeds:
            buttons.append(NamedButton('feeds'))

        buttons.extend([
            NamedButton('friends'),
            NamedButton('blocked'),
            NamedButton('update'),
        ])

        if c.user.name in g.admins:
            buttons += [NamedButton('security')]

        buttons += [NamedButton('deactivate')]

        return [PageNameNav('nomenu', title = _("preferences")),
                NavMenu(buttons, base_path = "/prefs", type="tabmenu")]


class PrefOptions(Templated):
    """Preference form for updating language and display options"""
    def __init__(self, done=False, error_style_override=None, generic_error=None):
        themes = []
        use_other_theme = True
        if feature.is_enabled('stylesheets_everywhere'):
            for theme in StylesheetsEverywhere.get_all():
                if theme.is_enabled:
                    themes.append(theme)
                if theme.id == c.user.pref_default_theme_sr:
                    use_other_theme = False
                    theme.checked = True

        feature_autoexpand_media_previews = feature.is_enabled("autoexpand_media_previews")
        Templated.__init__(self, done=done,
                error_style_override=error_style_override,
                feature_autoexpand_media_previews=feature_autoexpand_media_previews,
                generic_error=generic_error, themes=themes, use_other_theme=use_other_theme)


class PrefFeeds(Templated):
    pass

class PrefSecurity(Templated):
    pass


re_promoted = re.compile(r"/promoted.*", re.I)

class PrefUpdate(Templated):
    """Preference form for updating email address and passwords"""
    def __init__(self, email=True, password=True, verify=False, dest=None, subscribe=False):
        is_promoted = dest and re.match(re_promoted, urlparse(dest).path) != None
        self.email = email
        self.password = password
        self.verify = verify
        self.dest = dest
        self.subscribe = subscribe or is_promoted
        Templated.__init__(self)

class PrefApps(Templated):
    """Preference form for managing authorized third-party applications."""

    def __init__(self, my_apps, developed_apps):
        self.my_apps = my_apps
        self.developed_apps = developed_apps
        super(PrefApps, self).__init__()

    def render_developed_app(self, app, collapsed):
        base_template = self.template()
        developed_app_fn = base_template.get_def('developed_app')
        res = developed_app_fn.render(app, collapsed=collapsed)
        return spaceCompress(res)

    def render_editable_developer(self, app, dev):
        base_template = self.template()
        editable_developer_fn = base_template.get_def('editable_developer')
        res = editable_developer_fn.render(app, dev)
        return spaceCompress(res)


class PrefDeactivate(Templated):
    """Preference form for deactivating a user's own account."""
    def __init__(self):
        self.has_paypal_subscription = c.user.has_paypal_subscription
        if self.has_paypal_subscription:
            self.paypal_subscr_id = c.user.gold_subscr_id
            self.paypal_url = paypal_subscription_url()
        Templated.__init__(self)


class MessagePage(Reddit):
    """Defines the content for /message/*"""
    def __init__(self, *a, **kw):
        if not kw.has_key('show_sidebar'):
            kw['show_sidebar'] = False

        source = kw.pop("source", None)

        Reddit.__init__(self, *a, **kw)

        if is_api():
            self.replybox = None
        else:
            self.replybox = UserText(
                item=None,
                creating=True,
                post_form='comment',
                display=False,
                cloneable=True,
                source=source,
            )

    def content(self):
        return self.content_stack((self.replybox,
                                   self.infobar,
                                   self.nav_menu,
                                   self._content))

    def build_toolbars(self):
        if isinstance(c.site, MultiReddit):
            mod_srs = c.site.srs_with_perms(c.user, "mail")
            sr_path = bool(mod_srs)
        elif (not isinstance(c.site, FakeSubreddit) and
                c.site.is_moderator_with_perms(c.user, "mail")):
            sr_path = True
        else:
            sr_path = False

        buttons =  [NamedButton('compose', sr_path=sr_path),
                    NamedButton('inbox', aliases = ["/message/comments",
                                                    "/message/unread",
                                                    "/message/messages",
                                                    "/message/mentions",
                                                    "/message/selfreply"],
                                sr_path = False),
                    NamedButton('sent', sr_path = False)]
        if c.user_is_loggedin and c.user.is_moderator_somewhere:
            buttons.append(ModeratorMailButton(menu.modmail, "moderator",
                                               sr_path = False))
        if not c.default_sr:
            buttons.append(ModeratorMailButton(
                _("%(site)s mail") % {'site': c.site.name}, "moderator",
                aliases = ["/about/message/inbox",
                           "/about/message/unread"]))
        return [PageNameNav('nomenu', title = _("message")),
                NavMenu(buttons, base_path = "/message", type="tabmenu")]

class MessageCompose(Templated):
    """Compose message form."""
    def __init__(self, to='', subject='', message='', captcha=None,
                 admin_check=True, restrict_recipient=False):
        from r2.models.admintools import admintools

        if admin_check:
            self.admins = admintools.admin_list()

        Templated.__init__(self, to=to, subject=subject, message=message,
                           captcha=captcha, admin_check=admin_check,
                           restrict_recipient=restrict_recipient)


class ModeratorMessageCompose(MessageCompose):
    def __init__(self, mod_srs, only_as_subreddit=False, **kw):
        self.mod_srs = sorted(mod_srs, key=lambda sr: sr.name.lower())
        self.only_as_subreddit = only_as_subreddit
        MessageCompose.__init__(self, admin_check=False, **kw)


class BoringPage(Reddit):
    """parent class For rendering all sorts of uninteresting,
    sortless, navless form-centric pages.  The top navmenu is
    populated only with the text provided with pagename and the page
    title is 'reddit.com: pagename'"""

    extension_handling= False

    def __init__(self, pagename, css_class=None, **context):
        self.pagename = pagename
        name = c.site.name or g.default_sr
        if css_class:
            self.css_class = css_class
        if "title" not in context:
            context['title'] = "%s: %s" % (name, pagename)

        Reddit.__init__(self, **context)

    def build_toolbars(self):
        if not isinstance(c.site, DefaultSR):
            return [PageNameNav('subreddit', title = self.pagename)]
        else:
            return [PageNameNav('nomenu', title = self.pagename)]

class HelpPage(BoringPage):
    def build_toolbars(self):
        return [PageNameNav('help', title = self.pagename)]

class FormPage(BoringPage):
    create_reddit_box  = False
    submit_box         = False
    """intended for rendering forms with no rightbox needed or wanted"""
    def __init__(self, pagename, show_sidebar = False, *a, **kw):
        BoringPage.__init__(self, pagename,  show_sidebar = show_sidebar,
                            *a, **kw)

class LoginPage(BoringPage):
    enable_login_cover = False
    short_title = "log in"

    """a boring page which provides the Login/register form"""
    def __init__(self, **context):
        self.dest = context.get('dest', '')
        context['loginbox'] = False
        context['show_sidebar'] = False
        context['page_classes'] = ['login-page']

        if c.render_style == "compact":
            title = self.short_title
        else:
            title = _("sign up or log in")

        BoringPage.__init__(self, title, **context)

        if self.dest:
            u = UrlParser(self.dest)
            # Display a preview message for OAuth2 client authorizations
            if u.path in ['/api/v1/authorize', '/api/v1/authorize.compact']:
                client_id = u.query_dict.get("client_id")
                self.client = client_id and OAuth2Client.get_token(client_id)
                if self.client:
                    self.infobar = ClientInfoBar(self.client,
                                                 strings.oauth_login_msg)
                else:
                    self.infobar = None

    def content(self):
        kw = {}
        for x in ('user_login', 'user_reg'):
            kw[x] = getattr(self, x) if hasattr(self, x) else ''
        login_content = self.login_template(dest = self.dest, **kw)
        return self.content_stack((self.infobar, login_content))

    @classmethod
    def login_template(cls, **kw):
        return Login(**kw)

class RegisterPage(LoginPage):
    short_title = "sign up"
    @classmethod
    def login_template(cls, **kw):
        return Register(**kw)


class Login(Templated):
    """The two-unit login and register form."""
    def __init__(self, user_reg = '', user_login = '', dest='', is_popup=False):
        Templated.__init__(self, user_reg = user_reg, user_login = user_login,
                           dest = dest, captcha = Captcha(),
                           is_popup=is_popup,
                           registration_info=RegistrationInfo())

class Register(Login):
    pass


class RegistrationInfo(Templated):
    def __init__(self):
        html = unsafe(self.get_registration_info_html())
        Templated.__init__(self, content_html=html)

    @classmethod
    @memoize('registration_info_html', stale=True, time=10*60)
    def get_registration_info_html(cls):
        try:
            wp = WikiPage.get(Frontpage, g.wiki_page_registration_info)
        except tdb_cassandra.NotFound:
            return ''
        else:
            return wikimarkdown(wp.content, include_toc=False, target='_blank')


class OAuth2AuthorizationPage(BoringPage):
    show_mobilewebredirectbar = False

    def __init__(self, client, redirect_uri, scope, state, duration,
                 response_type):
        if duration == "permanent":
            expiration = None
        else:
            expiration = (
                datetime.datetime.now(g.tz)
                + datetime.timedelta(seconds=OAuth2AccessToken._ttl + 1))
        content = OAuth2Authorization(client=client,
                                      redirect_uri=redirect_uri,
                                      scope=scope,
                                      state=state,
                                      duration=duration,
                                      expiration=expiration,
                                      response_type=response_type,
                                      )
        BoringPage.__init__(self, _("request for permission"),
                            show_sidebar=False, content=content,
                            short_title=_("permission"))

class OAuth2Authorization(Templated):
    pass

class SearchPage(BoringPage):
    """Search results page"""
    searchbox = False
    extra_page_classes = ['search-page']

    def __init__(self, pagename, prev_search,
                 search_params={},
                 simple=False, restrict_sr=False, site=None,
                 syntax=None, converted_data=None, facets={}, sort=None,
                 recent=None, subreddits=None,
                 *a, **kw):
        if not (feature.is_enabled('legacy_search') or c.user.pref_legacy_search):
            self.extra_page_classes = self.extra_page_classes + ['combined-search-page']
        self.searchbar = SearchBar(prev_search=prev_search,
                                   search_params=search_params,
                                   site=site,
                                   simple=simple, restrict_sr=restrict_sr,
                                   syntax=syntax, converted_data=converted_data)
        self.subreddits = subreddits

        # generate the over18 redirect url for the current search if needed
        if kw['nav_menus'] and not c.over18 and feature.is_enabled('safe_search'):
            u = UrlParser(add_sr('/search'))
            if prev_search:
                u.update_query(q=prev_search)
            if restrict_sr:
                u.update_query(restrict_sr='on')
            u.update_query(**search_params)
            u.update_query(over18='yes')
            over18_url = u.unparse()
            kw['nav_menus'].append(MenuLink(title=_('enable NSFW results'),
                                            url=over18_url))

        self.sr_facets = SubredditFacets(prev_search=prev_search, facets=facets,
                                         sort=sort, recent=recent)
        BoringPage.__init__(self, pagename, robots='noindex', *a, **kw)

    def content(self):
        if feature.is_enabled('legacy_search') or c.user.pref_legacy_search:
            return self.content_stack((self.searchbar, self.sr_facets, self.infobar,
                                   self.nav_menu, self.subreddits, self._content))

        return self.content_stack((self.searchbar, self.infobar,
                                   self.subreddits, self._content,
                                   self.sr_facets))


class MenuLink(Templated):
    pass


class TakedownPage(BoringPage):
    def __init__(self, link):
        BoringPage.__init__(self, getattr(link, "takedown_title", _("bummer")),
                            content = TakedownPane(link))

    def render(self, *a, **kw):
        response = BoringPage.render(self, *a, **kw)
        return response


class TakedownPane(Templated):
    def __init__(self, link, *a, **kw):
        self.link = link
        self.explanation = getattr(self.link, "explanation",
                                   _("this page is no longer available due to a copyright claim."))
        Templated.__init__(self, *a, **kw)


class CommentVisitsBox(Templated):
    def __init__(self, visits, *a, **kw):
        self.visits = list(reversed(visits))
        Templated.__init__(self, *a, **kw)

class LinkInfoPage(Reddit):
    """Renders the varied /info pages for a link.  The Link object is
    passed via the link argument and the content passed to this class
    will be rendered after a one-element listing consisting of that
    link object.

    In addition, the rendering is reordered so that any nav_menus
    passed to this class will also be rendered underneath the rendered
    Link.
    """

    create_reddit_box = False
    extra_page_classes = ['single-page']
    metadata_image_widths = (320, 216)

    def __init__(self, link = None, comment = None, disable_comments=False,
                 link_title = '', subtitle = None, num_duplicates = None,
                 show_promote_button=False, sr_detail=False,
                 campaign_fullname=promote.NO_CAMPAIGN, click_url=None,
                 *a, **kw):

        c.permalink_page = True
        expand_children = kw.get("expand_children", not bool(comment))

        wrapper = default_thing_wrapper(expand_children=expand_children)


        # link_listing will be the one-element listing at the top
        self.link_listing = wrap_links(link, wrapper=wrapper, sr_detail=sr_detail)
        things = self.link_listing.things
        self.link = things[0]

        # add click tracker
        self.link.campaign = campaign_fullname

        # don't need to track clicks on self posts since they've
        # already been clicked at this point
        if not self.link.is_self:
            promote.add_trackers(
                things,
                c.site,
                adserver_click_urls={campaign_fullname: click_url},
            )

        self.disable_comments = disable_comments

        if promote.is_promo(self.link) and not promote.is_promoted(self.link):
            self.link.votable = False

        link_title = ((self.link.title) if hasattr(self.link, 'title') else '')

        # defaults whether or not there is a comment
        params = {'title':_force_unicode(link_title), 'site' : c.site.name}
        title = strings.link_info_title % params
        short_description = None
        if link and link.selftext and not (link._spam or link._deleted):
            short_description = _truncate(link.selftext.strip(), MAX_DESCRIPTION_LENGTH)
        # only modify the title if the comment/author are neither deleted nor spam
        if comment and not comment._deleted and not comment._spam:
            author = Account._byID(comment.author_id, data=True)

            if not author._deleted and not author._spam:
                params = {'author' : author.name, 'title' : _force_unicode(link_title)}
                title = strings.permalink_title % params
                short_description = _truncate(comment.body.strip(), MAX_DESCRIPTION_LENGTH) if comment.body else None

        self.subtitle = subtitle

        if hasattr(self.link, "shortlink"):
            self.shortlink = self.link.shortlink

        self.og_data = self._build_og_data(
            _force_unicode(link_title),
            short_description,
        )

        self.twitter_card = self._build_twitter_card_data(
            _force_unicode(link_title),
            short_description,
        )
        hook = hooks.get_hook('comments_page.twitter_card')
        hook.call(tags=self.twitter_card, sr_name=c.site.name,
                  id36=self.link._id36)

        if hasattr(self.link, "dart_keyword"):
            c.custom_dart_keyword = self.link.dart_keyword

        # if we're already looking at the 'duplicates' page, we can
        # avoid doing this lookup twice
        if num_duplicates is None:
            builder = url_links_builder(self.link.url,
                                        exclude=self.link._fullname,
                                        public_srs_only=True)
            self.num_duplicates = len(builder.get_items()[0])
        else:
            self.num_duplicates = num_duplicates

        self.show_promote_button = show_promote_button
        if link._deleted or link._spam:
            robots = "noindex,nofollow"
        elif comment:
            # We don't want crawlers to index the comment permalink pages.
            robots = "noindex"
        else:
            robots = None

        if 'extra_js_config' not in kw:
            kw['extra_js_config'] = {}

        kw['extra_js_config'].update({
            "cur_link": link._fullname,
        });

        if c.can_embed:
            from r2.lib import embeds
            kw['extra_js_config'].update({
                "embed_inject_template": websafe(embeds.get_inject_template()),
            })

        Reddit.__init__(self, title = title, short_description=short_description, robots=robots, *a, **kw)

    def _build_og_data(self, link_title, meta_description):
        sr_fragment = "/r/" + c.site.name if not c.default_sr else get_domain()
        data = {
            "site_name": "reddit",
            "title": u"%s • %s" % (link_title, sr_fragment),
            "description": self._build_og_description(meta_description),
            "ttl": "600",  # re-fetch frequently to update vote/comment count
        }
        if not self.link.nsfw:
            image_data = self._build_og_image()
            for key, value in image_data.iteritems():
                # Although the spec[0] and their docs[1] say 'og:image' and
                # 'og:image:url' are equivalent, Facebook doesn't actually take
                # the thumbnail from the latter form.  Even if that gets fixed,
                # it's likely the authors of other scrapers haven't read the
                # spec in-depth, either, so we'll just keep on doing the more
                # well-supported thing.
                #
                # [0]: http://ogp.me/#structured
                # [1]: https://developers.facebook.com/docs/sharing/webmasters#images
                if key == 'url':
                    data['image'] = value
                else:
                    data["image:%s" % key] = value

        return data

    def _build_og_image(self):
        if self.link.media_object:
            media_embed = media.get_media_embed(self.link.media_object)
            if media_embed and media_embed.public_thumbnail_url:
                return {
                    'url': media_embed.public_thumbnail_url,
                    'width': media_embed.width,
                    'height': media_embed.height,
                }

        if self.link.url and url_is_embeddable_image(self.link.url):
            return {'url': self.link.url}

        preview_object = self.link.preview_image
        if preview_object:
            for width in self.metadata_image_widths:
                try:
                    return {
                        'url': g.image_resizing_provider.resize_image(
                                    preview_object, width),
                        'width': width,
                    }
                except image_resizing.NotLargeEnough:
                    pass

        if self.link.has_thumbnail and self.link.thumbnail:
            # This is really not a great thumbnail for facebook right now
            # because it's so small, but it's better than nothing.
            data = {'url': self.link.thumbnail}

            # Some old posts don't have a recorded size for whatever reason, so
            # let's just ignore dimensions for them.
            if hasattr(self.link, 'thumbnail_size'):
                width, height = self.link.thumbnail_size
                data['width'] = width
                data['height'] = height
            return data

        # Default to the reddit icon if we've got nothing else.  Force it to be
        # absolute because not all scrapers handle relative protocols or paths
        # well.
        return {'url': static('icon.png', absolute=True)}

    def _build_og_description(self, meta_description):
        if self.link.selftext:
            return meta_description

        return strings.link_info_og_description % {
            "score": self.link.score,
            "num_comments": self.link.num_comments,
        }

    def _build_twitter_card_data(self, link_title, meta_description):
        """Build a set of data for Twitter's Summary Cards:
        https://dev.twitter.com/cards/types/summary
        https://dev.twitter.com/cards/markup
        """

        # Twitter limits us to 70 characters for the title.  Even though it's
        # at the end, we'd like to always show the whole subreddit name, so
        # let's truncate the title while still ensuring the entire thing is
        # under the limit.
        sr_fragment = u" • /r/" + c.site.name if not c.default_sr else get_domain()
        max_link_title_length = 70 - len(sr_fragment)

        return {
            "site": "reddit", # The twitter account of the site.
            "card": "summary",
            "title": _truncate(link_title, max_link_title_length) + sr_fragment
            # Twitter will fall back to any defined OpenGraph attributes, so we
            # don't need to define 'twitter:image' or 'twitter:description'.
        }

    def build_toolbars(self):
        base_path = "/%s/%s/" % (self.link._id36, title_to_url(self.link.title))
        base_path = _force_utf8(base_path)


        def info_button(name, **fmt_args):
            return NamedButton(name, dest = '/%s%s' % (name, base_path),
                               aliases = ['/%s/%s' % (name, self.link._id36)],
                               fmt_args = fmt_args)
        buttons = []
        if not self.disable_comments:
            buttons.append(info_button('comments'))

            if self.num_duplicates > 0:
                buttons.append(info_button('duplicates', num=self.num_duplicates))

        if self.show_promote_button:
            buttons.append(NavButton(menu.promote, 'promoted', sr_path=False))

        toolbar = [NavMenu(buttons, base_path = "", type="tabmenu")]

        if not isinstance(c.site, DefaultSR):
            toolbar.insert(0, PageNameNav('subreddit'))

        if c.user_is_admin:
            from admin_pages import AdminLinkMenu
            toolbar.append(AdminLinkMenu(self.link))

        return toolbar

    def content(self):
        if self.disable_comments:
            comment_area = InfoBar(message=_("comments disabled"))
        else:
            panes = [self.nav_menu, self._content]

            comment_area = PaneStack([
                PaneStack(
                    panes
                )],
                title=self.subtitle,
                title_buttons=getattr(self, "subtitle_buttons", []),
                css_class="commentarea",
            )

        return self.content_stack((
            self.infobar,
            self.link_listing,
            comment_area,
            self.popup_panes,
        ))

    def build_popup_panes(self):
        panes = super(LinkInfoPage, self).build_popup_panes()

        if self.link.locked:
            panes.append(Popup('locked-popup', LockedInterstitial()))

        return panes


    def rightbox(self):
        rb = Reddit.rightbox(self)

        if (c.site and not c.default_sr and c.render_style == 'html' and
                feature.is_enabled('read_next')):
            link = self.link

            def wrapper_fn(thing):
                w = Wrapped(thing)
                w.render_class = ReadNextLink
                return w

            query_obj = c.site.get_links('hot', 'all')
            builder = IDBuilder(query_obj,
                                wrap=wrapper_fn,
                                skip=True, num=10)
            listing = ReadNextListing(builder).listing()
            if len(listing.things):
                rb.append(ReadNext(c.site, listing.render()))

        if not (self.link.promoted and not c.user_is_sponsor):
            if c.user_is_admin:
                from admin_pages import AdminLinkInfoBar
                rb.insert(1, AdminLinkInfoBar(a=self.link))
            else:
                rb.insert(1, LinkInfoBar(a=self.link))
        return rb

    def page_classes(self):
        classes = Reddit.page_classes(self)

        if self.link.flair_css_class:
            for css_class in self.link.flair_css_class.split():
                classes.add('post-linkflair-' + css_class)

        if c.user_is_loggedin and self.link.author == c.user:
            classes.add("post-submitter")

        time_ago = datetime.datetime.now(g.tz) - self.link._date
        delta = datetime.timedelta
        steps = [
            delta(minutes=10),
            delta(hours=6),
            delta(hours=24),
        ]
        for step in steps:
            if time_ago < step:
                if step < delta(hours=1):
                    step_str = "%dm" % (step.total_seconds() / 60)
                else:
                    step_str = "%dh" % (step.total_seconds() / (60 * 60))
                classes.add("post-under-%s-old" % step_str)

        return classes

class LinkCommentSep(Templated):
    pass

class CommentPane(Templated):
    def cache_key(self):
        num = self.article.num_comments
        # bit of triage: we don't care about 10% changes in comment
        # trees once they get to a certain length.  The cache is only a few
        # min long anyway.
        if num > 1000:
            num = (num / 100) * 100
        elif num > 100:
            num = (num / 10) * 10

        cache_key_args = [
            self.article._fullname,
            self.article.contest_mode,
            self.article.locked,
            num,
            self.sort,
            self.num,
            c.lang,
            self.can_reply,
            c.render_style,
            c.domain_prefix,
            c.secure,
            c.user.pref_show_flair,
            c.can_embed,
            self.max_depth,
            self.edits_visible,
        ]

        if feature_utils.is_tracking_link_enabled(self.article):
            cache_key_args.append("utm_comment_links")

        _id = make_key_id(*cache_key_args)
        key = "pane:%s" % _id
        return key

    def __init__(self, article, sort, comment, context, num, **kw):
        from r2.models import Builder, CommentBuilder, NestedListing
        from r2.controllers.reddit_base import UnloggedUser

        self.sort = sort
        self.num = num
        self.article = article

        self.max_depth = kw.get('max_depth')
        self.edits_visible = kw.get("edits_visible")

        is_html = c.render_style == "html"

        if is_html:
            timer = g.stats.get_timer("service_time.CommentPaneCache")
        else:
            timer = g.stats.get_timer(
                "service_time.CommentPaneCache.%s" % c.render_style)
        timer.start()

        try_cache = (
            not comment and
            not context and
            is_html and
            not c.user_is_admin and
            not (c.user_is_loggedin and c.user._id == article.author_id)
        )

        if c.user_is_loggedin:
            sr = article.subreddit_slow
            try_cache &= not bool(sr.can_ban(c.user))

            user_threshold = c.user.pref_min_comment_score
            default_threshold = Account._defaults["pref_min_comment_score"]
            try_cache &= user_threshold == default_threshold

        if c.user_is_loggedin:
            self.can_reply = article.can_comment_slow(c.user)
        else:
            # assume that the common case is for loggedin users to see reply
            # buttons and do the same for loggedout users so they can use the
            # same cached page. reply buttons will be hidden client side for
            # loggedout users
            self.can_reply = not article.archived_slow and not article.locked

        builder = CommentBuilder(
            article, sort, comment=comment, context=context, num=num, **kw)

        if try_cache and c.user_is_loggedin:
            builder._get_comments()
            timer.intermediate("build_comments")
            for comment in builder.comments:
                if comment.author_id == c.user._id:
                    try_cache = False
                    break

        if not try_cache:
            listing = NestedListing(builder, parent_name=article._fullname)
            listing_for_user = listing.listing()
            timer.intermediate("build_listing")
            self.rendered = listing_for_user.render()
            timer.intermediate("render_listing")
        else:
            g.log.debug("using comment page cache")
            key = self.cache_key()
            self.rendered = g.commentpanecache.get(key)

            if self.rendered:
                cache_hit = True

                if c.user_is_loggedin:
                    # don't need the builder to make a listing so stop its timer
                    builder.timer.stop("waiting")

            else:
                cache_hit = False

                # spoof an unlogged in user
                user = c.user
                logged_in = c.user_is_loggedin
                try:
                    c.user = UnloggedUser([c.lang])
                    # Preserve the viewing user's flair preferences.
                    c.user.pref_show_flair = user.pref_show_flair

                    c.user_is_loggedin = False

                    # make the comment listing. if the user is loggedin we
                    # already made the builder retrieve/build the comment tree
                    # and lookup the comments.
                    listing = NestedListing(
                        builder, parent_name=article._fullname)
                    generic_listing = listing.listing()

                    if logged_in:
                        timer.intermediate("build_listing")
                    else:
                        timer.intermediate("build_comments_and_listing")

                    self.rendered = generic_listing.render()
                    timer.intermediate("render_listing")
                finally:
                    # undo the spoofing
                    c.user = user
                    c.user_is_loggedin = logged_in

                try:
                    g.commentpanecache.set(
                        key,
                        self.rendered,
                        time=g.commentpane_cache_time
                    )
                except MemcachedError as e:
                    g.log.warning("Ignored exception (%r) on commentpane "
                                  "write for %r", e, request.path)

            # figure out what needs to be updated on the listing
            if c.user_is_loggedin:
                updates = []

                # wrap the comments so the builder will customize them for
                # the loggedin user
                wrapped_for_user = Builder.wrap_items(builder, builder.comments)
                timer.intermediate("wrap_comments_for_user")

                for t in wrapped_for_user:
                    if not hasattr(t, "likes"):
                        # this is for MoreComments and MoreRecursion
                        continue

                    is_friend = (getattr(t, "friend", False) and
                                 not t.author._deleted)
                    is_enemy = getattr(t, "enemy", False)

                    update = {}
                    if is_friend:
                        update['friend'] = True
                    if is_enemy:
                        update['enemy'] = True
                    if t.likes:
                        update['voted'] = 1
                    if t.likes is False:
                        update['voted'] = -1
                    if t.saved:
                        update['saved'] = True
                    if t.user_gilded:
                        update['gilded'] = (t.gilded_message, t.gildings)

                    if update:
                        update['id'] = t._fullname
                        updates.append(update)

                self.rendered += ThingUpdater(updates=updates).render()
                timer.intermediate("thingupdater")

        if try_cache:
            if cache_hit:
                timer.stop("hit")
            else:
                timer.stop("miss")
        else:
            timer.stop("uncached")

    def listing_iter(self, l):
        for t in l:
            yield t
            for x in self.listing_iter(getattr(t, "child", [])):
                yield x

    def render(self, *a, **kw):
        return self.rendered

class ThingUpdater(Templated):
    pass


class LinkInfoBar(Templated):
    """Right box for providing info about a link."""
    def __init__(self, a = None):
        if a:
            a = Wrapped(a)
        Templated.__init__(self, a = a, datefmt = datefmt)

class EditReddit(Reddit):
    """Container for the about page for a reddit"""
    extension_handling= False

    def __init__(self, *a, **kw):
        from r2.lib.menus import menu

        try:
            key = kw.pop("location")
            title = menu[key]
        except KeyError:
            is_moderator = c.user_is_loggedin and \
                c.site.is_moderator(c.user) or c.user_is_admin

            title = (_('subreddit settings') if is_moderator else
                     _('about %(site)s') % dict(site=c.site.name))

        Reddit.__init__(self, title=title, *a, **kw)

    def build_toolbars(self):
        return [PageNameNav('subreddit', title=self.title)]


class SubredditsPage(Reddit):
    """container for rendering a list of reddits.  The corner
    searchbox is hidden and its functionality subsumed by an in page
    SearchBar for searching over reddits.  As a result this class
    takes the same arguments as SearchBar, which it uses to construct
    self.searchbar"""
    searchbox    = False
    submit_box   = False
    def __init__(self, prev_search = '',
                 title = '', loginbox = True, infotext = None, show_interestbar=False,
                 search_params = {}, *a, **kw):
        Reddit.__init__(self, title = title, loginbox = loginbox, infotext = infotext,
                        *a, **kw)
        self.searchbar = SearchBar(
            prev_search = prev_search,
            header=_('search subreddits by name'),
            search_params={},
            simple=True,
            subreddit_search=True,
            search_path="/subreddits/search",
        )
        self.sr_infobar = InfoBar(message = strings.sr_subscribe)
        self.interestbar = InterestBar(True) if show_interestbar else None

    def build_toolbars(self):
        buttons =  [NavButton(menu.popular, ""),
                    NamedButton("new")]
        if c.user_is_admin:
            buttons.append(NamedButton("banned"))
        if c.user.employee:
            buttons.append(NamedButton("employee"))
        if c.user.gold or c.user.gold_charter:
            buttons.append(NamedButton("gold"))
        if c.user_is_admin:
            buttons.append(NamedButton("quarantine"))
        if c.user_is_admin:
            buttons.append(NamedButton("featured"))
        if c.user_is_loggedin:
            #add the aliases to "my reddits" stays highlighted
            buttons.append(NamedButton("mine",
                                       aliases=['/subreddits/mine/subscriber',
                                                '/subreddits/mine/contributor',
                                                '/subreddits/mine/moderator']))

        return [PageNameNav('subreddits'),
                NavMenu(buttons, base_path = '/subreddits', type="tabmenu")]

    def content(self):
        return self.content_stack((self.interestbar, self.searchbar,
                                   self.nav_menu, self.sr_infobar,
                                   self._content))

    def rightbox(self):
        ps = Reddit.rightbox(self)
        srs = Subreddit.user_subreddits(c.user, ids=False, limit=None)
        srs.sort(key=lambda sr: sr.name.lower())
        subscribe_box = SubscriptionBox(srs,
                                        multi_text=strings.subscribed_multi)
        num_reddits = len(subscribe_box.srs)
        ps.append(SideContentBox(_("your front page subreddits (%s)") %
                                 num_reddits, [subscribe_box]))
        return ps

class MySubredditsPage(SubredditsPage):
    """Same functionality as SubredditsPage, without the search box."""

    def content(self):
        return self.content_stack((self.nav_menu, self.infobar, self._content))


def votes_visible(user):
    """Determines whether to show/hide a user's votes.  They are visible:
     * if the current user is the user in question
     * if the user has a preference showing votes
     * if the current user is an administrator
    """
    return ((c.user_is_loggedin and c.user.name == user.name) or
            user.pref_public_votes or
            c.user_is_admin)


class ProfilePage(Reddit):
    """Container for a user's profile page.  As such, the Account
    object of the user must be passed in as the first argument, along
    with the current sub-page (to determine the title to be rendered
    on the page)"""

    searchbox         = False
    create_reddit_box = False
    submit_box        = False
    extra_page_classes = ['profile-page']

    def __init__(self, user, *a, **kw):
        self.user     = user
        Reddit.__init__(self, *a, **kw)

    def build_toolbars(self):
        path = "/user/%s/" % self.user.name
        main_buttons = [NavButton(menu.overview, '/', aliases = ['/overview']),
                   NamedButton('comments'),
                   NamedButton('submitted'),
                   NamedButton('gilded')]

        if votes_visible(self.user):
            main_buttons += [
                NamedButton('upvoted'),
                NamedButton('downvoted'),
            ]

        if c.user_is_loggedin and (c.user._id == self.user._id or
                                   c.user_is_admin):
            main_buttons += [NamedButton('hidden'), NamedButton('saved')]

        if c.user_is_sponsor:
            main_buttons += [NamedButton('promoted')]

        toolbar = [PageNameNav('nomenu', title = self.user.name),
                   NavMenu(main_buttons, base_path = path, type="tabmenu")]

        if c.user_is_admin:
            from admin_pages import AdminProfileMenu
            toolbar.append(AdminProfileMenu(path))

        return toolbar

    def page_classes(self):
        classes = Reddit.page_classes(self)

        if c.user_is_admin:
            if self.user.in_timeout:
                if self.user.timeout_expiration:
                    classes.add("user-in-timeout-temp")
                else:
                    classes.add("user-in-timeout-perma")
            if self.user._spam:
                classes.add("user-spam")
            if self.user._banned:
                classes.add("user-banned")
            if self.user._deleted:
                classes.add("user-deleted")

        return classes

    def rightbox(self):
        rb = Reddit.rightbox(self)

        tc = TrophyCase(self.user)
        helplink = HelpLink("/wiki/awards", _("what's this?"))
        scb = SideContentBox(title=_("trophy case"),
                 helplink=helplink, content=[tc],
                 extra_class="trophy-area")

        rb.push(scb)

        multis = LabeledMulti.by_owner(self.user, load_subreddits=False)

        public_multis = [m for m in multis if m.is_public()]
        if public_multis:
            scb = SideContentBox(title=_("public multireddits"), content=[
                SidebarMultiList(public_multis)
            ])
            rb.push(scb)

        hidden_multis = [m for m in multis if m.is_hidden()]
        if c.user == self.user and hidden_multis:
            scb = SideContentBox(title=_("hidden multireddits"), content=[
                SidebarMultiList(hidden_multis)
            ])
            rb.push(scb)

        if c.user_is_admin:
            from r2.lib.pages.admin_pages import AdminNotesSidebar
            from admin_pages import AdminSidebar

            rb.push(AdminSidebar(self.user))
            rb.push(AdminNotesSidebar('user', self.user.name))
        elif c.user_is_sponsor:
            from admin_pages import SponsorSidebar
            rb.push(SponsorSidebar(self.user))

        mod_sr_ids = Subreddit.reverse_moderator_ids(self.user)
        all_mod_srs = Subreddit._byID(mod_sr_ids, data=True,
                                      return_dict=False, stale=True)
        mod_srs = [sr for sr in all_mod_srs if sr.can_view_in_modlist(c.user)]
        if mod_srs:
            rb.push(SideContentBox(title=_("moderator of"),
                                   content=[SidebarModList(mod_srs)]))

        if (c.user == self.user or c.user.employee or
            self.user.pref_public_server_seconds):
            seconds_bar = ServerSecondsBar(self.user)
            if seconds_bar.message or seconds_bar.gift_message:
                rb.push(seconds_bar)

        rb.push(ProfileBar(self.user))

        return rb

class TrophyCase(Templated):
    def __init__(self, user):
        self.user = user
        self.trophies = []
        self.invisible_trophies = []

        for trophy in Trophy.by_account(user):
            if trophy._thing2.awardtype == 'invisible':
                self.invisible_trophies.append(trophy)
            else:
                self.trophies.append(trophy)

        Templated.__init__(self)


class SidebarMultiList(Templated):
    def __init__(self, multis):
        Templated.__init__(self)
        multis.sort(key=lambda multi: multi.name.lower())
        self.multis = multis


class SidebarModList(Templated):
    def __init__(self, subreddits):
        Templated.__init__(self)
        # primary sort is desc. subscribers, secondary is name
        self.subreddits = sorted(subreddits,
                                 key=lambda sr: (-sr._ups, sr.name.lower()))


class ProfileBar(Templated):
    """Draws a right box for info about the user (karma, etc)"""
    def __init__(self, user):
        Templated.__init__(self, user=user)
        if c.user_is_loggedin:
            self.viewing_self = user._id == c.user._id
            self.show_private_info = self.viewing_self or c.user_is_admin
        else:
            self.viewing_self = False
            self.show_private_info = False

        self.show_users_gold_expiration = (self.show_private_info or
            user.pref_show_gold_expiration) and user.gold
        self.show_private_gold_info = (self.show_private_info and
            (user.gold or user.gold_creddits > 0 or user.num_gildings > 0))

        if self.show_users_gold_expiration:
            gold_days_left = (user.gold_expiration -
                              datetime.datetime.now(g.tz)).days

            if gold_days_left < 1:
                self.gold_remaining = _("less than a day")
            else:
                # Round remaining gold to number of days
                precision = 60 * 60 * 24
                self.gold_remaining = timeuntil(user.gold_expiration,
                                                precision)

        if c.user_is_loggedin:
            if user.gold and self.show_private_info:
                if user.has_paypal_subscription:
                    self.paypal_subscr_id = user.gold_subscr_id
                    self.paypal_url = paypal_subscription_url()
                if user.has_stripe_subscription:
                    self.stripe_customer_id = user.gold_subscr_id

            if user.gold_creddits > 0 and self.show_private_info:
                msg = ungettext("%(creddits)s gold creddit to give",
                                "%(creddits)s gold creddits to give",
                                user.gold_creddits)
                msg = msg % dict(creddits=user.gold_creddits)
                self.gold_creddit_message = msg

            if user.num_gildings > 0 and self.show_private_info:
                gildings_msg = ungettext(
                    "%(gildings)s gilding given out",
                    "%(gildings)s gildings given out",
                    user.num_gildings)
                gildings_msg = gildings_msg % dict(gildings=user.num_gildings)
                self.num_gildings_message = gildings_msg

            if not self.viewing_self:
                self.goldlink = "/gold?goldtype=gift&recipient=" + user.name
                self.giftmsg = _("give reddit gold to %(user)s to show "
                                 "your appreciation") % {'user': user.name}
            elif not user.gold:
                self.goldlink = "/gold/about"
                self.giftmsg = _("get extra features and help support reddit "
                                 "with a reddit gold subscription")
            elif gold_days_left < 7 and not user.gold_will_autorenew:
                self.goldlink = "/gold/about"
                self.giftmsg = _("renew your reddit gold")

            if not self.viewing_self:
                self.is_friend = user._id in c.user.friends

            if self.show_private_info:
                self.all_karmas = user.all_karmas()


class ServerSecondsBar(Templated):
    my_message = _("you have helped pay for *%(time)s* of reddit server time.")
    their_message = _("/u/%(user)s has helped pay for *%%(time)s* of reddit server "
                      "time.")

    my_gift_message = _("gifts on your behalf have helped pay for *%(time)s* of "
                        "reddit server time.")
    their_gift_message = _("gifts on behalf of /u/%(user)s have helped pay for "
                           "*%%(time)s* of reddit server time.")

    def make_message(self, seconds, my_message, their_message):
        if not seconds:
            return ''

        delta = datetime.timedelta(seconds=seconds)
        server_time = precise_format_timedelta(delta, threshold=5,
                                                locale=c.locale)
        if c.user == self.user:
            message = my_message
        else:
            message = their_message % {'user': self.user.name}
        return message % {'time': server_time}

    def __init__(self, user):
        Templated.__init__(self)

        self.is_public = user.pref_public_server_seconds
        self.is_user = c.user == user
        self.user = user

        seconds = 0.
        gold_payments = gold_payments_by_user(user)

        for payment in gold_payments:
            seconds += calculate_server_seconds(payment.pennies, payment.date)

        try:
            q = (Bid.query().filter(Bid.account_id == user._id)
                    .filter(Bid.status == Bid.STATUS.CHARGE)
                    .filter(Bid.transaction > 0))
            selfserve_payments = list(q)
        except NotFound:
            selfserve_payments = []

        for payment in selfserve_payments:
            pennies = payment.charge_amount * 100
            seconds += calculate_server_seconds(pennies, payment.date)
        self.message = self.make_message(seconds, self.my_message,
                                         self.their_message)

        seconds = 0.
        gold_gifts = gold_received_by_user(user)

        for payment in gold_gifts:
            pennies = days_to_pennies(payment.days)
            seconds += calculate_server_seconds(pennies, payment.date)
        self.gift_message = self.make_message(seconds, self.my_gift_message,
                                              self.their_gift_message)


class MenuArea(Templated):
    """Draws the gray box at the top of a page for sort menus"""
    def __init__(self, menus = []):
        Templated.__init__(self, menus = menus)


class InfoBar(Templated):
    """Draws the yellow box at the top of a page for info"""
    def __init__(self, message='', extra_class=''):
        Templated.__init__(self, message=message, extra_class=extra_class)


class RedditInfoBar(InfoBar):
    def __init__(self, message='', extra_class='', show_icon=False):
        self.show_icon = show_icon
        super(RedditInfoBar, self).__init__(
            message=message,
            extra_class=extra_class,
        )


class WelcomeBar(InfoBar):
    def __init__(self):
        messages = g.live_config.get("welcomebar_messages")
        if messages:
            message = random.choice(messages).split(" / ")
        else:
            message = (_("reddit is a platform for internet communities"),
                       _("where your votes shape what the world is talking about."))
        InfoBar.__init__(self, message=message)

class NewsletterBar(InfoBar):
    pass

class ClientInfoBar(InfoBar):
    """Draws the message the top of a login page before OAuth2 authorization"""
    def __init__(self, client, *args, **kwargs):
        kwargs.setdefault("extra_class", "client-info")
        InfoBar.__init__(self, *args, **kwargs)
        self.client = client


class LocationBar(Templated): pass

class MobileWebRedirectBar(Templated):
    pass

class SidebarMessage(Templated):
    """An info message box on the sidebar."""
    def __init__(self, message, extra_class=None):
        Templated.__init__(self, message=message, extra_class=extra_class)

class RedditError(BoringPage):
    show_infobar = False
    site_tracking = False

    def __init__(self, title, message, image=None, sr_description=None,
            include_message_mods_link=False, explanation=None):
        content = ErrorPage(
            title=title,
            message=message,
            image=image,
            sr_description=sr_description,
            include_message_mods_link=include_message_mods_link,
            explanation=explanation,
        )
        BoringPage.__init__(self, title, loginbox=False,
                            show_sidebar = False,
                            content=content)

class ErrorPage(Templated):
    """Wrapper for an error message"""
    def __init__(self, title, message, image=None, explanation=None, **kwargs):
        if not image:
            letter = random.choice(['a', 'b', 'c', 'd', 'e'])
            image = 'reddit404' + letter + '.png'
        # Normalize explanation strings.
        if explanation:
            explanation = explanation.lower().rstrip('.') + '.'
        Templated.__init__(self,
                           title=title,
                           message=message,
                           image_url=image,
                           explanation=explanation,
                           **kwargs)


class InterstitialPage(BoringPage):
    show_infobar = False

    def __init__(self, title, content=None):
        BoringPage.__init__(
            self,
            title,
            loginbox=False,
            show_sidebar=False,
            show_welcomebar=False,
            robots='noindex,nofollow',
            content=content,
        )

    def page_classes(self):
        classes = super(BoringPage, self).page_classes()
        if 'quarantine' in classes:
            classes.remove('quarantine')
        return classes


class Interstitial(Templated):
    """Generic template for rendering an interstitial page's content."""

    def __init__(self, image=None, title=None, message=None, sr_name=None,
                 sr_description=None, **kwargs):
        Templated.__init__(
            self,
            image=image,
            title=title,
            message=message,
            sr_name=sr_name,
            sr_description=sr_description,
            **kwargs
        )


class AdminInterstitial(Interstitial):
    """The admin password verification form."""
    pass


class BannedInterstitial(Interstitial):
    """The banned subreddit message."""
    pass


class BannedUserInterstitial(BannedInterstitial):
    """The message shown when viewing a banned user profile."""
    pass


class UserBlockedInterstitial(BannedInterstitial):
    """The message shown when viewing a blocked user profile."""
    pass


class InTimeoutInterstitial(BannedInterstitial):
    """The message shown to a user in timeout."""
    def __init__(self, timeout_days_remaining=0, hide_message=False):
        self.timeout_days_remaining = timeout_days_remaining
        self.hide_message = hide_message
        super(InTimeoutInterstitial, self).__init__()


class PrivateInterstitial(Interstitial):
    """The interstitial shown on private subreddits."""
    pass


class GoldOnlyInterstitial(Interstitial):
    """Interstitial for gold-only subreddits."""
    pass


class QuarantineInterstitial(Interstitial):
    """The opt in page for viewing quarantined content."""

    def __init__(self, sr_name, logged_in, email_verified):
        can_opt_in = logged_in and email_verified
        Interstitial.__init__(
            self,
            sr_name=sr_name,
            logged_in=logged_in,
            can_opt_in=can_opt_in,
        )


class Over18Interstitial(Interstitial):
    """The no-longer-creepy 'over 18' check page for nsfw content."""
    pass


class LockedInterstitial(Interstitial):
    """The error message shown when attempting to comment on a locked post."""
    pass


class ArchivedInterstitial(Interstitial):
    """The error message shown when attempting to comment on an archived post."""
    def __init__(self):
        days = g.ARCHIVE_AGE.days
        months = days // 30
        super(ArchivedInterstitial, self).__init__(
            archive_age_months=months,
        )


class DeletedUserInterstitial(Interstitial):
    """The deleted user message."""
    pass


class Popup(Templated):
    """Generic template for rendering a modal."""
    def __init__(self, popup_id=None, content=None, **kwargs):
        Templated.__init__(
            self,
            popup_id=popup_id,
            content=content,
            **kwargs
        )


class SubredditTopBar(CachedTemplate):

    """The horizontal strip at the top of most pages for navigating
    user-created reddits."""
    def __init__(self):
        self._my_reddits = None
        self._pop_reddits = None
        name = '' if not c.user_is_loggedin else c.user.name
        # poor man's expiration, with random initial time
        t = int(time.time()) / 3600
        if c.user_is_loggedin:
            t += c.user._id

        # HACK: depends on something in the page's content calling
        # Subreddit.default_subreddits so that c.location is set prior to this
        # template being added to the header. set c.location as an attribute so
        # it is added to the render cache key.
        self.location = c.location or "no_location"
        self.my_subreddits_dropdown = self.my_reddits_dropdown()
        CachedTemplate.__init__(self, name=name, t=t, over18=c.over18)

    @property
    def my_reddits(self):
        if self._my_reddits is None:
            self._my_reddits = Subreddit.user_subreddits(c.user, ids=False)
        return self._my_reddits

    @property
    def pop_reddits(self):
        if self._pop_reddits is None:
            defaults = Subreddit.default_subreddits(ids=False)
            # sort the default subreddits by "popularity" descending
            defaults = sorted(defaults, key=lambda sr: sr._downs, reverse=True)
            self._pop_reddits = defaults
        return self._pop_reddits

    def my_reddits_dropdown(self):
        drop_down_buttons = []
        for sr in sorted(self.my_reddits, key = lambda sr: sr.name.lower()):
            drop_down_buttons.append(SubredditButton(sr))
        drop_down_buttons.append(NavButton(menu.edit_subscriptions,
                                           sr_path = False,
                                           css_class = 'bottom-option',
                                           dest = '/subreddits/'))
        return SubredditMenu(drop_down_buttons,
                             title = _('my subreddits'),
                             type = 'srdrop')

    def subscribed_reddits(self):
        srs = [SubredditButton(sr) for sr in
                        sorted(self.my_reddits,
                               key = lambda sr: sr._downs,
                               reverse=True)
                        ]
        return NavMenu(srs,
                       type='flatlist', separator = '-',
                       css_class = 'sr-bar')

    def popular_reddits(self, exclude_mine=False):
        exclude = self.my_reddits if exclude_mine else []
        buttons = [SubredditButton(sr) for sr in self.pop_reddits
                                       if sr not in exclude]

        return NavMenu(buttons,
                       type='flatlist', separator = '-',
                       css_class = 'sr-bar', _id = 'sr-bar')

    def special_reddits(self):
        css_classes = {Random: "random",
                       RandomSubscription: "gold"}
        reddits = [Frontpage, All, Random]
        if getattr(c.site, "over_18", False):
            reddits.append(RandomNSFW)
        if c.user_is_loggedin:
            if c.user.gold:
                reddits.append(RandomSubscription)
            if c.user.friends:
                reddits.append(Friends)
            if c.user.is_moderator_somewhere:
                reddits.append(Mod)
        return NavMenu([SubredditButton(sr, css_class=css_classes.get(sr))
                        for sr in reddits],
                       type = 'flatlist', separator = '-',
                       css_class = 'sr-bar')

    def sr_bar (self):
        sep = '<span class="separator">&nbsp;|&nbsp;</span>'
        menus = []
        menus.append(self.special_reddits())
        menus.append(RawString(sep))

        if not c.user_is_loggedin:
            menus.append(self.popular_reddits())
        else:
            menus.append(self.subscribed_reddits())

            # if the user has more than ~10 subscriptions the top bar will be
            # completely full and anything we add to it won't be seen
            if len(self.my_reddits) < 10:
                menus.append(RawString(sep))
                menus.append(self.popular_reddits(exclude_mine=True))

        return menus


class MultiInfoBar(Templated):
    def __init__(self, multi, srs, user):
        Templated.__init__(self)
        self.multi = wrap_things(multi)[0]
        self.can_edit = multi.can_edit(user)
        self.can_copy = c.user_is_loggedin
        self.can_rename = c.user_is_loggedin and multi.owner == c.user
        srs.sort(key=lambda sr: sr.name.lower())
        self.description_md = multi.description_md
        self.srs = srs
        self.subreddit_selector = SubredditSelector(
                placeholder=_("add subreddit"),
                class_name="sr-name",
                include_user_subscriptions=False,
                show_add=True,
            )

        self.color_options = Subreddit.KEY_COLORS

        self.icon_options = g.multi_icons

        explore_sr = g.live_config["listing_chooser_explore_sr"]
        if explore_sr:
            self.share_url = "/r/%(sr)s/submit?url=%(url)s" % {
                "sr": explore_sr,
                "url": g.origin + self.multi.path,
            }
        else:
            self.share_url = None


class SubscriptionBox(Templated):
    """The list of reddits a user is currently subscribed to to go in
    the right pane."""
    def __init__(self, srs, multi_text=None):
        self.srs = srs
        self.goldlink = None
        self.goldmsg = None
        self.prelink = None
        self.multi_path = None
        self.multi_text = multi_text

        # Construct MultiReddit path
        if multi_text:
            self.multi_path = '/r/' + '+'.join([sr.name for sr in srs])

        if len(srs) > Subreddit.sr_limit and c.user_is_loggedin:
            if not c.user.gold:
                self.goldlink = "/gold"
                self.goldmsg = _("raise it to %s") % Subreddit.gold_limit
                self.prelink = ["/wiki/faq#wiki_how_many_subreddits_can_i_subscribe_to.3F",
                                _("%s visible") % Subreddit.sr_limit]
            else:
                self.goldlink = "/gold/about"
                extra = min(len(srs) - Subreddit.sr_limit,
                            Subreddit.gold_limit - Subreddit.sr_limit)
                visible = min(len(srs), Subreddit.gold_limit)
                bonus = {"bonus": extra}
                self.goldmsg = _("%(bonus)s bonus subreddits") % bonus
                self.prelink = ["/wiki/faq#wiki_how_many_subreddits_can_i_subscribe_to.3F",
                                _("%s visible") % visible]

        Templated.__init__(self, srs=srs, goldlink=self.goldlink,
                           goldmsg=self.goldmsg)

    @property
    def reddits(self):
        return wrap_links(self.srs)


class ModSRInfoBar(Templated):
    pass


class FilteredInfoBar(Templated):
    def __init__(self):
        self.css_class = None
        if c.site.filtername == "all":
            self.css_class = "gold-accent"
        Templated.__init__(self)


class AllInfoBar(Templated):
    def __init__(self, site, user):
        self.sr = site
        self.allminus_url = None
        self.css_class = None
        if isinstance(site, AllMinus) and c.user.gold:
            self.description = (strings.r_all_minus_description + "\n\n" +
                " ".join("/r/" + sr.name for sr in site.exclude_srs))
            self.css_class = "gold-accent"
        else:
            self.description = strings.r_all_description
            sr_ids = Subreddit.user_subreddits(user)
            srs = Subreddit._byID(
                sr_ids, data=True, return_dict=False, stale=True)
            if srs:
                self.allminus_url = '/r/all-' + '-'.join([sr.name for sr in srs])

        self.gilding_listing = False
        if request.path.startswith("/comments/gilded"):
            self.gilding_listing = True

        Templated.__init__(self)


class CreateSubreddit(Templated):
    """reddit creation form."""
    def __init__(self, site = None, name = '', captcha=None):
        allow_image_upload = site and not site.quarantine
        feature_autoexpand_media_previews = feature.is_enabled("autoexpand_media_previews")
        Templated.__init__(self,
                           site=site,
                           name=name,
                           captcha=captcha,
                           comment_sorts=CommentSortMenu.visible_options(),
                           allow_image_upload=allow_image_upload,
                           feature_autoexpand_media_previews=feature_autoexpand_media_previews,
                           )
        self.color_options = Subreddit.KEY_COLORS
        self.subreddit_selector = SubredditSelector(
                placeholder=_("add subreddit"),
                class_name="sr-name",
                include_user_subscriptions=False,
                show_add=True,
            )


class SubredditStylesheetBase(Templated):
    """Base subreddit stylesheet page."""
    def __init__(self, stylesheet_contents, **kwargs):
        raw_images = ImagesByWikiPage.get_images(c.site, "config/stylesheet")
        images = {name: make_url_protocol_relative(url)
                  for name, url in raw_images.iteritems()}
        super(SubredditStylesheetBase, self).__init__(
            images=images,
            stylesheet_contents=stylesheet_contents,
            **kwargs
        )


class SubredditStylesheet(SubredditStylesheetBase):
    """form for editing or creating subreddit stylesheets"""
    def __init__(self, site=None, stylesheet_contents=''):
        allow_image_upload = site and not site.quarantine
        super(SubredditStylesheet, self).__init__(
            stylesheet_contents=stylesheet_contents,
            site=site,
            allow_image_upload=allow_image_upload,
        )

    @staticmethod
    def find_preview_comments(sr):
        comments = queries.get_sr_comments(sr)
        comments = list(comments)
        if not comments:
            comments = queries.get_all_comments()
            comments = list(comments)

        return Thing._by_fullname(comments[:25], data=True, return_dict=False)

    @staticmethod
    def find_preview_links(sr):
        # try to find a link to use, otherwise give up and return
        links = normalized_hot([sr._id])
        if not links:
            links = normalized_hot(Subreddit.default_subreddits())

        if links:
            links = links[:25]
            links = Link._by_fullname(links, data=True, return_dict=False)

        return links

    @staticmethod
    def rendered_link(links, media, compress, stickied=False):
        with c.user.safe_set_attr:
            c.user.pref_compress = compress
            c.user.pref_media = media
        links = wrap_links(links, show_nums=True, num=1)
        for wrapped in links:
            wrapped.stickied = stickied
        delattr(c.user, "pref_compress")
        delattr(c.user, "pref_media")
        return links.render(style="html")

    @staticmethod
    def rendered_comment(comments, gilded=False):
        wrapped = wrap_links(comments, num=1)
        if gilded:
            for w in wrapped:
                w.gilded_message = "this comment was fake-gilded"
        return wrapped.render(style="html")


class SubredditStylesheetSource(SubredditStylesheetBase):
    """A view of the unminified source of a subreddit's stylesheet."""
    pass


class AutoModeratorConfig(Templated):
    """A view of a subreddit's AutoModerator configuration."""
    def __init__(self, automoderator_config):
        Templated.__init__(self, automoderator_config=automoderator_config)


class RawCode(Templated):
    """A "raw code" view of a wiki page - not rendered as markdown."""
    def __init__(self, code):
        Templated.__init__(self, code=code)


class CssError(Templated):
    """Rendered error returned to the stylesheet editing page via ajax"""
    def __init__(self, error):
        # error is an instance of cssfilter.py:ValidationError
        Templated.__init__(self, error = error)

    @property
    def message(self):
        return _(self.error.message_key) % self.error.message_params

class UploadedImage(Templated):
    "The page rendered in the iframe during an upload of a header image"
    def __init__(self,status,img_src, name="", errors = {}, form_id = ""):
        self.errors = list(errors.iteritems())
        Templated.__init__(self, status=status, img_src=img_src, name = name,
                           form_id = form_id)

    def render(self, *a, **kw):
        return responsive(Templated.render(self, *a, **kw))

class Thanks(Templated):
    """The page to claim reddit gold trophies"""
    def __init__(self, secret=None):
        if secret and secret.startswith("cr_"):
            status = "creddits"
        elif c.user.gold:
            status = "gold"
        else:
            status = "mundane"

        Templated.__init__(self, status=status, secret=secret)

class GoldThanks(Templated):
    """An actual 'Thanks for buying gold!' landing page"""
    pass

class Gold(Templated):
    def __init__(self, goldtype, period, months, signed,
                 email, recipient, giftmessage, can_subscribe=True,
                 edit=False):

        if c.user.employee:
            user_creddits = 50
        else:
            user_creddits = c.user.gold_creddits

        Templated.__init__(self, goldtype = goldtype, period = period,
                           months = months, signed = signed,
                           email=email,
                           recipient=recipient,
                           giftmessage=giftmessage,
                           user_creddits = user_creddits,
                           can_subscribe=can_subscribe,
                           edit=edit)


class Creddits(Templated):
    pass


class GoldPayment(Templated):
    def __init__(self, goldtype, period, months, signed,
                 recipient, giftmessage, passthrough, thing,
                 clone_template=False, thing_type=None):
        desc = None

        if period == "monthly" or 1 <= months < 12:
            unit_price = g.gold_month_price
            if period == 'monthly':
                price = unit_price
            else:
                price = unit_price * months
        else:
            unit_price = g.gold_year_price
            if period == 'yearly':
                price = unit_price
            else:
                years = months / 12
                price = unit_price * years

        if c.user.employee:
            user_creddits = 50
        else:
            user_creddits = c.user.gold_creddits

        if (goldtype in ("gift", "code", "onetime") and
                months <= user_creddits):
            can_use_creddits = True
        else:
            can_use_creddits = False

        if goldtype == "autorenew":
            if period == "monthly":
                paypal_buttonid = g.PAYPAL_BUTTONID_AUTORENEW_BYMONTH
                summary = strings.gold_summary_autorenew_monthly % dict(
                    user=c.user.name,
                    price=price,
                )
            elif period == "yearly":
                paypal_buttonid = g.PAYPAL_BUTTONID_AUTORENEW_BYYEAR
                summary = strings.gold_summary_autorenew_yearly % dict(
                    user=c.user.name,
                    price=price,
                )

            quantity = None
            stripe_key = g.secrets['stripe_public_key']
            coinbase_button_id = None

        elif goldtype == "onetime":
            if months < 12:
                paypal_buttonid = g.PAYPAL_BUTTONID_ONETIME_BYMONTH
                quantity = months
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sMO' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)
            else:
                paypal_buttonid = g.PAYPAL_BUTTONID_ONETIME_BYYEAR
                quantity = months / 12
                months = quantity * 12
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sYR' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)

            summary = strings.gold_summary_onetime % dict(
                amount=Score.somethings(months, "month"),
                user=c.user.name,
                price=price,
            )

            stripe_key = g.secrets['stripe_public_key']

        else:
            if months < 12:
                if goldtype == "code":
                    paypal_buttonid = g.PAYPAL_BUTTONID_GIFTCODE_BYMONTH
                else:
                    paypal_buttonid = g.PAYPAL_BUTTONID_CREDDITS_BYMONTH
                quantity = months
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sMO' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)
            else:
                if goldtype == "code":
                    paypal_buttonid = g.PAYPAL_BUTTONID_GIFTCODE_BYYEAR
                else:
                    paypal_buttonid = g.PAYPAL_BUTTONID_CREDDITS_BYYEAR
                quantity = months / 12
                months = quantity * 12
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sYR' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)

            if goldtype == "creddits":
                summary = strings.gold_summary_creddits % dict(
                    amount=Score.somethings(months, "creddit"),
                    price=price,
                )
            elif goldtype == "gift":
                if clone_template:
                    if thing_type == "comment":
                        format = strings.gold_summary_gilding_comment
                    elif thing_type == "link":
                        format = strings.gold_summary_gilding_link
                elif thing:
                    if isinstance(thing, Comment):
                        format = strings.gold_summary_gilding_page_comment
                        desc = thing.body
                    else:
                        format = strings.gold_summary_gilding_page_link
                        desc = thing.markdown_link_slow()
                elif signed:
                    format = strings.gold_summary_signed_gift
                else:
                    format = strings.gold_summary_anonymous_gift

                if not clone_template:
                    summary = format % dict(
                        amount=Score.somethings(months, "month"),
                        recipient=recipient and
                                  recipient.name.replace('_', '&#95;'),
                        price=price,
                    )
                else:
                    # leave the replacements to javascript
                    summary = format
            elif goldtype == "code":
                summary = strings.gold_summary_gift_code % dict(
                    amount=Score.somethings(months, "month"),
                    price=price,
                )
            else:
                raise ValueError("wtf is %r" % goldtype)

            stripe_key = g.secrets['stripe_public_key']

        Templated.__init__(self, goldtype=goldtype, period=period,
                           months=months, quantity=quantity,
                           unit_price=unit_price, price=price,
                           summary=summary, giftmessage=giftmessage,
                           can_use_creddits=can_use_creddits,
                           passthrough=passthrough,
                           thing=thing, clone_template=clone_template,
                           description=desc, thing_type=thing_type,
                           paypal_buttonid=paypal_buttonid,
                           stripe_key=stripe_key,
                           coinbase_button_id=coinbase_button_id,
                           user_creddits=user_creddits,
                           )


class GoldSubscription(Templated):
    def __init__(self, user):
        if user.has_stripe_subscription:
            details = get_subscription_details(user)
        else:
            details = None

        if details:
            self.has_stripe_subscription = True
            date = details['next_charge_date']
            next_charge_date = format_date(date, format="short",
                                           locale=c.locale)
            credit_card_last4 = details['credit_card_last4']
            amount = format_currency(float(details['pennies']) / 100, 'USD',
                                     locale=c.locale)
            text = _("you have a credit card gold subscription. your card "
                     "(ending in %(last4)s) will be charged %(amount)s on "
                     "%(date)s.")
            self.text = text % dict(last4=credit_card_last4,
                                    amount=amount,
                                    date=next_charge_date)
            self.user_fullname = user._fullname
        else:
            self.has_stripe_subscription = False

        if user.has_paypal_subscription:
            self.has_paypal_subscription = True
            self.paypal_subscr_id = user.gold_subscr_id
            self.paypal_url = paypal_subscription_url()
        else:
            self.has_paypal_subscription = False

        self.stripe_key = g.secrets['stripe_public_key']
        Templated.__init__(self)

class CreditGild(Templated):
    """Page for credit card payments for gilding."""
    pass

class GoldGiftCodeEmail(Templated):
    """Email sent to a logged-out person that purchases a reddit
    gold gift code."""
    pass


class Gilding(Templated):
    pass


class ReportForm(CachedTemplate):
    def __init__(self, thing=None, **kw):
        self.rules = []
        self.system_rules = []
        self.thing_fullname = thing._fullname
        self.kind = None
        subreddit = None

        if isinstance(thing, (Comment, Link)):
            subreddit = thing.subreddit_slow
            self.kind = thing.__class__.__name__.lower()

        if (subreddit and
                feature.is_enabled("subreddit_rules", subreddit=subreddit.name)):
            for rule in SubredditRules.get_rules(subreddit, self.kind):
                self.rules.append(rule["short_name"])
            if self.rules:
                self.system_rules = SITEWIDE_RULES
                self.rules_page_link = "/r/%s/about/rules" % subreddit.name
        if not self.rules:
            self.rules = OLD_SITEWIDE_RULES
            self.rules_page_link = "/help/contentpolicy"

        Templated.__init__(self)


class SubredditReportForm(CachedTemplate):
    def __init__(self, thing=None, filter_by_kind=True, **kw):
        self.rules = []
        self.system_rules = SITEWIDE_RULES
        self.thing_fullname = thing._fullname
        self.kind = None
        subreddit = None

        if isinstance(thing, Comment, Link):
            subreddit = thing.subreddit_slow
            self.sr_name = subreddit.name
            if filter_by_kind:
                self.kind = thing.__class__.__name__.lower()
        else:
            self.sr_name = None

        if (subreddit and
                feature.is_enabled("subreddit_rules", subreddit=subreddit.name)):
            self.rules = SubredditRules.get_rules(subreddit, self.kind)

        Templated.__init__(self)


class ReportFormTemplates(Templated):
    def __init__(self):
        super(ReportFormTemplates, self).__init__(
            system_rules=SITEWIDE_RULES,
            rules_page_link="/help/contentpolicy",
        )


class FraudForm(Templated):
    pass


class Password(Templated):
    """Form encountered when 'recover password' is clicked in the LoginFormWide."""
    def __init__(self, success=False):
        Templated.__init__(self, success = success)

class PasswordReset(Templated):
    """Template for generating an email to the user who wishes to
    reset their password (step 2 of password recovery, after they have
    entered their user name in Password.)"""
    pass

class MessageNotificationEmail(Templated):
    """Notification e-mail that a user has received a new message."""
    pass

class MessageNotificationEmailsUnsubscribe(Templated):
    """The page we show users when they unsubscribe from notification
    emails."""
    pass

class PasswordChangeEmail(Templated):
    """Notification e-mail that a user's password has changed."""
    pass

class EmailChangeEmail(Templated):
    """Notification e-mail that a user's e-mail has changed."""
    pass

class VerifyEmail(Templated):
    pass

class Promo_Email(Templated):
    def __init__(self, *args, **kwargs):
        # if total_budget_dollars is passed,
        # format into printable_total_budget
        if 'total_budget_dollars' in kwargs:
            locale = c.locale or g.locale
            self.printable_total_budget = format_currency(
                kwargs['total_budget_dollars'], 'USD', locale=locale)
        super(Promo_Email, self).__init__(*args, **kwargs)

class SuspiciousPaymentEmail(Templated):
    def __init__(self, user, link):
        Templated.__init__(self, user=user, link=link)


class ResetPassword(Templated):
    """Form for actually resetting a lost password, after the user has
    clicked on the link provided to them in the Password_Reset email
    (step 3 of password recovery.)"""
    pass


class Captcha(Templated):
    """Container for rendering robot detection device."""
    def __init__(self, error=None):
        self.error = _('try entering those letters again') if error else ""
        self.iden = get_captcha()
        Templated.__init__(self)

class PermalinkMessage(Templated):
    """renders the box on comment pages that state 'you are viewing a
    single comment's thread'"""
    def __init__(self, comments_url):
        Templated.__init__(self, comments_url = comments_url)

class PaneStack(Templated):
    """Utility class for storing and rendering a list of block elements."""

    def __init__(self, panes=[], div_id = None, css_class=None, div=False,
                 title="", title_buttons = []):
        div = div or div_id or css_class or False
        self.div_id    = div_id
        self.css_class = css_class
        self.div       = div
        self.stack     = list(panes)
        self.title = title
        self.title_buttons = title_buttons
        Templated.__init__(self)

    def append(self, item):
        """Appends an element to the end of the current stack"""
        self.stack.append(item)

    def push(self, item):
        """Prepends an element to the top of the current stack"""
        self.stack.insert(0, item)

    def insert(self, *a):
        """inerface to list.insert on the current stack"""
        return self.stack.insert(*a)


class HtmlPaneStack(PaneStack):
    """Same as panestack, but won't show up in json responses."""
    pass


class SearchForm(Templated):
    """The simple search form in the header of the page.  prev_search
    is the previous search."""
    def __init__(self, prev_search='', search_params={}, site=None,
                 simple=True, restrict_sr=False, subreddit_search=False,
                 syntax=None, search_path="/search"):
        Templated.__init__(self, prev_search=prev_search,
                           search_params=search_params, site=site,
                           simple=simple, restrict_sr=restrict_sr,
                           subreddit_search=subreddit_search, syntax=syntax,
                           search_path=search_path)

        # generate the over18 redirect url for the current search if needed
        if not c.over18 and feature.is_enabled('safe_search'):
            u = UrlParser(add_sr(search_path))
            if prev_search:
                u.update_query(q=prev_search)
            if restrict_sr:
                u.update_query(restrict_sr='on')
            u.update_query(**search_params)
            u.update_query(over18='yes')
            self.over18_url = u.unparse()
        else:
            self.over18_url = None


class SearchBar(Templated):
    """More detailed search box for /search and /subreddits pages.

    Displays the previous search as well

    """
    def __init__(self, header=None, prev_search='', search_params={},
                 simple=False, restrict_sr=False, site=None, syntax=None,
                 subreddit_search=False, converted_data=None,
                 search_path="/search"):
        if header is None:
            header = _("search")
        self.header = header
        self.prev_search  = prev_search
        self.converted_data = converted_data

        self.search_form = SearchForm(
            prev_search=prev_search,
            search_params=search_params,
            site=site,
            subreddit_search=subreddit_search,
            simple=simple,
            restrict_sr=restrict_sr,
            syntax=syntax,
            search_path=search_path,
        )
        Templated.__init__(self)


class SubredditFacets(Templated):
    def __init__(self, prev_search='', facets={}, sort=None, recent=None):
        self.prev_search = prev_search

        Templated.__init__(self, facets=facets, sort=sort, recent=recent)


class NewLink(Templated):
    """Render the link submission form"""
    def __init__(self, captcha=None, url='', title='', text='', selftext='',
                 resubmit=False, default_sr=None,
                 extra_subreddits=None, show_link=True, show_self=True):

        self.show_link = show_link
        self.show_self = show_self

        tabs = []
        if show_link:
            tabs.append(('link', ('link-desc', 'url-field')))
        if show_self:
            tabs.append(('text', ('text-desc', 'text-field')))

        if self.show_self and self.show_link:
            all_fields = set(chain(*(parts for (tab, parts) in tabs)))
            buttons = []

            if selftext == 'true' or text != '':
                self.default_tab = tabs[1][0]
            else:
                self.default_tab = tabs[0][0]

            for tab_name, parts in tabs:
                to_show = ','.join('#' + p for p in parts)
                to_hide = ','.join('#' + p for p in all_fields if p not in parts)
                onclick = "return select_form_tab(this, '%s', '%s');"
                onclick = onclick % (to_show, to_hide)
                if tab_name == self.default_tab:
                    self.default_show = to_show
                    self.default_hide = to_hide

                buttons.append(JsButton(tab_name, onclick=onclick, css_class=tab_name + "-button"))

            self.formtabs_menu = JsNavMenu(buttons, type = 'formtab')

        self.resubmit = resubmit
        self.default_sr = default_sr
        self.extra_subreddits = extra_subreddits

        Templated.__init__(self, captcha = captcha, url = url,
                         title = title, text = text)


class Share(Templated):
    pass

class Mail_Opt(Templated):
    pass

class OptOut(Templated):
    pass

class OptIn(Templated):
    pass


class Button(Wrapped):
    cachable = True
    extension_handling = False
    def __init__(self, link, **kw):
        Wrapped.__init__(self, link, **kw)
        if link is None:
            self.title = ""
            self.add_props(c.user, [self])


    @classmethod
    def add_props(cls, user, wrapped):
        # unlike most wrappers we can guarantee that there is a link
        # that this wrapper is wrapping.
        Link.add_props(user, [w for w in wrapped if hasattr(w, "_fullname")])
        for w in wrapped:
            # caching: store the user name since each button has a modhash
            w.user_name = c.user.name if c.user_is_loggedin else ""
            if not hasattr(w, '_fullname'):
                w._fullname = None

    def render(self, *a, **kw):
        res = Wrapped.render(self, *a, **kw)
        return responsive(res, True)

class ButtonLite(Button):
    def render(self, *a, **kw):
        return Wrapped.render(self, *a, **kw)

class ButtonDemoPanel(Templated):
    """The page for showing the different styles of embedable voting buttons"""
    pass

class ContactUs(Templated):
    pass


class WidgetDemoPanel(Templated):
    """Demo page for the .embed widget."""
    pass


class UserAwards(Templated):
    """For drawing the regular-user awards page."""
    def __init__(self):
        from r2.models import Award, Trophy
        Templated.__init__(self)

        self.regular_winners = []
        self.manuals = []
        self.invisibles = []

        for award in Award._all_awards():
            if award.awardtype == 'regular':
                trophies = Trophy.by_award(award)
                # Don't show awards that nobody's ever won
                # (e.g., "9-Year Club")
                if trophies:
                    winner = trophies[0]._thing1.name
                    self.regular_winners.append( (award, winner, trophies[0]) )
            elif award.awardtype == 'manual':
                self.manuals.append(award)
            elif award.awardtype == 'invisible':
                self.invisibles.append(award)
            else:
                raise NotImplementedError


class AdminAwards(Templated):
    """The admin page for editing awards"""
    def __init__(self):
        from r2.models import Award
        Templated.__init__(self)
        self.awards = Award._all_awards()

class AdminAwardGive(Templated):
    """The interface for giving an award"""
    def __init__(self, award, recipient='', desc='', url='', hours=''):
        now = datetime.datetime.now(g.display_tz)
        if desc:
            self.description = desc
        elif award.awardtype == 'regular':
            self.description = "??? -- " + now.strftime("%Y-%m-%d")
        else:
            self.description = ""
        self.url = url
        self.recipient = recipient
        self.hours = hours

        Templated.__init__(self, award = award)

class AdminAwardWinners(Templated):
    """The list of winners of an award"""
    def __init__(self, award):
        trophies = Trophy.by_award(award)
        Templated.__init__(self, award = award, trophies = trophies)


class AdminCreddits(Templated):
    """The admin interface for giving creddits to a user."""
    def __init__(self, recipient):
        self.recipient = recipient
        Templated.__init__(self)


class AdminGold(Templated):
    """The admin interface for giving or taking days of gold for a user."""
    def __init__(self, recipient):
        self.recipient = recipient
        Templated.__init__(self)


class Ads(Templated):
    def __init__(self):
        Templated.__init__(self)
        self.ad_url = g.ad_domain + "/ads/"
        self.frame_id = "ad-frame"


class ReadNext(Templated):
    def __init__(self, sr, links):
        Templated.__init__(self)
        self.sr = sr
        self.links = links


class Embed(Templated):
    """wrapper for embedding /help into reddit as if it were not on a separate wiki."""
    def __init__(self,content = ''):
        Templated.__init__(self, content = content)


def wrapped_flair(user, subreddit, force_show_flair):
    if isinstance(subreddit, FakeSubreddit):
        # FakeSubreddits don't show user flair
        return False, 'right', '', ''
    elif not (force_show_flair or subreddit.flair_enabled):
        return False, 'right', '', ''

    enabled = user.flair_enabled_in_sr(subreddit._id)
    position = subreddit.flair_position
    text = user.flair_text(subreddit._id)
    css_class = user.flair_css_class(subreddit._id)

    return enabled, position, text, css_class

class WrappedUser(CachedTemplate):
    cachable = False
    FLAIR_CSS_PREFIX = 'flair-'

    def __init__(self, user, attribs = [], context_thing = None, gray = False,
                 subreddit = None, force_show_flair = None,
                 flair_template = None, flair_text_editable = False,
                 include_flair_selector = False):
        if not subreddit:
            subreddit = c.site

        attribs.sort()
        author_cls = 'author'

        author_title = ''
        if gray:
            author_cls += ' gray'
        for tup in attribs:
            author_cls += " " + tup[2]
            # Hack: '(' should be in tup[3] iff this friend has a note
            if tup[1] == 'F' and '(' in tup[3]:
                author_title = tup[3]

        flair = wrapped_flair(user, subreddit or c.site, force_show_flair)
        flair_enabled, flair_position, flair_text, flair_css_class = flair
        has_flair = bool(
            c.user.pref_show_flair and (flair_text or flair_css_class))

        if flair_template:
            flair_template_id = flair_template._id
            flair_text = flair_template.text
            flair_css_class = flair_template.css_class
            has_flair = True
        else:
            flair_template_id = None

        if flair_css_class:
            # This is actually a list of CSS class *suffixes*. E.g., "a b c"
            # should expand to "flair-a flair-b flair-c".
            flair_css_class = ' '.join(self.FLAIR_CSS_PREFIX + c
                                       for c in flair_css_class.split())

        if include_flair_selector:
            if (not getattr(c.site, 'flair_self_assign_enabled', True)
                and not (c.user_is_admin
                         or c.site.is_moderator_with_perms(c.user, 'flair'))):
                include_flair_selector = False

        target = None
        ip_span = None
        context_deleted = None
        if context_thing:
            target = getattr(context_thing, 'target', None)
            ip_span = getattr(context_thing, 'ip_span', None)
            context_deleted = context_thing.deleted

        karma = ''
        context_thing_fullname = ''
        show_details_link = False

        if c.user_is_admin:
            karma = ' (%d)' % user.link_karma

            if user.in_timeout:
                if user.timeout_expiration:
                    author_cls += " user-in-timeout-temp"
                else:
                    author_cls += " user-in-timeout-perma"

            if context_thing:
                context_thing_fullname = context_thing._fullname

                if isinstance(context_thing, Wrapped):
                    unwrapped_thing = context_thing.lookups[0]
                else:
                    unwrapped_thing = context_thing
                if isinstance(unwrapped_thing, (Link, Comment)):
                    show_details_link = True

            if user._spam:
                author_cls += " user-spam"
            if user._banned:
                author_cls += " user-banned"

        CachedTemplate.__init__(self,
                                name = user.name,
                                force_show_flair = force_show_flair,
                                has_flair = has_flair,
                                flair_enabled = flair_enabled,
                                flair_position = flair_position,
                                flair_text = flair_text,
                                flair_text_editable = flair_text_editable,
                                flair_css_class = flair_css_class,
                                flair_template_id = flair_template_id,
                                include_flair_selector = include_flair_selector,
                                author_cls = author_cls,
                                author_title = author_title,
                                attribs = attribs,
                                context_thing_fullname = context_thing_fullname,
                                show_details_link = show_details_link,
                                karma = karma,
                                ip_span = ip_span,
                                context_deleted = context_deleted,
                                fullname = user._fullname,
                                user_deleted = user._deleted)

class UserTableItem(Templated):
    type = ''
    remove_action = 'unfriend'
    cells = ('user', 'age', 'sendmessage', 'remove')

    @property
    def executed_message(self):
        return _("added")

    def __init__(self, user, editable=True, **kw):
        self.user = user
        self.editable = editable
        self.author_cls = ''

        if c.user_is_admin and user._spam:
            self.author_cls = 'banned-user'

        Templated.__init__(self, **kw)

    def __repr__(self):
        return '<UserTableItem "%s">' % self.user.name

class RelTableItem(UserTableItem):
    def __init__(self, rel, **kw):
        self._id = rel._id
        self.rel = rel
        UserTableItem.__init__(self, rel._thing2, **kw)

    @property
    def _fullname(self):
        # needed for paging (see Listing.listing())
        return self.rel._fullname

    @property
    def container_name(self):
        return c.site._fullname

class FriendTableItem(RelTableItem):
    remove_access_required = False
    type = 'friend'

    @property
    def cells(self):
        if c.user.gold:
            return ('user', 'sendmessage', 'note', 'age', 'remove')
        return ('user', 'sendmessage', 'remove')

    @property
    def container_name(self):
        return c.user._fullname

class EnemyTableItem(RelTableItem):
    remove_access_required = False
    type = 'enemy'
    cells = ('user', 'age', 'remove')

    @property
    def container_name(self):
        return c.user._fullname

class BannedTableItem(RelTableItem):
    type = 'banned'
    cells = ('user', 'age', 'sendmessage', 'remove', 'note', 'temp')

    @property
    def executed_message(self):
        return _("banned")


class MutedTableItem(RelTableItem):
    type = 'muted'
    cells = ('user', 'age', 'remove', 'note')

    @property
    def executed_message(self):
        return _("muted")


class WikiBannedTableItem(BannedTableItem):
    type = 'wikibanned'

class ContributorTableItem(RelTableItem):
    type = 'contributor'

class WikiMayContributeTableItem(RelTableItem):
    type = 'wikicontributor'

class InvitedModTableItem(RelTableItem):
    type = 'moderator_invite'
    cells = ('user', 'age', 'permissions', 'permissionsctl')

    @property
    def executed_message(self):
        return _("invited")

    def is_editable(self, user):
        if not c.user_is_loggedin:
            return False
        elif c.user_is_admin:
            return True
        return c.site.is_unlimited_moderator(c.user)

    def __init__(self, rel, editable=True, **kw):
        if editable:
            self.cells += ('remove',)
        editable = self.is_editable(rel._thing2)
        self.permissions = ModeratorPermissions(rel._thing2, self.type,
                                                rel.get_permissions(),
                                                editable=editable)
        RelTableItem.__init__(self, rel, editable=editable, **kw)

class ModTableItem(InvitedModTableItem):
    type = 'moderator'

    @property
    def executed_message(self):
        return _("added")

    def is_editable(self, user):
        if not c.user_is_loggedin:
            return False
        elif c.user_is_admin:
            return True
        return c.user != user and c.site.can_demod(c.user, user)


class ModToolsPage(Reddit):
    """A mod tool page."""

    def __init__(self, **kwargs):
        super(ModToolsPage, self).__init__(
            page_classes=['modtools-page'],
            **kwargs
        )


class Rules(Templated):
    """Show subreddit rules for everyone and add edit controls for mods."""
    def __init__(self, title, kind_labels):
        self.title = title
        self.can_edit = c.user_is_loggedin and (c.user_is_admin or
            c.site.is_moderator_with_perms(c.user, 'config'))
        self.rules = SubredditRules.get_rules(c.site)
        self.site_rules = SITEWIDE_RULES
        self.kind_labels = kind_labels
        Templated.__init__(self)


class FlairPane(Templated):
    def __init__(self, num, after, reverse, name, user):
        # Make sure c.site isn't stale before rendering.
        c.site = Subreddit._byID(c.site._id, data=True, stale=False)

        tabs = [
            ('grant', _('grant flair'), FlairList(num, after, reverse, name,
                                                  user)),
            ('templates', _('user flair templates'),
             FlairTemplateList(USER_FLAIR)),
            ('link_templates', _('link flair templates'),
             FlairTemplateList(LINK_FLAIR)),
        ]

        Templated.__init__(
            self,
            tabs=TabbedPane(tabs, linkable=True),
            flair_enabled=c.site.flair_enabled,
            flair_position=c.site.flair_position,
            link_flair_position=c.site.link_flair_position,
            flair_self_assign_enabled=c.site.flair_self_assign_enabled,
            link_flair_self_assign_enabled=
                c.site.link_flair_self_assign_enabled)

class FlairList(Templated):
    """List of users who are tagged with flair within a subreddit."""

    def __init__(self, num, after, reverse, name, user):
        Templated.__init__(self, num=num, after=after, reverse=reverse,
                           name=name, user=user)

    @property
    def flair(self):
        if self.user:
            return [FlairListRow(self.user)]

        if self.name:
            # user lookup was requested, but no user was found, so abort
            return []

        query = Flair._query(
            Flair.c._thing1_id == c.site._id,
            Flair.c._name == 'flair',
            sort=asc('_thing2_id'),
            eager_load=True,
            thing_data=True,
        )

        # To maintain API compatibility we can't use the `before` or `after`s
        # returned by Builder.get_items(), since we use different logic to
        # determine them. We also need to fetch an extra item to be *sure*
        # there's a next page.
        builder = FlairListBuilder(query, wrap=FlairListRow.from_rel,
                                   after=self.after, reverse=self.reverse,
                                   num=self.num + 1)

        items = builder.get_items()[0]

        if not items:
            return []

        have_more = False
        if self.num and len(items) > self.num:
            if self.reverse:
                have_more = items.pop(0)
            else:
                have_more = items.pop()

        # FlairLists are unusual in that afters that aren't in the queryset
        # work correctly due to the filter just doing a gt (or lt) on
        # the after's `_id`. They also use _thing2's fullname instead
        # of the fullname of the rel for pagination.
        before = items[0].user._fullname
        after = items[-1].user._fullname

        links = []
        show_next = have_more or self.reverse
        if (not self.reverse and self.after) or (self.reverse and have_more):
            links.append(FlairNextLink(before, previous=True,
                                       needs_border=show_next))
        if show_next:
            links.append(FlairNextLink(after, previous=False))

        return items + links


class FlairListRow(Templated):
    def __init__(self, user):
        self.user = user
        Templated.__init__(self,
                           flair_text=user.flair_text(c.site._id),
                           flair_css_class=user.flair_css_class(c.site._id))

    @classmethod
    def from_rel(cls, rel):
        instance = cls(rel._thing2)
        # Needed by the builder to do wrapped -> unwrapped lookups
        instance._id = rel._id
        return instance


class FlairNextLink(Templated):
    def __init__(self, after, previous=False, needs_border=False):
        Templated.__init__(self, after=after, previous=previous,
                           needs_border=needs_border)

class FlairCsv(Templated):
    class LineResult:
        def __init__(self):
            self.errors = {}
            self.warnings = {}
            self.status = 'skipped'
            self.ok = False

        def error(self, field, desc):
            self.errors[field] = desc

        def warn(self, field, desc):
            self.warnings[field] = desc

    def __init__(self):
        Templated.__init__(self, results_by_line=[])

    def add_line(self):
        self.results_by_line.append(self.LineResult())
        return self.results_by_line[-1]

class FlairTemplateList(Templated):
    def __init__(self, flair_type):
        Templated.__init__(self, flair_type=flair_type)

    @property
    def templates(self):
        ids = FlairTemplateBySubredditIndex.get_template_ids(
                c.site._id, flair_type=self.flair_type)
        fts = FlairTemplate._byID(ids)
        return [FlairTemplateEditor(fts[i], self.flair_type) for i in ids]

class FlairTemplateEditor(Templated):
    def __init__(self, flair_template, flair_type):
        Templated.__init__(self,
                           id=flair_template._id,
                           text=flair_template.text,
                           css_class=flair_template.css_class,
                           text_editable=flair_template.text_editable,
                           sample=FlairTemplateSample(flair_template,
                                                      flair_type),
                           position=getattr(c.site, 'flair_position', 'right'),
                           flair_type=flair_type)

    def render(self, *a, **kw):
        res = Templated.render(self, *a, **kw)
        if not g.template_debug:
            res = spaceCompress(res)
        return res

class FlairTemplateSample(Templated):
    """Like a read-only version of FlairTemplateEditor."""
    def __init__(self, flair_template, flair_type):
        if flair_type == USER_FLAIR:
            wrapped_user = WrappedUser(c.user, subreddit=c.site,
                                       force_show_flair=True,
                                       flair_template=flair_template)
        else:
            wrapped_user = None
        Templated.__init__(self,
                           flair_template=flair_template,
                           wrapped_user=wrapped_user, flair_type=flair_type)

class FlairPrefs(CachedTemplate):
    def __init__(self):
        sr_flair_enabled = getattr(c.site, 'flair_enabled', False)
        user_flair_enabled = getattr(c.user, 'flair_%s_enabled' % c.site._id,
                                     True)
        sr_flair_self_assign_enabled = getattr(
            c.site, 'flair_self_assign_enabled', True)
        wrapped_user = WrappedUser(c.user, subreddit=c.site,
                                   force_show_flair=True,
                                   include_flair_selector=True)
        CachedTemplate.__init__(
            self,
            sr_flair_enabled=sr_flair_enabled,
            sr_flair_self_assign_enabled=sr_flair_self_assign_enabled,
            user_flair_enabled=user_flair_enabled,
            wrapped_user=wrapped_user)

class FlairSelectorLinkSample(CachedTemplate):
    def __init__(self, link, site, flair_template):
        flair_position = getattr(site, 'link_flair_position', 'right')
        admin = bool(c.user_is_admin
                     or site.is_moderator_with_perms(c.user, 'flair'))
        CachedTemplate.__init__(
            self,
            title=link.title,
            flair_position=flair_position,
            flair_template_id=flair_template._id,
            flair_text=flair_template.text,
            flair_css_class=flair_template.css_class,
            flair_text_editable=admin or flair_template.text_editable,
            )

class FlairSelector(CachedTemplate):
    """Provide user with flair options according to subreddit settings."""
    def __init__(self, user, site, link=None):
        admin = bool(
            c.user_is_admin or site.is_moderator_with_perms(c.user, 'flair'))

        if link:
            flair_type = LINK_FLAIR
            target = link
            target_name = link._fullname
            attr_pattern = 'flair_%s'
            position = getattr(site, 'link_flair_position', 'right')
            target_wrapper = (
                lambda flair_template: FlairSelectorLinkSample(
                    link, site, flair_template))
            self_assign_enabled = (
                c.user._id == link.author_id
                and site.link_flair_self_assign_enabled)
        else:
            flair_type = USER_FLAIR
            target = user
            target_name = user.name
            position = getattr(site, 'flair_position', 'right')
            attr_pattern = 'flair_%s_%%s' % c.site._id
            target_wrapper = (
                lambda flair_template: WrappedUser(
                    user, subreddit=site, force_show_flair=True,
                    flair_template=flair_template,
                    flair_text_editable=admin or template.text_editable))
            self_assign_enabled = site.flair_self_assign_enabled

        text = getattr(target, attr_pattern % 'text', '')
        css_class = getattr(target, attr_pattern % 'css_class', '')
        templates, matching_template = self._get_templates(
                site, flair_type, text, css_class)

        if self_assign_enabled or admin:
            choices = [target_wrapper(template) for template in templates]
        else:
            choices = []

        # If one of the templates is already selected, modify its text to match
        # the user's current flair.
        if matching_template:
            for choice in choices:
                if choice.flair_template_id == matching_template:
                    if choice.flair_text_editable:
                        choice.flair_text = text
                    break

        Templated.__init__(self, text=text, css_class=css_class,
                           position=position, choices=choices,
                           matching_template=matching_template,
                           target_name=target_name)

    def render(self, *a, **kw):
        return responsive(CachedTemplate.render(self, *a, **kw), True)

    def _get_templates(self, site, flair_type, text, css_class):
        ids = FlairTemplateBySubredditIndex.get_template_ids(
            site._id, flair_type)
        template_dict = FlairTemplate._byID(ids)
        templates = [template_dict[i] for i in ids]
        for template in templates:
            if template.covers((text, css_class)):
                matching_template = template._id
                break
        else:
             matching_template = None
        return templates, matching_template


class DetailsPage(LinkInfoPage):
    extension_handling= False

    def __init__(self, thing, *args, **kwargs):
        from admin_pages import Details
        after = kwargs.pop('after', None)
        reverse = kwargs.pop('reverse', False)
        count = kwargs.pop('count', None)
        self.details = None

        if isinstance(thing, (Link, Comment)):
            self.details = Details(thing, after=after, reverse=reverse,
                                   count=count)

        if isinstance(thing, Link):
            link = thing
            comment = None
            content = self.details
        elif isinstance(thing, Comment):
            comment = thing
            link = Link._byID(comment.link_id, data=True)
            content = PaneStack()
            content.append(PermalinkMessage(link.make_permalink_slow()))
            content.append(LinkCommentSep())
            content.append(CommentPane(link, CommentSortMenu.operator('new'),
                                   comment, None, 1))
            content.append(self.details)

        kwargs['content'] = content
        LinkInfoPage.__init__(self, link, comment, *args, **kwargs)

    def rightbox(self):
        rb = LinkInfoPage.rightbox(self)

        if c.user_is_admin:
            from admin_pages import AdminDetailsBar
            rb.append(AdminDetailsBar(from_page='details'))

        return rb


class PromotePage(Reddit):
    create_reddit_box  = False
    submit_box         = False
    extension_handling = False
    searchbox          = False

    @classmethod
    def get_menu(cls):
        if c.user_is_sponsor:
            buttons = [
                NavButton(menu['new_promo'], dest='/promoted/new_promo'),
                NavButton(menu['current_promos'], dest='/sponsor/promoted',
                          aliases=['/sponsor']),
                NavButton('inventory', '/sponsor/inventory'),
                NavButton('report', '/sponsor/report'),
                NavButton('underdelivered', '/sponsor/promoted/underdelivered'),
                NavButton('house ads', '/sponsor/promoted/house'),
                NavButton('reported links', '/sponsor/promoted/reported'),
                NavButton('fraud', '/sponsor/promoted/fraud'),
                NavButton('lookup user', '/sponsor/lookup_user'),
            ]
            return NavMenu(buttons, type='flatlist')
        else:
            buttons = [
                NamedButton('new_promo'),
                NamedButton('my_current_promos', dest=''),
            ]
            return NavMenu(buttons, base_path='/promoted', type='flatlist')

    def __init__(self, nav_menus=None, *a, **kw):
        menu = self.get_menu()

        if nav_menus:
            nav_menus.insert(0, menu)
        else:
            nav_menus = [menu]

        kw['show_sidebar'] = False
        auction_announcement = not feature.is_enabled('ads_auction')
        Reddit.__init__(self, nav_menus=nav_menus,
            auction_announcement=auction_announcement, *a, **kw)


class PromoteLinkBase(Templated):
    min_start = None
    max_start = None
    max_end = None

    def __init__(self, **kw):
        self.mobile_targeting_enabled = feature.is_enabled("mobile_targeting")
        Templated.__init__(self, **kw)

    def get_locations(self):
        # geotargeting
        def location_sort(location_tuple):
            code, name, default = location_tuple
            if code == '':
                return -2
            elif code == 'US':
                return -1
            else:
                return name

        countries = [(code, country['name'], False) for code, country
                                                    in g.locations.iteritems()]
        countries.append(('', _('none'), True))

        countries = sorted(countries, key=location_sort)
        regions = {}
        metros = {}
        for code, country in g.locations.iteritems():
            if 'regions' in country and country['regions']:
                regions[code] = [('', _('all'), True)]

                for region_code, region in country['regions'].iteritems():
                    if region['metros']:
                        region_tuple = (region_code, region['name'], False)
                        regions[code].append(region_tuple)
                        if c.user_is_sponsor:
                            metros[region_code] = []
                        else:
                            metros[region_code] = [('', _('all'), True)]

                        for metro_code, metro in region['metros'].iteritems():
                            metro_tuple = (metro_code, metro['name'], False)
                            metros[region_code].append(metro_tuple)
                        metros[region_code].sort(key=location_sort)
                regions[code].sort(key=location_sort)

        self.countries = countries
        self.regions = regions
        self.metros = metros

        ads_auction_enabled = feature.is_enabled('ads_auction')
        self.force_auction = (ads_auction_enabled and not c.user_is_sponsor)
        self.auction_optional = (ads_auction_enabled and c.user_is_sponsor)

        self.cpc_pricing = feature.is_enabled('cpc_pricing')

    def get_collections(self):
        self.collections = [cl.__dict__ for cl in Collection.get_all()]

    def get_mobile_versions(self):
        self.ios_versions = g.ios_versions
        self.android_versions = g.android_versions


class PromoteLinkNew(PromoteLinkBase):
    def __init__(self, images=None, *a, **kw):
        images = images or {}
        self.images = images
        PromoteLinkBase.__init__(self, **kw)


class PromoteLinkEdit(PromoteLinkBase):
    def __init__(self, link, listing, *a, **kw):
        self.setup(link, listing)
        PromoteLinkBase.__init__(self, **kw)

    def setup(self, link, listing):
        self.bids = []
        self.author = Account._byID(link.author_id, data=True)

        if c.user_is_sponsor:
            try:
                bids = Bid.lookup(thing_id=link._id)
            except NotFound:
                pass
            else:
                bids.sort(key=lambda x: x.date, reverse=True)
                bidders = Account._byID(set(bid.account_id for bid in bids),
                                        data=True, return_dict=True)
                for bid in bids:
                    status = Bid.STATUS.name[bid.status].lower()
                    bidder = bidders[bid.account_id]
                    row = Storage(
                        status=status,
                        bidder=bidder.name,
                        date=bid.date,
                        transaction=bid.transaction,
                        campaign=bid.campaign,
                        pay_id=bid.pay_id,
                        amount_str=format_currency(bid.bid, 'USD',
                                                   locale=c.locale),
                        charge_str=format_currency(bid.charge or bid.bid, 'USD',
                                                   locale=c.locale),
                    )
                    self.bids.append(row)

        min_start, max_start, max_end = promote.get_date_limits(
            link, c.user_is_sponsor)

        default_end = min_start + datetime.timedelta(days=7)
        default_start = min_start

        self.min_start = min_start.strftime("%m/%d/%Y")
        self.max_start = max_start.strftime("%m/%d/%Y")
        self.max_end = max_end.strftime("%m/%d/%Y")
        self.default_start = default_start.strftime("%m/%d/%Y")
        self.default_end = default_end.strftime("%m/%d/%Y")

        self.link = link
        self.listing = listing
        campaigns = list(PromoCampaign._by_link(link._id))
        self.campaigns = RenderableCampaign.from_campaigns(link, campaigns)
        self.promotion_log = PromotionLog.get(link)

        if c.user_is_sponsor:
            self.min_budget_dollars = 0
            self.max_budget_dollars = 0
        else:
            self.min_budget_dollars = g.min_total_budget_pennies / 100.
            self.max_budget_dollars = g.max_total_budget_pennies / 100.

        self.default_budget_dollars = g.default_total_budget_pennies / 100.

        if c.user_is_sponsor:
            self.min_bid_dollars = 0.
            self.max_bid_dollars = 0.
        else:
            self.min_bid_dollars = g.min_bid_pennies / 100.
            self.max_bid_dollars = g.max_bid_pennies / 100.

        self.priorities = [
            (p.name, p.text, p.description, p.default,
            p.inventory_override, p == PROMOTE_PRIORITIES['auction'])
            for p in PROMOTE_PRIORITIES.values()
        ]

        self.get_locations()
        self.get_collections()
        self.get_mobile_versions()

        user_srs = [sr for sr in Subreddit.user_subreddits(c.user, ids=False)
                    if sr.can_submit(c.user, promotion=True) and sr.allow_ads]
        top_srs = sorted(user_srs, key=lambda sr: sr._ups, reverse=True)[:20]
        extra_subreddits = [(_("suggestions:"), top_srs)]
        self.subreddit_selector = SubredditSelector(
            extra_subreddits=extra_subreddits, include_user_subscriptions=False)
        self.inventory = {}
        message = _("Create your ad on this page. Have questions? "
                    "Check out the [Help Center](%(help_center)s) "
                    "or [/r/selfserve](%(selfserve)s).")
        message %= {
            'help_center': 'https://reddit.zendesk.com/hc/en-us/categories/200352595-Advertising',
            'selfserve': 'https://www.reddit.com/r/selfserve'
        }
        self.infobar = RedditInfoBar(message=message)
        self.price_dict = PromotionPrices.get_price_dict(self.author)

        self.frequency_cap_min = g.frequency_cap_min

        self.ads_auction_enabled = feature.is_enabled('ads_auction')


class RenderableCampaign(Templated):
    def __init__(self, link, campaign, transaction, is_pending, is_live,
                 is_complete, is_edited_live, full_details=True,
                 hide_after_seen=False):
        self.link = link
        self.campaign = campaign

        self.ads_auction_enabled = feature.is_enabled('ads_auction')
        if self.ads_auction_enabled:
            self.is_auction = campaign.is_auction
        else:
            self.is_auction = False

        # Permission to edit is always granted when:
        # 1) Advertiser is sponsor
        # 2) Campaign is auction
        # 3) Auction is not enabled and campaign is not live
        if (c.user_is_sponsor or campaign.is_auction or
                (not self.ads_auction_enabled and not is_live)):
            self.editable = True
        else:
            self.editable = False

        # Convert total_budget_pennies to dollars for UI
        self.total_budget_dollars = campaign.total_budget_pennies / 100.

        if full_details:
            if not self.campaign.is_house and not self.campaign.is_auction:
                self.spent = promote.get_spent_amount(campaign)
            else:
                self.spent = campaign.adserver_spent_pennies / 100.
        else:
            self.spent = 0.

        self.paid = bool(transaction and not transaction.is_void())
        self.free = campaign.is_freebie()
        self.is_pending = is_pending
        self.is_live = is_live
        self.is_complete = is_complete
        self.is_edited_live = is_edited_live
        self.needs_refund = (is_complete and c.user_is_sponsor and
                             (transaction and not transaction.is_refund()) and
                             self.spent < campaign.total_budget_dollars)
        self.pay_url = promote.pay_url(link, campaign)
        sr_name = random.choice(campaign.target.subreddit_names)
        self.view_live_url = promote.view_live_url(link, campaign, sr_name)
        self.refund_url = promote.refund_url(link, campaign)

        if campaign.location:
            self.country = campaign.location.country or ''
            self.region = campaign.location.region or ''
            self.metro = campaign.location.metro or ''
        else:
            self.country, self.region, self.metro = '', '', ''
        self.location_str = campaign.location_str
        if campaign.target.is_collection:
            self.targeting_data = campaign.target.collection.name
        else:
            sr_name = campaign.target.subreddit_name
            # LEGACY: sponsored.js uses blank to indicate no targeting, meaning
            # targeted to the frontpage
            self.targeting_data = '' if sr_name == Frontpage.name else sr_name

        self.platform = campaign.platform
        self.mobile_os = campaign.mobile_os
        self.ios_devices = campaign.ios_devices
        self.ios_versions = campaign.ios_version_range
        self.android_devices = campaign.android_devices
        self.android_versions = campaign.android_version_range

        self.pause_ads_enabled = feature.is_enabled('pause_ads')

        # If ads_auction not enabled, default cost_basis to fixed_cpm
        if not feature.is_enabled('ads_auction'):
            self.cost_basis = PROMOTE_COST_BASIS.name[PROMOTE_COST_BASIS.fixed_cpm]
        elif campaign.cost_basis != PROMOTE_COST_BASIS.fixed_cpm:
            self.cost_basis = PROMOTE_COST_BASIS.name[campaign.cost_basis]
        else:
            self.cost_basis = PROMOTE_COST_BASIS.name[PROMOTE_COST_BASIS.cpm]
        self.bid_pennies = campaign.bid_pennies

        self.printable_bid = format_currency(campaign.bid_dollars, 'USD',
            locale=c.locale)

        Templated.__init__(self)

    @classmethod
    def from_campaigns(cls, link, campaigns,
                       full_details=True, hide_after_seen=False):
        campaigns, is_single = tup(campaigns, ret_is_single=True)

        if full_details:
            transactions = promote.get_transactions(link, campaigns)
            live_campaigns = promote.live_campaigns_by_link(link)
        else:
            transactions = {}
            live_campaigns = []

        ret = []
        now = promote.promo_datetime_now()
        for camp in campaigns:
            transaction = transactions.get(camp._id)
            is_pending = promote.is_pending(camp)
            is_live = camp in live_campaigns
            is_charged_or_refunded = (transaction and
                (transaction.is_charged() or transaction.is_refund()))
            is_expired_house = camp.is_house and camp.end_date < now
            is_live_or_pending = is_live or is_pending
            is_edited_live = promote.is_edited_live(link)
            is_complete = (not is_edited_live and
                (is_charged_or_refunded and
                not is_live_or_pending) or
                is_expired_house)
            rc = cls(link, camp, transaction, is_pending, is_live, is_complete,
                     is_edited_live, full_details, hide_after_seen)
            ret.append(rc)
        if is_single:
            return ret[0]
        else:
            return ret

    def render_html(self):
        return spaceCompress(self.render(style='html'))


class RefundPage(Reddit):
    def __init__(self, link, campaign):
        self.link = link
        self.campaign = campaign
        self.listing = wrap_links(link, skip=False)
        billable_impressions = promote.get_billable_impressions(campaign)
        billable_amount = promote.get_billable_amount(campaign,
                                                      billable_impressions)
        refund_amount = promote.get_refund_amount(campaign, billable_amount)
        self.billable_impressions = billable_impressions
        self.billable_amount = billable_amount
        self.refund_amount = refund_amount
        self.printable_total_budget = format_currency(
            campaign.total_budget_dollars, 'USD', locale=c.locale)
        self.printable_bid = format_currency(campaign.bid_dollars, 'USD',
            locale=c.locale)
        self.traffic_url = '/traffic/%s/%s' % (link._id36, campaign._id36)
        Reddit.__init__(self, title="refund", show_sidebar=False)

class PromotePost(PromoteLinkBase):
    def __init__(self):
        PromoteLinkBase.__init__(self)


class SponsorLookupUser(PromoteLinkBase):
    def __init__(self, id_user=None, email=None, email_users=None):
        PromoteLinkBase.__init__(
            self, id_user=id_user, email=email, email_users=email_users or [])




class SponsorLookupUser(PromoteLinkBase):
    def __init__(self, id_user=None, email=None, email_users=None):
        PromoteLinkBase.__init__(
            self, id_user=id_user, email=email, email_users=email_users or [])


class TabbedPane(Templated):
    def __init__(self, tabs, linkable=False):
        """Renders as tabbed area where you can choose which tab to
        render. Tabs is a list of tuples (tab_name, tab_pane)."""
        buttons = []
        for tab_name, title, pane in tabs:
            onclick = "return select_tab_menu(this, '%s')" % tab_name
            buttons.append(JsButton(title, tab_name=tab_name, onclick=onclick))

        self.tabmenu = JsNavMenu(buttons, type = 'tabmenu')
        self.tabs = tabs

        Templated.__init__(self, linkable=linkable)

class LinkChild(object):
    def __init__(self, link, load=False, expand=False, nofollow=False,
                 position_inline=False):
        self.link = link
        self.expand = expand
        self.load = load or expand
        self.nofollow = nofollow
        self.position_inline = position_inline

    def content(self):
        return ''

def make_link_child(item, show_media_preview=False):
    link_child = None
    editable = False
    expandable = getattr(item, 'expand_children', False)

    # if the item has a media_object, try to make a MediaEmbed for rendering
    if not c.secure:
        media_object = item.media_object
    else:
        media_object = item.secure_media_object

    if media_object:
        media_embed = None
        expand = False
        position_inline = False

        if isinstance(media_object, basestring):
            media_embed = media_object
        else:
            is_autoexpand_type = media_object.get('type') in g.autoexpand_media_types
            expand = expandable and (show_media_preview or is_autoexpand_type)
            position_inline = expandable and is_autoexpand_type

            try:
                media_embed = media.get_media_embed(media_object)
            except TypeError:
                g.log.warning("link %s has a bad media object" % item)
                media_embed = None

            if media_embed:
                if media_embed.sandbox:
                    should_authenticate = (item.subreddit.type in Subreddit.private_types or
                        item.subreddit.quarantine)
                    media_embed = MediaEmbed(
                        media_domain=g.media_domain,
                        height=media_embed.height + 10,
                        width=media_embed.width + 10,
                        scrolling=media_embed.scrolling,
                        id36=item._id36,
                        authenticated=should_authenticate,
                    )
                else:
                    media_embed = media_embed.content
            else:
                g.log.debug("media_object without media_embed %s" % item)

        if media_embed:
            link_child = MediaChild(item,
                                    media_embed,
                                    load=True,
                                    expand=expand,
                                    position_inline=position_inline)

    # if the item is_self, add a selftext child
    elif item.is_self:
        if not item.selftext: item.selftext = u''

        expand = expandable
        position_inline = expandable
        editable = (expand and
                    item.author == c.user and
                    not item._deleted)
        link_child = SelfTextChild(item,
                                   expand=expand,
                                   nofollow=item.nofollow,
                                   position_inline=position_inline)
    # if the item has a preview image and is on the whitelist, show it
    elif (feature.is_enabled('media_previews') and
            item.preview_object and
            media.allowed_media_preview_url(item.url)):
        media_object = media.get_preview_image(
            item.preview_object,
            include_censored=item.nsfw,
        )
        expand = show_media_preview and expandable

        if media_object:
            media_preview = MediaPreview(
                media_object=media_object,
                id36=item._id36,
                url=item.url,
            )
            link_child = MediaChild(
                item,
                media_preview,
                load=True,
                expand=expand,
                position_inline=False,
            )

    return link_child, editable


class MediaChild(LinkChild):
    """renders when the user hits the expando button to expand media
       objects, like embedded videos"""
    css_style = "video"
    def __init__(self, link, content, **kw):
        self._content = content
        LinkChild.__init__(self, link, **kw)

    def content(self):
        if isinstance(self._content, basestring):
            return self._content
        return self._content.render()

class MediaEmbed(Templated):
    """The actual rendered iframe for a media child"""

    def __init__(self, *args, **kwargs):
        authenticated = kwargs.pop("authenticated", False)
        if authenticated:
            mac = hmac.new(g.secrets["media_embed"], kwargs["id36"],
                           hashlib.sha1)
            self.credentials = "/" + mac.hexdigest()
        else:
            self.credentials = ""
        Templated.__init__(self, *args, **kwargs)


class MediaPreview(Templated):
    """Rendered html container for a media child"""

    def __init__(self, media_object, id36, url, **kwargs):
        self.media_content = media_object["content"]
        self.width = media_object["width"]
        self.id36 = id36
        self.url = url
        super(MediaPreview, self).__init__(**kwargs)


class SelfTextChild(LinkChild):
    css_style = "selftext"

    def content(self):
        u = UserText(self.link, self.link.selftext,
                     editable = c.user == self.link.author,
                     nofollow = self.nofollow,
                     expunged=self.link.expunged)
        return u.render()

class UserText(CachedTemplate):
    cachable = False

    def __init__(self,
                 item,
                 text = '',
                 have_form = True,
                 editable = False,
                 creating = False,
                 nofollow = False,
                 target = None,
                 display = True,
                 post_form = 'editusertext',
                 cloneable = False,
                 extra_css = '',
                 textarea_class = '',
                 name = "text",
                 expunged=False,
                 include_errors=True,
                 show_embed_help=False,
                 admin_takedown=False,
                 data_attrs={},
                 source=None,
                ):

        css_class = "usertext"
        if cloneable:
            css_class += " cloneable"
        if extra_css:
            css_class += " " + extra_css

        if text is None:
            text = ''

        # set the attribute for admin takedowns
        if getattr(item, 'admin_takedown', False):
            admin_takedown = True

        fullname = ''
        # Do not pass fullname on deleted things, unless we're admin
        if hasattr(item, '_fullname'):
            if not getattr(item, 'deleted', False) or c.user_is_admin:
                fullname = item._fullname

        CachedTemplate.__init__(self,
                                fullname = fullname,
                                text = text,
                                have_form = have_form,
                                editable = editable,
                                creating = creating,
                                nofollow = nofollow,
                                target = target,
                                display = display,
                                post_form = post_form,
                                cloneable = cloneable,
                                css_class = css_class,
                                textarea_class = textarea_class,
                                name = name,
                                expunged=expunged,
                                include_errors=include_errors,
                                show_embed_help=show_embed_help,
                                admin_takedown=admin_takedown,
                                data_attrs=data_attrs,
                                source=source,
                               )

class MediaEmbedBody(CachedTemplate):
    """What's rendered inside the iframe that contains media objects"""
    def render(self, *a, **kw):
        res = CachedTemplate.render(self, *a, **kw)
        return responsive(res, True)


class PaymentForm(Templated):
    countries = sorted({c['name'] for c in g.locations.values()})

    default_country = g.locations.get("US").get("name")

    def __init__(self, link, campaign, **kw):
        self.link = link
        self.duration = strings.time_label
        self.duration %= {'num': campaign.ndays,
                          'time': ungettext("day", "days", campaign.ndays)}
        self.start_date = campaign.start_date.strftime("%m/%d/%Y")
        self.end_date = campaign.end_date.strftime("%m/%d/%Y")
        self.campaign_id36 = campaign._id36
        self.budget = format_currency(campaign.total_budget_dollars, 'USD',
            locale=c.locale)
        Templated.__init__(self, **kw)


class Bookings(object):
    def __init__(self):
        self.subreddit = 0
        self.collection = 0

    def __add__(self, other):
        if isinstance(other, int) and other == 0:
            return self

        added = Bookings()
        added.subreddit = self.subreddit + other.subreddit
        added.collection = self.collection + other.collection

        return added

    def __radd__(self, other):
        return self.__add__(other)

    def __repr__(self):
        if self.subreddit and not self.collection:
            return format_number(self.subreddit)
        elif self.collection and not self.subreddit:
            return "%s*" % format_number(self.collection)
        elif not self.subreddit and not self.collection:
            return format_number(0)
        else:
            nums = tuple(map(format_number, (self.subreddit, self.collection)))
            return "%s (%s*)" % nums


class PromoteInventory(PromoteLinkBase):
    def __init__(self, start, end, target):
        Templated.__init__(self)
        self.start = start
        self.end = end
        self.default_start = start.strftime('%m/%d/%Y')
        self.default_end = end.strftime('%m/%d/%Y')
        self.target = target
        self.display_name = target.pretty_name
        p = request.GET.copy()
        self.csv_url = '%s.csv?%s' % (request.path, urlencode(p))
        if target.is_collection:
            self.sr_input = None
            self.collection_input = target.collection.name
            self.targeting_type = "collection"
        else:
            self.sr_input = target.subreddit_name
            self.collection_input = None
            self.targeting_type = "collection" if target.subreddit_name == Frontpage.name else "one"
        self.setup()

    def as_csv(self):
        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow(tuple(self.header))

        for row in self.rows:
            if not row.is_total:
                outrow = [row.info['author']]
            else:
                outrow = [row.info['title']]
            outrow.extend(row.columns)
            writer.writerow(outrow)

        return out.getvalue()

    def setup(self):
        srs = self.target.subreddits_slow
        campaigns_by_date = inventory.get_campaigns_by_date(
            srs, self.start, self.end)
        link_ids = {camp.link_id for camp
                    in chain.from_iterable(campaigns_by_date.itervalues())}
        links_by_id = Link._byID(link_ids, data=True)
        dates = inventory.get_date_range(self.start, self.end)
        total_by_date = {date: Bookings() for date in dates}
        imps_by_link = defaultdict(lambda: {date: Bookings() for date in dates})
        for date, campaigns in campaigns_by_date.iteritems():
            for camp in campaigns:
                link = links_by_id[camp.link_id]
                daily_impressions = camp.impressions / camp.ndays
                if camp.target.is_collection:
                    total_by_date[date].collection += daily_impressions
                    imps_by_link[link._id][date].collection += daily_impressions
                else:
                    total_by_date[date].subreddit += daily_impressions
                    imps_by_link[link._id][date].subreddit += daily_impressions

        account_ids = {link.author_id for link in links_by_id.itervalues()}
        accounts_by_id = Account._byID(account_ids, data=True)

        self.header = ['link'] + [date.strftime("%m/%d/%Y") for date in dates] + ['total']
        rows = []
        for link_id, imps_by_date in imps_by_link.iteritems():
            link = links_by_id[link_id]
            author = accounts_by_id[link.author_id]
            info = {
                'author': author.name,
                'edit_url': promote.promo_edit_url(link),
            }
            row = Storage(info=info, is_total=False)
            row.columns = ([str(imps_by_date[date]) for date in dates] +
                [str(sum(imps_by_date.values()))])
            rows.append(row)
        rows.sort(key=lambda row: row.info['author'].lower())

        total_row = Storage(
            info={'title': 'total'},
            is_total=True,
            columns=([str(total_by_date[date]) for date in dates] +
                [str(sum(total_by_date.values()))]),
        )
        rows.append(total_row)

        predicted_pageviews_by_sr = inventory.get_predicted_pageviews(srs)
        predicted_pageviews = sum(pageviews for pageviews
                                  in predicted_pageviews_by_sr.itervalues())
        predicted_row = Storage(
            info={'title': 'predicted'},
            is_total=True,
            columns=([format_number(predicted_pageviews) for date in dates] +
                [format_number(sum([predicted_pageviews for date in dates]))]),
        )
        rows.append(predicted_row)

        available_pageviews = inventory.get_available_pageviews(
            self.target, self.start, self.end)
        remaining_row = Storage(
            info={'title': 'remaining'},
            is_total=True,
            columns=([format_number(available_pageviews[date]) for date in dates] +
                [format_number(sum(available_pageviews.values()))]),
        )
        rows.append(remaining_row)

        self.rows = rows

        default_sr = None
        if not self.target.is_collection and self.sr_input:
            default_sr = Subreddit._by_name(self.sr_input)
        self.subreddit_selector = SubredditSelector(
                default_sr=default_sr,
                include_user_subscriptions=False)

        self.get_locations()
        self.get_collections()


ReportKey = namedtuple("ReportKey", ["date", "link", "campaign"])
ReportItem = namedtuple("ReportItem",
    ["bid", "fp_imps", "sr_imps", "fp_clicks", "sr_clicks"])


class PromoteReport(PromoteLinkBase):
    def __init__(self, links, link_text, owner_name, bad_links, start, end,
                 group_by_date=False):
        self.links = links
        self.start = start
        self.end = end
        self.default_start = start.strftime('%m/%d/%Y')
        self.default_end = end.strftime('%m/%d/%Y')
        self.group_by_date = group_by_date

        if links:
            self.make_report()
            p = request.GET.copy()
            self.csv_url = '%s.csv?%s' % (request.path, urlencode(p))
        else:
            self.link_report = []
            self.campaign_report = []
            self.csv_url = None

        Templated.__init__(self, link_text=link_text, owner_name=owner_name,
                           bad_links=bad_links)

    def as_csv(self):
        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow((_("start date"), self.start.strftime('%m/%d/%Y')))
        writer.writerow((_("end date"), self.end.strftime('%m/%d/%Y')))
        writer.writerow([])
        writer.writerow((_("links"),))
        if self.group_by_date:
            outrow = [_("date")]
        else:
            outrow = []
        outrow.extend([_("id"), _("owner"), _("url"), _("comments"),
            _("upvotes"), _("downvotes"), _("clicks"), _("impressions")])
        writer.writerow(outrow)
        for row in self.link_report:
            if self.group_by_date:
                outrow = [row['date']]
            else:
                outrow = []
            outrow.extend([row['id36'], row['owner'], row['url'],
                row['comments'], row['upvotes'], row['downvotes'],
                row['clicks'], row['impressions']])
            writer.writerow(outrow)

        writer.writerow([])
        writer.writerow((_("campaigns"),))
        if self.group_by_date:
            outrow = [_("date")]
        else:
            outrow = []
        outrow.extend([_("link id"), _("owner"), _("campaign id"), _("target"),
            _("bid"), _("frontpage clicks"), _("frontpage impressions"),
            _("subreddit clicks"), _("subreddit impressions"),
            _("total clicks"), _("total impressions")])
        writer.writerow(outrow)
        for row in self.campaign_report:
            if self.group_by_date:
                outrow = [row['date']]
            else:
                outrow = []
            outrow.extend([row['link'], row['owner'], row['campaign'],
                row['target'], row['bid'], row['fp_clicks'],
                row['fp_impressions'], row['sr_clicks'], row['sr_impressions'],
                row['total_clicks'], row['total_impressions']])
            writer.writerow(outrow)
        return out.getvalue()

    @classmethod
    def get_traffic(self, campaigns, start, end):
        campaigns_by_name = {camp._fullname: camp for camp in campaigns}
        codenames = campaigns_by_name.keys()

        start_date = start.date()
        ndays = (end - start).days
        dates = {start_date + datetime.timedelta(days=i) for i in xrange(ndays)}

        # traffic database uses datetimes with no timezone, also need to shift
        # start, end to account for campaigns launching at 12:00 EST
        start = (start - promote.timezone_offset).replace(tzinfo=None)
        end = (end - promote.timezone_offset).replace(tzinfo=None)

        # start and end are dates so we need to subtract an hour from end to
        # only include 24 hours per day
        end -= datetime.timedelta(hours=1)

        fp_imps_by_date = {d: defaultdict(int) for d in dates}
        sr_imps_by_date = {d: defaultdict(int) for d in dates}
        fp_clicks_by_date = {d: defaultdict(int) for d in dates}
        sr_clicks_by_date = {d: defaultdict(int) for d in dates}

        imps = traffic.TargetedImpressionsByCodename.campaign_history(
            codenames, start, end)
        clicks = traffic.TargetedClickthroughsByCodename.campaign_history(
            codenames, start, end)

        for date, codename, sr, (uniques, pageviews) in imps:
            # convert from utc hour to campaign date
            traffic_date = (date + promote.timezone_offset).date()

            if sr == '':
                # LEGACY: traffic uses '' to indicate Frontpage
                fp_imps_by_date[traffic_date][codename] += pageviews
            else:
                sr_imps_by_date[traffic_date][codename] += pageviews

        for date, codename, sr, (uniques, pageviews) in clicks:
            traffic_date = (date + promote.timezone_offset).date()

            if sr == '':
                # NOTE: clicks use hourly uniques
                fp_clicks_by_date[traffic_date][codename] += uniques
            else:
                sr_clicks_by_date[traffic_date][codename] += uniques

        traffic_by_key = {}
        for camp in campaigns:
            fullname = camp._fullname
            bid = camp.total_budget_pennies / max(camp.ndays, 1)
            camp_ndays = max(1, (camp.end_date - camp.start_date).days)
            camp_start = camp.start_date.date()
            days = xrange(camp_ndays)
            camp_dates = {camp_start + datetime.timedelta(days=i) for i in days}

            for date in camp_dates.intersection(dates):
                fp_imps = fp_imps_by_date[date][fullname]
                sr_imps = sr_imps_by_date[date][fullname]
                fp_clicks = fp_clicks_by_date[date][fullname]
                sr_clicks = sr_clicks_by_date[date][fullname]
                key = ReportKey(date, camp.link_id, camp._fullname)
                item = ReportItem(bid, fp_imps, sr_imps, fp_clicks, sr_clicks)
                traffic_by_key[key] = item
        return traffic_by_key

    def make_report(self):
        campaigns = PromoCampaign._by_link([link._id for link in self.links])
        campaigns = filter(promote.charged_or_not_needed, campaigns)
        traffic_by_key = self.get_traffic(campaigns, self.start, self.end)

        def group_and_combine(items_by_key, group_on=None):
            # combine all items whose keys have the same value for the
            # attributes in group_on, and create new keys with None values for
            # the attributes we aren't grouping on.
            by_group = defaultdict(list)
            for item_key, item in items_by_key.iteritems():
                attrs = [getattr(item_key, a) if a in group_on else None
                    for a in ReportKey._fields]
                group_key = ReportKey(*attrs)
                by_group[group_key].append(item)

            new_items_by_key = {}
            for group_key, items in by_group.iteritems():
                bid = fp_imps = sr_imps = fp_clicks = sr_clicks = 0
                for item in items:
                    bid += item.bid
                    fp_imps += item.fp_imps
                    sr_imps += item.sr_imps
                    fp_clicks += item.fp_clicks
                    sr_clicks += item.sr_clicks
                item = ReportItem(bid, fp_imps, sr_imps, fp_clicks, sr_clicks)
                new_items_by_key[group_key] = item
            return new_items_by_key

        # make the campaign report
        if not self.group_by_date:
            traffic_by_key = group_and_combine(
                traffic_by_key, group_on=["link", "campaign"])

        owners = Account._byID([link.author_id for link in self.links],
                               data=True)
        links_by_id = {link._id: link for link in self.links}
        camps_by_name = {camp._fullname: camp for camp in campaigns}

        self.campaign_report_totals = {
            'fp_clicks': 0,
            'fp_imps': 0,
            'sr_clicks': 0,
            'sr_imps': 0,
            'total_clicks': 0,
            'total_imps': 0,
            'bid': 0,
        }
        self.campaign_report = []
        for rk in sorted(traffic_by_key):
            item = traffic_by_key[rk]
            link = links_by_id[rk.link]
            camp = camps_by_name[rk.campaign]

            self.campaign_report_totals['fp_clicks'] += item.fp_clicks
            self.campaign_report_totals['fp_imps'] += item.fp_imps
            self.campaign_report_totals['sr_clicks'] += item.sr_clicks
            self.campaign_report_totals['sr_imps'] += item.sr_imps
            self.campaign_report_totals['bid'] += item.bid

            self.campaign_report.append({
                'date': rk.date,
                'link': link._id36,
                'owner': owners[link.author_id].name,
                'campaign': camp._id36,
                'target': camp.target.pretty_name,
                'bid': format_currency(item.bid, 'USD', locale=c.locale),
                'fp_impressions': item.fp_imps,
                'sr_impressions': item.sr_imps,
                'fp_clicks': item.fp_clicks,
                'sr_clicks': item.sr_clicks,
                'total_impressions': item.fp_imps + item.sr_imps,
                'total_clicks': item.fp_clicks + item.sr_clicks,
            })
        crt = self.campaign_report_totals
        crt['total_clicks'] = crt['sr_clicks'] + crt['fp_clicks']
        crt['total_imps'] = crt['sr_imps'] + crt['fp_imps']
        crt['bid'] = format_currency(crt['bid'], 'USD', locale=c.locale)
        # make the link report
        traffic_by_key = group_and_combine(
                traffic_by_key, group_on=["link", "date"])

        self.link_report = []
        for rk in sorted(traffic_by_key):
            item = traffic_by_key[rk]
            link = links_by_id[rk.link]
            self.link_report.append({
                'date': rk.date,
                'owner': owners[link.author_id].name,
                'id36': link._id36,
                'comments': link.num_comments,
                'upvotes': link._ups,
                'downvotes': link._downs,
                'clicks': item.fp_clicks + item.sr_clicks,
                'impressions': item.fp_imps + item.sr_imps,
                'url': link.url,
            })


class RawString(Templated):
   def __init__(self, s):
       self.s = s

   def render(self, *a, **kw):
       return unsafe(self.s)


class TryCompact(Reddit):
    def __init__(self, dest, **kw):
        dest = dest or "/"
        u = UrlParser(dest)
        u.set_extension("compact")
        self.compact = u.unparse()

        u.update_query(keep_extension = True)
        self.like = u.unparse()

        u.set_extension("mobile")
        self.mobile = u.unparse()
        Reddit.__init__(self, **kw)

class AccountActivityPage(BoringPage):
    def __init__(self):
        super(AccountActivityPage, self).__init__(_("account activity"))

    def content(self):
        return UserIPHistory()

class UserIPHistory(Templated):
    def __init__(self):
        self.my_apps = OAuth2Client._by_user_grouped(c.user)
        self.ips = ips_by_account_id(c.user._id)

        if not c.user_is_admin:
            self.ips = [
                ip
                for ip in self.ips
                if not ip_address(ip[0]).is_private
            ]
        super(UserIPHistory, self).__init__()

class ApiHelp(Templated):
    def __init__(self, api_docs, *a, **kw):
        self.api_docs = api_docs
        super(ApiHelp, self).__init__(*a, **kw)


class AwardReceived(Templated):
    pass

class ConfirmAwardClaim(Templated):
    pass

class TimeSeriesChart(Templated):
    def __init__(self, id, title, interval, columns, rows,
                 latest_available_data=None, classes=[],
                 make_period_link=None):
        self.id = id
        self.title = title
        self.interval = interval
        self.columns = columns
        self.rows = rows
        self.latest_available_data = (latest_available_data or
                                      datetime.datetime.utcnow())
        self.classes = " ".join(classes)
        self.make_period_link = make_period_link

        Templated.__init__(self)

class InterestBar(Templated):
    def __init__(self, has_subscribed):
        self.has_subscribed = has_subscribed
        Templated.__init__(self)

class Goldvertisement(Templated):
    def __init__(self):
        now = datetime.datetime.now(GOLD_TIMEZONE)
        today = now.date()
        tomorrow = now + datetime.timedelta(days=1)
        end_time = tomorrow.replace(hour=0, minute=0, second=0, microsecond=0)
        revenue_today = gold_revenue_volatile(today)
        yesterday = today - datetime.timedelta(days=1)
        revenue_yesterday = gold_revenue_steady(yesterday)
        revenue_goal = gold_goal_on(today)
        revenue_goal_yesterday = gold_goal_on(yesterday)

        if revenue_goal:
            self.percent_filled = int((revenue_today / revenue_goal) * 100)
        else:
            self.percent_filled = 0

        if revenue_goal_yesterday:
            self.percent_filled_yesterday = int((revenue_yesterday /
                                                 revenue_goal_yesterday) * 100)
        else:
            self.percent_filled_yesterday = 0

        seconds = get_current_value_of_month()
        delta = datetime.timedelta(seconds=seconds)
        self.hours_paid = precise_format_timedelta(
            delta, threshold=5, locale=c.locale)

        self.time_left_today = timeuntil(end_time, precision=60)
        if c.user.employee:
            self.goal_today = revenue_goal / 100.0
            self.goal_yesterday = revenue_goal_yesterday / 100.0

        if c.user_is_loggedin:
            self.default_type = "autorenew"
        else:
            self.default_type = "code"

        Templated.__init__(self)

class LinkCommentsSettings(Templated):
    def __init__(self, link, sort, suggested_sort):
        Templated.__init__(self)
        self.sr = link.subreddit_slow
        self.link = link
        self.is_author = c.user_is_loggedin and c.user._id == link.author_id
        self.contest_mode = link.contest_mode
        self.stickied = link.is_stickied(self.sr)
        self.stickies_full = self.sr.has_max_stickies
        self.sendreplies = link.sendreplies
        self.can_edit = (
            c.user_is_loggedin and
            (c.user_is_admin or
                self.sr.is_moderator_with_perms(c.user, "posts"))
        )
        self.can_sticky = False
        if self.can_edit:
            if self.stickied:
                # always allow un-stickying things
                self.can_sticky = True
            # non deleted/spam self-posts by mods are eligible for stickying
            else:
                self.can_sticky = link.is_stickyable()
        self.sort = sort
        self.suggested_sort = suggested_sort

class ModeratorPermissions(Templated):
    def __init__(self, user, permissions_type, permissions,
                 editable=False, embedded=False):
        self.user = user
        self.permissions = permissions
        Templated.__init__(self, permissions_type=permissions_type,
                           editable=editable, embedded=embedded)

    def items(self):
        return self.permissions.iteritems()

class ListingChooser(Templated):
    def __init__(self):
        Templated.__init__(self)
        self.sections = defaultdict(list)
        self.add_item("global", _("subscribed"), site=Frontpage,
                      description=_("your front page"))
        self.add_item("global", _("explore"), path="/explore")
        if c.user_is_loggedin and c.user.gold:
            self.add_item("other", _("everything"),
                          path="/me/f/all",
                          extra_class="gold-perks",
                          description=_("from all subreddits"))
        else:
            self.add_item("other", _("everything"), site=All,
                          description=_("from all subreddits"))
        if c.user_is_loggedin and c.user.is_moderator_somewhere:
            self.add_item("other", _("moderating"), site=Mod,
                          description=_("subreddits you mod"))

        self.add_item("other", _("saved"), path='/user/%s/saved' % c.user.name)

        gold_multi = g.live_config["listing_chooser_gold_multi"]
        if c.user_is_loggedin and c.user.gold and gold_multi:
            self.add_item("other", name=_("gold perks"), path=gold_multi,
                          extra_class="gold-perks")

        self.show_samples = False
        if c.user_is_loggedin:
            multis = LabeledMulti.by_owner(c.user, load_subreddits=False)
            multis.sort(key=lambda multi: multi.name.lower())
            for multi in multis:
                if not multi.is_hidden():
                    self.add_item("multi", multi.name, site=multi)

            explore_sr = g.live_config["listing_chooser_explore_sr"]
            if explore_sr:
                sr = Subreddit._by_name(explore_sr, stale=True)
                self.add_item("multi", name=_("explore multis"), site=sr)

            self.show_samples = not multis

        if self.show_samples:
            self.add_samples()

        self.selected_item = self.find_selected()
        if self.selected_item:
            self.selected_item["selected"] = True

    def add_item(self, section, name, path=None, site=None, description=None,
                 extra_class=None):
        self.sections[section].append({
            "name": name,
            "description": description,
            "path": path or site.user_path,
            "site": site,
            "selected": False,
            "extra_class": extra_class,
        })

    def add_samples(self):
        for path in g.live_config["listing_chooser_sample_multis"]:
            self.add_item(
                section="sample",
                name=path.rpartition('/')[2],
                path=path,
            )

    def find_selected(self):
        path = request.path
        matching = []
        for item in chain(*self.sections.values()):
            if item["site"]:
                if item["site"] == c.site:
                    matching.append(item)
            elif path.startswith(item["path"]):
                matching.append(item)

        matching.sort(key=lambda item: len(item["path"]), reverse=True)
        return matching[0] if matching else None

class PolicyView(Templated):
    pass


class PolicyPage(BoringPage):
    css_class = 'policy-page'
    show_infobar = False

    def __init__(self, pagename=None, content=None, **kw):
        BoringPage.__init__(self, pagename=pagename, show_sidebar=False,
            content=content, **kw)
        self.welcomebar = None

    def build_toolbars(self):
        toolbars = BoringPage.build_toolbars(self)
        policies_buttons = [
            NavButton(_('privacy policy'), '/privacypolicy'),
            NavButton(_('user agreement'), '/useragreement'),
            NavButton(_('content policy'), '/contentpolicy'),
        ]
        policies_menu = NavMenu(policies_buttons, type='tabmenu',
                                base_path='/help')
        toolbars.append(policies_menu)
        return toolbars


class GoogleTagManagerJail(Templated):
    pass


class GoogleTagManager(Templated):
    pass


class Newsletter(BoringPage):
    extra_page_classes = ['newsletter']

    def __init__(self, pagename=None, content=None, **kw):
        BoringPage.__init__(self, pagename=pagename, show_sidebar=False,
                            content=content, **kw)


class SubscribeButton(Templated):
    def __init__(self, sr, bubble_class=None):
        Templated.__init__(self)
        self.sr = sr
        self.data_attrs = {"sr_name": sr.name}
        if bubble_class:
            self.data_attrs["bubble_class"] = bubble_class


class QuarantineOptoutButton(Templated):
    def __init__(self, sr, bubble_class=None):
        Templated.__init__(self)
        self.sr = sr
        self.data_attrs = {"sr_name": sr.name}
        if bubble_class:
            self.data_attrs["bubble_class"] = bubble_class


class SubredditSelector(Templated):
    def __init__(self, default_sr=None, extra_subreddits=None, required=False,
                 include_searches=True, include_user_subscriptions=True, class_name=None,
                 placeholder=None, show_add=False):
        Templated.__init__(self)

        self.placeholder = placeholder
        self.class_name = class_name
        self.show_add = show_add

        if extra_subreddits:
            self.subreddits = extra_subreddits
        else:
            self.subreddits = []

        if include_user_subscriptions:
            self.subreddits.append((
                _('your subscribed subreddits'),
                Subreddit.user_subreddits(c.user, ids=False)
            ))

        self.default_sr = default_sr
        self.required = required
        if include_searches:
            self.sr_searches = simplejson.dumps(
                popular_searches(include_over_18=c.over18)
            )
        else:
            self.sr_searches = simplejson.dumps({})
        self.include_searches = include_searches

    @property
    def subreddit_names(self):
        groups = []
        for title, subreddits in self.subreddits:
            names = [sr.name for sr in subreddits if sr.can_submit(c.user)]
            names.sort(key=str.lower)
            groups.append((title, names))
        return groups


class ListingSuggestions(Templated):
    def __init__(self):
        Templated.__init__(self)

        self.suggestion_type = None
        if c.default_sr:
            if c.user_is_loggedin and random.randint(0, 1) == 1:
                self.suggestion_type = "explore"
                return

            if c.user_is_loggedin:
                multis = LabeledMulti.by_owner(c.user, load_subreddits=False)
            else:
                multis = []

            if multis and c.site in multis:
                multis.remove(c.site)

            if multis:
                self.suggestion_type = "multis"
                if len(multis) <= 3:
                    self.suggestions = multis
                else:
                    self.suggestions = random.sample(multis, 3)
            else:
                self.suggestion_type = "random"


class UnreadMessagesSuggestions(Templated):
    """Let a user mark all as read if they have > 1 page of unread messages."""
    pass


class ExploreItem(Templated):
    """For managing recommended content."""

    def __init__(self, item_type, rec_src, sr, link, comment=None):
        """Constructor.

        item_type - string that helps templates know how to render this item.
        rec_src - code that lets us track where the rec originally came from,
            useful for comparing performance of data sources or algorithms
        sr and link are required
        comment is optional

        See r2.lib.recommender for valid values of item_type and rec_src.

        """
        self.sr = sr
        self.link = link
        self.comment = comment
        self.type = item_type
        self.src = rec_src
        Templated.__init__(self)

    def is_over18(self):
        return self.sr.over_18 or self.link.is_nsfw


class ExploreItemListing(Templated):
    def __init__(self, recs, settings):
        self.things = []
        self.settings = settings
        if recs:
            links, srs = zip(*[(rec.link, rec.sr) for rec in recs])
            wrapped_links = {l._id: l for l in wrap_links(links).things}
            wrapped_srs = {sr._id: sr for sr in wrap_things(*srs)}
            for rec in recs:
                if rec.link._id in wrapped_links:
                    rec.link = wrapped_links[rec.link._id]
                    rec.sr = wrapped_srs[rec.sr._id]
                    self.things.append(rec)
        Templated.__init__(self)


class TrendingSubredditsBar(Templated):
    def __init__(self, subreddit_names, comment_url, comment_count):
        Templated.__init__(self)
        self.subreddit_names = subreddit_names
        self.comment_url = comment_url
        self.comment_count = comment_count
        self.comment_label, self.comment_label_cls = \
            comment_label(comment_count)


class GeotargetNotice(Templated):
    def __init__(self, city_target=False):
        self.targeting_level = "city" if city_target else "country"
        if city_target:
            text = _("this promoted link uses city level targeting and may "
                     "have been shown to you because of your location. "
                     "([learn more](%(link)s))")
        else:
            text = _("this promoted link uses country level targeting and may "
                     "have been shown to you because of your location. "
                     "([learn more](%(link)s))")
        more_link = "/wiki/targetingbycountrycity"
        self.text = text % {"link": more_link}
        Templated.__init__(self)


class ShareClose(Templated):
    pass
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pages import *
from admin_pages import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.config import feature
from r2.lib.db.thing import NotFound
from r2.lib.menus import (
  JsButton,
  NavButton,
  NavMenu,
  Styled,
)
from r2.lib.wrapped import Wrapped
from r2.models import Comment, LinkListing, Link, Message, PromotedLink, Report
from r2.models import IDBuilder, Thing
from r2.lib.utils import tup
from r2.lib.strings import Score
from r2.lib.promote import *
from datetime import datetime
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _, ungettext

class PrintableButtons(Styled):
    cachable = False

    def __init__(self, style, thing,
                 show_delete = False, show_report = True,
                 show_distinguish = False, show_lock = False,
                 show_unlock = False, show_marknsfw = False,
                 show_unmarknsfw = False, is_link=False,
                 show_flair=False, show_rescrape=False,
                 show_givegold=False, show_sticky_comment=False,
                 **kw):
        show_ignore = thing.show_reports
        approval_checkmark = getattr(thing, "approval_checkmark", None)
        show_approve = (thing.show_spam or show_ignore or
                        (is_link and approval_checkmark is None)) and not thing._deleted

        Styled.__init__(self, style = style,
                        thing = thing,
                        fullname = thing._fullname,
                        can_ban = thing.can_ban and not thing._deleted,
                        show_spam = thing.show_spam,
                        show_reports = thing.show_reports,
                        show_ignore = show_ignore,
                        approval_checkmark = approval_checkmark,
                        show_delete = show_delete,
                        show_approve = show_approve,
                        show_report = show_report,
                        show_distinguish = show_distinguish,
                        show_sticky_comment=show_sticky_comment,
                        show_lock = show_lock,
                        show_unlock = show_unlock,
                        show_marknsfw = show_marknsfw,
                        show_unmarknsfw = show_unmarknsfw,
                        show_flair = show_flair,
                        show_rescrape=show_rescrape,
                        show_givegold=show_givegold,
                        **kw)
        
class BanButtons(PrintableButtons):
    def __init__(self, thing,
                 show_delete = False, show_report = True):
        PrintableButtons.__init__(self, "banbuttons", thing)

class LinkButtons(PrintableButtons):
    def __init__(self, thing, comments = True, delete = True, report = True):
        # is the current user the author?
        is_author = (c.user_is_loggedin and thing.author and
                     c.user.name == thing.author.name)
        # do we show the report button?
        show_report = not is_author and not thing._deleted and report

        show_share = ((c.user_is_loggedin or not g.read_only_mode) and
                      not thing.subreddit.quarantine and
                      not thing.disable_comments and
                      not thing._deleted)

        # if they are the author, can they edit it?
        thing_editable = getattr(thing, 'editable', True)
        thing_takendown = getattr(thing, 'admin_takedown', False)
        editable = is_author and thing_editable and not thing_takendown

        show_lock = show_unlock = False
        lockable = thing.can_ban and not thing.archived
        if lockable:
            show_lock = not thing.locked
            show_unlock = not show_lock

        show_marknsfw = show_unmarknsfw = False
        show_rescrape = False
        if thing.can_ban or is_author or (thing.promoted and c.user_is_sponsor):
            if not thing.nsfw:
                show_marknsfw = True
            else:
                show_unmarknsfw = True

            if (not thing.is_self and
                    not (thing.has_thumbnail or thing.media_object)):
                show_rescrape = True

        show_givegold = thing.can_gild and (c.permalink_page or c.profilepage)

        # do we show the delete button?
        show_delete = is_author and delete and not thing._deleted
        # disable the delete button for live sponsored links
        if (is_promoted(thing) and not c.user_is_sponsor):
            show_delete = False

        # do we show the distinguish button? among other things,
        # we never want it to appear on link listings -- only
        # comments pages
        show_distinguish = (is_author and
                            (thing.can_ban or  # Moderator distinguish
                             c.user.employee or  # Admin distinguish
                             c.user_special_distinguish)
                            and getattr(thing, "expand_children", False))

        permalink = thing.permalink

        kw = {}
        if thing.promoted is not None:
            if getattr(thing, "campaign", False):
                permalink = update_query(permalink, {
                    "campaign": thing.campaign,
                })

            now = datetime.now(g.tz)
            kw = dict(promo_url = promo_edit_url(thing),
                      promote_status = getattr(thing, "promote_status", 0),
                      user_is_sponsor = c.user_is_sponsor,
                      traffic_url = promo_traffic_url(thing),
                      is_author = thing.is_author,
                      )

            if c.user_is_sponsor:
                kw["is_awaiting_fraud_review"] = is_awaiting_fraud_review(thing)
                kw["payment_flagged_reason"] = thing.payment_flagged_reason
                kw["hide_after_seen"] = getattr(thing, "hide_after_seen", False)
                kw["show_approval"] = thing.promoted and not thing._deleted

        PrintableButtons.__init__(self, 'linkbuttons', thing, 
                                  # user existence and preferences
                                  is_loggedin = c.user_is_loggedin,
                                  # comment link params
                                  comment_label = thing.comment_label,
                                  commentcls = thing.commentcls,
                                  permalink  = permalink,
                                  # button visibility
                                  saved = thing.saved,
                                  editable = editable, 
                                  deleted = thing._deleted,
                                  hidden = thing.hidden, 
                                  ignore_reports = thing.ignore_reports,
                                  show_delete = show_delete,
                                  show_report = show_report and c.user_is_loggedin,
                                  mod_reports=thing.mod_reports,
                                  user_reports=thing.user_reports,
                                  show_distinguish = show_distinguish,
                                  distinguished=thing.distinguished,
                                  show_lock = show_lock,
                                  show_unlock = show_unlock,
                                  show_marknsfw = show_marknsfw,
                                  show_unmarknsfw = show_unmarknsfw,
                                  show_flair = thing.can_flair,
                                  show_rescrape=show_rescrape,
                                  show_givegold=show_givegold,
                                  show_comments = comments,
                                  show_share=show_share,
                                  # promotion
                                  promoted = thing.promoted,
                                  is_link = True,
                                  **kw)

class CommentButtons(PrintableButtons):
    def __init__(self, thing, delete = True, report = True):
        # is the current user the author?
        is_author = thing.is_author

        # if they are the author, can they edit it?
        thing_editable = getattr(thing, 'editable', True)
        thing_takendown = getattr(thing, 'admin_takedown', False)
        editable = is_author and thing_editable and not thing_takendown

        # do we show the report button?
        show_report = not is_author and report and thing.can_reply
        # do we show the delete button?
        show_delete = is_author and delete and not thing._deleted
        suppress_reply_buttons = getattr(thing, 'suppress_reply_buttons', False)

        if thing.link.is_archived(thing.subreddit):
            suppress_reply_buttons = True

        show_distinguish = (is_author and
                            (thing.can_ban or  # Moderator distinguish
                             c.user.employee or  # Admin distinguish
                             c.user_special_distinguish))

        show_sticky_comment = (feature.is_enabled('sticky_comments') and
                               thing.is_stickyable and
                               is_author and
                               thing.can_ban)

        show_givegold = thing.can_gild

        embed_button = False
        
        show_admin_context = c.user_is_admin

        if thing.can_embed:
            embed_button = JsButton("embed",
                css_class="embed-comment",
                data={
                    "media": g.media_domain or g.domain,
                    "comment": thing.permalink,
                    "link": thing.link.make_permalink(thing.subreddit),
                    "title": thing.link.title,
                    "root": ("true" if thing.parent_id is None else "false"),
                })

            embed_button.build()

        PrintableButtons.__init__(self, "commentbuttons", thing,
                                  is_author = is_author, 
                                  profilepage = c.profilepage,
                                  permalink = thing.permalink,
                                  saved = thing.saved,
                                  editable = editable,
                                  ignore_reports = thing.ignore_reports,
                                  full_comment_path = thing.full_comment_path,
                                  full_comment_count = thing.full_comment_count,
                                  deleted = thing.deleted,
                                  parent_permalink = thing.parent_permalink, 
                                  can_reply = thing.can_reply,
                                  locked = thing.link.locked,
                                  suppress_reply_buttons = suppress_reply_buttons,
                                  show_report=show_report,
                                  mod_reports=thing.mod_reports,
                                  user_reports=thing.user_reports,
                                  show_distinguish = show_distinguish,
                                  distinguished=thing.distinguished,
                                  show_sticky_comment=show_sticky_comment,
                                  show_delete = show_delete,
                                  show_givegold=show_givegold,
                                  embed_button=embed_button,
                                  show_admin_context=show_admin_context,
        )

class MessageButtons(PrintableButtons):
    def __init__(self, thing, delete = False, report = True):
        was_comment = getattr(thing, 'was_comment', False)
        permalink = thing.permalink
        # don't allow replying to self unless it's modmail
        valid_recipient = (thing.author_id != c.user._id or
                           thing.sr_id)

        can_reply = (c.user_is_loggedin and
                     getattr(thing, "repliable", True) and
                     valid_recipient)
        can_block = True
        can_mute = False
        is_admin_message = False
        show_distinguish = c.user.employee and c.user._id == thing.author_id
        del_on_recipient = (isinstance(thing, Message) and
                            thing.del_on_recipient)

        if not was_comment:
            first_message = thing
            if getattr(thing, 'first_message', False):
                first_message = Message._byID(thing.first_message, data=True)

            if thing.sr_id:
                sr = thing.subreddit_slow
                is_admin_message = '/r/%s' % sr.name == g.admin_message_acct

                if (sr.is_muted(first_message.author_slow) or
                        (first_message.to_id and
                            sr.is_muted(first_message.recipient_slow))):
                    can_reply = False

                can_mute = sr.can_mute(c.user, thing.author_slow)

        if not was_comment and thing.display_author:
            can_block = False

        if was_comment:
            link = thing.link_slow
            if link.is_archived(thing.subreddit) or link.locked:
                can_reply = False

        # Allow comment-reply messages to have links to the full thread.
        if was_comment:
            self.full_comment_path = thing.link_permalink
            self.full_comment_count = thing.full_comment_count

        PrintableButtons.__init__(self, "messagebuttons", thing,
                                  profilepage = c.profilepage,
                                  permalink = permalink,
                                  was_comment = was_comment,
                                  unread = thing.new,
                                  user_is_recipient = thing.user_is_recipient,
                                  can_reply = can_reply,
                                  parent_id = getattr(thing, "parent_id", None),
                                  show_report = True,
                                  show_delete = False,
                                  can_block = can_block,
                                  can_mute = can_mute,
                                  is_admin_message = is_admin_message,
                                  del_on_recipient=del_on_recipient,
                                  show_distinguish=show_distinguish,
                                  distinguished=thing.distinguished,
                                 )


def make_wrapper(parent_wrapper = Wrapped, **params):
    def wrapper_fn(thing):
        w = parent_wrapper(thing)
        for k, v in params.iteritems():
            setattr(w, k, v)
        return w
    return wrapper_fn


# formerly ListingController.builder_wrapper
def default_thing_wrapper(**params):
    def _default_thing_wrapper(thing):
        w = Wrapped(thing)
        style = params.get('style', c.render_style)
        if isinstance(thing, Link):
            if thing.promoted is not None:
                w.render_class = PromotedLink
            elif style == 'htmllite':
                w.score_fmt = Score.safepoints
            w.should_incr_counts = style != 'htmllite'
        return w
    params['parent_wrapper'] = _default_thing_wrapper
    return make_wrapper(**params)

# TODO: move this into lib somewhere?
def wrap_links(links, wrapper = default_thing_wrapper(),
               listing_cls = LinkListing, 
               num = None, show_nums = False, nextprev = False, **kw):
    links = tup(links)
    if not all(isinstance(x, basestring) for x in links):
        links = [x._fullname for x in links]
    b = IDBuilder(links, num = num, wrap = wrapper, **kw)
    l = listing_cls(b, nextprev = nextprev, show_nums = show_nums)
    return l.listing()


def hot_links_by_url_listing(url, sr=None, num=None, **kw):
    try:
        links_for_url = Link._by_url(url, sr)
    except NotFound:
        links_for_url = []

    links_for_url.sort(key=lambda link: link._hot, reverse=True)
    listing = wrap_links(links_for_url, num=num, **kw)
    return listing


def wrap_things(*things):
    """Instantiate Wrapped for each thing, calling add_props if available."""
    if not things:
        return []

    wrapped = [Wrapped(thing) for thing in things]
    if hasattr(things[0], 'add_props'):
        # assume all things are of the same type and use the first thing's
        # add_props to process the list.
        things[0].add_props(c.user, wrapped)
    return wrapped
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.pages.pages import (
    AutoModeratorConfig,
    RawCode,
    Reddit,
    SubredditStylesheetSource,
)
from pylons import tmpl_context as c
from r2.lib.wrapped import Templated
from r2.lib.menus import PageNameNav
from r2.lib.validator.wiki import this_may_revise
from r2.lib.filters import wikimarkdown, safemarkdown
from pylons.i18n import _

class WikiView(Templated):
    def __init__(self, content, edit_by, edit_date, may_revise=False,
                 page=None, diff=None, renderer='wiki'):
        self.page_content_md = content
        if renderer == 'wiki':
            self.page_content = wikimarkdown(content)
        elif renderer == 'reddit':
            self.page_content = safemarkdown(content)
        elif renderer == 'stylesheet':
            self.page_content = SubredditStylesheetSource(content).render()
        elif renderer == "automoderator":
            self.page_content = AutoModeratorConfig(content).render()
        elif renderer == "rawcode":
            self.page_content = RawCode(content).render()

        self.renderer = renderer
        self.page = page
        self.diff = diff
        self.edit_by = edit_by
        self.may_revise = may_revise
        self.edit_date = edit_date
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiPageNotFound(Templated):
    def __init__(self, page):
        self.page = page
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiPageListing(Templated):
    def __init__(self, pages, linear_pages, page=None):
        self.pages = pages
        self.page = page
        self.linear_pages = linear_pages
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiEditPage(Templated):
    def __init__(self, page_content='', previous='', page=None):
        self.page_content = page_content
        self.page = page
        self.previous = previous
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiPageSettings(Templated):
    def __init__(self, settings, mayedit, show_editors=True,
                 show_settings=True, page=None, **context):
        self.permlevel = settings['permlevel']
        self.listed = settings['listed']
        self.show_settings = show_settings
        self.show_editors = show_editors
        self.page = page
        self.base_url = c.wiki_base_url
        self.mayedit = mayedit
        Templated.__init__(self)

class WikiPageRevisions(Templated):
    def __init__(self, revisions, page=None):
        self.listing = revisions
        self.page = page
        Templated.__init__(self)

class WikiPageDiscussions(Templated):
    def __init__(self, listing, page=None):
        self.listing = listing
        self.page = page
        Templated.__init__(self)

class WikiBasePage(Reddit):
    extra_page_classes = ['wiki-page']
    
    def __init__(self, content, page=None, may_revise=False,
                 actionless=False, alert=None, description=None, 
                 showtitle=False, **context):
        pageactions = []
        if not actionless and page:
            pageactions += [(page, _("view"), False, 'wikiview')]
            if may_revise:
                pageactions += [('edit', _("edit"), True, 'wikiedit')]
            pageactions += [('revisions/%s' % page, _("history"), False, 'wikirevisions')]
            pageactions += [('discussions', _("talk"), True, 'wikidiscussions')]
            if c.is_wiki_mod and may_revise:
                pageactions += [('settings', _("settings"), True, 'wikisettings')]

        action = context.get('wikiaction', (page, 'wiki'))
        
        if alert:
            context['infotext'] = alert
        elif c.wikidisabled:
            context['infotext'] = _("this wiki is currently disabled, only mods may interact with this wiki")
        
        self.pageactions = pageactions
        self.page = page
        self.base_url = c.wiki_base_url
        self.action = action
        self.description = description
        
        if showtitle:
            self.pagetitle = action[1]
        else:
            self.pagetitle = None

        page_classes = None

        if page and "title" not in context:
            context["title"] = _("%(page)s - %(site)s") % {
                "site": c.site.name,
                "page": page}
            page_classes = ['wiki-page-%s' % page.replace('/', '-')]

        Reddit.__init__(self, extra_js_config={'wiki_page': page}, 
                        show_wiki_actions=True, page_classes=page_classes,
                        content=content, short_title=page, **context)

    def content(self):
        return self._content

class WikiPageView(WikiBasePage):
    def __init__(self, content, page, diff=None, renderer='wiki', **context):
        may_revise = context.get('may_revise')
        if not content and not context.get('alert'):
            if may_revise:
                context['alert'] = _("this page is empty, edit it to add some content.")
        content = WikiView(content, context.get('edit_by'), context.get('edit_date'), 
                           may_revise=may_revise, page=page, diff=diff, renderer=renderer)
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiNotFound(WikiBasePage):
    def __init__(self, page, **context):
        content = WikiPageNotFound(page)
        context['alert'] = _("page %s does not exist in this subreddit") % page
        context['actionless'] = True
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiCreate(WikiBasePage):
    def __init__(self, page, **context):
        context['alert'] = _("page %s does not exist in this subreddit") % page
        context['actionless'] = True
        content = WikiEditPage(page=page)
        WikiBasePage.__init__(self, content, page, **context)

class WikiEdit(WikiBasePage):
    def __init__(self, content, previous, page, **context):
        content = WikiEditPage(content, previous, page)
        context['wikiaction'] = ('edit', _("editing"))
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiSettings(WikiBasePage):
    def __init__(self, settings, mayedit, page, restricted, **context):
        content = WikiPageSettings(settings, mayedit, page=page, **context)
        if restricted:
            context['alert'] = _("This page is restricted, only moderators may edit it.")
        context['wikiaction'] = ('settings', _("settings"))
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiRevisions(WikiBasePage):
    def __init__(self, revisions, page, **context):
        content = WikiPageRevisions(revisions, page)
        context['wikiaction'] = ('revisions/%s' % page, _("revisions"))
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiRecent(WikiBasePage):
    def __init__(self, revisions, **context):
        content = WikiPageRevisions(revisions)
        context['wikiaction'] = ('revisions', _("Viewing recent revisions for /r/%s") % c.wiki_id)
        WikiBasePage.__init__(self, content, showtitle=True, **context)

class WikiListing(WikiBasePage):
    def __init__(self, pages, linear_pages, **context):
        content = WikiPageListing(pages, linear_pages)
        context['wikiaction'] = ('pages', _("Viewing pages for /r/%s") % c.wiki_id)
        description = [_("Below is a list of pages in this wiki visible to you in this subreddit.")]
        WikiBasePage.__init__(self, content, description=description, showtitle=True, **context)

class WikiDiscussions(WikiBasePage):
    def __init__(self, listing, page, **context):
        content = WikiPageDiscussions(listing, page)
        context['wikiaction'] = ('discussions', _("discussions"))
        description = [_("Discussions are site-wide links to this wiki page."),
                       _("Submit a link to this wiki page or see other discussions about this wiki page.")]
        WikiBasePage.__init__(self, content, page=page, description=description, **context)

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import config
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import N_

from r2.lib.wrapped import Templated
from r2.lib.pages import LinkInfoBar, Reddit
from r2.lib.menus import (
    NamedButton,
    NavButton,
    menu,
    NavMenu,
    OffsiteButton,
)
from r2.lib.utils import timesince

def admin_menu(**kwargs):
    buttons = [
        OffsiteButton("traffic", "/traffic"),
        NavButton(menu.awards, "awards"),
        NavButton(menu.errors, "error log"),
    ]

    admin_menu = NavMenu(buttons, title='admin tools', base_path='/admin',
                         type="lightdrop", **kwargs)
    return admin_menu

class AdminSidebar(Templated):
    def __init__(self, user):
        Templated.__init__(self)
        self.user = user


class SponsorSidebar(Templated):
    def __init__(self, user):
        Templated.__init__(self)
        self.user = user


class Details(Templated):
    def __init__(self, link, *a, **kw):
        Templated.__init__(self, *a, **kw)
        self.link = link


class AdminPage(Reddit):
    create_reddit_box  = False
    submit_box         = False
    extension_handling = False
    show_sidebar = False

    def __init__(self, nav_menus = None, *a, **kw):
        Reddit.__init__(self, nav_menus = nav_menus, *a, **kw)

class AdminProfileMenu(NavMenu):
    def __init__(self, path):
        NavMenu.__init__(self, [], base_path = path,
                         title = 'admin', type="tabdrop")


class AdminLinkMenu(NavMenu):
    def __init__(self, link):
        NavMenu.__init__(self, [], title='admin', type="tabdrop")


class AdminNotesSidebar(Templated):
    EMPTY_MESSAGE = {
        "domain": N_("No notes for this domain"),
        "ip": N_("No notes for this IP address"),
        "subreddit": N_("No notes for this subreddit"),
        "user": N_("No notes for this user"),
    }

    SYSTEMS = {
        "domain": N_("domain"),
        "ip": N_("IP address"),
        "subreddit": N_("subreddit"),
        "user": N_("user"),
    }

    def __init__(self, system, subject):
        from r2.models.admin_notes import AdminNotesBySystem

        self.system = system
        self.subject = subject
        self.author = c.user.name
        self.notes = AdminNotesBySystem.in_display_order(system, subject)
        # Convert timestamps for easier reading/translation
        for note in self.notes:
            note["timesince"] = timesince(note["when"])
        Templated.__init__(self)


class AdminLinkInfoBar(LinkInfoBar):
    pass


class AdminDetailsBar(Templated):
    pass


if config['r2.import_private']:
    from r2admin.lib.pages import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
"""View models for the traffic statistic pages on reddit."""

import collections
import datetime
import pytz
import urllib

from pylons.i18n import _
from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

import babel.core
from babel.dates import format_datetime
from babel.numbers import format_currency

from r2.lib import promote
from r2.lib.db.sorts import epoch_seconds
from r2.lib.menus import menu
from r2.lib.menus import NavButton, NamedButton, PageNameNav, NavMenu
from r2.lib.pages.pages import Reddit, TimeSeriesChart, TabbedPane
from r2.lib.promote import cost_per_mille, cost_per_click
from r2.lib.template_helpers import format_number
from r2.lib.utils import Storage, to_date, timedelta_by_name
from r2.lib.wrapped import Templated
from r2.models import Thing, Link, PromoCampaign, traffic
from r2.models.subreddit import Subreddit, _DefaultSR


COLORS = Storage(UPVOTE_ORANGE="#ff5700",
                 DOWNVOTE_BLUE="#9494ff",
                 MISCELLANEOUS="#006600")


class TrafficPage(Reddit):
    """Base page template for pages rendering traffic graphs."""

    extension_handling = False
    extra_page_classes = ["traffic"]

    def __init__(self, content):
        Reddit.__init__(self, title=_("traffic stats"), content=content)

    def build_toolbars(self):
        main_buttons = [NavButton(menu.sitewide, "/"),
                        NamedButton("languages"),
                        NamedButton("adverts")]

        toolbar = [PageNameNav("nomenu", title=self.title),
                   NavMenu(main_buttons, base_path="/traffic", type="tabmenu")]

        return toolbar


class SitewideTrafficPage(TrafficPage):
    """Base page for sitewide traffic overview."""

    extra_page_classes = TrafficPage.extra_page_classes + ["traffic-sitewide"]

    def __init__(self):
        TrafficPage.__init__(self, SitewideTraffic())


class LanguageTrafficPage(TrafficPage):
    """Base page for interface language traffic summaries or details."""

    def __init__(self, langcode):
        if langcode:
            content = LanguageTraffic(langcode)
        else:
            content = LanguageTrafficSummary()

        TrafficPage.__init__(self, content)


class AdvertTrafficPage(TrafficPage):
    """Base page for advert traffic summaries or details."""

    def __init__(self, code):
        if code:
            content = AdvertTraffic(code)
        else:
            content = AdvertTrafficSummary()
        TrafficPage.__init__(self, content)


class RedditTraffic(Templated):
    """A generalized content pane for traffic reporting."""

    make_period_link = None

    def __init__(self, place):
        self.place = place

        self.traffic_last_modified = traffic.get_traffic_last_modified()
        self.traffic_lag = (datetime.datetime.utcnow() -
                            self.traffic_last_modified)

        self.make_tables()

        Templated.__init__(self)

    def make_tables(self):
        """Create tables to put in the main table area of the page.

        See the stub implementations below for ways to hook into this process
        without completely overriding this method.

        """

        self.tables = []

        for interval in ("month", "day", "hour"):
            columns = [
                dict(color=COLORS.UPVOTE_ORANGE,
                     title=_("uniques by %s" % interval),
                     shortname=_("uniques")),
                dict(color=COLORS.DOWNVOTE_BLUE,
                     title=_("pageviews by %s" % interval),
                     shortname=_("pageviews")),
            ]

            data = self.get_data_for_interval(interval, columns)

            title = _("traffic by %s" % interval)
            graph = TimeSeriesChart("traffic-" + interval,
                                    title,
                                    interval,
                                    columns,
                                    data,
                                    self.traffic_last_modified,
                                    classes=["traffic-table"],
                                    make_period_link=self.make_period_link,
                                   )
            self.tables.append(graph)

        try:
            self.dow_summary = self.get_dow_summary()
        except NotImplementedError:
            self.dow_summary = None
        else:
            uniques_total = collections.Counter()
            pageviews_total = collections.Counter()
            days_total = collections.Counter()

            # don't include the latest (likely incomplete) day
            for date, (uniques, pageviews) in self.dow_summary[1:]:
                dow = date.weekday()
                uniques_total[dow] += uniques
                pageviews_total[dow] += pageviews
                days_total[dow] += 1

            # make a summary of the averages for each day of the week
            self.dow_summary = []
            for dow in xrange(7):
                day_count = days_total[dow]
                if day_count:
                    avg_uniques = uniques_total[dow] / day_count
                    avg_pageviews = pageviews_total[dow] / day_count
                    self.dow_summary.append((dow,
                                             (avg_uniques, avg_pageviews)))
                else:
                    self.dow_summary.append((dow, (0, 0)))

            # calculate the averages for *any* day of the week
            mean_uniques = sum(r[1][0] for r in self.dow_summary) / 7.0
            mean_pageviews = sum(r[1][1] for r in self.dow_summary) / 7.0
            self.dow_means = (round(mean_uniques), round(mean_pageviews))

    def get_dow_summary(self):
        """Return day-interval data to be aggregated by day of week.

        If implemented, a summary table will be shown on the traffic page
        with the average per day of week over the data interval given.

        """
        raise NotImplementedError()

    def get_data_for_interval(self, interval, columns):
        """Return data for the main overview at the interval given.

        This data will be shown as a set of graphs at the top of the page and a
        table for monthly and daily data (hourly is present but hidden by
        default.)

        """
        raise NotImplementedError()


def make_subreddit_traffic_report(subreddits=None, num=None):
    """Return a report of subreddit traffic in the last full month.

    If given a list of subreddits, those subreddits will be put in the report
    otherwise the top subreddits by pageviews will be automatically chosen.

    """

    if subreddits:
        subreddit_summary = traffic.PageviewsBySubreddit.last_month(subreddits)
    else:
        subreddit_summary = traffic.PageviewsBySubreddit.top_last_month(num)

    report = []
    for srname, data in subreddit_summary:
        if srname == _DefaultSR.name:
            name = _("[frontpage]")
            url = None
        elif srname in Subreddit._specials:
            name = "[%s]" % srname
            url = None
        else:
            name = "/r/%s" % srname
            url = name + "/about/traffic"

        report.append(((name, url), data))
    return report


class SitewideTraffic(RedditTraffic):
    """An overview of all traffic to the site."""
    def __init__(self):
        self.subreddit_summary = make_subreddit_traffic_report(num=250)
        RedditTraffic.__init__(self, g.domain)

    def get_dow_summary(self):
        return traffic.SitewidePageviews.history("day")

    def get_data_for_interval(self, interval, columns):
        return traffic.SitewidePageviews.history(interval)


class LanguageTrafficSummary(Templated):
    """An overview of traffic by interface language on the site."""

    def __init__(self):
        # convert language codes to real names
        language_summary = traffic.PageviewsByLanguage.top_last_month()
        locale = c.locale
        self.language_summary = []
        for language_code, data in language_summary:
            name = LanguageTraffic.get_language_name(language_code, locale)
            self.language_summary.append(((language_code, name), data))
        Templated.__init__(self)


class AdvertTrafficSummary(RedditTraffic):
    """An overview of traffic for all adverts on the site."""

    def __init__(self):
        RedditTraffic.__init__(self, _("adverts"))

    def make_tables(self):
        # overall promoted link traffic
        impressions = traffic.AdImpressionsByCodename.historical_totals("day")
        clicks = traffic.ClickthroughsByCodename.historical_totals("day")
        data = traffic.zip_timeseries(impressions, clicks)

        columns = [
            dict(color=COLORS.UPVOTE_ORANGE,
                 title=_("total impressions by day"),
                 shortname=_("impressions")),
            dict(color=COLORS.DOWNVOTE_BLUE,
                 title=_("total clicks by day"),
                 shortname=_("clicks")),
        ]

        self.totals = TimeSeriesChart("traffic-ad-totals",
                                      _("ad totals"),
                                      "day",
                                      columns,
                                      data,
                                      self.traffic_last_modified,
                                      classes=["traffic-table"])

        # get summary of top ads
        advert_summary = traffic.AdImpressionsByCodename.top_last_month()
        things = AdvertTrafficSummary.get_things(ad for ad, data
                                                 in advert_summary)
        self.advert_summary = []
        for id, data in advert_summary:
            name = AdvertTrafficSummary.get_ad_name(id, things=things)
            url = AdvertTrafficSummary.get_ad_url(id, things=things)
            self.advert_summary.append(((name, url), data))

    @staticmethod
    def split_codename(codename):
        """Codenames can be "fullname_campaign". Rend the parts asunder."""
        split_code = codename.split("_")
        fullname = "_".join(split_code[:2])
        campaign = "_".join(split_code[2:])
        return fullname, campaign

    @staticmethod
    def get_things(codes):
        """Fetch relevant things for a list of ad codenames in batch."""
        fullnames = [AdvertTrafficSummary.split_codename(code)[0]
                     for code in codes
                     if code.startswith(Thing._type_prefix)]
        return Thing._by_fullname(fullnames, data=True, return_dict=True)

    @staticmethod
    def get_sr_name(name):
        """Return the display name for a subreddit."""
        if name == g.default_sr:
            return _("frontpage")
        else:
            return "/r/" + name

    @staticmethod
    def get_ad_name(code, things=None):
        """Return a human-readable name for an ad given its codename.

        Optionally, a dictionary of things can be passed in so lookups can
        be done in batch upstream.

        """

        if not things:
            things = AdvertTrafficSummary.get_things([code])

        thing = things.get(code)
        campaign = None

        # if it's not at first a thing, see if it's a thing with campaign
        # appended to it.
        if not thing:
            fullname, campaign = AdvertTrafficSummary.split_codename(code)
            thing = things.get(fullname)

        if not thing:
            if code.startswith("dart_"):
                srname = code.split("_", 1)[1]
                srname = AdvertTrafficSummary.get_sr_name(srname)
                return "DART: " + srname
            else:
                return code
        elif isinstance(thing, Link):
            return "Link: " + thing.title
        elif isinstance(thing, Subreddit):
            srname = AdvertTrafficSummary.get_sr_name(thing.name)
            name = "300x100: " + srname
            if campaign:
                name += " (%s)" % campaign
            return name

    @staticmethod
    def get_ad_url(code, things):
        """Given a codename, return the canonical URL for its traffic page."""
        thing = things.get(code)
        if isinstance(thing, Link):
            return "/traffic/%s" % thing._id36
        return "/traffic/adverts/%s" % code


class LanguageTraffic(RedditTraffic):
    def __init__(self, langcode):
        self.langcode = langcode
        name = LanguageTraffic.get_language_name(langcode)
        RedditTraffic.__init__(self, name)

    def get_data_for_interval(self, interval, columns):
        return traffic.PageviewsByLanguage.history(interval, self.langcode)

    @staticmethod
    def get_language_name(language_code, locale=None):
        if not locale:
            locale = c.locale

        try:
            lang_locale = babel.core.Locale.parse(language_code, sep="-")
        except (babel.core.UnknownLocaleError, ValueError):
            return language_code
        else:
            return lang_locale.get_display_name(locale)


class AdvertTraffic(RedditTraffic):
    def __init__(self, code):
        self.code = code
        name = AdvertTrafficSummary.get_ad_name(code)
        RedditTraffic.__init__(self, name)

    def get_data_for_interval(self, interval, columns):
        columns[1]["title"] = _("impressions by %s" % interval)
        columns[1]["shortname"] = _("impressions")

        columns += [
            dict(shortname=_("unique clicks")),
            dict(color=COLORS.MISCELLANEOUS,
                 title=_("clicks by %s" % interval),
                 shortname=_("total clicks")),
        ]

        imps = traffic.AdImpressionsByCodename.history(interval, self.code)
        clicks = traffic.ClickthroughsByCodename.history(interval, self.code)
        return traffic.zip_timeseries(imps, clicks)


class SubredditTraffic(RedditTraffic):
    def __init__(self):
        RedditTraffic.__init__(self, "/r/" + c.site.name)

        if c.user_is_sponsor:
            fullname = c.site._fullname
            codes = traffic.AdImpressionsByCodename.recent_codenames(fullname)
            self.codenames = [(code,
                               AdvertTrafficSummary.split_codename(code)[1])
                               for code in codes]

    @staticmethod
    def make_period_link(interval, date):
        date = date.replace(tzinfo=g.tz)  # won't be necessary after tz fixup
        if interval == "month":
            if date.month != 12:
                end = date.replace(month=date.month + 1)
            else:
                end = date.replace(month=1, year=date.year + 1)
        else:
            end = date + timedelta_by_name(interval)

        query = urllib.urlencode({
            "syntax": "cloudsearch",
            "restrict_sr": "on",
            "sort": "top",
            "q": "timestamp:{:d}..{:d}".format(int(epoch_seconds(date)),
                                               int(epoch_seconds(end))),
        })
        return "/r/%s/search?%s" % (c.site.name, query)

    def get_dow_summary(self):
        return traffic.PageviewsBySubreddit.history("day", c.site.name)

    def get_data_for_interval(self, interval, columns):
        pageviews = traffic.PageviewsBySubreddit.history(interval, c.site.name)

        if interval == "day":
            columns.append(dict(color=COLORS.MISCELLANEOUS,
                                title=_("subscriptions by day"),
                                shortname=_("subscriptions")))

            sr_name = c.site.name
            subscriptions = traffic.SubscriptionsBySubreddit.history(interval,
                                                                     sr_name)

            return traffic.zip_timeseries(pageviews, subscriptions)
        else:
            return pageviews


def _clickthrough_rate(impressions, clicks):
    """Return the click-through rate percentage."""
    if impressions:
        return (float(clicks) / impressions) * 100.
    else:
        return 0


def _is_promo_preliminary(end_date):
    """Return if results are preliminary for this promotion.

    Results are preliminary until 1 day after the promotion ends.

    """

    now = datetime.datetime.now(g.tz)
    return end_date + datetime.timedelta(days=1) > now


def get_promo_traffic(thing, start, end):
    """Get traffic for a Promoted Link or PromoCampaign"""
    if isinstance(thing, Link):
        imp_fn = traffic.AdImpressionsByCodename.promotion_history
        click_fn = traffic.ClickthroughsByCodename.promotion_history
    elif isinstance(thing, PromoCampaign):
        imp_fn = traffic.TargetedImpressionsByCodename.promotion_history
        click_fn = traffic.TargetedClickthroughsByCodename.promotion_history

    imps = imp_fn(thing._fullname, start.replace(tzinfo=None),
                  end.replace(tzinfo=None))
    clicks = click_fn(thing._fullname, start.replace(tzinfo=None),
                      end.replace(tzinfo=None))

    if imps and not clicks:
        clicks = [(imps[0][0], (0,))]
    elif clicks and not imps:
        imps = [(clicks[0][0], (0,))]

    history = traffic.zip_timeseries(imps, clicks, order="ascending")
    return history


def get_billable_traffic(campaign):
    """Get traffic for dates when PromoCampaign is active."""
    start, end = promote.get_traffic_dates(campaign)
    return get_promo_traffic(campaign, start, end)


def is_early_campaign(campaign):
    # traffic by campaign was only recorded starting 2012/9/12
    return campaign.end_date < datetime.datetime(2012, 9, 12, 0, 0, tzinfo=g.tz)


def is_launched_campaign(campaign):
    now = datetime.datetime.now(g.tz).date()
    return (promote.charged_or_not_needed(campaign) and
            campaign.start_date.date() <= now)


class PromotedLinkTraffic(Templated):
    def __init__(self, thing, campaign, before, after):
        self.thing = thing
        self.campaign = campaign
        self.before = before
        self.after = after
        self.period = datetime.timedelta(days=7)
        self.prev = None
        self.next = None
        self.has_live_campaign = False
        self.has_early_campaign = False
        self.detail_name = ('campaign %s' % campaign._id36 if campaign
                                                           else 'all campaigns')

        editable = c.user_is_sponsor or c.user._id == thing.author_id
        self.traffic_last_modified = traffic.get_traffic_last_modified()
        self.traffic_lag = (datetime.datetime.utcnow() -
                            self.traffic_last_modified)
        self.make_hourly_table(campaign or thing)
        self.make_campaign_table()
        Templated.__init__(self)

    @classmethod
    def make_campaign_table_row(cls, id, start, end, target, location,
            budget_dollars, spent, paid_impressions, impressions, clicks,
            is_live, is_active, url, is_total):

        if impressions:
            cpm = format_currency(promote.cost_per_mille(spent, impressions),
                                  'USD', locale=c.locale)
        else:
            cpm = '---'

        if clicks:
            cpc = format_currency(promote.cost_per_click(spent, clicks), 'USD',
                                  locale=c.locale)
            ctr = format_number(_clickthrough_rate(impressions, clicks))
        else:
            cpc = '---'
            ctr = '---'

        return {
            'id': id,
            'start': start,
            'end': end,
            'target': target,
            'location': location,
            'budget': format_currency(budget_dollars, 'USD', locale=c.locale),
            'spent': format_currency(spent, 'USD', locale=c.locale),
            'impressions_purchased': format_number(paid_impressions),
            'impressions_delivered': format_number(impressions),
            'cpm': cpm,
            'clicks': format_number(clicks),
            'cpc': cpc,
            'ctr': ctr,
            'live': is_live,
            'active': is_active,
            'url': url,
            'csv': url + '.csv',
            'total': is_total,
        }

    def make_campaign_table(self):
        campaigns = PromoCampaign._by_link(self.thing._id)

        total_budget_dollars = 0.
        total_spent = 0
        total_paid_impressions = 0
        total_impressions = 0
        total_clicks = 0

        self.campaign_table = []
        for camp in campaigns:
            if not is_launched_campaign(camp):
                continue

            is_live = camp.is_live_now()
            self.has_early_campaign |= is_early_campaign(camp)
            self.has_live_campaign |= is_live

            history = get_billable_traffic(camp)
            impressions, clicks = 0, 0
            for date, (imp, click) in history:
                impressions += imp
                clicks += click

            start = to_date(camp.start_date).strftime('%Y-%m-%d')
            end = to_date(camp.end_date).strftime('%Y-%m-%d')
            target = camp.target.pretty_name
            location = camp.location_str
            spent = promote.get_spent_amount(camp)
            is_active = self.campaign and self.campaign._id36 == camp._id36
            url = '/traffic/%s/%s' % (self.thing._id36, camp._id36)
            is_total = False
            campaign_budget_dollars = camp.total_budget_dollars
            row = self.make_campaign_table_row(camp._id36,
                                               start=start,
                                               end=end,
                                               target=target,
                                               location=location,
                                               budget_dollars=campaign_budget_dollars,
                                               spent=spent,
                                               paid_impressions=camp.impressions,
                                               impressions=impressions,
                                               clicks=clicks,
                                               is_live=is_live,
                                               is_active=is_active,
                                               url=url,
                                               is_total=is_total)
            self.campaign_table.append(row)

            total_budget_dollars += campaign_budget_dollars
            total_spent += spent
            total_paid_impressions += camp.impressions
            total_impressions += impressions
            total_clicks += clicks

        # total row
        start = '---'
        end = '---'
        target = '---'
        location = '---'
        is_live = False
        is_active = not self.campaign
        url = '/traffic/%s' % self.thing._id36
        is_total = True
        row = self.make_campaign_table_row(_('total'),
                                           start=start,
                                           end=end,
                                           target=target,
                                           location=location,
                                           budget_dollars=total_budget_dollars,
                                           spent=total_spent,
                                           paid_impressions=total_paid_impressions,
                                           impressions=total_impressions,
                                           clicks=total_clicks,
                                           is_live=is_live,
                                           is_active=is_active,
                                           url=url,
                                           is_total=is_total)
        self.campaign_table.append(row)

    def check_dates(self, thing):
        """Shorten range for display and add next/prev buttons."""
        start, end = promote.get_traffic_dates(thing)

        # Check date of latest traffic (campaigns can end early).
        history = list(get_promo_traffic(thing, start, end))
        if history:
            end = max(date for date, data in history)
            end = end.replace(tzinfo=g.tz)  # get_promo_traffic returns tz naive
                                            # datetimes but is actually g.tz

        if self.period:
            display_start = self.after
            display_end = self.before

            if not display_start and not display_end:
                display_end = end
                display_start = end - self.period
            elif not display_end:
                display_end = display_start + self.period
            elif not display_start:
                display_start = display_end - self.period

            if display_start > start:
                p = request.GET.copy()
                p.update({
                    'after': None,
                    'before': display_start.strftime('%Y%m%d%H'),
                })
                self.prev = '%s?%s' % (request.path, urllib.urlencode(p))
            else:
                display_start = start

            if display_end < end:
                p = request.GET.copy()
                p.update({
                    'after': display_end.strftime('%Y%m%d%H'),
                    'before': None,
                })
                self.next = '%s?%s' % (request.path, urllib.urlencode(p))
            else:
                display_end = end
        else:
            display_start, display_end = start, end

        return display_start, display_end

    @classmethod
    def get_hourly_traffic(cls, thing, start, end):
        """Retrieve hourly traffic for a Promoted Link or PromoCampaign."""
        history = get_promo_traffic(thing, start, end)
        computed_history = []
        for date, data in history:
            imps, clicks = data
            ctr = _clickthrough_rate(imps, clicks)

            date = date.replace(tzinfo=pytz.utc)
            date = date.astimezone(pytz.timezone("EST"))
            datestr = format_datetime(
                date,
                locale=c.locale,
                format="yyyy-MM-dd HH:mm zzz",
            )
            computed_history.append((date, datestr, data + (ctr,)))
        return computed_history

    def make_hourly_table(self, thing):
        start, end = self.check_dates(thing)
        self.history = self.get_hourly_traffic(thing, start, end)

        self.total_impressions, self.total_clicks = 0, 0
        for date, datestr, data in self.history:
            imps, clicks, ctr = data
            self.total_impressions += imps
            self.total_clicks += clicks
        if self.total_impressions > 0:
            self.total_ctr = _clickthrough_rate(self.total_impressions,
                                                self.total_clicks)
        # XXX: _is_promo_preliminary correctly expects tz-aware datetimes
        # because it's also used with datetimes from promo code. this hack
        # relies on the fact that we're storing UTC w/o timezone info.
        # TODO: remove this when traffic is correctly using timezones.
        end_aware = end.replace(tzinfo=g.tz)
        self.is_preliminary = _is_promo_preliminary(end_aware)

    @classmethod
    def as_csv(cls, thing):
        """Return the traffic data in CSV format for reports."""

        import csv
        import cStringIO

        start, end = promote.get_traffic_dates(thing)
        history = cls.get_hourly_traffic(thing, start, end)

        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow((_("date and time (UTC)"),
                         _("impressions"),
                         _("clicks"),
                         _("click-through rate (%)")))
        for date, datestr, values in history:
            # flatten (date, datestr, value-tuple) to (date, value1, value2...)
            writer.writerow((date,) + values)

        return out.getvalue()


class SubredditTrafficReport(Templated):
    def __init__(self):
        self.srs, self.invalid_srs, self.report = [], [], []

        self.textarea = request.params.get("subreddits")
        if self.textarea:
            requested_srs = [srname.strip()
                             for srname in self.textarea.splitlines()]
            subreddits = Subreddit._by_name(requested_srs)

            for srname in requested_srs:
                if srname in subreddits:
                    self.srs.append(srname)
                else:
                    self.invalid_srs.append(srname)

            if subreddits:
                self.report = make_subreddit_traffic_report(subreddits.values())

            param = urllib.quote(self.textarea)
            self.csv_url = "/traffic/subreddits/report.csv?subreddits=" + param

        Templated.__init__(self)

    def as_csv(self):
        """Return the traffic data in CSV format for reports."""

        import csv
        import cStringIO

        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow((_("subreddit"),
                         _("uniques"),
                         _("pageviews")))
        for (name, url), (uniques, pageviews) in self.report:
            writer.writerow((name, uniques, pageviews))

        return out.getvalue()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.config import feature
from r2.lib.menus import CommentSortMenu
from r2.lib.validator.validator import (
    VBoolean,
    VInt,
    VLang,
    VOneOf,
    VSRByName,
)
from r2.lib.errors import errors
from r2.models import Subreddit, NotFound

# Validators that map directly to Account._preference_attrs
# The key MUST be the same string as the value in _preference_attrs
# Non-preference validators should be added to to the controller
# method directly (see PostController.POST_options)
PREFS_VALIDATORS = dict(
    pref_clickgadget=VBoolean('clickgadget'),
    pref_organic=VBoolean('organic'),
    pref_newwindow=VBoolean('newwindow'),
    pref_public_votes=VBoolean('public_votes'),
    pref_hide_from_robots=VBoolean('hide_from_robots'),
    pref_hide_ups=VBoolean('hide_ups'),
    pref_hide_downs=VBoolean('hide_downs'),
    pref_over_18=VBoolean('over_18'),
    pref_research=VBoolean('research'),
    pref_numsites=VInt('numsites', 1, 100),
    pref_lang=VLang('lang'),
    pref_media=VOneOf('media', ('on', 'off', 'subreddit')),
    # pref_media_preview=VOneOf('media_preview', ('on', 'off', 'subreddit')),
    pref_compress=VBoolean('compress'),
    pref_domain_details=VBoolean('domain_details'),
    pref_min_link_score=VInt('min_link_score', -100, 100),
    pref_min_comment_score=VInt('min_comment_score', -100, 100),
    pref_num_comments=VInt('num_comments', 1, g.max_comments,
                           default=g.num_comments),
    pref_highlight_controversial=VBoolean('highlight_controversial'),
    pref_default_comment_sort=VOneOf('default_comment_sort',
                                     CommentSortMenu.visible_options()),
    pref_ignore_suggested_sort=VBoolean("ignore_suggested_sort"),
    pref_show_stylesheets=VBoolean('show_stylesheets'),
    pref_show_flair=VBoolean('show_flair'),
    pref_show_link_flair=VBoolean('show_link_flair'),
    pref_no_profanity=VBoolean('no_profanity'),
    pref_label_nsfw=VBoolean('label_nsfw'),
    pref_show_promote=VBoolean('show_promote'),
    pref_mark_messages_read=VBoolean("mark_messages_read"),
    pref_threaded_messages=VBoolean("threaded_messages"),
    pref_collapse_read_messages=VBoolean("collapse_read_messages"),
    pref_email_messages=VBoolean("email_messages"),
    pref_private_feeds=VBoolean("private_feeds"),
    pref_store_visits=VBoolean('store_visits'),
    pref_hide_ads=VBoolean("hide_ads"),
    pref_show_trending=VBoolean("show_trending"),
    pref_highlight_new_comments=VBoolean("highlight_new_comments"),
    pref_show_gold_expiration=VBoolean("show_gold_expiration"),
    pref_monitor_mentions=VBoolean("monitor_mentions"),
    pref_hide_locationbar=VBoolean("hide_locationbar"),
    pref_use_global_defaults=VBoolean("use_global_defaults"),
    pref_creddit_autorenew=VBoolean("creddit_autorenew"),
    pref_enable_default_themes=VBoolean("enable_default_themes", False),
    pref_default_theme_sr=VSRByName("theme_selector", required=False,
        return_srname=True),
    pref_other_theme=VSRByName("other_theme", required=False,
        return_srname=True),
    pref_beta=VBoolean('beta'),
    pref_legacy_search=VBoolean('legacy_search'),
    pref_threaded_modmail=VBoolean('threaded_modmail', False),
)


def set_prefs(user, prefs):
    for k, v in prefs.iteritems():
        if k == 'pref_beta' and v and not getattr(user, 'pref_beta', False):
            # If a user newly opted into beta, we want to subscribe them
            # to the beta subreddit.
            try:
                sr = Subreddit._by_name(g.beta_sr)
                if not sr.is_subscriber(user):
                    sr.add_subscriber(user)
            except NotFound:
                g.log.warning("Could not find beta subreddit '%s'. It may "
                              "need to be created." % g.beta_sr)

        setattr(user, k, v)

def filter_prefs(prefs, user):
    # replace stylesheet_override with other_theme if it doesn't exist
    if feature.is_enabled('stylesheets_everywhere', user=user):
        if not prefs["pref_default_theme_sr"]:
            if prefs.get("pref_other_theme", False):
                prefs["pref_default_theme_sr"] = prefs["pref_other_theme"]

    for pref_key in prefs.keys():
        if pref_key not in user._preference_attrs:
            del prefs[pref_key]

    #temporary. eventually we'll change pref_clickgadget to an
    #integer preference
    prefs['pref_clickgadget'] = 5 if prefs['pref_clickgadget'] else 0
    if user.pref_show_promote is None:
        prefs['pref_show_promote'] = None
    elif not prefs.get('pref_show_promote'):
        prefs['pref_show_promote'] = False

    if not prefs.get("pref_over_18") or not user.pref_over_18:
        prefs['pref_no_profanity'] = True

    if prefs.get("pref_no_profanity") or user.pref_no_profanity:
        prefs['pref_label_nsfw'] = True

    # don't update the hide_ads pref if they don't have gold
    if not user.gold:
        del prefs['pref_hide_ads']
        del prefs['pref_show_gold_expiration']

    if not (user.gold or user.is_moderator_somewhere):
        prefs['pref_highlight_new_comments'] = True

    # check stylesheet override
    if (feature.is_enabled('stylesheets_everywhere', user=user) and
            prefs['pref_default_theme_sr']):
        override_sr = Subreddit._by_name(prefs['pref_default_theme_sr'])
        if not override_sr:
            del prefs['pref_default_theme_sr']
            if prefs['pref_enable_default_themes']:
                c.errors.add(c.errors.add(errors.SUBREDDIT_REQUIRED, field="stylesheet_override"))
        else:
            if override_sr.can_view(user):
                prefs['pref_default_theme_sr'] = override_sr.name
            else:
                # don't update if they can't view the chosen subreddit
                c.errors.add(errors.SUBREDDIT_NO_ACCESS, field='stylesheet_override')
                del prefs['pref_default_theme_sr']
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import config

from validator import *

if config['r2.import_private']:
    from r2admin.lib.validator import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from os.path import normpath
from functools import wraps
import datetime
import re

from pylons.i18n import _

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.models.wiki import WikiPage, WikiRevision, WikiBadRevision
from r2.lib.validator import (
    Validator,
    VSrModerator,
    set_api_docs,
)
from r2.lib.db import tdb_cassandra

MAX_PAGE_NAME_LENGTH = g.wiki_max_page_name_length

MAX_SEPARATORS = g.wiki_max_page_separators

def this_may_revise(page=None):
    if not c.user_is_loggedin:
        return False
    
    if c.user_is_admin:
        return True
    
    return may_revise(c.site, c.user, page)

def this_may_view(page):
    user = c.user if c.user_is_loggedin else None
    if user and c.user_is_admin:
        return True
    return may_view(c.site, user, page)

def may_revise(sr, user, page=None):    
    if sr.is_moderator_with_perms(user, 'wiki'):
        # Mods may always contribute to non-config pages
        if not page or not page.special:
            return True
    
    if page and page.restricted and not page.special:
        # People may not contribute to restricted pages
        # (Except for special pages)
        return False

    if sr.is_wikibanned(user):
        # Users who are wiki banned in the subreddit may not contribute
        return False
    
    if sr.is_banned(user):
        # If the user is banned from the subreddit, do not allow them to contribute
        return False
    
    if page and not may_view(sr, user, page):
        # Users who are not allowed to view the page may not contribute to the page
        return False
    
    if user.wiki_override == False:
        # global ban
        return False
    
    if page and page.has_editor(user._id36):
        # If the user is an editor on the page, they may edit
        return True

    if (page and page.special and
            sr.is_moderator_with_perms(user, 'config')):
        return True

    if page and page.special:
        # If this is a special page
        # (and the user is not a mod or page editor)
        # They should not be allowed to revise
        return False
    
    if page and page.permlevel > 0:
        # If the page is beyond "anyone may contribute"
        # A normal user should not be allowed to revise
        return False
    
    if sr.is_wikicontributor(user):
        # If the user is a wiki contributor, they may revise
        return True
    
    if sr.wikimode != 'anyone':
        # If the user is not a page editor or wiki contributor,
        # and the mode is not everyone,
        # the user may not edit.
        return False
    
    if not sr.wiki_can_submit(user):
        # If the user can not submit to the subreddit
        # They should not be able to contribute
        return False

    # Use global karma for the frontpage wiki
    karma_sr = sr if sr.wiki_use_subreddit_karma else None

    # Use link or comment karma, whichever is greater
    karma = max(user.karma('link', karma_sr), user.karma('comment', karma_sr))

    if karma < (sr.wiki_edit_karma or 0):
        # If the user has too few karma, they should not contribute
        return False
    
    age = (datetime.datetime.now(g.tz) - user._date).days
    if age < (sr.wiki_edit_age or 0):
        # If they user's account is too young
        # They should not contribute
        return False
    
    # Otherwise, allow them to contribute
    return True

def may_view(sr, user, page):
    # User being None means not logged in
    mod = sr.is_moderator_with_perms(user, 'wiki') if user else False
    
    if mod:
        # Mods may always view
        return True
    
    if page.special:
        level = WikiPage.get_special_view_permlevel(page.name)
    else:
        level = page.permlevel
    
    if level < 2:
        # Everyone may view in levels below 2
        return True
    
    if level == 2:
        # Only mods may view in level 2
        return mod
    
    # In any other obscure level,
    # (This should not happen but just in case)
    # nobody may view.
    return False

def normalize_page(page):
    # Ensure there is no side effect if page is None
    page = page or ""
    
    # Replace spaces with underscores
    page = page.replace(' ', '_')
    
    # Case insensitive page names
    page = page.lower()
    
    # Normalize path (And avoid normalizing empty to ".")
    if page:
        page = normpath(page)
    
    # Chop off initial "/", just in case it exists
    page = page.lstrip('/')
    
    return page

class AbortWikiError(Exception):
    pass

page_match_regex = re.compile(r'^[\w_\-/]+\Z')

class VWikiModerator(VSrModerator):
    def __init__(self, fatal=False, *a, **kw):
        VSrModerator.__init__(self, param='page', fatal=fatal, *a, **kw)

    def run(self, page):
        self.perms = ['wiki']
        if page and WikiPage.is_special(page):
            self.perms += ['config']
        VSrModerator.run(self)

class VWikiPageName(Validator):
    def __init__(self, param, error_on_name_normalized=False, *a, **kw):
        self.error_on_name_normalized = error_on_name_normalized
        Validator.__init__(self, param, *a, **kw)
    
    def run(self, page):
        original_page = page
        
        try:
            page = str(page) if page else ""
        except UnicodeEncodeError:
            return self.set_error('INVALID_PAGE_NAME', code=400)
        
        page = normalize_page(page)
        
        if page and not page_match_regex.match(page):
            return self.set_error('INVALID_PAGE_NAME', code=400)
        
        # If no page is specified, give the index page
        page = page or "index"
        
        if WikiPage.is_impossible(page):
            return self.set_error('INVALID_PAGE_NAME', code=400)
        
        if self.error_on_name_normalized and page != original_page:
            self.set_error('PAGE_NAME_NORMALIZED')
        
        return page

class VWikiPage(VWikiPageName):
    def __init__(self, param, required=True, restricted=True, modonly=False,
                 allow_hidden_revision=True, **kw):
        self.restricted = restricted
        self.modonly = modonly
        self.allow_hidden_revision = allow_hidden_revision
        self.required = required
        VWikiPageName.__init__(self, param, **kw)
    
    def run(self, page):
        page = VWikiPageName.run(self, page)
        
        if self.has_errors:
            return
        
        if (not c.is_wiki_mod) and self.modonly:
            return self.set_error('MOD_REQUIRED', code=403)
        
        try:
            wp = self.validpage(page)
        except AbortWikiError:
            return

        return wp
    
    def validpage(self, page):
        try:
            wp = WikiPage.get(c.site, page)
            if self.restricted and wp.restricted:
                if not (c.is_wiki_mod or wp.special):
                    self.set_error('RESTRICTED_PAGE', code=403)
                    raise AbortWikiError
            if not this_may_view(wp):
                self.set_error('MAY_NOT_VIEW', code=403)
                raise AbortWikiError
            return wp
        except tdb_cassandra.NotFound:
            if self.required:
                self.set_error('PAGE_NOT_FOUND', code=404)
                raise AbortWikiError
            return None
    
    def validversion(self, version, pageid=None):
        if not version:
            return
        try:
            r = WikiRevision.get(version, pageid)
            if r.admin_deleted and not c.user_is_admin:
                self.set_error('INVALID_REVISION', code=404)
                raise AbortWikiError
            if not self.allow_hidden_revision and (r.is_hidden and not c.is_wiki_mod):
                self.set_error('HIDDEN_REVISION', code=403)
                raise AbortWikiError
            return r
        except (tdb_cassandra.NotFound, WikiBadRevision, ValueError):
            self.set_error('INVALID_REVISION', code=404)
            raise AbortWikiError

    def param_docs(self, param=None):
        return {param or self.param: _('the name of an existing wiki page')}

class VWikiPageAndVersion(VWikiPage):    
    def run(self, page, *versions):
        wp = VWikiPage.run(self, page)
        if self.has_errors:
            return
        validated = []
        for v in versions:
            try:
                validated += [self.validversion(v, wp._id) if v and wp else None]
            except AbortWikiError:
                return
        return tuple([wp] + validated)
    
    def param_docs(self):
        doc = dict.fromkeys(self.param, _('a wiki revision ID'))
        doc.update(VWikiPage.param_docs(self, self.param[0]))
        return doc

class VWikiPageRevise(VWikiPage):
    def __init__(self, param, required=False, *k, **kw):
        VWikiPage.__init__(self, param, required=required, *k, **kw)
    
    def may_not_create(self, page):
        if not page:
            # Should not happen, but just in case
            self.set_error('EMPTY_PAGE_NAME', 403)
            return
        
        page = normalize_page(page)
        
        if WikiPage.is_automatically_created(page):
            return {'reason': 'PAGE_CREATED_ELSEWHERE'}
        elif WikiPage.is_special(page):
            if not (c.user_is_admin or
                    c.site.is_moderator_with_perms(c.user, 'config')):
                self.set_error('RESTRICTED_PAGE', code=403)
                return
        elif (not c.user_is_admin) and WikiPage.is_restricted(page):
            self.set_error('RESTRICTED_PAGE', code=403)
            return
        elif page.count('/') > MAX_SEPARATORS:
            return {'reason': 'PAGE_NAME_MAX_SEPARATORS', 'max_separators': MAX_SEPARATORS}
        elif len(page) > MAX_PAGE_NAME_LENGTH:
            return {'reason': 'PAGE_NAME_LENGTH', 'max_length': MAX_PAGE_NAME_LENGTH}
    
    def run(self, page, previous=None):
        wp = VWikiPage.run(self, page)
        if self.has_errors:
            return
        if not this_may_revise(wp):
            if not wp:
                return self.set_error('PAGE_NOT_FOUND', code=404)
            # No abort code on purpose, controller will handle
            self.set_error('MAY_NOT_REVISE')
            return (None, None)
        if not wp:
            # No abort code on purpose, controller will handle
            error = self.may_not_create(page)
            if error:
                self.set_error('WIKI_CREATE_ERROR', msg_params=error)
            return (None, None)
        if previous:
            try:
                prev = self.validversion(previous, wp._id)
            except AbortWikiError:
                return
            return (wp, prev)
        return (wp, None)
    
    def param_docs(self):
        docs = {self.param[0]: _('the name of an existing page or a new page to create')}
        if 'previous' in self.param:
            docs['previous'] = _('the starting point revision for this edit')
        return docs
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import cgi
import json
from collections import OrderedDict
from decimal import Decimal

from pylons import request, response
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import _
from pylons.controllers.util import abort

from r2.config import feature
from r2.config.extensions import api_type, is_api
from r2.lib import utils, captcha, promote, totp, ratelimit
from r2.lib.filters import unkeep_space, websafe, _force_unicode, _force_utf8
from r2.lib.filters import markdown_souptest
from r2.lib.db import tdb_cassandra
from r2.lib.db.operators import asc, desc
from r2.lib.souptest import (
    SoupError,
    SoupDetectedCrasherError,
    SoupUnsupportedEntityError,
)
from r2.lib.template_helpers import add_sr
from r2.lib.jsonresponse import JQueryResponse, JsonResponse
from r2.lib.permissions import ModeratorPermissionSet
from r2.models import *
from r2.models.rules import MAX_RULES_PER_SUBREDDIT
from r2.models.promo import Location
from r2.lib.authorize import Address, CreditCard
from r2.lib.utils import constant_time_compare
from r2.lib.require import require, require_split, RequirementException
from r2.lib import signing

from r2.lib.errors import errors, RedditError, UserRequiredException
from r2.lib.errors import VerifiedUserRequiredException

from copy import copy
from datetime import datetime, timedelta
from curses.ascii import isprint
import re, inspect
from itertools import chain
from functools import wraps


def can_view_link_comments(article):
    return (article.subreddit_slow.can_view(c.user) and
            article.can_view_promo(c.user))


class Validator(object):
    notes = None
    default_param = None
    def __init__(self, param=None, default=None, post=True, get=True, url=True,
                 get_multiple=False, body=False, docs=None):
        if param:
            self.param = param
        else:
            self.param = self.default_param

        self.default = default
        self.post, self.get, self.url, self.docs = post, get, url, docs
        self.get_multiple = get and get_multiple
        self.body = body
        self.has_errors = False

    def set_error(self, error, msg_params={}, field=False, code=None):
        """
        Adds the provided error to c.errors and flags that it is come
        from the validator's param
        """
        if field is False:
            field = self.param

        c.errors.add(error, msg_params=msg_params, field=field, code=code)
        self.has_errors = True

    def param_docs(self):
        param_info = {}
        for param in filter(None, tup(self.param)):
            param_info[param] = None
        return param_info

    def __call__(self, url):
        self.has_errors = False
        a = []
        if self.param:
            for p in utils.tup(self.param):
                # cgi.FieldStorage is falsy even if it has a filled value
                # property. :(
                post_val = request.POST.get(p)
                if self.post and (post_val or
                                  isinstance(post_val, cgi.FieldStorage)):
                    val = request.POST[p]
                elif ((self.get_multiple and
                      (self.get_multiple == True or
                       p in self.get_multiple)) and
                      request.GET.getall(p)):
                    val = request.GET.getall(p)
                elif self.get and request.GET.get(p):
                    val = request.GET[p]
                elif self.url and url.get(p):
                    val = url[p]
                elif self.body:
                    val = request.body
                else:
                    val = self.default
                a.append(val)
        try:
            return self.run(*a)
        except TypeError, e:
            if str(e).startswith('run() takes'):
                # Prepend our class name so we know *which* run()
                raise TypeError('%s.%s' % (type(self).__name__, str(e)))
            else:
                raise


def build_arg_list(fn, env):
    """given a fn and and environment the builds a keyword argument list
    for fn"""
    kw = {}
    argspec = inspect.getargspec(fn)

    # if there is a **kw argument in the fn definition,
    # just pass along the environment
    if argspec[2]:
        kw = env
    #else for each entry in the arglist set the value from the environment
    else:
        #skip self
        argnames = argspec[0][1:]
        for name in argnames:
            if name in env:
                kw[name] = env[name]
    return kw

def _make_validated_kw(fn, simple_vals, param_vals, env):
    for validator in simple_vals:
        validator(env)
    kw = build_arg_list(fn, env)
    for var, validator in param_vals.iteritems():
        kw[var] = validator(env)
    return kw

def set_api_docs(fn, simple_vals, param_vals, extra_vals=None):
    doc = fn._api_doc = getattr(fn, '_api_doc', {})
    param_info = doc.get('parameters', {})
    notes = doc.get('notes', [])
    for validator in chain(simple_vals, param_vals.itervalues()):
        param_docs = validator.param_docs()
        if validator.docs:
            param_docs.update(validator.docs)
        param_info.update(param_docs)
        if validator.notes:
            notes.append(validator.notes)
    if extra_vals:
        param_info.update(extra_vals)
    doc['parameters'] = param_info
    doc['notes'] = notes

def _validators_handle_csrf(simple_vals, param_vals):
    for validator in chain(simple_vals, param_vals.itervalues()):
        if getattr(validator, 'handles_csrf', False):
            return True
    return False

def validate(*simple_vals, **param_vals):
    """Validation decorator that delegates error handling to the controller.

    Runs the validators specified and calls self.on_validation_error to
    process each error. This allows controllers to define their own fatal
    error processing logic.
    """
    def val(fn):
        @wraps(fn)
        def newfn(self, *a, **env):
            try:
                kw = _make_validated_kw(fn, simple_vals, param_vals, env)
            except RedditError as err:
                self.on_validation_error(err)

            for err in c.errors:
                self.on_validation_error(c.errors[err])

            try:
                return fn(self, *a, **kw)
            except RedditError as err:
                self.on_validation_error(err)

        set_api_docs(newfn, simple_vals, param_vals)
        newfn.handles_csrf = _validators_handle_csrf(simple_vals, param_vals)
        return newfn
    return val


def api_validate(response_type=None, add_api_type_doc=False):
    """
    Factory for making validators for API calls, since API calls come
    in two flavors: responsive and unresponsive.  The machinary
    associated with both is similar, and the error handling identical,
    so this function abstracts away the kw validation and creation of
    a Json-y responder object.
    """
    def wrap(response_function):
        def _api_validate(*simple_vals, **param_vals):
            def val(fn):
                @wraps(fn)
                def newfn(self, *a, **env):
                    renderstyle = request.params.get("renderstyle")
                    if renderstyle:
                        c.render_style = api_type(renderstyle)
                    elif not c.extension:
                        # if the request URL included an extension, don't
                        # touch the render_style, since it was already set by
                        # set_extension. if no extension was provided, default
                        # to response_type.
                        c.render_style = api_type(response_type)

                    # generate a response object
                    if response_type == "html" and not request.params.get('api_type') == "json":
                        responder = JQueryResponse()
                    else:
                        responder = JsonResponse()

                    response.content_type = responder.content_type

                    try:
                        kw = _make_validated_kw(fn, simple_vals, param_vals, env)
                        return response_function(self, fn, responder,
                                                 simple_vals, param_vals, *a, **kw)
                    except UserRequiredException:
                        responder.send_failure(errors.USER_REQUIRED)
                        return self.api_wrapper(responder.make_response())
                    except VerifiedUserRequiredException:
                        responder.send_failure(errors.VERIFIED_USER_REQUIRED)
                        return self.api_wrapper(responder.make_response())

                extra_param_vals = {}
                if add_api_type_doc:
                    extra_param_vals = {
                        "api_type": "the string `json`",
                    }

                set_api_docs(newfn, simple_vals, param_vals, extra_param_vals)
                newfn.handles_csrf = _validators_handle_csrf(simple_vals,
                                                             param_vals)
                return newfn
            return val
        return _api_validate
    return wrap


@api_validate("html")
def noresponse(self, self_method, responder, simple_vals, param_vals, *a, **kw):
    self_method(self, *a, **kw)
    return self.api_wrapper({})

@api_validate("html")
def textresponse(self, self_method, responder, simple_vals, param_vals, *a, **kw):
    return self_method(self, *a, **kw)

@api_validate()
def json_validate(self, self_method, responder, simple_vals, param_vals, *a, **kw):
    if c.extension != 'json':
        abort(404)

    val = self_method(self, responder, *a, **kw)
    if val is None:
        val = responder.make_response()
    return self.api_wrapper(val)

def _validatedForm(self, self_method, responder, simple_vals, param_vals,
                  *a, **kw):
    # generate a form object
    form = responder(request.POST.get('id', "body"))

    # clear out the status line as a courtesy
    form.set_text(".status", "")

    # do the actual work
    val = self_method(self, form, responder, *a, **kw)

    # add data to the output on some errors
    for validator in chain(simple_vals, param_vals.values()):
        if (isinstance(validator, VCaptcha) and
            (form.has_errors('captcha', errors.BAD_CAPTCHA) or
             (form.has_error() and c.user.needs_captcha()))):
            form.new_captcha()
        elif (isinstance(validator, (VRatelimit, VThrottledLogin)) and
              form.has_errors('ratelimit', errors.RATELIMIT)):
            form.ratelimit(validator.seconds)
    if val:
        return val
    else:
        return self.api_wrapper(responder.make_response())

@api_validate("html", add_api_type_doc=True)
def validatedForm(self, self_method, responder, simple_vals, param_vals,
                  *a, **kw):
    return _validatedForm(self, self_method, responder, simple_vals, param_vals,
                          *a, **kw)

@api_validate("html", add_api_type_doc=True)
def validatedMultipartForm(self, self_method, responder, simple_vals,
                           param_vals, *a, **kw):
    def wrapped_self_method(*a, **kw):
        val = self_method(*a, **kw)
        if val:
            return val
        else:
            data = json.dumps(responder.make_response())
            response.content_type = "text/html"
            return ('<html><head><script type="text/javascript">\n'
                    'parent.$.handleResponse()(%s)\n'
                    '</script></head></html>') % filters.websafe_json(data)
    return _validatedForm(self, wrapped_self_method, responder, simple_vals,
                          param_vals, *a, **kw)


jsonp_callback_rx = re.compile("\\A[\\w$\\.\"'[\\]]+\\Z")
def valid_jsonp_callback(callback):
    return jsonp_callback_rx.match(callback)


#### validators ####
class nop(Validator):
    def run(self, x):
        return x

class VLang(Validator):
    @staticmethod
    def validate_lang(lang, strict=False):
        if lang in g.all_languages:
            return lang
        else:
            if not strict:
                return g.lang
            else:
                raise ValueError("invalid language %r" % lang)
    def run(self, lang):
        return VLang.validate_lang(lang)

    def param_docs(self):
        return {
            self.param: "a valid IETF language tag (underscore separated)",
        }


class VRequired(Validator):
    def __init__(self, param, error, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self._error = error

    def error(self, e = None):
        if not e: e = self._error
        if e:
            self.set_error(e)

    def run(self, item):
        if not item:
            self.error()
        else:
            return item

class VThing(Validator):
    def __init__(self, param, thingclass, redirect = True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.thingclass = thingclass
        self.redirect = redirect

    def run(self, thing_id):
        if thing_id:
            try:
                tid = int(thing_id, 36)
                thing = self.thingclass._byID(tid, True)
                if thing.__class__ != self.thingclass:
                    raise TypeError("Expected %s, got %s" %
                                    (self.thingclass, thing.__class__))
                return thing
            except (NotFound, ValueError):
                if self.redirect:
                    abort(404, 'page not found')
                else:
                    return None

    def param_docs(self):
        return {
            self.param: "The base 36 ID of a " + self.thingclass.__name__
        }

class VLink(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Link, redirect=redirect, *a, **kw)

class VPromoCampaign(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, PromoCampaign, *a, **kw)

class VCommentByID(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Comment, redirect=redirect, *a, **kw)


class VAward(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Award, redirect=redirect, *a, **kw)

class VAwardByCodename(Validator):
    def run(self, codename, required_fullname=None):
        if not codename:
            return self.set_error(errors.NO_TEXT)

        try:
            a = Award._by_codename(codename)
        except NotFound:
            a = None

        if a and required_fullname and a._fullname != required_fullname:
            return self.set_error(errors.INVALID_OPTION)
        else:
            return a

class VTrophy(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Trophy, redirect=redirect, *a, **kw)

class VMessage(Validator):
    def run(self, message_id):
        if message_id:
            try:
                aid = int(message_id, 36)
                return Message._byID(aid, True)
            except (NotFound, ValueError):
                abort(404, 'page not found')


class VCommentID(Validator):
    def run(self, cid):
        if cid:
            try:
                cid = int(cid, 36)
                return Comment._byID(cid, True)
            except (NotFound, ValueError):
                pass

class VMessageID(Validator):
    def run(self, cid):
        if cid:
            try:
                cid = int(cid, 36)
                m = Message._byID(cid, True)
                if not m.can_view_slow():
                    abort(403, 'forbidden')
                return m
            except (NotFound, ValueError):
                pass

class VCount(Validator):
    def run(self, count):
        if count is None:
            count = 0
        try:
            return max(int(count), 0)
        except ValueError:
            return 0

    def param_docs(self):
        return {
            self.param: "a positive integer (default: 0)",
        }


class VLimit(Validator):
    def __init__(self, param, default=25, max_limit=100, **kw):
        self.default_limit = default
        self.max_limit = max_limit
        Validator.__init__(self, param, **kw)

    def run(self, limit):
        default = c.user.pref_numsites
        if not default or c.render_style in ("compact", api_type("compact")):
            default = self.default_limit  # TODO: ini param?

        if limit is None:
            return default

        try:
            i = int(limit)
        except ValueError:
            return default

        return min(max(i, 1), self.max_limit)

    def param_docs(self):
        return {
            self.param: "the maximum number of items desired "
                        "(default: %d, maximum: %d)" % (self.default_limit,
                                                        self.max_limit),
        }

class VCssMeasure(Validator):
    measure = re.compile(r"\A\s*[\d\.]+\w{0,3}\s*\Z")
    def run(self, value):
        return value if value and self.measure.match(value) else ''


class VLength(Validator):
    only_whitespace = re.compile(r"\A\s*\Z", re.UNICODE)

    def __init__(self, param, max_length,
                 min_length=0,
                 empty_error = errors.NO_TEXT,
                 length_error = errors.TOO_LONG,
                 short_error=errors.TOO_SHORT,
                 **kw):
        Validator.__init__(self, param, **kw)
        self.max_length = max_length
        self.min_length = min_length
        self.length_error = length_error
        self.short_error = short_error
        self.empty_error = empty_error

    def run(self, text, text2 = ''):
        text = text or text2
        if self.empty_error and (not text or self.only_whitespace.match(text)):
            self.set_error(self.empty_error, code=400)
        elif len(text) > self.max_length:
            self.set_error(self.length_error, {'max_length': self.max_length}, code=400)
        elif len(text) < self.min_length:
            self.set_error(self.short_error, {'min_length': self.min_length},
                           code=400)
        else:
            return text

    def param_docs(self):
        return {
            self.param:
                "a string no longer than %d characters" % self.max_length,
        }

class VUploadLength(VLength):
    def run(self, upload, text2=''):
        # upload is expected to be a FieldStorage object
        if isinstance(upload, cgi.FieldStorage):
            return VLength.run(self, upload.value, text2)
        else:
            self.set_error(self.empty_error, code=400)

    def param_docs(self):
        kibibytes = self.max_length / 1024
        return {
            self.param:
                "file upload with maximum size of %d KiB" % kibibytes,
        }

class VPrintable(VLength):
    def run(self, text, text2 = ''):
        text = VLength.run(self, text, text2)

        if text is None:
            return None

        try:
            if all(isprint(str(x)) for x in text):
                return str(text)
        except UnicodeEncodeError:
            pass

        self.set_error(errors.BAD_STRING, code=400)
        return None

    def param_docs(self):
        return {
            self.param: "a string up to %d characters long,"
                        " consisting of printable characters."
                            % self.max_length,
        }

class VTitle(VLength):
    def __init__(self, param, max_length = 300, **kw):
        VLength.__init__(self, param, max_length, **kw)

    def param_docs(self):
        return {
            self.param: "title of the submission. "
                        "up to %d characters long" % self.max_length,
        }

class VMarkdown(Validator):
    def __init__(self, param, renderer='reddit'):
        Validator.__init__(self, param)
        self.renderer = renderer

    def run(self, text, text2=''):
        text = text or text2
        try:
            markdown_souptest(text, renderer=self.renderer)
            return text
        except SoupError as e:
            # Could happen if someone does `&#00;`. It's not a security issue,
            # it's just unacceptable.
            # TODO: give a better indication to the user of what happened
            if isinstance(e, SoupUnsupportedEntityError):
                abort(400)
                return

            import sys
            user = "???"
            if c.user_is_loggedin:
                user = c.user.name

            # work around CRBUG-464270
            if isinstance(e, SoupDetectedCrasherError):
                # We want a general idea of how often this is triggered, and
                # by what
                g.log.warning("CHROME HAX by %s: %s" % (user, text))
                abort(400)
                return

            g.log.error("HAX by %s: %s" % (user, text))
            s = sys.exc_info()
            # reraise the original error with the original stack trace
            raise s[1], None, s[2]

    def param_docs(self):
        return {
            tup(self.param)[0]: "raw markdown text",
        }


class VMarkdownLength(VMarkdown):
    def __init__(self, param, renderer='reddit', max_length=10000,
                 empty_error=errors.NO_TEXT, length_error=errors.TOO_LONG):
        VMarkdown.__init__(self, param, renderer)
        self.max_length = max_length
        self.empty_error = empty_error
        self.length_error = length_error

    def run(self, text, text2=''):
        text = text or text2
        text = VLength(self.param, self.max_length,
                       empty_error=self.empty_error,
                       length_error=self.length_error).run(text)
        if text:
            return VMarkdown.run(self, text)
        else:
            return ''


class VSavedCategory(Validator):
    savedcategory_rx = re.compile(r"\A[a-z0-9 _]{1,20}\Z")

    def run(self, name):
        if not name:
            return
        name = name.lower()
        valid = self.savedcategory_rx.match(name)
        if not valid:
            self.set_error('BAD_SAVE_CATEGORY')
            return
        return name

    def param_docs(self):
        return {
            self.param: "a category name",
        }


class VSubredditName(VRequired):
    def __init__(self, item, allow_language_srs=False, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_SR_NAME, *a, **kw)
        self.allow_language_srs = allow_language_srs

    def run(self, name):
        if name:
            name = sr_path_rx.sub('\g<name>', name.strip())

        valid_name = Subreddit.is_valid_name(
            name, allow_language_srs=self.allow_language_srs)

        if not valid_name:
            self.set_error(self._error, code=400)
            return

        return str(name)

    def param_docs(self):
        return {
            self.param: "subreddit name",
        }


class VAvailableSubredditName(VSubredditName):
    def run(self, name):
        name = VSubredditName.run(self, name)
        if name:
            try:
                a = Subreddit._by_name(name)
                return self.error(errors.SUBREDDIT_EXISTS)
            except NotFound:
                return name


class VSRByName(Validator):
    def __init__(self, sr_name, required=True, return_srname=False):
        self.required = required
        self.return_srname = return_srname
        Validator.__init__(self, sr_name)

    def run(self, sr_name):
        if not sr_name:
            if self.required:
                self.set_error(errors.BAD_SR_NAME, code=400)
        else:
            sr_name = sr_path_rx.sub('\g<name>', sr_name.strip())
            try:
                sr = Subreddit._by_name(sr_name)
                if self.return_srname:
                    return sr.name
                else:
                    return sr
            except NotFound:
                self.set_error(errors.SUBREDDIT_NOEXIST, code=400)

    def param_docs(self):
        return {
            self.param: "subreddit name",
        }


class VSRByNames(Validator):
    """Returns a dict mapping subreddit names to subreddit objects.

    sr_names_csv - a comma delimited string of subreddit names
    required - if true (default) an empty subreddit name list is an error

    """
    def __init__(self, sr_names_csv, required=True):
        self.required = required
        Validator.__init__(self, sr_names_csv)

    def run(self, sr_names_csv):
        if sr_names_csv:
            sr_names = [sr_path_rx.sub('\g<name>', s.strip())
                        for s in sr_names_csv.split(',')]
            return Subreddit._by_name(sr_names)
        elif self.required:
            self.set_error(errors.BAD_SR_NAME, code=400)
        return {}

    def param_docs(self):
        return {
            self.param: "comma-delimited list of subreddit names",
        }


class VSubredditTitle(Validator):
    def run(self, title):
        if not title:
            self.set_error(errors.NO_TITLE)
        elif len(title) > 100:
            self.set_error(errors.TITLE_TOO_LONG)
        else:
            return title

class VSubredditDesc(Validator):
    def run(self, description):
        if description and len(description) > 500:
            self.set_error(errors.DESC_TOO_LONG)
        return unkeep_space(description or '')


class VAvailableSubredditRuleName(Validator):
    def __init__(self, short_name, updating=False):
        Validator.__init__(self, short_name)
        self.updating = updating

    def run(self, short_name):
        short_name = VLength(
            self.param,
            max_length=50,
            min_length=1,
        ).run(short_name.strip())
        if not short_name:
            return None

        if SubredditRules.get_rule(c.site, short_name):
            self.set_error(errors.SR_RULE_EXISTS)
        elif not self.updating:
            number_rules = len(SubredditRules.get_rules(c.site))
            if number_rules >= MAX_RULES_PER_SUBREDDIT:
                self.set_error(errors.SR_RULE_TOO_MANY)
                return None
        return short_name


class VSubredditRule(Validator):
    def run(self, short_name):
        short_name = VLength(
            self.param,
            max_length=50,
            min_length=1,
        ).run(short_name.strip())
        if not short_name:
            self.set_error(errors.SR_RULE_DOESNT_EXIST)
            return None

        rule = SubredditRules.get_rule(c.site, short_name)
        if not rule:
            self.set_error(errors.SR_RULE_DOESNT_EXIST)
        else:
            return rule


class VAccountByName(VRequired):
    def __init__(self, param, error = errors.USER_DOESNT_EXIST, *a, **kw):
        VRequired.__init__(self, param, error, *a, **kw)

    def run(self, name):
        if name:
            try:
                return Account._by_name(name)
            except NotFound: pass
        return self.error()

    def param_docs(self):
        return {self.param: "A valid, existing reddit username"}


class VFriendOfMine(VAccountByName):
    def run(self, name):
        # Must be logged in
        VUser().run()
        maybe_friend = VAccountByName.run(self, name)
        if maybe_friend:
            friend_rel = Account.get_friend(c.user, maybe_friend)
            if friend_rel:
                return friend_rel
            else:
                self.error(errors.NOT_FRIEND)
        return None


def fullname_regex(thing_cls = None, multiple = False):
    pattern = "[%s%s]" % (Relation._type_prefix, Thing._type_prefix)
    if thing_cls:
        pattern += utils.to36(thing_cls._type_id)
    else:
        pattern += r"[0-9a-z]+"
    pattern += r"_[0-9a-z]+"
    if multiple:
        pattern = r"(%s *,? *)+" % pattern
    return re.compile(r"\A" + pattern + r"\Z")

class VByName(Validator):
    # Lookup tdb_sql.Thing or tdb_cassandra.Thing objects by fullname.
    splitter = re.compile('[ ,]+')
    def __init__(self, param, thing_cls=None, multiple=False, limit=None,
                 error=errors.NO_THING_ID, ignore_missing=False,
                 backend='sql', **kw):
        # Limit param only applies when multiple is True
        if not multiple and limit is not None:
            raise TypeError('multiple must be True when limit is set')
        self.thing_cls = thing_cls
        self.re = fullname_regex(thing_cls)
        self.multiple = multiple
        self.limit = limit
        self._error = error
        self.ignore_missing = ignore_missing
        self.backend = backend

        Validator.__init__(self, param, **kw)

    def run(self, items):
        if self.backend == 'cassandra':
            # tdb_cassandra.Thing objects can't use the regex
            if items and self.multiple:
                items = [item for item in self.splitter.split(items)]
                if self.limit and len(items) > self.limit:
                    return self.set_error(errors.TOO_MANY_THING_IDS)
            if items:
                try:
                    return tdb_cassandra.Thing._by_fullname(
                        items,
                        ignore_missing=self.ignore_missing,
                        return_dict=False,
                    )
                except tdb_cassandra.NotFound:
                    pass
        else:
            if items and self.multiple:
                items = [item for item in self.splitter.split(items)
                         if item and self.re.match(item)]
                if self.limit and len(items) > self.limit:
                    return self.set_error(errors.TOO_MANY_THING_IDS)
            if items and (self.multiple or self.re.match(items)):
                try:
                    return Thing._by_fullname(
                        items,
                        return_dict=False,
                        ignore_missing=self.ignore_missing,
                        data=True,
                    )
                except NotFound:
                    pass

        return self.set_error(self._error)

    def param_docs(self):
        thingtype = (self.thing_cls or Thing).__name__.lower()
        if self.multiple:
            return {
                self.param: ("A comma-separated list of %s [fullnames]"
                             "(#fullnames)" % thingtype)
            }
        else:
            return {
                self.param: "[fullname](#fullnames) of a %s" % thingtype,
            }

class VByNameIfAuthor(VByName):
    def run(self, fullname):
        thing = VByName.run(self, fullname)
        if thing:
            if c.user_is_loggedin and thing.author_id == c.user._id:
                return thing
        return self.set_error(errors.NOT_AUTHOR)

    def param_docs(self):
        return {
            self.param: "[fullname](#fullnames) of a thing created by the user",
        }

class VCaptcha(Validator):
    default_param = ('iden', 'captcha')

    def run(self, iden, solution):
        if c.user.needs_captcha():
            valid_captcha = captcha.valid_solution(iden, solution)
            if not valid_captcha:
                self.set_error(errors.BAD_CAPTCHA)
            g.stats.action_event_count("captcha", valid_captcha)

    def param_docs(self):
        return {
            self.param[0]: "the identifier of the CAPTCHA challenge",
            self.param[1]: "the user's response to the CAPTCHA challenge",
        }


class VUser(Validator):
    def run(self):
        if not c.user_is_loggedin:
            raise UserRequiredException


class VNotInTimeout(Validator):
    def run(self, target_fullname=None, fatal=True, action_name=None,
            details_text="", target=None, subreddit=None):
        if c.user_is_loggedin and c.user.in_timeout:
            g.events.timeout_forbidden_event(
                action_name,
                details_text=details_text,
                target=target,
                target_fullname=target_fullname,
                subreddit=subreddit,
                request=request,
                context=c,
            )
            if fatal:
                request.environ['REDDIT_ERROR_NAME'] = 'IN_TIMEOUT'
                abort(403, errors.IN_TIMEOUT)
            return False


class VVerifyPassword(Validator):
    def __init__(self, param, fatal=True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.fatal = fatal

    def run(self, password):
        VUser().run()
        if not valid_password(c.user, password):
            if self.fatal:
                abort(403)
            self.set_error(errors.WRONG_PASSWORD)
            return None
        # bcrypt wants a bytestring
        return _force_utf8(password)

    def param_docs(self):
        return {
            self.param: "the current user's password",
        }


class VModhash(Validator):
    handles_csrf = True
    default_param = 'uh'

    def __init__(self, param=None, fatal=True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.fatal = fatal

    def run(self, modhash):
        # OAuth authenticated requests do not require CSRF protection.
        if c.oauth_user:
            return

        VUser().run()

        if modhash is None:
            modhash = request.headers.get('X-Modhash')

        hook = hooks.get_hook("modhash.validate")
        result = hook.call_until_return(modhash=modhash)

        # if no plugins validate the hash, just check if it's the user name
        if result is None:
            result = (modhash == c.user.name)

        if not result:
            g.stats.simple_event("event.modhash.invalid")
            if self.fatal:
                abort(403)
            self.set_error('INVALID_MODHASH')

    def param_docs(self):
        return {
            '%s / X-Modhash header' % self.param: 'a [modhash](#modhashes)',
        }


class VModhashIfLoggedIn(Validator):
    handles_csrf = True
    default_param = 'uh'

    def __init__(self, param=None, fatal=True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.fatal = fatal

    def run(self, modhash):
        if c.user_is_loggedin:
            VModhash(fatal=self.fatal).run(modhash)

    def param_docs(self):
        return {
            '%s / X-Modhash header' % self.param: 'a [modhash](#modhashes)',
        }


class VAdmin(Validator):
    def run(self):
        if not c.user_is_admin:
            abort(404, "page not found")

def make_or_admin_secret_cls(base_cls):
    class VOrAdminSecret(base_cls):
        handles_csrf = True

        def run(self, secret=None):
            '''If validation succeeds, return True if the secret was used,
            False otherwise'''
            if secret and constant_time_compare(secret,
                                                g.secrets["ADMINSECRET"]):
                return True
            super(VOrAdminSecret, self).run()

            if request.method.upper() != "GET":
                VModhash(fatal=True).run(request.POST.get("uh"))

            return False
    return VOrAdminSecret

VAdminOrAdminSecret = make_or_admin_secret_cls(VAdmin)

class VVerifiedUser(VUser):
    def run(self):
        VUser.run(self)
        if not c.user.email_verified:
            raise VerifiedUserRequiredException

class VGold(VUser):
    notes = "*Requires a subscription to [reddit gold](/gold/about)*"
    def run(self):
        VUser.run(self)
        if not c.user.gold:
            abort(403, 'forbidden')

class VSponsorAdmin(VVerifiedUser):
    """
    Validator which checks c.user_is_sponsor
    """
    def user_test(self, thing):
        return (thing.author_id == c.user._id)

    def run(self, link_id = None):
        VVerifiedUser.run(self)
        if c.user_is_sponsor:
            return
        abort(403, 'forbidden')

VSponsorAdminOrAdminSecret = make_or_admin_secret_cls(VSponsorAdmin)

class VSponsor(VUser):
    """
    Not intended to be used as a check for c.user_is_sponsor, but
    rather is the user allowed to use the sponsored link system.
    If a link or campaign is passed in, it also checks whether the user is
    allowed to edit that particular sponsored link.
    """
    def user_test(self, thing):
        return (thing.author_id == c.user._id)

    def run(self, link_id=None, campaign_id=None):
        assert not (link_id and campaign_id), 'Pass link or campaign, not both'

        VUser.run(self)
        if c.user_is_sponsor:
            return
        elif campaign_id:
            pc = None
            try:
                if '_' in campaign_id:
                    pc = PromoCampaign._by_fullname(campaign_id, data=True)
                else:
                    pc = PromoCampaign._byID36(campaign_id, data=True)
            except (NotFound, ValueError):
                pass
            if pc:
                link_id = pc.link_id
        if link_id:
            try:
                if '_' in link_id:
                    t = Link._by_fullname(link_id, True)
                else:
                    aid = int(link_id, 36)
                    t = Link._byID(aid, True)
                if self.user_test(t):
                    return
            except (NotFound, ValueError):
                pass
            abort(403, 'forbidden')


class VVerifiedSponsor(VSponsor):
    def run(self, *args, **kwargs):
        VVerifiedUser().run()

        return super(VVerifiedSponsor, self).run(*args, **kwargs)


class VEmployee(VVerifiedUser):
    """Validate that user is an employee."""
    def run(self):
        if not c.user.employee:
            abort(403, 'forbidden')
        VVerifiedUser.run(self)


class VSrModerator(Validator):
    def __init__(self, fatal=True, perms=(), *a, **kw):
        # If True, abort rather than setting an error
        self.fatal = fatal
        self.perms = utils.tup(perms)
        super(VSrModerator, self).__init__(*a, **kw)

    def run(self):
        if not (c.user_is_loggedin
                and c.site.is_moderator_with_perms(c.user, *self.perms)
                or c.user_is_admin):
            if self.fatal:
                abort(403, "forbidden")
            return self.set_error('MOD_REQUIRED', code=403)


class VCanDistinguish(VByName):
    def run(self, thing_name, how):
        if c.user_is_loggedin:
            can_distinguish = False
            item = VByName.run(self, thing_name)

            if not item:
                abort(404)

            if item.author_id == c.user._id:
                if isinstance(item, Message) and c.user.employee:
                    return True
                subreddit = item.subreddit_slow

                if (how in ("yes", "no") and
                        subreddit.can_distinguish(c.user)):
                    can_distinguish = True
                elif (how in ("special", "no") and
                        c.user_special_distinguish):
                    can_distinguish = True
                elif (how in ("admin", "no") and
                        c.user.employee):
                    can_distinguish = True

                if can_distinguish:
                    # Don't allow distinguishing for users in timeout
                    VNotInTimeout().run(target=item, subreddit=subreddit)
                    return can_distinguish

        abort(403,'forbidden')

    def param_docs(self):
        return {}

class VSrCanAlter(VByName):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            can_alter = False
            subreddit = None
            item = VByName.run(self, thing_name)

            if item.author_id == c.user._id:
                can_alter = True
            elif item.promoted and c.user_is_sponsor:
                can_alter = True
            else:
                # will throw a legitimate 500 if this isn't a link or
                # comment, because this should only be used on links and
                # comments
                subreddit = item.subreddit_slow
                if subreddit.can_distinguish(c.user):
                    can_alter = True

            if can_alter:
                # Don't allow mod actions for users who are in timeout
                VNotInTimeout().run(target=item, subreddit=subreddit)
                return can_alter

        abort(403,'forbidden')

class VSrCanBan(VByName):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = VByName.run(self, thing_name)

            if isinstance(item, (Link, Comment)):
                sr = item.subreddit_slow
                if sr.is_moderator_with_perms(c.user, 'posts'):
                    return True
            elif isinstance(item, Message):
                sr = item.subreddit_slow
                if sr and sr.is_moderator_with_perms(c.user, 'mail'):
                    return True
        abort(403,'forbidden')

class VSrSpecial(VByName):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = VByName.run(self, thing_name)
            # will throw a legitimate 500 if this isn't a link or
            # comment, because this should only be used on links and
            # comments
            subreddit = item.subreddit_slow
            if subreddit.is_special(c.user):
                return True
        abort(403,'forbidden')


class VSubmitParent(VByName):
    def run(self, fullname, fullname2):
        # for backwards compatibility (with iphone app)
        fullname = fullname or fullname2
        parent = VByName.run(self, fullname) if fullname else None

        if not parent:
            # for backwards compatibility (normally 404)
            abort(403, "forbidden")

        if not isinstance(parent, (Comment, Link, Message)):
            # for backwards compatibility (normally 400)
            abort(403, "forbidden")

        if not c.user_is_loggedin:
            # in practice this is handled by VUser
            abort(403, "forbidden")

        if parent.author_id in c.user.enemies:
            self.set_error(errors.USER_BLOCKED)

        if isinstance(parent, Message):
            return parent

        elif isinstance(parent, Link):
            sr = parent.subreddit_slow

            if parent.is_archived(sr):
                self.set_error(errors.TOO_OLD)
            elif parent.locked and not sr.can_distinguish(c.user):
                self.set_error(errors.THREAD_LOCKED)

            if self.has_errors or parent.can_comment_slow(c.user):
                return parent

        elif isinstance(parent, Comment):
            sr = parent.subreddit_slow

            if parent._deleted:
                self.set_error(errors.DELETED_COMMENT)

            elif parent._spam:
                # Only author, mod or admin can reply to removed comments
                can_reply = (c.user_is_loggedin and
                             (parent.author_id == c.user._id or
                              c.user_is_admin or
                              sr.is_moderator(c.user)))
                if not can_reply:
                    self.set_error(errors.DELETED_COMMENT)

            link = Link._byID(parent.link_id, data=True)

            if link.is_archived(sr):
                self.set_error(errors.TOO_OLD)
            elif link.locked and not sr.can_distinguish(c.user):
                self.set_error(errors.THREAD_LOCKED)

            if self.has_errors or link.can_comment_slow(c.user):
                return parent

        abort(403, "forbidden")

    def param_docs(self):
        return {
            self.param[0]: "[fullname](#fullnames) of parent thing",
        }

class VSubmitSR(Validator):
    def __init__(self, srname_param, linktype_param=None, promotion=False):
        self.require_linktype = False
        self.promotion = promotion

        if linktype_param:
            self.require_linktype = True
            Validator.__init__(self, (srname_param, linktype_param))
        else:
            Validator.__init__(self, srname_param)

    def run(self, sr_name, link_type = None):
        if not sr_name:
            self.set_error(errors.SUBREDDIT_REQUIRED)
            return None

        try:
            sr_name = sr_path_rx.sub('\g<name>', str(sr_name).strip())
            sr = Subreddit._by_name(sr_name)
        except (NotFound, AttributeError, UnicodeEncodeError):
            self.set_error(errors.SUBREDDIT_NOEXIST)
            return

        if not c.user_is_loggedin or not sr.can_submit(c.user, self.promotion):
            self.set_error(errors.SUBREDDIT_NOTALLOWED)
            return

        if not sr.allow_ads and self.promotion:
            self.set_error(errors.SUBREDDIT_DISABLED_ADS)
            return

        if self.require_linktype:
            if link_type not in ('link', 'self'):
                self.set_error(errors.INVALID_OPTION)
                return
            elif link_type == "link" and not sr.can_submit_link(c.user):
                self.set_error(errors.NO_LINKS)
                return
            elif link_type == "self" and not sr.can_submit_text(c.user):
                self.set_error(errors.NO_SELFS)
                return

        return sr

    def param_docs(self):
        return {
            self.param[0]: "name of a subreddit",
        }

class VSubscribeSR(VByName):
    def __init__(self, srid_param, srname_param):
        VByName.__init__(self, (srid_param, srname_param))

    def run(self, sr_id, sr_name):
        if sr_id:
            return VByName.run(self, sr_id)
        elif not sr_name:
            return

        try:
            sr = Subreddit._by_name(str(sr_name).strip())
        except (NotFound, AttributeError, UnicodeEncodeError):
            self.set_error(errors.SUBREDDIT_NOEXIST)
            return

        return sr

    def param_docs(self):
        return {
            self.param[0]: "the name of a subreddit",
        }


RE_GTM_ID = re.compile(r"^GTM-[A-Z0-9]+$")

class VGTMContainerId(Validator):
    def run(self, value):
        if not value:
            return g.googletagmanager

        if RE_GTM_ID.match(value):
            return value
        else:
            abort(404)


class VCollection(Validator):
    def run(self, name):
        collection = Collection.by_name(name)
        if collection:
            return collection
        self.set_error(errors.COLLECTION_NOEXIST)


class VPromoTarget(Validator):
    default_param = ("targeting", "sr", "collection")

    def run(self, targeting, sr_name, collection_name):
        if targeting == "collection" and collection_name == "none":
            return Target(Frontpage.name)
        elif targeting == "none":
            return Target(Frontpage.name)
        elif targeting == "collection":
            collection = VCollection("collection").run(collection_name)
            if collection:
                return Target(collection)
            else:
                # VCollection added errors so no need to do anything
                return
        elif targeting == "one":
            sr = VSubmitSR("sr", promotion=True).run(sr_name)
            if sr:
                return Target(sr.name)
            else:
                # VSubmitSR added errors so no need to do anything
                return
        else:
            self.set_error(errors.INVALID_TARGET, field="targeting")


class VOSVersion(Validator):
    def __init__(self, param, os, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.os = os

    def assign_error(self):
        self.set_error(errors.INVALID_OS_VERSION, field="os_version")

    def run(self, version_range):
        if not version_range:
            return

        # check that string conforms to `min,max` format
        try:
            min, max = version_range.split(',')
        except ValueError:
            self.assign_error()
            return

        # check for type errors
        # (max can be empty string, otherwise both float)
        type_errors = False
        if max == '':
            # check that min is a float
            try:
                min = float(min)
            except ValueError:
                type_errors = True
        else:
            # check that min and max are both floats
            try:
                min, max = float(min), float(max)
                # ensure than min is less-or-equal-to max
                if min > max:
                    type_errors = True
            except ValueError:
                type_errors = True

        if type_errors == True:
            self.assign_error()
            return

        for endpoint in (min, max):
            if endpoint != '':
                # check that the version is in the global config
                if endpoint not in getattr(g, '%s_versions' % self.os):
                    self.assign_error()
                    return

        return [str(min), str(max)]


MIN_PASSWORD_LENGTH = 6

class VPassword(Validator):
    def run(self, password):
        if not (password and len(password) >= MIN_PASSWORD_LENGTH):
            self.set_error(errors.SHORT_PASSWORD, {"chars": MIN_PASSWORD_LENGTH})
            self.set_error(errors.BAD_PASSWORD)
        else:
            return password.encode('utf8')

    def param_docs(self):
        return {
            self.param[0]: "the password"
        }


class VPasswordChange(VPassword):
    def run(self, password, verify):
        base = super(VPasswordChange, self).run(password)

        if self.has_errors:
            return base

        if (verify != password):
            self.set_error(errors.BAD_PASSWORD_MATCH)
        else:
            return base

    def param_docs(self):
        return {
            self.param[0]: "the new password",
            self.param[1]: "the password again (for verification)",
        }

MIN_USERNAME_LENGTH = 3
MAX_USERNAME_LENGTH = 20

user_rx = re.compile(r"\A[\w-]+\Z", re.UNICODE)

def chkuser(x):
    if x is None:
        return None
    try:
        if any(ch.isspace() for ch in x):
            return None
        return str(x) if user_rx.match(x) else None
    except TypeError:
        return None
    except UnicodeEncodeError:
        return None

class VUname(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_USERNAME, *a, **kw)
    def run(self, user_name):
        length = 0 if not user_name else len(user_name)
        if (length < MIN_USERNAME_LENGTH or length > MAX_USERNAME_LENGTH):
            msg_params = {
                'min': MIN_USERNAME_LENGTH,
                'max': MAX_USERNAME_LENGTH,
            }
            self.set_error(errors.USERNAME_TOO_SHORT, msg_params=msg_params)
            self.set_error(errors.BAD_USERNAME)
            return
        user_name = chkuser(user_name)
        if not user_name:
            self.set_error(errors.USERNAME_INVALID_CHARACTERS)
            self.set_error(errors.BAD_USERNAME)
            return
        else:
            try:
                a = Account._by_name(user_name, True)
                if a._deleted:
                   return self.set_error(errors.USERNAME_TAKEN_DEL)
                else:
                   return self.set_error(errors.USERNAME_TAKEN)
            except NotFound:
                return user_name

    def param_docs(self):
        return {
            self.param[0]: "a valid, unused, username",
        }

class VLoggedOut(Validator):
    def run(self):
        if c.user_is_loggedin:
            self.set_error(errors.LOGGED_IN)


class AuthenticationFailed(Exception):
    pass


class LoginRatelimit(object):
    def __init__(self, category, key):
        self.category = category
        self.key = key

    def __str__(self):
        return "login-%s-%s" % (self.category, self.key)

    def __hash__(self):
        return hash(str(self))


class VThrottledLogin(VRequired):
    def __init__(self, params):
        VRequired.__init__(self, params, error=errors.WRONG_PASSWORD)
        self.vlength = VLength("user", max_length=100)
        self.seconds = None

    def get_ratelimits(self, account):
        is_previously_seen_ip = request.ip in [
            j for i in IPsByAccount.get(account._id, column_count=1000)
            for j in i.itervalues()
        ]

        # We want to maintain different rate-limit buckets depending on whether
        # we have seen the IP logging in before.  If someone is trying to brute
        # force an account from an unfamiliar location, we will rate limit
        # *all* requests from unfamiliar locations that try to access the
        # account, while still maintaining a separate rate-limit for IP
        # addresses we have seen use the account before.
        #
        # Finally, we also rate limit IPs themselves that appear to be trying
        # to log into accounts they have never logged into before.  This goes
        # into a separately maintained bucket.
        if is_previously_seen_ip:
            ratelimits = {
                LoginRatelimit("familiar", account._id): g.RL_LOGIN_MAX_REQS,
            }
        else:
            ratelimits = {
                LoginRatelimit("unfamiliar", account._id): g.RL_LOGIN_MAX_REQS,
                LoginRatelimit("ip", request.ip): g.RL_LOGIN_IP_MAX_REQS,
            }

        hooks.get_hook("login.ratelimits").call(
            ratelimits=ratelimits,
            familiar=is_previously_seen_ip,
        )

        return ratelimits

    def run(self, username, password):
        ratelimits = {}

        try:
            if username:
                username = username.strip()
                username = self.vlength.run(username)
                username = chkuser(username)

            if not username:
                raise AuthenticationFailed

            try:
                account = Account._by_name(username)
            except NotFound:
                raise AuthenticationFailed

            hooks.get_hook("account.spotcheck").call(account=account)
            if account._banned:
                raise AuthenticationFailed

            # if already logged in, you're exempt from your own ratelimit
            # (e.g. to allow account deletion regardless of DoS)
            ratelimit_exempt = (account == c.user)
            if not ratelimit_exempt:
                time_slice = ratelimit.get_timeslice(g.RL_RESET_SECONDS)
                ratelimits = self.get_ratelimits(account)
                now = int(time.time())

                for rl, max_requests in ratelimits.iteritems():
                    try:
                        failed_logins = ratelimit.get_usage(str(rl), time_slice)

                        if failed_logins >= max_requests:
                            self.seconds = time_slice.end - now
                            period_end = datetime.utcfromtimestamp(
                                time_slice.end).replace(tzinfo=pytz.UTC)
                            remaining_text = utils.timeuntil(period_end)
                            self.set_error(
                                errors.RATELIMIT, {'time': remaining_text},
                                field='ratelimit', code=429)
                            g.stats.event_count('login.throttle', rl.category)
                            return False
                    except ratelimit.RatelimitError as e:
                        g.log.info("ratelimitcache error (login): %s", e)

            try:
                str(password)
            except UnicodeEncodeError:
                password = password.encode("utf8")

            if not valid_password(account, password):
                raise AuthenticationFailed
            g.stats.event_count('login', 'success')
            return account
        except AuthenticationFailed:
            g.stats.event_count('login', 'failure')
            if ratelimits:
                for rl in ratelimits:
                    try:
                        ratelimit.record_usage(str(rl), time_slice)
                    except ratelimit.RatelimitError as e:
                        g.log.info("ratelimitcache error (login): %s", e)
            self.error()
            return False

    def param_docs(self):
        return {
            self.param[0]: "a username",
            self.param[1]: "the user's password",
        }


class VSanitizedUrl(Validator):
    def run(self, url):
        return utils.sanitize_url(url)

    def param_docs(self):
        return {self.param: "a valid URL"}


class VUrl(VRequired):
    def __init__(self, item, allow_self=True, require_scheme=False,
                 valid_schemes=utils.VALID_SCHEMES, *a, **kw):
        self.allow_self = allow_self
        self.require_scheme = require_scheme
        self.valid_schemes = valid_schemes
        VRequired.__init__(self, item, errors.NO_URL, *a, **kw)

    def run(self, url):
        if not url:
            return self.error(errors.NO_URL)

        url = utils.sanitize_url(url, require_scheme=self.require_scheme,
                                 valid_schemes=self.valid_schemes)
        if not url:
            return self.error(errors.BAD_URL)

        try:
            url.encode('utf-8')
        except UnicodeDecodeError:
            return self.error(errors.BAD_URL)

        if url == 'self':
            if self.allow_self:
                return url
            else:
                self.error(errors.BAD_URL)
        else:
            return url

    def param_docs(self):
        return {self.param: "a valid URL"}


class VRedirectUri(VUrl):
    def __init__(self, item, valid_schemes=None, *a, **kw):
        VUrl.__init__(self, item, allow_self=False, require_scheme=True,
                      valid_schemes=valid_schemes, *a, **kw)

    def param_docs(self):
        doc = "a valid URI"
        if self.valid_schemes:
            doc += " with one of the following schemes: "
            doc += ", ".join(self.valid_schemes)
        return {self.param: doc}


class VShamedDomain(Validator):
    def run(self, url):
        if not url:
            return

        is_shamed, domain, reason = is_shamed_domain(url)

        if is_shamed:
            self.set_error(errors.DOMAIN_BANNED, dict(domain=domain,
                                                      reason=reason))

class VExistingUname(VRequired):
    def __init__(self, item, allow_deleted=False, *a, **kw):
        self.allow_deleted = allow_deleted
        VRequired.__init__(self, item, errors.NO_USER, *a, **kw)

    def run(self, name):
        if name:
            name = name.strip()
        if name and name.startswith('~') and c.user_is_admin:
            try:
                user_id = int(name[1:])
                return Account._byID(user_id, True)
            except (NotFound, ValueError):
                self.error(errors.USER_DOESNT_EXIST)

        # make sure the name satisfies our user name regexp before
        # bothering to look it up.
        name = chkuser(name)
        if name:
            try:
                return Account._by_name(name)
            except NotFound:
                if self.allow_deleted:
                    try:
                        return Account._by_name(name, allow_deleted=True)
                    except NotFound:
                        pass

                self.error(errors.USER_DOESNT_EXIST)
        else:
            self.error()

    def param_docs(self):
        return {
            self.param: 'the name of an existing user'
        }

class VMessageRecipient(VExistingUname):
    def run(self, name):
        if not name:
            return self.error()
        is_subreddit = False
        if name.startswith('/r/'):
            name = name[3:]
            is_subreddit = True
        elif name.startswith('#'):
            name = name[1:]
            is_subreddit = True

        # A user in timeout should only be able to message us, the admins.
        if (c.user.in_timeout and
                not (is_subreddit and
                     '/r/%s' % name == g.admin_message_acct)):
            abort(403, 'forbidden')

        if is_subreddit:
            try:
                s = Subreddit._by_name(name)
                if isinstance(s, FakeSubreddit):
                    raise NotFound, "fake subreddit"
                if s._spam:
                    raise NotFound, "banned subreddit"
                if s.is_muted(c.user) and not c.user_is_admin:
                    self.set_error(errors.USER_MUTED)
                return s
            except NotFound:
                self.set_error(errors.SUBREDDIT_NOEXIST)
        else:
            account = VExistingUname.run(self, name)
            if account and account._id in c.user.enemies:
                self.set_error(errors.USER_BLOCKED)
            else:
                return account

class VUserWithEmail(VExistingUname):
    def run(self, name):
        user = VExistingUname.run(self, name)
        if not user or not hasattr(user, 'email') or not user.email:
            return self.error(errors.NO_EMAIL_FOR_USER)
        return user


class VBoolean(Validator):
    def run(self, val):
        if val is True or val is False:
            # val is already a bool object, no processing needed
            return val
        lv = str(val).lower()
        if lv == 'off' or lv == '' or lv[0] in ("f", "n"):
            return False
        return bool(val)

    def param_docs(self):
        return {
            self.param: 'boolean value',
        }

class VNumber(Validator):
    def __init__(self, param, min=None, max=None, coerce = True,
                 error=errors.BAD_NUMBER, num_default=None,
                 *a, **kw):
        self.min = self.cast(min) if min is not None else None
        self.max = self.cast(max) if max is not None else None
        self.coerce = coerce
        self.error = error
        self.num_default = num_default
        Validator.__init__(self, param, *a, **kw)

    def cast(self, val):
        raise NotImplementedError

    def _set_error(self):
        if self.max is None and self.min is None:
            range = ""
        elif self.max is None:
            range = _("%(min)d to any") % dict(min=self.min)
        elif self.min is None:
            range = _("any to %(max)d") % dict(max=self.max)
        else:
            range = _("%(min)d to %(max)d") % dict(min=self.min, max=self.max)
        self.set_error(self.error, msg_params=dict(range=range))

    def run(self, val):
        if not val:
            return self.num_default
        try:
            val = self.cast(val)
            if self.min is not None and val < self.min:
                if self.coerce:
                    val = self.min
                else:
                    raise ValueError, ""
            elif self.max is not None and val > self.max:
                if self.coerce:
                    val = self.max
                else:
                    raise ValueError, ""
            return val
        except ValueError:
            self._set_error()

class VInt(VNumber):
    def cast(self, val):
        return int(val)

    def param_docs(self):
        if self.min is not None and self.max is not None:
            description = "an integer between %d and %d" % (self.min, self.max)
        elif self.min is not None:
            description = "an integer greater than %d" % self.min
        elif self.max is not None:
            description = "an integer less than %d" % self.max
        else:
            description = "an integer"

        if self.num_default is not None:
            description += " (default: %d)" % self.num_default

        return {self.param: description}


class VFloat(VNumber):
    def cast(self, val):
        return float(val)


class VDecimal(VNumber):
    def cast(self, val):
        return Decimal(val)


class VCssName(Validator):
    """
    returns a name iff it consists of alphanumeric characters and
    possibly "-", and is below the length limit.
    """

    r_css_name = re.compile(r"\A[a-zA-Z0-9\-]{1,100}\Z")

    def run(self, name):
        if name:
            if self.r_css_name.match(name):
                return name
            else:
                self.set_error(errors.BAD_CSS_NAME)
        return ''

    def param_docs(self):
        return {
            self.param: "a valid subreddit image name",
        }

class VColor(Validator):
    """Validate a string as being a 6 digit hex color starting with #"""
    color = re.compile(r"\A#[a-f0-9]{6}\Z", re.IGNORECASE)

    def run(self, color):
        if color:
            if self.color.match(color):
                return color.lower()
            else:
                self.set_error(errors.BAD_COLOR)
        return ''

    def param_docs(self):
        return {
            self.param: "a 6-digit rgb hex color, e.g. `#AABBCC`",
        }


class VMenu(Validator):
    def __init__(self, param, menu_cls, remember = True, **kw):
        self.nav = menu_cls
        self.remember = remember
        param = (menu_cls.name, param)
        Validator.__init__(self, param, **kw)

    def run(self, sort, where):
        if self.remember:
            pref = "%s_%s" % (where, self.nav.name)
            user_prefs = copy(c.user.sort_options) if c.user else {}
            user_pref = user_prefs.get(pref)

            # check to see if a default param has been set
            if not sort:
                sort = user_pref

        # validate the sort
        if sort not in self.nav._options:
            sort = self.nav._default

        # commit the sort if changed and if this is a POST request
        if (self.remember and c.user_is_loggedin and sort != user_pref
            and request.method.upper() == 'POST'):
            user_prefs[pref] = sort
            c.user.sort_options = user_prefs
            user = c.user
            user._commit()

        return sort

    def param_docs(self):
        return {
            self.param[0]: 'one of (%s)' % ', '.join("`%s`" % s
                                                  for s in self.nav._options),
        }


class VRatelimit(Validator):
    def __init__(self, rate_user=False, rate_ip=False, prefix='rate_',
                 error=errors.RATELIMIT, fatal=False, *a, **kw):
        self.rate_user = rate_user
        self.rate_ip = rate_ip
        self.name = prefix
        self.cache_prefix = "rl:%s" % self.name
        self.error = error
        self.fatal = fatal
        self.seconds = None
        Validator.__init__(self, *a, **kw)

    def run(self):
        if g.disable_ratelimit:
            return

        if c.user_is_loggedin:
            hook = hooks.get_hook("account.is_ratelimit_exempt")
            ratelimit_exempt = hook.call_until_return(account=c.user)
            if ratelimit_exempt:
                self._record_event(self.name, 'exempted')
                return

        to_check = []
        if self.rate_user and c.user_is_loggedin:
            to_check.append('user' + str(c.user._id36))
            self._record_event(self.name, 'check_user')
        if self.rate_ip:
            to_check.append('ip' + str(request.ip))
            self._record_event(self.name, 'check_ip')

        r = g.ratelimitcache.get_multi(to_check, prefix=self.cache_prefix)
        if r:
            expire_time = max(r.values())
            time = utils.timeuntil(expire_time)

            g.log.debug("rate-limiting %s from %s" % (self.name, r.keys()))
            for key in r.keys():
                if key.startswith('user'):
                    self._record_event(self.name, 'user_limit_hit')
                elif key.startswith('ip'):
                    self._record_event(self.name, 'ip_limit_hit')

            # when errors have associated field parameters, we'll need
            # to add that here
            if self.error == errors.RATELIMIT:
                from datetime import datetime
                delta = expire_time - datetime.now(g.tz)
                self.seconds = delta.total_seconds()
                if self.seconds < 3:  # Don't ratelimit within three seconds
                    return
                if self.fatal:
                    abort(429)
                self.set_error(errors.RATELIMIT, {'time': time},
                               field='ratelimit', code=429)
            else:
                if self.fatal:
                    abort(429)
                self.set_error(self.error)

    @classmethod
    def ratelimit(cls, rate_user=False, rate_ip=False, prefix="rate_",
                  seconds=None):
        name = prefix
        cache_prefix = "rl:%s" % name

        if seconds is None:
            seconds = g.RL_RESET_SECONDS

        expire_time = datetime.now(g.tz) + timedelta(seconds=seconds)

        to_set = {}
        if rate_user and c.user_is_loggedin:
            to_set['user' + str(c.user._id36)] = expire_time
            cls._record_event(name, 'set_user_limit')

        if rate_ip:
            to_set['ip' + str(request.ip)] = expire_time
            cls._record_event(name, 'set_ip_limit')

        g.ratelimitcache.set_multi(to_set, prefix=cache_prefix, time=seconds)

    @classmethod
    def _record_event(cls, name, event):
        g.stats.event_count('VRatelimit.%s' % name, event, sample_rate=0.1)


class VRatelimitImproved(Validator):
    """Enforce ratelimits on a function.

    This is a newer version of VRatelimit that uses the ratelimit lib.
    """

    class RateLimit(ratelimit.RateLimit):
        """A RateLimit with defaults specialized for VRatelimitImproved.

        Arguments:
            event_action: The type of the action the user took, for logging.
            event_type: Part of the key in the rate limit cache.
            limit: The RateLimit.limit value. Allowed hits per batch of seconds.
            seconds: The RateLimit.seconds value. How may seconds per batch.
            event_id_fn: Nullary function that derives an id from the current
                context.
        """
        sample_rate = 0.1

        def __init__(self,
                     event_action, event_type, limit, seconds, event_id_fn):
            ratelimit.RateLimit.__init__(self)
            self.event_name = 'VRatelimitImproved.' + event_action
            self.event_type = event_type
            self.event_id_fn = event_id_fn
            self.limit = limit
            self.seconds = seconds

        @property
        def key(self):
            return 'ratelimit-%s-%s' % (self.event_type, self.event_id_fn())

    def __init__(self, user_limit=None, ip_limit=None, error=errors.RATELIMIT,
                 *a, **kw):
        """
        At least one of user_limit and ip_limit should be set for this function
        to have any effect.

        Arguments:
            user_limit: RateLimit -- The per-user rate limit.
            ip_limit: RateLimit -- The per-IP rate limit.
            error -- the error message to use when the limit is exceeded.
        """
        self.user_limit = user_limit
        self.ip_limit = ip_limit
        self.error = error

        # _validatedForm passes self.seconds to the current form's javascript.
        self.seconds = None
        Validator.__init__(self, *a, **kw)

    def run(self):
        if g.disable_ratelimit:
            return

        if c.user_is_loggedin:
            hook = hooks.get_hook("account.is_ratelimit_exempt")
            ratelimit_exempt = hook.call_until_return(account=c.user)
            if ratelimit_exempt:
                return

        if self.user_limit and c.user_is_loggedin:
            self._check_usage(self.user_limit)

        if self.ip_limit:
            self._check_usage(self.ip_limit)

    def _check_usage(self, rate_limit):
        """Check ratelimit usage and set an error if necessary."""
        if rate_limit.check():
            # Not rate limited.
            return

        g.log.debug('rate-limiting %s with %s used',
                    rate_limit.key, rate_limit.get_usage())
        # When errors have associated field parameters, we'll need
        # to add that here.
        if self.error == errors.RATELIMIT:
            period_end = datetime.utcfromtimestamp(
                rate_limit.timeslice.end).replace(tzinfo=pytz.UTC)
            time = utils.timeuntil(period_end)
            self.set_error(errors.RATELIMIT, {'time': time},
                            field='ratelimit', code=429)
        else:
            self.set_error(self.error)

    @classmethod
    def ratelimit(cls, user_limit=None, ip_limit=None):
        """Record usage of a resource."""
        if user_limit and c.user_is_loggedin:
            user_limit.record_usage()

        if ip_limit:
            ip_limit.record_usage()


class VShareRatelimit(VRatelimitImproved):
    USER_LIMIT = VRatelimitImproved.RateLimit(
        'share', 'user',
        limit=g.RL_SHARE_MAX_REQS,
        seconds=g.RL_RESET_SECONDS,
        event_id_fn=lambda: c.user._id36)

    IP_LIMIT = VRatelimitImproved.RateLimit(
        'share', 'ip',
        limit=g.RL_SHARE_MAX_REQS,
        seconds=g.RL_RESET_SECONDS,
        event_id_fn=lambda: request.ip)

    def __init__(self):
        super(VShareRatelimit, self).__init__(
            user_limit=self.USER_LIMIT, ip_limit=self.IP_LIMIT)

    @classmethod
    def ratelimit(cls):
        super(VShareRatelimit, cls).ratelimit(
            user_limit=cls.USER_LIMIT, ip_limit=cls.IP_LIMIT)


class VCommentIDs(Validator):
    def run(self, id_str):
        if id_str:
            try:
                cids = [int(i, 36) for i in id_str.split(',')]
                return cids
            except ValueError:
                abort(400)
        return []

    def param_docs(self):
        return {
            self.param: "a comma-delimited list of comment ID36s",
        }


class VOneTimeToken(Validator):
    def __init__(self, model, param, *args, **kwargs):
        self.model = model
        Validator.__init__(self, param, *args, **kwargs)

    def run(self, key):
        token = self.model.get_token(key)

        if token:
            return token
        else:
            self.set_error(errors.EXPIRED)
            return None

class VOneOf(Validator):
    def __init__(self, param, options = (), *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.options = options

    def run(self, val):
        if self.options and val not in self.options:
            self.set_error(errors.INVALID_OPTION, code=400)
            return self.default
        else:
            return val

    def param_docs(self):
        return {
            self.param: 'one of (%s)' % ', '.join("`%s`" % s
                                                  for s in self.options),
        }


class VList(Validator):
    def __init__(self, param, separator=",", choices=None,
                 error=errors.INVALID_OPTION, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.separator = separator
        self.choices = choices
        self.error = error

    def run(self, items):
        if not items:
            return None
        all_values = items.split(self.separator)
        if self.choices is None:
            return all_values

        values = []
        for val in all_values:
            if val in self.choices:
                values.append(val)
            else:
                msg_params = {"choice": val}
                self.set_error(self.error, msg_params=msg_params,
                               code=400)
        return values

    # Not i18n'able, but param_docs are not currently i18n'ed
    NICE_SEP = {",": "comma"}
    def param_docs(self):
        if self.choices:
            msg = ("A %(separator)s-separated list of items from "
                   "this set:\n\n%(choices)s")
            choices = "`" + "`  \n`".join(self.choices) + "`"
        else:
            msg = "A %(separator)s-separated list of items"
            choices = None

        sep = self.NICE_SEP.get(self.separator, self.separator)
        docs = msg % {"separator": sep, "choices": choices}
        return {self.param: docs}


class VFrequencyCap(Validator):
    def run(self, frequency_capped='false', frequency_cap=None):

        if frequency_capped == 'true':
            if frequency_cap and int(frequency_cap) >= g.frequency_cap_min:
                try:
                    return frequency_cap
                except (ValueError, TypeError):
                    self.set_error(errors.INVALID_FREQUENCY_CAP, code=400)
            else:
                self.set_error(
                    errors.FREQUENCY_CAP_TOO_LOW,
                    {'min': g.frequency_cap_min},
                    code=400
                )
        else:
            return None


class VPriority(Validator):
    def run(self, val):
        if c.user_is_sponsor:
            return (PROMOTE_PRIORITIES.get(val,
                PROMOTE_DEFAULT_PRIORITY(context=c)))
        elif feature.is_enabled('ads_auction'):
            return PROMOTE_DEFAULT_PRIORITY(context=c)
        else:
            return PROMOTE_PRIORITIES['standard']


class VLocation(Validator):
    default_param = ("country", "region", "metro")

    def run(self, country, region, metro):
        # some browsers are sending "null" rather than omitting the input when
        # the select is disabled
        country, region, metro = map(lambda val: None if val == "null" else val,
                                     [country, region, metro])

        if not (country or region or metro):
            return None

        # Sponsors should only be creating fixed-CPM campaigns, which we
        # cannot calculate region specific inventory for
        if c.user_is_sponsor and region and not (region and metro):
            invalid_region = True
        else:
            invalid_region = False

        # Non-sponsors can only create auctions (non-inventory), so they
        # can target country, country/region, and country/region/metro
        if not (country and not (region or metro) or
                (country and region and not metro) or
                (country and region and metro)):
            invalid_geotargets = True
        else:
            invalid_geotargets = False

        if (country not in g.locations or
                region and region not in g.locations[country]['regions'] or
                metro and metro not in g.locations[country]['regions'][region]['metros']):
            nonexistent_geotarget = True
        else:
            nonexistent_geotarget = False

        if invalid_region or invalid_geotargets or nonexistent_geotarget:
            self.set_error(errors.INVALID_LOCATION, code=400, field='location')
        else:
            return Location(country, region, metro)


class VImageType(Validator):
    def run(self, img_type):
        if not img_type in ('png', 'jpg'):
            return 'png'
        return img_type

    def param_docs(self):
        return {
            self.param: "one of `png` or `jpg` (default: `png`)",
        }


class ValidEmail(Validator):
    """Validates a single email. Returns the email on success."""

    def run(self, email):
        # Strip out leading/trailing whitespace, since the inclusion of that is
        # a common and easily-fixable user error.
        if email is not None:
            email = email.strip()

        if not email:
            self.set_error(errors.NO_EMAIL)
        elif not ValidEmails.email_re.match(email):
            self.set_error(errors.BAD_EMAIL)
        else:
            return email


class ValidEmails(Validator):
    """Validates a list of email addresses passed in as a string and
    delineated by whitespace, ',' or ';'.  Also validates quantity of
    provided emails.  Returns a list of valid email addresses on
    success"""

    separator = re.compile(r'[^\s,;]+')
    email_re  = re.compile(r'\A[^\s@]+@[^\s@]+\.[^\s@]+\Z')

    def __init__(self, param, num = 20, **kw):
        self.num = num
        Validator.__init__(self, param = param, **kw)

    def run(self, emails0):
        emails = set(self.separator.findall(emails0) if emails0 else [])
        failures = set(e for e in emails if not self.email_re.match(e))
        emails = emails - failures

        # make sure the number of addresses does not exceed the max
        if self.num > 0 and len(emails) + len(failures) > self.num:
            # special case for 1: there should be no delineators at all, so
            # send back original string to the user
            if self.num == 1:
                self.set_error(errors.BAD_EMAILS,
                             {'emails': '"%s"' % emails0})
            # else report the number expected
            else:
                self.set_error(errors.TOO_MANY_EMAILS,
                             {'num': self.num})
        # correct number, but invalid formatting
        elif failures:
            self.set_error(errors.BAD_EMAILS,
                         {'emails': ', '.join(failures)})
        # no emails
        elif not emails:
            self.set_error(errors.NO_EMAILS)
        else:
            # return single email if one is expected, list otherwise
            return list(emails)[0] if self.num == 1 else emails

class ValidEmailsOrExistingUnames(Validator):
    """Validates a list of mixed email addresses and usernames passed in
    as a string, delineated by whitespace, ',' or ';'.  Validates total
    quantity too while we're at it.  Returns a tuple of the form
    (e-mail addresses, user account objects)"""

    def __init__(self, param, num=20, **kw):
        self.num = num
        Validator.__init__(self, param=param, **kw)

    def run(self, items):
        # Use ValidEmails separator to break the list up
        everything = set(ValidEmails.separator.findall(items) if items else [])

        # Use ValidEmails regex to divide the list into e-mail and other
        emails = set(e for e in everything if ValidEmails.email_re.match(e))
        failures = everything - emails

        # Run the rest of the validator against the e-mails list
        ve = ValidEmails(self.param, self.num)
        if len(emails) > 0:
            ve.run(", ".join(emails))

        # ValidEmails will add to c.errors for us, so do nothing if that fails
        # Elsewise, on with the users
        if not ve.has_errors:
            users = set()  # set of accounts
            validusers = set()  # set of usernames to subtract from failures

            # Now steal from VExistingUname:
            for uname in failures:
                check = uname
                if re.match('/u/', uname):
                    check = check[3:]
                veu = VExistingUname(check)
                account = veu.run(check)
                if account:
                    validusers.add(uname)
                    users.add(account)

            # We're fine if all our failures turned out to be valid users
            if len(users) == len(failures):
                # ValidEmails checked to see if there were too many addresses,
                # check to see if there's enough left-over space for users
                remaining = self.num - len(emails)
                if len(users) > remaining:
                    if self.num == 1:
                        # We only wanted one, and we got it as an e-mail,
                        # so complain.
                        self.set_error(errors.BAD_EMAILS,
                                       {"emails": '"%s"' % items})
                    else:
                        # Too many total
                        self.set_error(errors.TOO_MANY_EMAILS,
                                       {"num": self.num})
                elif len(users) + len(emails) == 0:
                    self.set_error(errors.NO_EMAILS)
                else:
                    # It's all good!
                    return (emails, users)
            else:
                failures = failures - validusers
                self.set_error(errors.BAD_EMAILS,
                               {'emails': ', '.join(failures)})

class VCnameDomain(Validator):
    domain_re  = re.compile(r'\A([\w\-_]+\.)+[\w]+\Z')

    def run(self, domain):
        if (domain
            and (not self.domain_re.match(domain)
                 or domain.endswith('.' + g.domain)
                 or domain.endswith('.' + g.media_domain)
                 or len(domain) > 300)):
            self.set_error(errors.BAD_CNAME)
        elif domain:
            try:
                return str(domain).lower()
            except UnicodeEncodeError:
                self.set_error(errors.BAD_CNAME)

    def param_docs(self):
        # cnames are dead; don't advertise this.
        return {}


class VDate(Validator):
    """
    Date checker that accepts string inputs.

    Error conditions:
       * BAD_DATE on mal-formed date strings (strptime parse failure)

    """

    def __init__(self, param, format="%m/%d/%Y", required=True):
        self.format = format
        self.required = required
        Validator.__init__(self, param)

    def run(self, datestr):
        if not datestr and not self.required:
            return None

        try:
            dt = datetime.strptime(datestr, self.format)
            return dt.replace(tzinfo=g.tz)
        except (ValueError, TypeError):
            self.set_error(errors.BAD_DATE)


class VDestination(Validator):
    def __init__(self, param = 'dest', default = "", **kw):
        Validator.__init__(self, param, default, **kw)

    def run(self, dest):
        if not dest:
            dest = self.default or add_sr("/")

        ld = dest.lower()
        if ld.startswith(('/', 'http://', 'https://')):
            u = UrlParser(dest)

            if u.is_reddit_url(c.site) and u.is_web_safe_url():
                return dest

        return "/"

    def param_docs(self):
        return {
            self.param: 'destination url (must be same-domain)',
        }

class ValidAddress(Validator):
    def set_error(self, msg, field):
        Validator.set_error(self, errors.BAD_ADDRESS,
                            dict(message=msg), field = field)

    def run(self, firstName, lastName, company, address,
            city, state, zipCode, country, phoneNumber):
        if not firstName:
            self.set_error(_("please provide a first name"), "firstName")
        elif not lastName:
            self.set_error(_("please provide a last name"), "lastName")
        elif not address:
            self.set_error(_("please provide an address"), "address")
        elif not city:
            self.set_error(_("please provide your city"), "city")
        elif not state:
            self.set_error(_("please provide your state"), "state")
        elif not zipCode:
            self.set_error(_("please provide your zip or post code"), "zip")
        elif not country:
            self.set_error(_("please provide your country"), "country")

        # Make sure values don't exceed max length defined in the authorize.net
        # xml schema: https://api.authorize.net/xml/v1/schema/AnetApiSchema.xsd
        max_lengths = [
            (firstName, 50, 'firstName'), # (argument, max len, form field name)
            (lastName, 50, 'lastName'),
            (company, 50, 'company'),
            (address, 60, 'address'),
            (city, 40, 'city'),
            (state, 40, 'state'),
            (zipCode, 20, 'zip'),
            (country, 60, 'country'),
            (phoneNumber, 255, 'phoneNumber')
        ]
        for (arg, max_length, form_field_name) in max_lengths:
            if arg and len(arg) > max_length:
                self.set_error(_("max length %d characters" % max_length), form_field_name)

        if not self.has_errors:
            return Address(firstName = firstName,
                           lastName = lastName,
                           company = company or "",
                           address = address,
                           city = city, state = state,
                           zip = zipCode, country = country,
                           phoneNumber = phoneNumber or "")

class ValidCard(Validator):
    valid_date = re.compile(r"\d\d\d\d-\d\d")
    def set_error(self, msg, field):
        Validator.set_error(self, errors.BAD_CARD,
                            dict(message=msg), field = field)

    def run(self, cardNumber, expirationDate, cardCode):
        has_errors = False

        cardNumber = cardNumber or ""
        if not (cardNumber.isdigit() and 13 <= len(cardNumber) <= 16):
            self.set_error(_("credit card numbers should be 13 to 16 digits"),
                           "cardNumber")
            has_errors = True

        if not self.valid_date.match(expirationDate or ""):
            self.set_error(_("dates should be YYYY-MM"), "expirationDate")
            has_errors = True
        else:
            now = datetime.now(g.tz)
            yyyy, mm = expirationDate.split("-")
            year = int(yyyy)
            month = int(mm)
            if month < 1 or month > 12:
                self.set_error(_("month must be in the range 01..12"), "expirationDate")
                has_errors = True
            elif datetime(year, month, 1) < datetime(now.year, now.month, 1):
                self.set_error(_("expiration date must be in the future"), "expirationDate")
                has_errors = True

        cardCode = cardCode or ""
        if not (cardCode.isdigit() and 3 <= len(cardCode) <= 4):
            self.set_error(_("card verification codes should be 3 or 4 digits"),
                           "cardCode")
            has_errors = True

        if not has_errors:
            return CreditCard(cardNumber = cardNumber,
                              expirationDate = expirationDate,
                              cardCode = cardCode)

class VTarget(Validator):
    target_re = re.compile("\A[\w_-]{3,20}\Z")
    def run(self, name):
        if name and self.target_re.match(name):
            return name

    def param_docs(self):
        # this is just for htmllite and of no interest to api consumers
        return {}

class VFlairAccount(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_FLAIR_TARGET, *a, **kw)

    def _lookup(self, name, allow_deleted):
        try:
            return Account._by_name(name, allow_deleted=allow_deleted)
        except NotFound:
            return None

    def run(self, name):
        if not name:
            return self.error()
        return (
            self._lookup(name, False)
            or self._lookup(name, True)
            or self.error())

    def param_docs(self):
        return {self.param: _("a user by name")}

class VFlairLink(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_FLAIR_TARGET, *a, **kw)

    def run(self, name):
        if not name:
            return self.error()
        try:
            return Link._by_fullname(name, data=True)
        except NotFound:
            return self.error()

    def param_docs(self):
        return {self.param: _("a [fullname](#fullname) of a link")}

class VFlairCss(VCssName):
    def __init__(self, param, max_css_classes=10, **kw):
        self.max_css_classes = max_css_classes
        VCssName.__init__(self, param, **kw)

    def run(self, css):
        if not css:
            return css

        names = css.split()
        if len(names) > self.max_css_classes:
            self.set_error(errors.TOO_MUCH_FLAIR_CSS)
            return ''

        for name in names:
            if not self.r_css_name.match(name):
                self.set_error(errors.BAD_CSS_NAME)
                return ''

        return css

class VFlairText(VLength):
    def __init__(self, param, max_length=64, **kw):
        VLength.__init__(self, param, max_length, **kw)

class VFlairTemplateByID(VRequired):
    def __init__(self, param, **kw):
        VRequired.__init__(self, param, None, **kw)

    def run(self, flair_template_id):
        try:
            return FlairTemplateBySubredditIndex.get_template(
                c.site._id, flair_template_id)
        except tdb_cassandra.NotFound:
            return None

class VOneTimePassword(Validator):
    allowed_skew = [-1, 0, 1]  # allow a period of skew on either side of now
    ratelimit = 3  # maximum number of tries per period

    def __init__(self, param, required):
        self.required = required
        Validator.__init__(self, param)

    @classmethod
    def validate_otp(cls, secret, password):
        # is the password a valid format and has it been used?
        try:
            key = "otp:used_%s_%d" % (c.user._id36, int(password))
        except (TypeError, ValueError):
            valid_and_unused = False
        else:
            # leave this key around for one more time period than the maximum
            # number of time periods we'll check for valid passwords
            key_ttl = totp.PERIOD * (len(cls.allowed_skew) + 1)
            valid_and_unused = g.gencache.add(key, True, time=key_ttl)

        # check the password (allowing for some clock-skew as 2FA-users
        # frequently travel at relativistic velocities)
        if valid_and_unused:
            for skew in cls.allowed_skew:
                expected_otp = totp.make_totp(secret, skew=skew)
                if constant_time_compare(password, expected_otp):
                    return True

        return False

    def run(self, password):
        # does the user have 2FA configured?
        secret = c.user.otp_secret
        if not secret:
            if self.required:
                self.set_error(errors.NO_OTP_SECRET)
            return

        # do they have the otp cookie instead?
        if c.otp_cached:
            return

        # make sure they're not trying this too much
        if not g.disable_ratelimit:
            current_password = totp.make_totp(secret)
            otp_ratelimit = ratelimit.SimpleRateLimit(
                name="otp_tries_%s_%s" % (c.user._id36, current_password),
                seconds=600,
                limit=self.ratelimit,
            )
            if not otp_ratelimit.record_and_check():
                self.set_error(errors.RATELIMIT, dict(time="30 seconds"))
                return

        # check the password
        if self.validate_otp(secret, password):
            return

        # if we got this far, their password was wrong, invalid or already used
        self.set_error(errors.WRONG_PASSWORD)

class VOAuth2ClientID(VRequired):
    default_param = "client_id"
    default_param_doc = _("an app")
    def __init__(self, param=None, *a, **kw):
        VRequired.__init__(self, param, errors.OAUTH2_INVALID_CLIENT, *a, **kw)

    def run(self, client_id):
        client_id = VRequired.run(self, client_id)
        if client_id:
            client = OAuth2Client.get_token(client_id)
            if client and not client.deleted:
                return client
            else:
                self.error()

    def param_docs(self):
        return {self.default_param: self.default_param_doc}

class VOAuth2ClientDeveloper(VOAuth2ClientID):
    default_param_doc = _("an app developed by the user")

    def run(self, client_id):
        client = super(VOAuth2ClientDeveloper, self).run(client_id)
        if not client or not client.has_developer(c.user):
            return self.error()
        return client

class VOAuth2Scope(VRequired):
    default_param = "scope"
    def __init__(self, param=None, *a, **kw):
        VRequired.__init__(self, param, errors.OAUTH2_INVALID_SCOPE, *a, **kw)

    def run(self, scope):
        scope = VRequired.run(self, scope)
        if scope:
            parsed_scope = OAuth2Scope(scope)
            if parsed_scope.is_valid():
                return parsed_scope
            else:
                self.error()

class VOAuth2RefreshToken(Validator):
    def __init__(self, param, *a, **kw):
        Validator.__init__(self, param, None, *a, **kw)

    def run(self, refresh_token_id):
        if refresh_token_id:
            try:
                token = OAuth2RefreshToken._byID(refresh_token_id)
            except tdb_cassandra.NotFound:
                self.set_error(errors.OAUTH2_INVALID_REFRESH_TOKEN)
                return None
            if not token.check_valid():
                self.set_error(errors.OAUTH2_INVALID_REFRESH_TOKEN)
                return None
            return token
        else:
            return None

class VPermissions(Validator):
    types = dict(
        moderator=ModeratorPermissionSet,
        moderator_invite=ModeratorPermissionSet,
    )

    def __init__(self, type_param, permissions_param, *a, **kw):
        Validator.__init__(self, (type_param, permissions_param), *a, **kw)

    def run(self, type, permissions):
        permission_class = self.types.get(type)
        if not permission_class:
            self.set_error(errors.INVALID_PERMISSION_TYPE, field=self.param[0])
            return (None, None)
        try:
            perm_set = permission_class.loads(permissions, validate=True)
        except ValueError:
            self.set_error(errors.INVALID_PERMISSIONS, field=self.param[1])
            return (None, None)
        return type, perm_set


class VJSON(Validator):
    def run(self, json_str):
        if not json_str:
            return self.set_error('JSON_PARSE_ERROR', code=400)
        else:
            try:
                return json.loads(json_str)
            except ValueError:
                return self.set_error('JSON_PARSE_ERROR', code=400)

    def param_docs(self):
        return {
            self.param: "JSON data",
        }


class VValidatedJSON(VJSON):
    """Apply validators to the values of JSON formatted data."""
    class ArrayOf(object):
        """A JSON array of objects with the specified schema."""
        def __init__(self, spec):
            self.spec = spec

        def run(self, data):
            if not isinstance(data, list):
                raise RedditError('JSON_INVALID', code=400)

            validated_data = []
            for item in data:
                validated_data.append(self.spec.run(item))
            return validated_data

        def spec_docs(self):
            spec_lines = []
            spec_lines.append('[')
            if hasattr(self.spec, 'spec_docs'):
                array_docs = self.spec.spec_docs()
            else:
                array_docs = self.spec.param_docs()[self.spec.param]
            for line in array_docs.split('\n'):
                spec_lines.append('  ' + line)
            spec_lines[-1] += ','
            spec_lines.append('  ...')
            spec_lines.append(']')
            return '\n'.join(spec_lines)


    class Object(object):
        """A JSON object with validators for specified fields."""
        def __init__(self, spec):
            self.spec = spec

        def run(self, data, ignore_missing=False):
            if not isinstance(data, dict):
                raise RedditError('JSON_INVALID', code=400)

            validated_data = {}
            for key, validator in self.spec.iteritems():
                try:
                    validated_data[key] = validator.run(data[key])
                except KeyError:
                    if ignore_missing:
                        continue
                    raise RedditError('JSON_MISSING_KEY', code=400,
                                      msg_params={'key': key})
            return validated_data

        def spec_docs(self):
            spec_docs = {}
            for key, validator in self.spec.iteritems():
                if hasattr(validator, 'spec_docs'):
                    spec_docs[key] = validator.spec_docs()
                elif hasattr(validator, 'param_docs'):
                    spec_docs.update(validator.param_docs())
                    if validator.docs:
                        spec_docs.update(validator.docs)

            # generate markdown json schema docs
            spec_lines = []
            spec_lines.append('{')
            for key in sorted(spec_docs.keys()):
                key_docs = spec_docs[key]
                # indent any new lines
                key_docs = key_docs.replace('\n', '\n  ')
                spec_lines.append('  "%s": %s,' % (key, key_docs))
            spec_lines.append('}')
            return '\n'.join(spec_lines)

    class PartialObject(Object):
        def run(self, data):
            super_ = super(VValidatedJSON.PartialObject, self)
            return super_.run(data, ignore_missing=True)

    def __init__(self, param, spec, **kw):
        VJSON.__init__(self, param, **kw)
        self.spec = spec

    def run(self, json_str):
        data = VJSON.run(self, json_str)
        if self.has_errors:
            return

        # Note: this relies on the fact that all validator errors are dumped
        # into a global (c.errors) and then checked by @validate.
        return self.spec.run(data)

    def docs_model(self):
        spec_md = self.spec.spec_docs()

        # indent for code formatting
        spec_md = '\n'.join(
            '    ' + line for line in spec_md.split('\n')
        )
        return spec_md

    def param_docs(self):
        return {
            self.param: 'json data:\n\n' + self.docs_model(),
        }


multi_name_rx = re.compile(r"\A[A-Za-z0-9][A-Za-z0-9_]{1,20}\Z")
multi_name_chars_rx = re.compile(r"[^A-Za-z0-9_]")

class VMultiPath(Validator):
    """Validates a multireddit path. Returns a path info dictionary.
    """
    def __init__(self, param, kinds=None, required=True, **kw):
        Validator.__init__(self, param, **kw)
        self.required = required
        self.kinds = tup(kinds or ('f', 'm'))

    @classmethod
    def normalize(self, path):
        if path[0] != '/':
            path = '/' + path
        path = path.lower().rstrip('/')
        return path

    def run(self, path):
        if not path and not self.required:
            return None
        try:
            require(path)
            path = self.normalize(path)
            require(path.startswith('/user/'))
            prefix, owner, kind, name = require_split(path, 5, sep='/')[1:]
            require(kind in self.kinds)
            owner = chkuser(owner)
            require(owner)
        except RequirementException:
            self.set_error('BAD_MULTI_PATH', code=400)
            return

        try:
            require(multi_name_rx.match(name))
        except RequirementException:
            invalid_char = multi_name_chars_rx.search(name)
            if invalid_char:
                char = invalid_char.group()
                if char == ' ':
                    reason = _('no spaces allowed')
                else:
                    reason = _("invalid character: '%s'") % char
            elif name[0] == '_':
                reason = _("can't start with a '_'")
            elif len(name) < 2:
                reason = _('that name is too short')
            elif len(name) > 21:
                reason = _('that name is too long')
            else:
                reason = _("that name isn't going to work")

            self.set_error('BAD_MULTI_NAME', {'reason': reason}, code=400)
            return

        return {'path': path, 'prefix': prefix, 'owner': owner, 'name': name}

    def param_docs(self):
        return {
            self.param: "multireddit url path",
        }


class VMultiByPath(Validator):
    """Validates a multireddit path.  Returns a LabeledMulti.
    """
    def __init__(self, param, require_view=True, require_edit=False, kinds=None):
        Validator.__init__(self, param)
        self.require_view = require_view
        self.require_edit = require_edit
        self.kinds = tup(kinds or ('f', 'm'))

    def run(self, path):
        path = VMultiPath.normalize(path)
        if not path.startswith('/user/'):
            return self.set_error('MULTI_NOT_FOUND', code=404)

        name = path.split('/')[-1]
        if not multi_name_rx.match(name):
            return self.set_error('MULTI_NOT_FOUND', code=404)

        try:
            multi = LabeledMulti._byID(path)
        except tdb_cassandra.NotFound:
            return self.set_error('MULTI_NOT_FOUND', code=404)

        if not multi or multi.kind not in self.kinds:
            return self.set_error('MULTI_NOT_FOUND', code=404)
        if not multi or (self.require_view and not multi.can_view(c.user)):
            return self.set_error('MULTI_NOT_FOUND', code=404)
        if self.require_edit and not multi.can_edit(c.user):
            return self.set_error('MULTI_CANNOT_EDIT', code=403)

        return multi

    def param_docs(self):
        return {
            self.param: "multireddit url path",
        }


sr_path_rx = re.compile(r"\A(/?r/)?(?P<name>.*?)/?\Z")
class VSubredditList(Validator):

    def __init__(self, param, limit=20, allow_language_srs=True):
        Validator.__init__(self, param)
        self.limit = limit
        self.allow_language_srs = allow_language_srs

    def run(self, subreddits):
        if not subreddits:
            return []

        # extract subreddit name if path provided
        subreddits = [sr_path_rx.sub('\g<name>', sr.strip())
                      for sr in subreddits.lower().strip().splitlines() if sr]

        for name in subreddits:
            valid_name = Subreddit.is_valid_name(
                name, allow_language_srs=self.allow_language_srs)
            if not valid_name:
                return self.set_error(errors.BAD_SR_NAME, code=400)

        unique_srs = set(subreddits)

        if subreddits:
            valid_srs = set(Subreddit._by_name(subreddits).keys())
            if unique_srs - valid_srs:
                return self.set_error(errors.SUBREDDIT_NOEXIST, code=400)

        if len(unique_srs) > self.limit:
            return self.set_error(
                errors.TOO_MANY_SUBREDDITS, {'max': self.limit}, code=400)

        # return list of subreddit names as entered
        return subreddits

    def param_docs(self):
        return {
            self.param: 'a list of subreddit names, line break delimited',
        }


class VResultTypes(Validator):
    """
    Validates a list of search result types, provided either as multiple
    GET parameters or as a comma separated list.  Returns a set.
    """
    def __init__(self, param):
        Validator.__init__(self, param, get_multiple=True)
        self.default = []
        self.options = {'link', 'sr'}

    def run(self, result_types):
        if result_types and ',' in result_types[0]:
            result_types = result_types[0].strip(',').split(',')

        # invalid values are ignored
        result_types = set(result_types) & self.options

        # for backwards compatibility, api and legacy default to link results
        if is_api():
            result_types = result_types or {'link'}
        elif feature.is_enabled('legacy_search') or c.user.pref_legacy_search:
            result_types = {'link'}
        else:
            result_types = result_types or {'link', 'sr'}

        return result_types

    def param_docs(self):
        return {
            self.param: (
                '(optional) comma-delimited list of result types '
                '(`%s`)' % '`, `'.join(self.options)
            ),
        }


class VSigned(Validator):
    """Validate if the request is properly signed.

    Checks the headers (mostly the User-Agent) are signed with
    :py:function:`~r2.lib.signing.valid_ua_signature` and in the case
    of POST and PUT ensure that any request.body included is also signed
    via :py:function:`~r2.lib.signing.valid_body_signature`.

    In :py:method:`run`, the signatures are combined as needed to generate a
    final signature that is generally the combination of the two.
    """

    def run(self):
        signature = signing.valid_ua_signature(request)

        # only check the request body when there should be one
        if request.method.upper() in ("POST", "PUT"):
            signature.update(signing.valid_post_signature(request))

        # add a simple event for each error as it appears (independent of
        # whether we're going to ignore them).
        for code, field in signature.errors:
            g.stats.simple_event(
                "signing.%s.invalid.%s" % (field, code.lower())
            )

        # persistent skew problems on android suggest something deeper is
        # wrong in v1.  Disable the expiration check for now!
        if signature.platform == "android" and signature.version == 1:
            signature.add_ignore(signing.ERRORS.EXPIRED_TOKEN)

        return signature


def need_provider_captcha():
    return False
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import deepcopy
from datetime import datetime
import cPickle as pickle
import logging
import operators
import re
import threading

from pylons import request
from pylons import tmpl_context as c
from pylons import app_globals as g

import sqlalchemy as sa

from r2.lib import filters
from r2.lib.utils import (
    iters,
    Results,
    simple_traceback,
    storage,
    tup,
)


dbm = g.dbm
predefined_type_ids = g.predefined_type_ids
log_format = logging.Formatter('sql: %(message)s')
max_val_len = 1000


class TransactionSet(threading.local):
    """A manager for SQL transactions.

    This implements a thread local meta-transaction which may span multiple
    databases.  The existing tdb_sql code calls add_engine before executing
    writes.  If thing.py calls begin then these calls will actually kick in
    and start a transaction that must be committed or rolled back by thing.py.

    Because this involves creating transactions at the connection level, this
    system implicitly relies on using the threadlocal strategy for the
    sqlalchemy engines.

    This system is a bit awkward, and should be replaced with something that
    doesn't use module-globals when doing a cleanup of tdb_sql.

    """

    def __init__(self):
        self.transacting_engines = set()
        self.transaction_begun = False

    def begin(self):
        """Indicate that a transaction has begun."""
        self.transaction_begun = True

    def add_engine(self, engine):
        """Add a database connection to the meta-transaction if active."""
        if not self.transaction_begun:
            return

        if engine not in self.transacting_engines:
            engine.begin()
            self.transacting_engines.add(engine)

    def commit(self):
        """Commit the meta-transaction."""
        try:
            for engine in self.transacting_engines:
                engine.commit()
        finally:
            self._clear()

    def rollback(self):
        """Roll back the meta-transaction."""
        try:
            for engine in self.transacting_engines:
                engine.rollback()
        finally:
            self._clear()

    def _clear(self):
        self.transacting_engines.clear()
        self.transaction_begun = False


transactions = TransactionSet()

MAX_THING_ID = 9223372036854775807 # http://www.postgresql.org/docs/8.3/static/datatype-numeric.html
MIN_THING_ID = 0

def make_metadata(engine):
    metadata = sa.MetaData(engine)
    metadata.bind.echo = g.sqlprinting
    return metadata

def create_table(table, index_commands=None):
    t = table
    if g.db_create_tables:
        #@@hackish?
        if not t.bind.has_table(t.name):
            t.create(checkfirst = False)
            if index_commands:
                for i in index_commands:
                    t.bind.execute(i)

def index_str(table, name, on, where = None, unique = False):
    if unique:
        index_str = 'create unique index'
    else:
        index_str = 'create index'
    index_str += ' idx_%s_' % name
    index_str += table.name
    index_str += ' on '+ table.name + ' (%s)' % on
    if where:
        index_str += ' where %s' % where
    return index_str


def index_commands(table, type):
    commands = []

    if type == 'thing':
        commands.append(index_str(table, 'date', 'date'))
        commands.append(index_str(table, 'deleted_spam', 'deleted, spam'))
        commands.append(index_str(table, 'hot', 'hot(ups, downs, date), date'))
        commands.append(index_str(table, 'score', 'score(ups, downs), date'))
        commands.append(index_str(table, 'controversy', 'controversy(ups, downs), date'))
    elif type == 'data':
        commands.append(index_str(table, 'key_value', 'key, substring(value, 1, %s)' \
                                  % max_val_len))

        #lower name
        commands.append(index_str(table, 'lower_key_value', 'key, lower(value)',
                                  where = "key = 'name'"))
        #ip
        commands.append(index_str(table, 'ip_network', 'ip_network(value)',
                                  where = "key = 'ip'"))
        #base_url
        commands.append(index_str(table, 'base_url', 'base_url(lower(value))',
                                  where = "key = 'url'"))
    elif type == 'rel':
        commands.append(index_str(table, 'thing1_name_date', 'thing1_id, name, date'))
        commands.append(index_str(table, 'thing2_name_date', 'thing2_id, name, date'))
        commands.append(index_str(table, 'thing1_id', 'thing1_id'))
        commands.append(index_str(table, 'thing2_id', 'thing2_id'))
        commands.append(index_str(table, 'name', 'name'))
        commands.append(index_str(table, 'date', 'date'))
    else:
        print "unknown index_commands() type %s" % type

    return commands

def get_type_table(metadata):
    table = sa.Table(g.db_app_name + '_type', metadata,
                     sa.Column('id', sa.Integer, primary_key = True),
                     sa.Column('name', sa.String, nullable = False))
    return table

def get_rel_type_table(metadata):
    table = sa.Table(g.db_app_name + '_type_rel', metadata,
                     sa.Column('id', sa.Integer, primary_key = True),
                     sa.Column('type1_id', sa.Integer, nullable = False),
                     sa.Column('type2_id', sa.Integer, nullable = False),
                     sa.Column('name', sa.String, nullable = False))
    return table

def get_thing_table(metadata, name):
    table = sa.Table(g.db_app_name + '_thing_' + name, metadata,
                     sa.Column('thing_id', sa.BigInteger, primary_key = True),
                     sa.Column('ups', sa.Integer, default = 0, nullable = False),
                     sa.Column('downs',
                               sa.Integer,
                               default = 0,
                               nullable = False),
                     sa.Column('deleted',
                               sa.Boolean,
                               default = False,
                               nullable = False),
                     sa.Column('spam',
                               sa.Boolean,
                               default = False,
                               nullable = False),
                     sa.Column('date',
                               sa.DateTime(timezone = True),
                               default = sa.func.now(),
                               nullable = False))
    table.thing_name = name
    return table

def get_data_table(metadata, name):
    data_table = sa.Table(g.db_app_name + '_data_' + name, metadata,
                          sa.Column('thing_id', sa.BigInteger, nullable = False,
                                    primary_key = True),
                          sa.Column('key', sa.String, nullable = False,
                                    primary_key = True),
                          sa.Column('value', sa.String),
                          sa.Column('kind', sa.String))
    return data_table

def get_rel_table(metadata, name):
    rel_table = sa.Table(g.db_app_name + '_rel_' + name, metadata,
                         sa.Column('rel_id', sa.BigInteger, primary_key = True),
                         sa.Column('thing1_id', sa.BigInteger, nullable = False),
                         sa.Column('thing2_id', sa.BigInteger, nullable = False),
                         sa.Column('name', sa.String, nullable = False),
                         sa.Column('date', sa.DateTime(timezone = True),
                                   default = sa.func.now(), nullable = False),
                         sa.UniqueConstraint('thing1_id', 'thing2_id', 'name'))
    rel_table.rel_name = name
    return rel_table

#get/create the type tables
def make_type_table():
    metadata = make_metadata(dbm.type_db)
    table = get_type_table(metadata)
    create_table(table)
    return table
type_table = make_type_table()

def make_rel_type_table():
    metadata = make_metadata(dbm.relation_type_db)
    table = get_rel_type_table(metadata)
    create_table(table)
    return table
rel_type_table = make_rel_type_table()

#lookup dicts
types_id = {}
types_name = {}
rel_types_id = {}
rel_types_name = {}

class ConfigurationError(Exception):
    pass

def check_type(table, name, insert_vals):
    # before hitting the db, check if we can get the type id from
    # the ini file
    type_id = predefined_type_ids.get(name)
    if type_id:
        return type_id
    elif len(predefined_type_ids) > 0:
        # flip the hell out if only *some* of the type ids are defined
        raise ConfigurationError("Expected typeid for %s" % name)

    # check for type in type table, create if not existent
    r = table.select(table.c.name == name).execute().fetchone()
    if not r:
        r = table.insert().execute(**insert_vals)
        type_id = r.inserted_primary_key[0]
    else:
        type_id = r.id
    return type_id

#make the thing tables
def build_thing_tables():
    for name, engines in dbm.things_iter():
        type_id = check_type(type_table,
                             name,
                             dict(name = name))

        tables = []
        for engine in engines:
            metadata = make_metadata(engine)

            #make thing table
            thing_table = get_thing_table(metadata, name)
            create_table(thing_table,
                         index_commands(thing_table, 'thing'))

            #make data tables
            data_table = get_data_table(metadata, name)
            create_table(data_table,
                         index_commands(data_table, 'data'))

            tables.append((thing_table, data_table))

        thing = storage(type_id = type_id,
                        name = name,
                        avoid_master_reads = dbm.avoid_master_reads.get(name),
                        tables = tables)

        types_id[type_id] = thing
        types_name[name] = thing
build_thing_tables()

#make relation tables
def build_rel_tables():
    for name, (type1_name, type2_name, engines) in dbm.rels_iter():
        type1_id = types_name[type1_name].type_id
        type2_id = types_name[type2_name].type_id
        type_id = check_type(rel_type_table,
                             name,
                             dict(name = name,
                                  type1_id = type1_id,
                                  type2_id = type2_id))

        tables = []
        for engine in engines:
            metadata = make_metadata(engine)

            #relation table
            rel_table = get_rel_table(metadata, name)
            create_table(rel_table, index_commands(rel_table, 'rel'))

            #make thing tables
            rel_t1_table = get_thing_table(metadata, type1_name)
            if type1_name == type2_name:
                rel_t2_table = rel_t1_table
            else:
                rel_t2_table = get_thing_table(metadata, type2_name)

            #build the data
            rel_data_table = get_data_table(metadata, 'rel_' + name)
            create_table(rel_data_table,
                         index_commands(rel_data_table, 'data'))

            tables.append((rel_table,
                           rel_t1_table,
                           rel_t2_table,
                           rel_data_table))

        rel = storage(type_id = type_id,
                      type1_id = type1_id,
                      type2_id = type2_id,
                      avoid_master_reads = dbm.avoid_master_reads.get(name),
                      name = name,
                      tables = tables)

        rel_types_id[type_id] = rel
        rel_types_name[name] = rel
build_rel_tables()

def get_write_table(tables):
    if g.disallow_db_writes:
        raise Exception("not so fast! writes are not allowed on this app.")
    else:
        return tables[0]

def add_request_info(select):
    def sanitize(txt):
        return "".join(x if x.isalnum() else "."
                       for x in filters._force_utf8(txt))

    tb = simple_traceback(limit=12)
    try:
        if (hasattr(request, 'path') and
            hasattr(request, 'ip') and
            hasattr(request, 'user_agent')):
            comment = '/*\n%s\n%s\n%s\n*/' % (
                tb or "", 
                sanitize(request.fullpath),
                sanitize(request.ip))
            return select.prefix_with(comment)
    except UnicodeDecodeError:
        pass

    return select


def get_table(kind, action, tables, avoid_master_reads = False):
    if action == 'write':
        #if this is a write, store the kind in the c.use_write_db dict
        #so that all future requests use the write db
        if not isinstance(c.use_write_db, dict):
            c.use_write_db = {}
        c.use_write_db[kind] = True

        return get_write_table(tables)
    elif action == 'read':
        #check to see if we're supposed to use the write db again
        if c.use_write_db and c.use_write_db.has_key(kind):
            return get_write_table(tables)
        else:
            if avoid_master_reads and len(tables) > 1:
                return dbm.get_read_table(tables[1:])
            return dbm.get_read_table(tables)


def get_thing_table(type_id, action = 'read' ):
    return get_table('t' + str(type_id), action,
                     types_id[type_id].tables,
                     avoid_master_reads = types_id[type_id].avoid_master_reads)

def get_rel_table(rel_type_id, action = 'read'):
    return get_table('r' + str(rel_type_id), action,
                     rel_types_id[rel_type_id].tables,
                     avoid_master_reads = rel_types_id[rel_type_id].avoid_master_reads)


#TODO does the type actually exist?
def make_thing(type_id, ups, downs, date, deleted, spam, id=None):
    table = get_thing_table(type_id, action = 'write')[0]

    params = dict(ups = ups, downs = downs,
                  date = date, deleted = deleted, spam = spam)

    if id:
        params['thing_id'] = id

    def do_insert(t):
        transactions.add_engine(t.bind)
        r = t.insert().execute(**params)
        new_id = r.inserted_primary_key[0]
        new_r = r.last_inserted_params()
        for k, v in params.iteritems():
            if new_r[k] != v:
                raise CreationError, ("There's shit in the plumbing. " +
                                      "expected %s, got %s" % (params,  new_r))
        return new_id

    try:
        id = do_insert(table)
        params['thing_id'] = id
        return id
    except sa.exc.DBAPIError, e:
        if not 'IntegrityError' in e.message:
            raise
        # wrap the error to prevent db layer bleeding out
        raise CreationError, "Thing exists (%s)" % str(params)


def set_thing_props(type_id, thing_id, **props):
    table = get_thing_table(type_id, action = 'write')[0]

    if not props:
        return

    #use real columns
    def do_update(t):
        transactions.add_engine(t.bind)
        new_props = dict((t.c[prop], val) for prop, val in props.iteritems())
        u = t.update(t.c.thing_id == thing_id, values = new_props)
        u.execute()

    do_update(table)

def incr_thing_prop(type_id, thing_id, prop, amount):
    table = get_thing_table(type_id, action = 'write')[0]
    
    def do_update(t):
        transactions.add_engine(t.bind)
        u = t.update(t.c.thing_id == thing_id,
                     values={t.c[prop] : t.c[prop] + amount})
        u.execute()

    do_update(table)

class CreationError(Exception): pass

#TODO does the type exist?
#TODO do the things actually exist?
def make_relation(rel_type_id, thing1_id, thing2_id, name, date=None):
    table = get_rel_table(rel_type_id, action = 'write')[0]
    transactions.add_engine(table.bind)
    
    if not date: date = datetime.now(g.tz)
    try:
        r = table.insert().execute(thing1_id = thing1_id,
                                   thing2_id = thing2_id,
                                   name = name, 
                                   date = date)
        return r.inserted_primary_key[0]
    except sa.exc.DBAPIError, e:
        if not 'IntegrityError' in e.message:
            raise
        # wrap the error to prevent db layer bleeding out
        raise CreationError, "Relation exists (%s, %s, %s)" % (name, thing1_id, thing2_id)
        

def set_rel_props(rel_type_id, rel_id, **props):
    t = get_rel_table(rel_type_id, action = 'write')[0]

    if not props:
        return

    #use real columns
    transactions.add_engine(t.bind)
    new_props = dict((t.c[prop], val) for prop, val in props.iteritems())
    u = t.update(t.c.rel_id == rel_id, values = new_props)
    u.execute()


def py2db(val, return_kind=False):
    if isinstance(val, bool):
        val = 't' if val else 'f'
        kind = 'bool'
    elif isinstance(val, (str, unicode)):
        kind = 'str'
    elif isinstance(val, (int, float, long)):
        kind = 'num'
    elif val is None:
        kind = 'none'
    else:
        kind = 'pickle'
        val = pickle.dumps(val)

    if return_kind:
        return (val, kind)
    else:
        return val

def db2py(val, kind):
    if kind == 'bool':
        val = True if val is 't' else False
    elif kind == 'num':
        try:
            val = int(val)
        except ValueError:
            val = float(val)
    elif kind == 'none':
        val = None
    elif kind == 'pickle':
        val = pickle.loads(val)

    return val


def update_data(table, thing_id, **vals):
    transactions.add_engine(table.bind)

    u = table.update(sa.and_(table.c.thing_id == thing_id,
                             table.c.key == sa.bindparam('_key')))

    inserts = []
    for key, val in vals.iteritems():
        val, kind = py2db(val, return_kind=True)

        uresult = u.execute(_key = key, value = val, kind = kind)
        if not uresult.rowcount:
            inserts.append({'key':key, 'value':val, 'kind': kind})

    #do one insert
    if inserts:
        i = table.insert(values = dict(thing_id = thing_id))
        i.execute(*inserts)


def create_data(table, thing_id, **vals):
    transactions.add_engine(table.bind)

    inserts = []
    for key, val in vals.iteritems():
        val, kind = py2db(val, return_kind=True)
        inserts.append(dict(key=key, value=val, kind=kind))

    if inserts:
        i = table.insert(values=dict(thing_id=thing_id))
        i.execute(*inserts)


def incr_data_prop(table, type_id, thing_id, prop, amount):
    t = table
    transactions.add_engine(t.bind)
    u = t.update(sa.and_(t.c.thing_id == thing_id,
                         t.c.key == prop),
                 values={t.c.value : sa.cast(t.c.value, sa.Float) + amount})
    u.execute()

def fetch_query(table, id_col, thing_id):
    """pull the columns from the thing/data tables for a list or single
    thing_id"""
    single = False

    if not isinstance(thing_id, iters):
        single = True
        thing_id = (thing_id,)
    
    s = sa.select([table], id_col.in_(thing_id))

    try:
        r = add_request_info(s).execute().fetchall()
    except Exception, e:
        dbm.mark_dead(table.bind)
        # this thread must die so that others may live
        raise
    return (r, single)

#TODO specify columns to return?
def get_data(table, thing_id):
    r, single = fetch_query(table, table.c.thing_id, thing_id)

    #if single, only return one storage, otherwise make a dict
    res = storage() if single else {}
    for row in r:
        val = db2py(row.value, row.kind)
        stor = res if single else res.setdefault(row.thing_id, storage())
        if single and row.thing_id != thing_id:
            raise ValueError, ("tdb_sql.py: there's shit in the plumbing." 
                               + " got %s, wanted %s" % (row.thing_id,
                                                         thing_id))
        stor[row.key] = val

    return res

def set_thing_data(type_id, thing_id, brand_new_thing, **vals):
    table = get_thing_table(type_id, action = 'write')[1]

    if brand_new_thing:
        return create_data(table, thing_id, **vals)
    else:
        return update_data(table, thing_id, **vals)

def incr_thing_data(type_id, thing_id, prop, amount):
    table = get_thing_table(type_id, action = 'write')[1]
    return incr_data_prop(table, type_id, thing_id, prop, amount)    

def get_thing_data(type_id, thing_id):
    table = get_thing_table(type_id)[1]
    return get_data(table, thing_id)

def get_thing(type_id, thing_id):
    table = get_thing_table(type_id)[0]
    r, single = fetch_query(table, table.c.thing_id, thing_id)

    #if single, only return one storage, otherwise make a dict
    res = {} if not single else None
    for row in r:
        stor = storage(ups = row.ups,
                       downs = row.downs,
                       date = row.date,
                       deleted = row.deleted,
                       spam = row.spam)
        if single:
            res = stor
            # check that we got what we asked for
            if row.thing_id != thing_id:
                raise ValueError, ("tdb_sql.py: there's shit in the plumbing." 
                                    + " got %s, wanted %s" % (row.thing_id,
                                                              thing_id))
        else:
            res[row.thing_id] = stor
    return res

def set_rel_data(rel_type_id, thing_id, brand_new_thing, **vals):
    table = get_rel_table(rel_type_id, action = 'write')[3]

    if brand_new_thing:
        return create_data(table, thing_id, **vals)
    else:
        return update_data(table, thing_id, **vals)

def incr_rel_data(rel_type_id, thing_id, prop, amount):
    table = get_rel_table(rel_type_id, action = 'write')[3]
    return incr_data_prop(table, rel_type_id, thing_id, prop, amount)

def get_rel_data(rel_type_id, rel_id):
    table = get_rel_table(rel_type_id)[3]
    return get_data(table, rel_id)

def get_rel(rel_type_id, rel_id):
    r_table = get_rel_table(rel_type_id)[0]
    r, single = fetch_query(r_table, r_table.c.rel_id, rel_id)
    
    res = {} if not single else None
    for row in r:
        stor = storage(thing1_id = row.thing1_id,
                       thing2_id = row.thing2_id,
                       name = row.name,
                       date = row.date)
        if single:
            res = stor
        else:
            res[row.rel_id] = stor
    return res

def del_rel(rel_type_id, rel_id):
    tables = get_rel_table(rel_type_id, action = 'write')
    table = tables[0]
    data_table = tables[3]

    transactions.add_engine(table.bind)
    transactions.add_engine(data_table.bind)

    table.delete(table.c.rel_id == rel_id).execute()
    data_table.delete(data_table.c.thing_id == rel_id).execute()

def sa_op(op):
    #if BooleanOp
    if isinstance(op, operators.or_):
        return sa.or_(*[sa_op(o) for o in op.ops])
    elif isinstance(op, operators.and_):
        return sa.and_(*[sa_op(o) for o in op.ops])
    elif isinstance(op, operators.not_):
        return sa.not_(*[sa_op(o) for o in op.ops])

    #else, assume op is an instance of op
    if isinstance(op, operators.eq):
        fn = lambda x,y: x == y
    elif isinstance(op, operators.ne):
        fn = lambda x,y: x != y
    elif isinstance(op, operators.gt):
        fn = lambda x,y: x > y
    elif isinstance(op, operators.lt):
        fn = lambda x,y: x < y
    elif isinstance(op, operators.gte):
        fn = lambda x,y: x >= y
    elif isinstance(op, operators.lte):
        fn = lambda x,y: x <= y
    elif isinstance(op, operators.in_):
        return sa.or_(op.lval.in_(op.rval))

    rval = tup(op.rval)

    if not rval:
        return '2+2=5'
    else:
        return sa.or_(*[fn(op.lval, v) for v in rval])

def translate_sort(table, column_name, lval = None, rewrite_name = True):
    if isinstance(lval, operators.query_func):
        fn_name = lval.__class__.__name__
        sa_func = getattr(sa.func, fn_name)
        return sa_func(translate_sort(table,
                                      column_name,
                                      lval.lval,
                                      rewrite_name))

    if rewrite_name:
        if column_name == 'id':
            return table.c.thing_id
        elif column_name == 'hot':
            return sa.func.hot(table.c.ups, table.c.downs, table.c.date)
        elif column_name == 'score':
            return sa.func.score(table.c.ups, table.c.downs)
        elif column_name == 'controversy':
            return sa.func.controversy(table.c.ups, table.c.downs)
    #else
    return table.c[column_name]

#TODO - only works with thing tables
def add_sort(sort, t_table, select):
    sort = tup(sort)

    prefixes = t_table.keys() if isinstance(t_table, dict) else None
    #sort the prefixes so the longest come first
    prefixes.sort(key = lambda x: len(x))
    cols = []

    def make_sa_sort(s):
        orig_col = s.col

        col = orig_col
        if prefixes:
            table = None
            for k in prefixes:
                if k and orig_col.startswith(k):
                    table = t_table[k]
                    col = orig_col[len(k):]
            if table is None:
                table = t_table[None]
        else:
            table = t_table

        real_col = translate_sort(table, col)

        #TODO a way to avoid overlap?
        #add column for the sort parameter using the sorted name
        select.append_column(real_col.label(orig_col))

        #avoids overlap temporarily
        select.use_labels = True

        #keep track of which columns we added so we can add joins later
        cols.append((real_col, table))

        #default to asc
        return (sa.desc(real_col) if isinstance(s, operators.desc)
                else sa.asc(real_col))
        
    sa_sort = [make_sa_sort(s) for s in sort]

    s = select.order_by(*sa_sort)

    return s, cols

def translate_thing_value(rval):
    if isinstance(rval, operators.timeago):
        return sa.text("current_timestamp - interval '%s'" % rval.interval)
    else:
        return rval

#will assume parameters start with a _ for consistency
def find_things(type_id, sort, limit, offset, constraints):
    table = get_thing_table(type_id)[0]
    constraints = deepcopy(constraints)

    s = sa.select([table.c.thing_id.label('thing_id')])
    
    for op in operators.op_iter(constraints):
        #assume key starts with _
        #if key.startswith('_'):
        key = op.lval_name
        op.lval = translate_sort(table, key[1:], op.lval)
        op.rval = translate_thing_value(op.rval)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if sort:
        s, cols = add_sort(sort, {'_': table}, s)

    if limit:
        s = s.limit(limit)

    if offset:
        s = s.offset(offset)

    try:
        r = add_request_info(s).execute()
    except Exception, e:
        dbm.mark_dead(table.bind)
        # this thread must die so that others may live
        raise
    return Results(r, lambda(row): row.thing_id)

def translate_data_value(alias, op):
    lval = op.lval
    need_substr = False if isinstance(lval, operators.query_func) else True
    lval = translate_sort(alias, 'value', lval, False)

    #add the substring func
    if need_substr:
        lval = sa.func.substring(lval, 1, max_val_len)
    
    op.lval = lval
        
    #convert the rval to db types
    #convert everything to strings for pg8.3
    op.rval = tuple(str(py2db(v)) for v in tup(op.rval))

#TODO sort by data fields
#TODO sort by id wants thing_id
def find_data(type_id, sort, limit, offset, constraints):
    t_table, d_table = get_thing_table(type_id)
    constraints = deepcopy(constraints)

    used_first = False
    s = None
    need_join = False
    have_data_rule = False
    first_alias = d_table.alias()
    s = sa.select([first_alias.c.thing_id.label('thing_id')])#, distinct=True)

    for op in operators.op_iter(constraints):
        key = op.lval_name
        vals = tup(op.rval)

        if key == '_id':
            op.lval = first_alias.c.thing_id
        elif key.startswith('_'):
            need_join = True
            op.lval = translate_sort(t_table, key[1:], op.lval)
            op.rval = translate_thing_value(op.rval)
        else:
            have_data_rule = True
            id_col = None
            if not used_first:
                alias = first_alias
                used_first = True
            else:
                alias = d_table.alias()
                id_col = first_alias.c.thing_id

            if id_col is not None:
                s.append_whereclause(id_col == alias.c.thing_id)
            
            s.append_column(alias.c.value.label(key))
            s.append_whereclause(alias.c.key == key)
            
            #add the substring constraint if no other functions are there
            translate_data_value(alias, op)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if not have_data_rule:
        raise Exception('Data queries must have at least one data rule.')

    #TODO in order to sort by data columns, this is going to need to be smarter
    if sort:
        need_join = True
        s, cols = add_sort(sort, {'_':t_table}, s)
            
    if need_join:
        s.append_whereclause(first_alias.c.thing_id == t_table.c.thing_id)

    if limit:
        s = s.limit(limit)

    if offset:
        s = s.offset(offset)

    try:
        r = add_request_info(s).execute()
    except Exception, e:
        dbm.mark_dead(t_table.bind)
        # this thread must die so that others may live
        raise

    return Results(r, lambda(row): row.thing_id)


def sort_thing_ids_by_data_value(type_id, thing_ids, value_name,
        limit=None, desc=False):
    """Order thing_ids by the value of a data column."""

    thing_table, data_table = get_thing_table(type_id)

    join = thing_table.join(data_table,
        data_table.c.thing_id == thing_table.c.thing_id)

    query = (sa.select(
            [thing_table.c.thing_id],
            sa.and_(
                thing_table.c.thing_id.in_(thing_ids),
                thing_table.c.deleted == False,
                thing_table.c.spam == False,
                data_table.c.key == value_name,
            )
        )
        .select_from(join)
    )

    sort_column = data_table.c.value
    if desc:
        sort_column = sa.desc(sort_column)
    query = query.order_by(sort_column)

    if limit:
        query = query.limit(limit)

    rows = query.execute()

    return Results(rows, lambda(row): row.thing_id)


def find_rels(ret_props, rel_type_id, sort, limit, offset, constraints):
    tables = get_rel_table(rel_type_id)
    r_table, t1_table, t2_table, d_table = tables
    constraints = deepcopy(constraints)

    t1_table, t2_table = t1_table.alias(), t2_table.alias()

    prop_to_column = {
        "_rel_id": r_table.c.rel_id.label('rel_id'),
        "_thing1_id": r_table.c.thing1_id.label('thing1_id'),
        "_thing2_id": r_table.c.thing2_id.label('thing2_id'),
        "_name": r_table.c.name.label('name'),
        "_date": r_table.c.date.label('date'),
    }

    if not ret_props:
        valid_props = ', '.join(prop_to_column.keys())
        raise ValueError("ret_props must contain at least one of " + valid_props)

    columns = []
    for prop in ret_props:
        if prop not in prop_to_column:
            raise ValueError("ret_props got unrecognized %s" % prop)

        columns.append(prop_to_column[prop])

    s = sa.select(columns)
    need_join1 = ('thing1_id', t1_table)
    need_join2 = ('thing2_id', t2_table)
    joins_needed = set()

    for op in operators.op_iter(constraints):
        #vals = con.rval
        key = op.lval_name
        prefix = key[:4]
        
        if prefix in ('_t1_', '_t2_'):
            #not a thing attribute
            key = key[4:]

            if prefix == '_t1_':
                join = need_join1
                joins_needed.add(join)
            elif prefix == '_t2_':
                join = need_join2
                joins_needed.add(join)

            table = join[1]
            op.lval = translate_sort(table, key, op.lval)
            op.rval = translate_thing_value(op.rval)
            #ors = [sa_op(con, key, v) for v in vals]
            #s.append_whereclause(sa.or_(*ors))

        elif prefix.startswith('_'):
            op.lval = r_table.c[key[1:]]

        else:
            alias = d_table.alias()
            s.append_whereclause(r_table.c.rel_id == alias.c.thing_id)
            s.append_column(alias.c.value.label(key))
            s.append_whereclause(alias.c.key == key)

            translate_data_value(alias, op)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if sort:
        s, cols = add_sort(
            sort=sort,
            t_table={'_': r_table, '_t1_': t1_table, '_t2_': t2_table},
            select=s,
        )

        #do we need more joins?
        for (col, table) in cols:
            if table == need_join1[1]:
                joins_needed.add(need_join1)
            elif table == need_join2[1]:
                joins_needed.add(need_join2)

    for j in joins_needed:
        col, table = j
        s.append_whereclause(r_table.c[col] == table.c.thing_id)

    if limit:
        s = s.limit(limit)

    if offset:
        s = s.offset(offset)

    try:
        r = add_request_info(s).execute()
    except Exception, e:
        dbm.mark_dead(r_table.bind)
        # this thread must die so that others may live
        raise

    def build_fn(row):
        # return Storage objects with just the requested props
        props = {}
        for prop in ret_props:
            db_prop = prop[1:]  # column name doesn't have _ prefix
            props[prop] = getattr(row, db_prop)
        return storage(**props)

    return Results(sa_ResultProxy=r, build_fn=build_fn)


if logging.getLogger('sqlalchemy').handlers:
    logging.getLogger('sqlalchemy').handlers[0].formatter = log_format


#inconsitencies:

#relationships assume their thing and data tables are in the same
#database. things don't make that assumption. in practice thing/data
#tables always go together.
#
#we create thing tables for a relationship's things that aren't on the
#same database as the relationship, although they're never used in
#practice. we could remove a healthy chunk of code if we removed that.
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import sqlalchemy as sa
import cPickle as pickle

class tdb_lite(object):
    def __init__(self, gc):
        self.gc = gc

    def make_metadata(self, engine):
        metadata = sa.MetaData(engine)
        metadata.bind.echo = self.gc.sqlprinting
        return metadata

    def index_str(self, table, name, on, where = None):
        index_str = 'create index idx_%s_' % name
        index_str += table.name
        index_str += ' on '+ table.name + ' (%s)' % on
        if where:
            index_str += ' where %s' % where
        return index_str

    def create_table(self, table, index_commands=None):
        t = table
        if self.gc.db_create_tables:
            #@@hackish?
            if not t.bind.has_table(t.name):
                t.create(checkfirst = False)
                if index_commands:
                    for i in index_commands:
                        t.bind.execute(i)

    def py2db(self, val, return_kind=False):
        if isinstance(val, bool):
            val = 't' if val else 'f'
            kind = 'bool'
        elif isinstance(val, (str, unicode)):
            kind = 'str'
        elif isinstance(val, (int, float, long)):
            kind = 'num'
        elif val is None:
            kind = 'none'
        else:
            kind = 'pickle'
            val = pickle.dumps(val)

        if return_kind:
            return (val, kind)
        else:
            return val

    def db2py(self, val, kind):
        if kind == 'bool':
            val = True if val is 't' else False
        elif kind == 'num':
            try:
                val = int(val)
            except ValueError:
                val = float(val)
        elif kind == 'none':
            val = None
        elif kind == 'pickle':
            val = pickle.loads(val)

        return val
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db._sorts import epoch_seconds, score, hot, _hot
from r2.lib.db._sorts import controversy, confidence, qa
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import inspect
import pytz
from datetime import datetime

from pylons import app_globals as g

from pycassa import ColumnFamily
from pycassa.pool import MaximumRetryException
from pycassa.cassandra.ttypes import ConsistencyLevel, NotFoundException
from pycassa.system_manager import (
    ASCII_TYPE,
    COUNTER_COLUMN_TYPE,
    DATE_TYPE,
    INT_TYPE,
    SystemManager,
    TIME_UUID_TYPE,
    UTF8_TYPE,
)
from pycassa.types import DateType
from pycassa.util import convert_uuid_to_time
from r2.lib.utils import tup, Storage
from r2.lib.sgm import sgm
from uuid import uuid1, UUID
from itertools import chain
import cPickle as pickle
from pycassa.util import OrderedDict
import base64

connection_pools = g.cassandra_pools
default_connection_pool = g.cassandra_default_pool

keyspace = 'reddit'
disallow_db_writes = g.disallow_db_writes
tz = g.tz
log = g.log
read_consistency_level = g.cassandra_rcl
write_consistency_level = g.cassandra_wcl
debug = g.debug
make_lock = g.make_lock
db_create_tables = g.db_create_tables

thing_types = {}

TRANSIENT_EXCEPTIONS = (MaximumRetryException,)

# The available consistency levels
CL = Storage(ANY    = ConsistencyLevel.ANY,
             ONE    = ConsistencyLevel.ONE,
             QUORUM = ConsistencyLevel.QUORUM,
             ALL    = ConsistencyLevel.ALL)

# the greatest number of columns that we're willing to accept over the
# wire for a given row (this should be increased if we start working
# with classes with lots of columns, like Account which has lots of
# karma_ rows, or we should not do that)
max_column_count = 10000

# the pycassa date serializer, for use when we can't set the right metadata
# to get pycassa to serialize dates for us
date_serializer = DateType()

class CassandraException(Exception):
    """Base class for Exceptions in tdb_cassandra"""
    pass

class InvariantException(CassandraException):
    """Exceptions that can only be caused by bugs in tdb_cassandra"""
    pass

class ConfigurationException(CassandraException):
    """Exceptions that are caused by incorrect configuration on the
       Cassandra server"""
    pass

class TdbException(CassandraException):
    """Exceptions caused by bugs in our callers or subclasses"""
    pass

class NotFound(CassandraException, NotFoundException):
    """Someone asked us for an ID that isn't stored in the DB at
       all. This is probably an end-user's fault."""
    pass

def will_write(fn):
    """Decorator to indicate that a given function intends to write
       out to Cassandra"""
    def _fn(*a, **kw):
        if disallow_db_writes:
            raise CassandraException("Not so fast! DB writes have been disabled")
        return fn(*a, **kw)
    return _fn

def get_manager(seeds):
    # n.b. does not retry against multiple servers
    server = seeds[0]
    return SystemManager(server)

class ThingMeta(type):
    def __init__(cls, name, bases, dct):
        type.__init__(cls, name, bases, dct)

        if hasattr(cls, '_ttl') and hasattr(cls._ttl, 'total_seconds'):
            cls._ttl = cls._ttl.total_seconds()

        if cls._use_db:
            if cls._type_prefix is None:
                # default to the class name
                cls._type_prefix = name

            if '_' in cls._type_prefix:
                raise TdbException("Cannot have _ in type prefix %r (for %r)"
                                   % (cls._type_prefix, name))

            if cls._type_prefix in thing_types:
                raise InvariantException("Redefining type %r?" % (cls._type_prefix))

            # if we weren't given a specific _cf_name, we can use the
            # classes's name
            cf_name = cls._cf_name or name

            thing_types[cls._type_prefix] = cls

            if not getattr(cls, "_read_consistency_level", None):
                cls._read_consistency_level = read_consistency_level
            if not getattr(cls, "_write_consistency_level", None):
                cls._write_consistency_level = write_consistency_level

            pool_name = getattr(cls, "_connection_pool", default_connection_pool)
            connection_pool = connection_pools[pool_name]
            cassandra_seeds = connection_pool.server_list

            try:
                cls._cf = ColumnFamily(connection_pool,
                                       cf_name,
                                       read_consistency_level = cls._read_consistency_level,
                                       write_consistency_level = cls._write_consistency_level)
            except NotFoundException:
                if not db_create_tables:
                    raise

                manager = get_manager(cassandra_seeds)

                # allow subclasses to add creation args or override base class ones
                extra_creation_arguments = {}
                for c in reversed(inspect.getmro(cls)):
                    creation_args = getattr(c, "_extra_schema_creation_args", {})
                    extra_creation_arguments.update(creation_args)

                log.warning("Creating Cassandra Column Family %s" % (cf_name,))
                with make_lock("cassandra_schema", 'cassandra_schema'):
                    manager.create_column_family(keyspace, cf_name,
                                                 comparator_type = cls._compare_with,
                                                 super=getattr(cls, '_super', False),
                                                 **extra_creation_arguments
                                                 )
                log.warning("Created Cassandra Column Family %s" % (cf_name,))

                # try again to look it up
                cls._cf = ColumnFamily(connection_pool,
                                       cf_name,
                                       read_consistency_level = cls._read_consistency_level,
                                       write_consistency_level = cls._write_consistency_level)

        cls._kind = name

    def __repr__(cls):
        return '<thing: %s>' % cls.__name__


class ThingBase(object):
    # base class for Thing

    __metaclass__ = ThingMeta

    _cf_name = None # the name of the ColumnFamily; defaults to the
                    # name of the class

    # subclasses must replace these

    _type_prefix = None # this must be present for classes with _use_db==True

    _use_db = False

    # the Cassandra column-comparator (internally orders column
    # names). In real life you can't change this without some changes
    # to tdb_cassandra to support other attr types
    _compare_with = UTF8_TYPE

    _value_type = None # if set, overrides all of the _props types
                       # below. Used for Views. One of 'int', 'float',
                       # 'bool', 'pickle', 'json', 'date', 'bytes', 'str'

    _int_props = ()
    _float_props = () # note that we can lose resolution on these
    _bool_props = ()
    _pickle_props = ()
    _json_props = ()
    _date_props = () # note that we can lose resolution on these
    _bytes_props = ()
    _str_props = () # at present we never actually read out of here
                    # since it's the default if none of the previous
                    # matches

    # the value that we assume a property to have if it is not found
    # in the DB. Note that we don't do type-checking here, so if you
    # want a default to be a boolean and want it to be storable you'll
    # also have to set it in _bool_props
    _defaults = {}

    # The default TTL in seconds to add to all columns. Note: if an
    # entire object is expected to have a TTL, it should be considered
    # immutable! (You don't want to write out an object with an author
    # and date, then go update author or add a new column, then have
    # the original columns expire. Then when you go to look it up, the
    # inherent properties author and/or date will be gone and only the
    # updated columns will be present.) This is an expected convention
    # and is not enforced.
    _ttl = None
    _warn_on_partial_ttl = True

    # A per-class dictionary of default TTLs that new columns of this
    # class should have
    _default_ttls = {}

    # A per-instance property defining the TTL of individual columns
    # (that must also appear in self._dirties)
    _column_ttls = {}

    # a timestamp property that will automatically be added to newly
    # created Things (disable by setting to None)
    _timestamp_prop = None

    # a per-instance property indicating that this object was
    # partially loaded: i.e. only some properties were requested from
    # the DB
    _partial = None

    # a per-instance property that specifies that the columns backing
    # these attributes are to be removed on _commit()
    _deletes = set()

    # thrift will materialize the entire result set for a slice range
    # in memory, meaning that we need to limit the maximum number of columns
    # we receive in a single get to avoid hurting the server. if this
    # value is true, we will make sure to do extra gets to retrieve all of
    # the columns in a row when there are more than the per-call maximum.
    _fetch_all_columns = False

    # request-local cache to avoid duplicate lookups from hitting C*
    _local_cache = g.cassandra_local_cache

    def __init__(self, _id = None, _committed = False, _partial = None, **kw):
        # things that have changed
        self._dirties = kw.copy()

        # what the original properties were when we went to Cassandra to
        # get them
        self._orig = {}

        self._defaults = self._defaults.copy()

        # whether this item has ever been created
        self._committed = _committed

        self._partial = None if _partial is None else frozenset(_partial)

        self._deletes = set()
        self._column_ttls = {}

        # our row key
        self._id = _id

        if not self._use_db:
            raise TdbException("Cannot make instances of %r" % (self.__class__,))

    @classmethod
    def _byID(cls, ids, return_dict=True, properties=None):
        ids, is_single = tup(ids, True)

        if properties is not None:
            asked_properties = frozenset(properties)
            willask_properties = set(properties)

        if not len(ids):
            if is_single:
                raise InvariantException("whastis?")
            return {}

        # all keys must be strings or directly convertable to strings
        assert all(isinstance(_id, basestring) or str(_id) for _id in ids)

        def reject_bad_partials(cached, still_need):
            # tell sgm that the match it found in the cache isn't good
            # enough if it's a partial that doesn't include our
            # properties. we still need to look those items up to get
            # the properties that we're after
            stillfind = set()

            for k, v in cached.iteritems():
                if properties is None:
                    if v._partial is not None:
                        # there's a partial in the cache but we're not
                        # looking for partials
                        stillfind.add(k)
                elif v._partial is not None and not asked_properties.issubset(v._partial):
                    # we asked for a partial, and this is a partial,
                    # but it doesn't have all of the properties that
                    # we need
                    stillfind.add(k)

                    # other callers in our request are now expecting
                    # to find the properties that were on that
                    # partial, so we'll have to preserve them
                    for prop in v._partial:
                        willask_properties.add(prop)

            for k in stillfind:
                del cached[k]
                still_need.add(k)

        def lookup(l_ids):
            if properties is None:
                rows = cls._cf.multiget(l_ids, column_count=max_column_count)

                # if we got max_column_count columns back for a row, it was
                # probably clipped. in this case, we should fetch the remaining
                # columns for that row and add them to the result.
                if cls._fetch_all_columns:
                    for key, row in rows.iteritems():
                        if len(row) == max_column_count:
                            last_column_seen = next(reversed(row))
                            cols = cls._cf.xget(key,
                                                column_start=last_column_seen,
                                                buffer_size=max_column_count)
                            row.update(cols)
            else:
                rows = cls._cf.multiget(l_ids, columns = willask_properties)

            l_ret = {}
            for t_id, row in rows.iteritems():
                t = cls._from_serialized_columns(t_id, row)
                if properties is not None:
                    # make sure that the item is marked as a _partial
                    t._partial = willask_properties
                l_ret[t._id] = t

            return l_ret

        ret = sgm(
            cache=cls._local_cache,
            keys=ids,
            miss_fn=lookup,
            prefix=cls._cache_prefix(),
            found_fn=reject_bad_partials,
        )

        if is_single and not ret:
            raise NotFound("<%s %r>" % (cls.__name__,
                                        ids[0]))
        elif is_single:
            assert len(ret) == 1
            return ret.values()[0]
        elif return_dict:
            return ret
        else:
            return filter(None, (ret.get(i) for i in ids))

    @property
    def _fullname(self):
        if self._type_prefix is None:
            raise TdbException("%r has no _type_prefix, so fullnames cannot be generated"
                               % self.__class__)

        return '%s_%s' % (self._type_prefix, self._id)

    @classmethod
    def _by_fullname(cls, fnames, return_dict=True, ignore_missing=False):
        if ignore_missing:
            raise NotImplementedError
        ids, is_single = tup(fnames, True)

        by_cls = {}
        for i in ids:
            typ, underscore, _id = i.partition('_')
            assert underscore == '_'

            by_cls.setdefault(thing_types[typ], []).append(_id)

        items = []
        for typ, ids in by_cls.iteritems():
            items.extend(typ._byID(ids).values())

        if is_single:
            try:
                return items[0]
            except IndexError:
                raise NotFound("<%s %r>" % (cls.__name__, ids[0]))
        elif return_dict:
            return dict((x._fullname, x) for x in items)
        else:
            d = dict((x._fullname, x) for x in items)
            return [d[fullname] for fullname in fnames]

    @classmethod
    def _cache_prefix(cls):
        return 'tdbcassandra_' + cls._type_prefix + '_'

    def _cache_key(self):
        if not self._id:
            raise TdbException('no cache key for uncommitted %r' % (self,))

        return self._cache_key_id(self._id)

    @classmethod
    def _cache_key_id(cls, t_id):
        return cls._cache_prefix() + t_id

    @classmethod
    def _wcl(cls, wcl, default = None):
        if wcl is not None:
            return wcl
        elif default is not None:
            return default
        return cls._write_consistency_level

    def _rcl(cls, rcl, default = None):
        if rcl is not None:
            return rcl
        elif default is not None:
            return default
        return cls._read_consistency_level

    @classmethod
    def _get_column_validator(cls, colname):
        return cls._cf.column_validators.get(colname,
                                             cls._cf.default_validation_class)

    @classmethod
    def _deserialize_column(cls, attr, val):
        if attr in cls._int_props or (cls._value_type and cls._value_type == 'int'):
            try:
                return int(val)
            except ValueError:
                return long(val)
        elif attr in cls._float_props or (cls._value_type and cls._value_type == 'float'):
            return float(val)
        elif attr in cls._bool_props or (cls._value_type and cls._value_type == 'bool'):
            # note that only the string "1" is considered true!
            return val == '1'
        elif attr in cls._pickle_props or (cls._value_type and cls._value_type == 'pickle'):
            return pickle.loads(val)
        elif attr in cls._json_props or (cls._value_type and cls._value_type == 'json'):
            return json.loads(val)
        elif attr in cls._date_props or attr == cls._timestamp_prop or (cls._value_type and cls._value_type == 'date'):
            return cls._deserialize_date(val)
        elif attr in cls._bytes_props or (cls._value_type and cls._value_type == 'bytes'):
            return val

        # otherwise we'll assume that it's a utf-8 string
        return val if isinstance(val, unicode) else val.decode('utf-8')

    @classmethod
    def _serialize_column(cls, attr, val):
        if (attr in chain(cls._int_props, cls._float_props) or
            (cls._value_type and cls._value_type in ('float', 'int'))):
            return str(val)
        elif attr in cls._bool_props or (cls._value_type and cls._value_type == 'bool'):
            # n.b. we "truncate" this to a boolean, so truthy but
            # non-boolean values are discarded
            return '1' if val else '0'
        elif attr in cls._pickle_props or (cls._value_type and cls._value_type == 'pickle'):
            return pickle.dumps(val)
        elif attr in cls._json_props or (cls._value_type and cls._value_type == 'json'):
            return json.dumps(val)
        elif (attr in cls._date_props or attr == cls._timestamp_prop or
              (cls._value_type and cls._value_type == 'date')):
            # the _timestamp_prop is handled in _commit(), not here
            validator = cls._get_column_validator(attr)
            if validator in ("DateType", "TimeUUIDType"):
                # pycassa will take it from here
                return val
            else:
                return cls._serialize_date(val)
        elif attr in cls._bytes_props or (cls._value_type and cls._value_type == 'bytes'):
            return val

        return unicode(val).encode('utf-8')

    @classmethod
    def _serialize_date(cls, date):
        return date_serializer.pack(date)

    @classmethod
    def _deserialize_date(cls, val):
        if isinstance(val, datetime):
            date = val
        elif isinstance(val, UUID):
            return convert_uuid_to_time(val)
        elif len(val) == 8: # cassandra uses 8-byte integer format for this
            date = date_serializer.unpack(val)
        else: # it's probably the old-style stringified seconds since epoch
            as_float = float(val)
            date = datetime.utcfromtimestamp(as_float)

        return date.replace(tzinfo=pytz.utc)

    @classmethod
    def _from_serialized_columns(cls, t_id, columns):
        d_columns = dict((attr, cls._deserialize_column(attr, val))
                         for (attr, val)
                         in columns.iteritems())
        return cls._from_columns(t_id, d_columns)

    @classmethod
    def _from_columns(cls, t_id, columns):
        """Given a dictionary of freshly deserialized columns
           construct an instance of cls"""
        t = cls()
        t._orig = columns
        t._id = t_id
        t._committed = True
        return t

    @property
    def _dirty(self):
        return len(self._dirties) or len(self._deletes) or not self._committed

    @will_write
    def _commit(self, write_consistency_level = None):
        if not self._dirty:
            return

        if self._id is None:
            raise TdbException("Can't commit %r without an ID" % (self,))

        if self._committed and self._ttl and self._warn_on_partial_ttl:
            log.warning("Using a full-TTL object %r in a mutable fashion"
                        % (self,))

        if not self._committed:
            # if this has never been committed we should also consider
            # the _orig columns as dirty (but "less dirty" than the
            # _dirties)
            upd = self._orig.copy()
            self._orig.clear()
            upd.update(self._dirties)
            self._dirties = upd

        # Cassandra values are untyped byte arrays, so we need to
        # serialize everything while filtering out anything that's
        # been dirtied but doesn't actually differ from what's already
        # in the DB
        updates = dict((attr, self._serialize_column(attr, val))
                       for (attr, val)
                       in self._dirties.iteritems()
                       if (attr not in self._orig or
                           val != self._orig[attr]))

        # n.b. deleted columns are applied *after* the updates. our
        # __setattr__/__delitem__ tries to make sure that this always
        # works

        if not self._committed and self._timestamp_prop and self._timestamp_prop not in updates:
            # auto-create timestamps on classes that request them

            # this serialize/deserialize is a bit funny: the process
            # of storing and retrieving causes us to lose some
            # resolution because of the floating-point representation,
            # so this is just to make sure that we have the same value
            # that the DB does after writing it out. Note that this is
            # the only property munged this way: other timestamp and
            # floating point properties may lose resolution
            s_now = self._serialize_date(datetime.now(tz))
            now = self._deserialize_date(s_now)

            timestamp_is_typed = self._get_column_validator(self._timestamp_prop) == "DateType"
            updates[self._timestamp_prop] = now if timestamp_is_typed else s_now
            self._dirties[self._timestamp_prop] = now

        if not updates and not self._deletes:
            self._dirties.clear()
            return

        # actually write out the changes to the CF
        wcl = self._wcl(write_consistency_level)
        with self._cf.batch(write_consistency_level = wcl) as b:
            if updates:
                for k, v in updates.iteritems():
                    b.insert(self._id,
                             {k: v},
                             ttl=self._column_ttls.get(k, self._ttl))
            if self._deletes:
                b.remove(self._id, self._deletes)

        self._orig.update(self._dirties)
        self._column_ttls.clear()
        self._dirties.clear()
        for k in self._deletes:
            try:
                del self._orig[k]
            except KeyError:
                pass
        self._deletes.clear()

        if not self._committed:
            self._on_create()
        else:
            self._on_commit()

        self._committed = True

        self.__class__._local_cache.set(self._cache_key(), self)

    def _revert(self):
        if not self._committed:
            raise TdbException("Revert to what?")

        self._dirties.clear()
        self._deletes.clear()
        self._column_ttls.clear()

    def _destroy(self):
        self._cf.remove(self._id,
                        write_consistency_level=self._write_consistency_level)

    def __getattr__(self, attr):
        if isinstance(attr, basestring) and attr.startswith('_'):
            # TODO: I bet this interferes with Views whose column names can
            # start with a _
            try:
                return self.__dict__[attr]
            except KeyError:
                raise AttributeError, attr

        if attr in self._deletes:
            raise AttributeError("%r has no %r because you deleted it", (self, attr))
        elif attr in self._dirties:
            return self._dirties[attr]
        elif attr in self._orig:
            return self._orig[attr]
        elif attr in self._defaults:
            return self._defaults[attr]
        elif self._partial is not None and attr not in self._partial:
            raise AttributeError("%r has no %r but you didn't request it" % (self, attr))
        else:
            raise AttributeError('%r has no %r' % (self, attr))

    def __setattr__(self, attr, val):
        if attr == '_id' and self._committed:
            raise ValueError('cannot change _id on a committed %r' % (self.__class__))

        if isinstance(attr, basestring) and attr.startswith('_'):
            # TODO: I bet this interferes with Views whose column names can
            # start with a _
            return object.__setattr__(self, attr, val)

        try:
            self._deletes.remove(attr)
        except KeyError:
            pass
        self._dirties[attr] = val
        if attr in self._default_ttls:
            self._column_ttls[attr] = self._default_ttls[attr]

    def __eq__(self, other):
        if self.__class__ != other.__class__:
            return False

        if self._partial or other._partial and self._partial != other._partial:
            raise ValueError("Can't compare incompatible partials")

        return self._id == other._id and self._t == other._t

    def __ne__(self, other):
        return not (self == other)

    @property
    def _t(self):
        """Emulate the _t property from tdb_sql: a dictionary of all
           values that are or will be stored in the database, (not
           including _defaults or unrequested properties on
           partials)"""
        ret = self._orig.copy()
        ret.update(self._dirties)
        for k in self._deletes:
            try:
                del ret[k]
            except KeyError:
                pass
        return ret

    # allow the dictionary mutation syntax; it makes working some some
    # keys a bit easier. Go through our regular
    # __getattr__/__setattr__ functions where all of the appropriate
    # work is done
    def __getitem__(self, key):
        return self.__getattr__(key)

    def __setitem__(self, key, value):
        return self.__setattr__(key, value)

    def __delitem__(self, key):
        try:
            del self._dirties[key]
        except KeyError:
            pass
        try:
            del self._column_ttls[key]
        except KeyError:
            pass
        self._deletes.add(key)

    def _get(self, key, default = None):
        try:
            return self.__getattr__(key)
        except AttributeError:
            if self._partial is not None and key not in self._partial:
                raise AttributeError("_get on unrequested key from partial")
            return default

    def _set_ttl(self, key, ttl):
        assert key in self._dirties
        assert isinstance(ttl, (long, int))
        self._column_ttls[key] = ttl

    def _on_create(self):
        """A hook executed on creation, good for creation of static
           Views. Subclasses should call their parents' hook(s) as
           well"""
        pass

    def _on_commit(self):
        """Executed on _commit other than creation."""
        pass

    @classmethod
    def _all(cls):
        # returns a query object yielding every single item in a
        # column family. it probably shouldn't be used except in
        # debugging
        return Query(cls, limit=None)

    def __repr__(self):
        # it's safe for subclasses to override this to e.g. put a Link
        # title or Account name in the repr(), but they must be
        # careful to check hasattr for the properties that they read
        # out, as __getattr__ itself may call __repr__ in constructing
        # its error messages
        id_str = self._id
        comm_str = '' if self._committed else ' (uncommitted)'
        part_str = '' if self._partial is None else ' (partial)'
        return "<%s %r%s%s>" % (self.__class__.__name__,
                              id_str,
                              comm_str, part_str)

    if debug:
        # we only want this with g.debug because overriding __del__ can play
        # hell with memory leaks
        def __del__(self):
            if not self._committed:
                # normally we'd log this with g.log or something, but we can't
                # guarantee that the thread destructing us has access to g
                print "Warning: discarding uncomitted %r; this is usually a bug" % (self,)
            elif self._dirty:
                print ("Warning: discarding dirty %r; this is usually a bug (_dirties=%r, _deletes=%r)"
                       % (self,self._dirties,self._deletes))

class Thing(ThingBase):
    _timestamp_prop = 'date'

    # alias _date property for consistency with tdb_sql things.
    @property
    def _date(self):
        return self.date

class UuidThing(ThingBase):
    _timestamp_prop = 'date'
    _extra_schema_creation_args = {
        'key_validation_class': TIME_UUID_TYPE
    }

    def __init__(self, **kw):
        ThingBase.__init__(self, _id=uuid1(), **kw)

    @classmethod
    def _byID(cls, ids, **kw):
        ids, is_single = tup(ids, ret_is_single=True)

        #Convert string ids to UUIDs before retrieving
        uuids = [UUID(id) if not isinstance(id, UUID) else id for id in ids]

        if len(uuids) == 0:
            return {}
        elif is_single:
            assert len(uuids) == 1
            uuids = uuids[0]

        return super(UuidThing, cls)._byID(uuids, **kw)

    @classmethod
    def _cache_key_id(cls, t_id):
        return cls._cache_prefix() + str(t_id)


def view_of(cls):
    """Register a class as a view of a Thing.

    When a Thing is created or destroyed the appropriate View method must be
    called to update the View. This can be done using Thing._on_create() for
    general Thing classes or create()/destroy() for DenormalizedRelation
    classes.

    """
    def view_of_decorator(view_cls):
        cls._views.append(view_cls)
        view_cls._view_of = cls
        return view_cls
    return view_of_decorator



class DenormalizedRelation(object):
    """A model of many-to-many relationships, indexed by thing1.

    Each thing1 is represented by a row. The relationships from that thing1 to
    a number of thing2s are represented by columns in that row. To query if
    relationships exist and what its value is ("name" in the PG model), we
    fetch the thing1's row, telling C* we're only interested in the columns
    representing the thing2s we are interested in. This allows negative lookups
    to be very fast because of the row-level bloom filter.

    This data model will generate VERY wide rows. Any column family based on
    it should have its row cache disabled.

    """
    __metaclass__ = ThingMeta
    _use_db = False
    _cf_name = None
    _compare_with = ASCII_TYPE
    _type_prefix = None
    _last_modified_name = None
    _write_last_modified = True
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)
    _ttl = None

    @classmethod
    def value_for(cls, thing1, thing2, **kw):
        """Return a value to store for a relationship between thing1/thing2."""
        raise NotImplementedError()

    @classmethod
    @will_write
    def create(cls, thing1, thing2s, **kw):
        """Create a relationship between thing1 and thing2s.

        If there are any other views of this data, they will be updated as
        well.

        Takes kwargs which can be used by views
        or value_for to get additional information.

        """
        thing2s = tup(thing2s)
        values = {thing2._id36 : cls.value_for(thing1, thing2, **kw)
                  for thing2 in thing2s}
        cls._cf.insert(thing1._id36, values, ttl=cls._ttl)

        for view in cls._views:
            view.create(thing1, thing2s, **kw)

        if cls._write_last_modified:
            from r2.models.last_modified import LastModified
            LastModified.touch(thing1._fullname, cls._last_modified_name)

    @classmethod
    @will_write
    def destroy(cls, thing1, thing2s):
        """Destroy relationships between thing1 and some thing2s."""
        thing2s = tup(thing2s)
        cls._cf.remove(thing1._id36, (thing2._id36 for thing2 in thing2s))

        for view in cls._views:
            view.destroy(thing1, thing2s)

    @classmethod
    def fast_query(cls, thing1, thing2s):
        """Find relationships between thing1 and various thing2s."""
        thing2s, thing2s_is_single = tup(thing2s, ret_is_single=True)

        if not thing1:
            return {}

        # don't bother looking up relationships for items that were created
        # since the last time the thing1 created a relationship of this type
        if cls._last_modified_name:
            from r2.models.last_modified import LastModified
            timestamp = LastModified.get(thing1._fullname,
                                         cls._last_modified_name)
            if timestamp:
                thing2s = [thing2 for thing2 in thing2s
                           if thing2._date <= timestamp]
            else:
                thing2s = []

        if not thing2s:
            return {}

        # fetch the row from cassandra. if it doesn't exist, thing1 has no
        # relation of this type to any thing2!
        try:
            columns = [thing2._id36 for thing2 in thing2s]
            results = cls._cf.get(thing1._id36, columns)
        except NotFoundException:
            results = {}

        # return the data in the expected format
        if not thing2s_is_single:
            # {(thing1, thing2) : value}
            thing2s_by_id = {thing2._id36 : thing2 for thing2 in thing2s}
            return {(thing1, thing2s_by_id[k]) : v
                    for k, v in results.iteritems()}
        else:
            if results:
                assert len(results) == 1
                return results.values()[0]
            else:
                raise NotFound("<%s %r>" % (cls.__name__, (thing1._id36,
                                                           thing2._id36)))


class ColumnQuery(object):
    """
    A query across a row of a CF.
    """
    _chunk_size = 100

    def __init__(self, cls, rowkeys, column_start="", column_finish="",
                 column_count=100, column_reversed=True,
                 column_to_obj=None,
                 obj_to_column=None):
        self.cls = cls
        self.rowkeys = rowkeys
        self.column_start = column_start
        self.column_finish = column_finish
        self._limit = column_count
        self.column_reversed = column_reversed
        self.column_to_obj = column_to_obj or self.default_column_to_obj
        self.obj_to_column = obj_to_column or self.default_obj_to_column
        self._rules = []    # dummy parameter to mimic tdb_sql queries

        # Sorting for TimeUuid objects
        if self.cls._compare_with == TIME_UUID_TYPE:
            def sort_key(i):
                return i.time
        else:
            def sort_key(i):
                return i
        self.sort_key = sort_key

    @staticmethod
    def combine(queries):
        raise NotImplementedError

    @staticmethod
    def default_column_to_obj(columns):
        """
        Mapping from column --> object.

        This default doesn't actually return the underlying object but we don't
        know how to do that without more information.
        """
        return columns

    @staticmethod
    def default_obj_to_column(objs):
        """
        Mapping from object --> column
        """
        objs, is_single = tup(objs, ret_is_single=True)
        columns = [{obj._id: obj._id} for obj in objs]

        if is_single:
            return columns[0]
        else:
            return columns

    def _after(self, thing):
        if thing:
            column_name = self.obj_to_column(thing).keys()[0]
            self.column_start = column_name
        else:
            self.column_start = ""

    def _after_id(self, column_name):
        self.column_start = column_name

    def _reverse(self):
        # Logic of standard reddit query is opposite of cassandra
        self.column_reversed = False

    def __iter__(self, yield_column_names=False):
        retrieved = 0
        column_start = self.column_start
        while retrieved < self._limit:
            try:
                column_count = min(self._chunk_size, self._limit - retrieved)
                if column_start:
                    column_count += 1   # cassandra includes column_start
                r = self.cls._cf.multiget(self.rowkeys,
                                          column_start=column_start,
                                          column_finish=self.column_finish,
                                          column_count=column_count,
                                          column_reversed=self.column_reversed)

                # multiget returns OrderedDict {rowkey: {column_name: column_value}}
                # combine into single OrderedDict of {column_name: column_value}
                nrows = len(r.keys())
                if nrows == 0:
                    return
                elif nrows == 1:
                    columns = r.values()[0]
                else:
                    r_combined = {}
                    for d in r.values():
                        r_combined.update(d)
                    columns = OrderedDict(sorted(r_combined.items(),
                                                 key=lambda t: self.sort_key(t[0]),
                                                 reverse=self.column_reversed))
            except NotFoundException:
                return

            retrieved += self._chunk_size

            if column_start:
                try:
                    del columns[column_start]
                except KeyError:
                    # This can happen when a timezone-aware datetime is
                    # passed in as a column_start, but non-timezone-aware
                    # datetimes are returned from cassandra, causing `del` to
                    # fail.
                    #
                    # Reversed queries include column_start in the results,
                    # while non-reversed queries do not.
                    if self.column_reversed:
                        columns.popitem(last=False)

            if not columns:
                return

            # Convert to list of columns
            l_columns = [{col_name: columns[col_name]} for col_name in columns]

            column_start = l_columns[-1].keys()[0]
            objs = self.column_to_obj(l_columns)

            if yield_column_names:
                column_names = [column.keys()[0] for column in l_columns]
                if len(column_names) == 1:
                    ret = (column_names[0], objs),
                else:
                    ret = zip(column_names, objs)
            else:
                ret = objs

            ret, is_single = tup(ret, ret_is_single=True)
            for r in ret:
                yield r

    def __repr__(self):
        return "<%s(%s-%r)>" % (self.__class__.__name__, self.cls.__name__,
                                self.rowkeys)

class MultiColumnQuery(object):
    def __init__(self, queries, num, sort_key=None):
        self.num = num
        self._queries = queries
        self.sort_key = sort_key    # python doesn't sort UUID1's correctly, need to pass in a sorter
        self._rules = []            # dummy parameter to mimic tdb_sql queries

    def _after(self, thing):
        for q in self._queries:
            q._after(thing)

    def _reverse(self):
        for q in self._queries:
            q._reverse()

    def __setattr__(self, attr, val):
        # Catch _limit to set on all queries
        if attr == '_limit':
             for q in self._queries:
                 q._limit = val
        else:
            object.__setattr__(self, attr, val)

    def __iter__(self):

        if self.sort_key:
            def sort_key(tup):
                # Need to point the supplied sort key at the correct item in
                # the (sortable, item, generator) tuple
                return self.sort_key(tup[0])
        else:
            def sort_key(tup):
                return tup[0]

        top_items = []
        for q in self._queries:
            try:
                gen = q.__iter__(yield_column_names=True)
                column_name, item = gen.next()
                top_items.append((column_name, item, gen))
            except StopIteration:
                pass
        top_items.sort(key=sort_key)

        def _update(top_items):
            # Remove the first item from combined query and update the list
            head = top_items.pop(0)
            item = head[1]
            gen = head[2]

            # Try to get a new item from the query that gave us the current one
            try:
                column_name, item = gen.next()
                top_items.append((column_name, item, gen)) # if multiple queues have the same item value the sort is somewhat undefined
                top_items.sort(key=sort_key)
            except StopIteration:
                pass

        num_ret = 0
        while top_items and num_ret < self.num:
            yield top_items[0][1]
            _update(top_items)
            num_ret += 1

class Query(object):
    """A query across a CF. Note that while you can query rows from a
       CF that has a RandomPartitioner, you won't get them in any sort
       of order"""
    def __init__(self, cls, after=None, properties=None, limit=100,
                 chunk_size=100, _max_column_count = max_column_count):
        self.cls = cls
        self.after = after
        self.properties = properties
        self.limit = limit
        self.chunk_size = chunk_size
        self.max_column_count = _max_column_count

    def __copy__(self):
        return Query(self.cls, after=self.after,
                     properties = self.properties,
                     limit=self.limit,
                     chunk_size=self.chunk_size,
                     _max_column_count = self.max_column_count)
    copy = __copy__

    def _dump(self):
        q = self.copy()
        q.after = q.limit = None

        for row in q:
            print row
            for col, val in row._t.iteritems():
                print '\t%s: %r' % (col, val)

    def __iter__(self):
        # n.b.: we aren't caching objects that we find this way in the
        # LocalCache. This may will need to be changed if we ever
        # start using OPP in Cassandra (since otherwise these types of
        # queries aren't useful for anything but debugging anyway)
        after = '' if self.after is None else self.after._id
        limit = self.limit

        if self.properties is None:
            r = self.cls._cf.get_range(start=after, row_count=limit,
                                       column_count = self.max_column_count)
        else:
            r = self.cls._cf.get_range(start=after, row_count=limit,
                                       columns = self.properties)

        for t_id, columns in r:
            if not columns:
                # a ghost row
                continue

            t = self.cls._from_serialized_columns(t_id, columns)
            yield t

class View(ThingBase):
    # Views are Things like any other, but may have special key
    # characteristics. Uses ColumnQuery for queries across a row.

    _timestamp_prop = None
    _value_type = 'str'

    _compare_with = UTF8_TYPE   # Type of the columns - should match _key_validation_class of _view_of class
    _view_of = None
    _write_consistency_level = CL.ONE   # Is this necessary?
    _query_cls = ColumnQuery

    @classmethod
    def _rowkey(cls, obj):
        """Mapping from _view_of object --> view rowkey. No default
        implementation is provided because this is the fundamental aspect of the
        view."""
        raise NotImplementedError

    @classmethod
    def _obj_to_column(cls, objs):
        """Mapping from _view_of object --> view column. Returns a
        single item dict {column name:column value} or list of dicts."""
        objs, is_single = tup(objs, ret_is_single=True)

        columns = [{obj._id: obj._id} for obj in objs]

        if len(columns) == 1:
            return columns[0]
        else:
            return columns

    @classmethod
    def _column_to_obj(cls, columns):
        """Mapping from view column --> _view_of object. Must be complement to
        _obj_to_column()."""
        columns, is_single = tup(columns, ret_is_single=True)

        ids = [column.keys()[0] for column in columns]

        if len(ids) == 1:
            ids = ids[0]
        return cls._view_of._byID(ids, return_dict=False)

    @classmethod
    def add_object(cls, obj, **kw):
        """Add a lookup to the view"""
        rowkey = cls._rowkey(obj)
        column = cls._obj_to_column(obj)
        cls._set_values(rowkey, column, **kw)

    @classmethod
    def query(cls, rowkeys, after=None, reverse=False, count=1000):
        """Return a query to get objects from the underlying _view_of class."""

        column_reversed = not reverse   # Reverse convention for cassandra is opposite

        q = cls._query_cls(cls, rowkeys, column_count=count,
                           column_reversed=column_reversed,
                           column_to_obj=cls._column_to_obj,
                           obj_to_column=cls._obj_to_column)
        q._after(after)
        return q

    def _values(self):
        """Retrieve the entire contents of the view"""
        # TODO: at present this only grabs max_column_count columns
        return self._t

    @classmethod
    def get_time_sorted_columns(cls, rowkey, limit=None):
        q = cls._cf.xget(rowkey, include_timestamp=True)
        r = sorted(q, key=lambda i: i[1][1]) # (col_name, (col_val, timestamp))
        if limit:
            r = r[:limit]
        return OrderedDict([(i[0], i[1][0]) for i in r])

    @classmethod
    @will_write
    def _set_values(cls, row_key, col_values,
                    write_consistency_level = None,
                    batch=None,
                    ttl=None):
        """Set a set of column values in a row of a view without
           looking up the whole row first"""
        # col_values =:= dict(col_name -> col_value)

        updates = dict((col_name, cls._serialize_column(col_name, col_val))
                       for (col_name, col_val) in col_values.iteritems())

        # if they didn't give us a TTL, use the default TTL for the
        # class. This will be further overwritten below per-column
        # based on the _default_ttls class dict. Note! There is no way
        # to use this API to express that you don't want a TTL if
        # there is a default set on either the row or the column
        default_ttl = ttl or cls._ttl

        def do_inserts(b):
            for k, v in updates.iteritems():
                b.insert(row_key, {k: v},
                         ttl=cls._default_ttls.get(k, default_ttl))

        if batch is None:
            batch = cls._cf.batch(write_consistency_level = cls._wcl(write_consistency_level))
            with batch as b:
                do_inserts(b)
        else:
            do_inserts(batch)

        # can we be smarter here?
        cls._local_cache.delete(cls._cache_key_id(row_key))

    @classmethod
    @will_write
    def _remove(cls, key, columns):
        cls._cf.remove(key, columns)
        cls._local_cache.delete(cls._cache_key_id(key))

class DenormalizedView(View):
    """Store the entire underlying object inside the View column."""

    @classmethod
    def is_date_prop(cls, attr):
        view_cls = cls._view_of
        return (view_cls._value_type == 'date' or
                attr in view_cls._date_props or
                view_cls._timestamp_prop and attr == view_cls._timestamp_prop)

    @classmethod
    def _thing_dumper(cls, thing):
        serialize_fn = cls._view_of._serialize_column
        serialized_columns = dict((attr, serialize_fn(attr, val)) for
            (attr, val) in thing._orig.iteritems())

        # Encode date props which may be binary
        for attr, val in serialized_columns.items():
            if cls.is_date_prop(attr):
                serialized_columns[attr] = base64.b64encode(val)

        dump = json.dumps(serialized_columns)
        return dump

    @classmethod
    def _thing_loader(cls, _id, dump):
        serialized_columns = json.loads(dump)

        # Decode date props
        for attr, val in serialized_columns.items():
            if cls.is_date_prop(attr):
                serialized_columns[attr] = base64.b64decode(val)

        obj = cls._view_of._from_serialized_columns(_id, serialized_columns)
        return obj

    @classmethod
    def _obj_to_column(cls, objs):
        objs = tup(objs)
        columns = []
        for o in objs:
            _id = o._id
            dump = cls._thing_dumper(o)
            columns.append({_id: dump})

        if len(columns) == 1:
            return columns[0]
        else:
            return columns

    @classmethod
    def _column_to_obj(cls, columns):
        columns = tup(columns)
        objs = []
        for column in columns:
            _id, dump = column.items()[0]
            obj = cls._thing_loader(_id, dump)
            objs.append(obj)

        if len(objs) == 1:
            return objs[0]
        else:
            return objs
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
from copy import deepcopy, copy
import cPickle as pickle
from datetime import datetime
from functools import partial
import hashlib
import itertools
import pytz
from time import mktime

from pylons import app_globals as g
from pylons import tmpl_context as c
from pylons import request

from r2.lib import amqp
from r2.lib import filters
from r2.lib.comment_tree import add_comments
from r2.lib.db import tdb_cassandra
from r2.lib.db.operators import and_, or_
from r2.lib.db.operators import asc, desc, timeago
from r2.lib.db.sorts import epoch_seconds
from r2.lib.db.thing import Thing, Merge
from r2.lib import utils
from r2.lib.utils import in_chunks, is_subdomain, SimpleSillyStub
from r2.lib.utils import fetch_things2, tup, UniqueIterator
from r2.lib.voting import prequeued_vote_key
from r2.models import (
    Account,
    Comment,
    Inbox,
    Link,
    LinksByAccount,
    Message,
    ModContribSR,
    ModeratorInbox,
    MultiReddit,
    PromoCampaign,
    Report,
    Subreddit,
    VotesByAccount,
)
from r2.models.last_modified import LastModified
from r2.models.promo import PROMOTE_STATUS, PromotionLog
from r2.models.query_cache import (
    cached_query,
    CachedQuery,
    CachedQueryMutator,
    filter_thing,
    FakeQuery,
    merged_cached_query,
    MergedCachedQuery,
    SubredditQueryCache,
    ThingTupleComparator,
    UserQueryCache,
)
from r2.models.vote import Vote


precompute_limit = 1000

db_sorts = dict(hot = (desc, '_hot'),
                new = (desc, '_date'),
                top = (desc, '_score'),
                controversial = (desc, '_controversy'))

def db_sort(sort):
    cls, col = db_sorts[sort]
    return cls(col)

db_times = dict(all = None,
                hour = Thing.c._date >= timeago('1 hour'),
                day = Thing.c._date >= timeago('1 day'),
                week = Thing.c._date >= timeago('1 week'),
                month = Thing.c._date >= timeago('1 month'),
                year = Thing.c._date >= timeago('1 year'))

# sorts for which there can be a time filter (by day, by week,
# etc). All of these but 'all' are done in mr_top, who knows about the
# structure of the stored CachedResults (so changes here may warrant
# changes there)
time_filtered_sorts = set(('top', 'controversial'))

#we need to define the filter functions here so cachedresults can be pickled
def filter_identity(x):
    return x

def filter_thing2(x):
    """A filter to apply to the results of a relationship query returns
    the object of the relationship."""
    return x._thing2


class CachedResults(object):
    """Given a query returns a list-like object that will lazily look up
    the query from the persistent cache. """
    def __init__(self, query, filter):
        self.query = query
        self.query._limit = precompute_limit
        self.filter = filter
        self.iden = self.get_query_iden(query)
        self.sort_cols = [s.col for s in self.query._sort]
        self.data = []
        self._fetched = False

    @classmethod
    def get_query_iden(cls, query):
        # previously in Query._iden()
        i = str(query._sort) + str(query._kind) + str(query._limit)

        if query._offset:
            i += str(query._offset)

        if query._rules:
            rules = copy(query._rules)
            rules.sort()
            for r in rules:
                i += str(r)

        return hashlib.sha1(i).hexdigest()

    @property
    def sort(self):
        return self.query._sort

    def fetch(self, force=False, stale=False):
        """Loads the query from the cache."""
        self.fetch_multi([self], force=force, stale=stale)

    @classmethod
    def fetch_multi(cls, crs, force=False, stale=False):
        unfetched = filter(lambda cr: force or not cr._fetched, crs)
        if not unfetched:
            return

        keys = [cr.iden for cr in unfetched]
        cached = g.permacache.get_multi(
            keys=keys,
            allow_local=not force,
            stale=stale,
        )
        for cr in unfetched:
            cr.data = cached.get(cr.iden) or []
            cr._fetched = True

    def make_item_tuple(self, item):
        """Given a single 'item' from the result of a query build the tuple
        that will be stored in the query cache. It is effectively the
        fullname of the item after passing through the filter plus the
        columns of the unfiltered item to sort by."""
        filtered_item = self.filter(item)
        lst = [filtered_item._fullname]
        for col in self.sort_cols:
            #take the property of the original 
            attr = getattr(item, col)
            #convert dates to epochs to take less space
            if isinstance(attr, datetime):
                attr = epoch_seconds(attr)
            lst.append(attr)
        return tuple(lst)

    def can_insert(self):
        """True if a new item can just be inserted rather than
           rerunning the query."""
         # This is only true in some circumstances: queries where
         # eligibility in the list is determined only by its sort
         # value (e.g. hot) and where addition/removal from the list
         # incurs an insertion/deletion event called on the query. So
         # the top hottest items in X some subreddit where the query
         # is notified on every submission/banning/unbanning/deleting
         # will work, but for queries with a time-component or some
         # other eligibility factor, it cannot be inserted this way.
        if self.query._sort in ([desc('_date')],
                                [desc('_hot'), desc('_date')],
                                [desc('_score'), desc('_date')],
                                [desc('_controversy'), desc('_date')]):
            if not any(r for r in self.query._rules
                       if r.lval.name == '_date'):
                # if no time-rule is specified, then it's 'all'
                return True
        return False

    def can_delete(self):
        "True if a item can be removed from the listing, always true for now."
        return True

    def _mutate(self, fn, willread=True):
        self.data = g.permacache.mutate(
            key=self.iden,
            mutation_fn=fn,
            default=[],
            willread=willread,
        )
        self._fetched=True

    def insert(self, items):
        """Inserts the item into the cached data. This only works
           under certain criteria, see can_insert."""
        self._insert_tuples([self.make_item_tuple(item) for item in tup(items)])

    def _insert_tuples(self, tuples):
        def _mutate(data):
            data = data or []
            item_tuples = tuples or []

            existing_fnames = {item[0] for item in data}
            new_fnames = {item[0] for item in item_tuples}

            mutated_length = len(existing_fnames.union(new_fnames))
            would_truncate = mutated_length >= precompute_limit
            if would_truncate and data:
                # only insert items that are already stored or new items
                # that are large enough that they won't be immediately truncated
                # out of storage
                # item structure is (name, sortval1[, sortval2, ...])
                smallest = data[-1]
                item_tuples = [item for item in item_tuples
                                    if (item[0] in existing_fnames or
                                        item[1:] >= smallest[1:])]

            if not item_tuples:
                return data

            # insert the items, remove the duplicates (keeping the
            # one being inserted over the stored value if applicable),
            # and sort the result
            data = filter(lambda x: x[0] not in new_fnames, data)
            data.extend(item_tuples)
            data.sort(reverse=True, key=lambda x: x[1:])
            if len(data) > precompute_limit:
                data = data[:precompute_limit]
            return data

        self._mutate(_mutate)

    def delete(self, items):
        """Deletes an item from the cached data."""
        fnames = set(self.filter(x)._fullname for x in tup(items))

        def _mutate(data):
            data = data or []
            return filter(lambda x: x[0] not in fnames,
                          data)

        self._mutate(_mutate)

    def _replace(self, tuples, lock=True):
        """Take pre-rendered tuples from mr_top and replace the
           contents of the query outright. This should be considered a
           private API"""
        if lock:
            def _mutate(data):
                return tuples
            self._mutate(_mutate, willread=False)
        else:
            self._fetched = True
            self.data = tuples
            g.permacache.pessimistically_set(self.iden, tuples)

    def update(self):
        """Runs the query and stores the result in the cache. This is
           only run by hand."""
        self.data = [self.make_item_tuple(i) for i in self.query]
        self._fetched = True
        g.permacache.set(self.iden, self.data)

    def __repr__(self):
        return '<CachedResults %s %s>' % (self.query._rules, self.query._sort)

    def __iter__(self):
        self.fetch()

        for x in self.data:
            yield x[0]

class MergedCachedResults(object):
    """Given two CachedResults, merges their lists based on the sorts
       of their queries."""
    # normally we'd do this by having a superclass of CachedResults,
    # but we have legacy pickled CachedResults that we don't want to
    # break

    def __init__(self, results):
        self.cached_results = results
        CachedResults.fetch_multi([r for r in results
                                   if isinstance(r, CachedResults)])
        CachedQuery._fetch_multi([r for r in results
                                   if isinstance(r, CachedQuery)])
        self._fetched = True

        self.sort = results[0].sort
        comparator = ThingTupleComparator(self.sort)
        # make sure they're all the same
        assert all(r.sort == self.sort for r in results[1:])

        all_items = []
        for cr in results:
            all_items.extend(cr.data)
        all_items.sort(cmp=comparator)
        self.data = all_items


    def __repr__(self):
        return '<MergedCachedResults %r>' % (self.cached_results,)

    def __iter__(self):
        for x in self.data:
            yield x[0]

    def update(self):
        for x in self.cached_results:
            x.update()

def make_results(query, filter = filter_identity):
    return CachedResults(query, filter)

def merge_results(*results):
    if not results:
        return []
    return MergedCachedResults(results)

def migrating_cached_query(model, filter_fn=filter_identity):
    """Returns a CachedResults object that has a new-style cached query
    attached as "new_query". This way, reads will happen from the old
    query cache while writes can be made to go to both caches until a
    backfill migration is complete."""

    decorator = cached_query(model, filter_fn)
    def migrating_cached_query_decorator(fn):
        wrapped = decorator(fn)
        def migrating_cached_query_wrapper(*args):
            new_query = wrapped(*args)
            old_query = make_results(new_query.query, filter_fn)
            old_query.new_query = new_query
            return old_query
        return migrating_cached_query_wrapper
    return migrating_cached_query_decorator


@cached_query(UserQueryCache)
def get_deleted_links(user_id):
    return Link._query(Link.c.author_id == user_id,
                       Link.c._deleted == True,
                       Link.c._spam == (True, False),
                       sort=db_sort('new'))


@cached_query(UserQueryCache)
def get_deleted_comments(user_id):
    return Comment._query(Comment.c.author_id == user_id,
                          Comment.c._deleted == True,
                          Comment.c._spam == (True, False),
                          sort=db_sort('new'))


@merged_cached_query
def get_deleted(user):
    return [get_deleted_links(user),
            get_deleted_comments(user)]


def get_links(sr, sort, time):
    return _get_links(sr._id, sort, time)

def _get_links(sr_id, sort, time):
    """General link query for a subreddit."""
    q = Link._query(Link.c.sr_id == sr_id,
                    sort = db_sort(sort),
                    data = True)

    if time != 'all':
        q._filter(db_times[time])

    res = make_results(q)

    return res

@cached_query(SubredditQueryCache)
def get_spam_links(sr_id):
    return Link._query(Link.c.sr_id == sr_id,
                       Link.c._spam == True,
                       sort = db_sort('new'))

@cached_query(SubredditQueryCache)
def get_spam_comments(sr_id):
    return Comment._query(Comment.c.sr_id == sr_id,
                          Comment.c._spam == True,
                          sort = db_sort('new'))


@cached_query(SubredditQueryCache)
def get_edited_comments(sr_id):
    return FakeQuery(sort=[desc("editted")])


@cached_query(SubredditQueryCache)
def get_edited_links(sr_id):
    return FakeQuery(sort=[desc("editted")])


@merged_cached_query
def get_edited(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_edited_links)
    if include_comments:
        queries.append(get_edited_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]


def moderated_srids(sr, user):
    if isinstance(sr, (ModContribSR, MultiReddit)):
        srs = Subreddit._byID(sr.sr_ids, return_dict=False)
        if user:
            srs = [sr for sr in srs
                   if sr.is_moderator_with_perms(user, 'posts')]
        return [sr._id for sr in srs]
    else:
        return [sr._id]

@merged_cached_query
def get_spam(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_spam_links)
    if include_comments:
        queries.append(get_spam_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

@cached_query(SubredditQueryCache)
def get_spam_filtered_links(sr_id):
    """ NOTE: This query will never run unless someone does an "update" on it,
        but that will probably timeout. Use insert_spam_filtered_links."""
    return Link._query(Link.c.sr_id == sr_id,
                       Link.c._spam == True,
                       Link.c.verdict != 'mod-removed',
                       sort = db_sort('new'))

@cached_query(SubredditQueryCache)
def get_spam_filtered_comments(sr_id):
    return Comment._query(Comment.c.sr_id == sr_id,
                          Comment.c._spam == True,
                          Comment.c.verdict != 'mod-removed',
                          sort = db_sort('new'))

@merged_cached_query
def get_spam_filtered(sr):
    return [get_spam_filtered_links(sr),
            get_spam_filtered_comments(sr)]

@cached_query(SubredditQueryCache)
def get_reported_links(sr_id):
    q = Link._query(Link.c.reported != 0,
                    Link.c._spam == False,
                    sort = db_sort('new'))
    if sr_id is not None:
        q._filter(Link.c.sr_id == sr_id)
    return q

@cached_query(SubredditQueryCache)
def get_reported_comments(sr_id):
    q = Comment._query(Comment.c.reported != 0,
                          Comment.c._spam == False,
                          sort = db_sort('new'))

    if sr_id is not None:
        q._filter(Comment.c.sr_id == sr_id)
    return q

@merged_cached_query
def get_reported(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_reported_links)
    if include_comments:
        queries.append(get_reported_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

@cached_query(SubredditQueryCache)
def get_unmoderated_links(sr_id):
    q = Link._query(Link.c.sr_id == sr_id,
                    Link.c._spam == (True, False),
                    sort = db_sort('new'))

    # Doesn't really work because will not return Links with no verdict
    q._filter(or_(and_(Link.c._spam == True, Link.c.verdict != 'mod-removed'),
                  and_(Link.c._spam == False, Link.c.verdict != 'mod-approved')))
    return q

@merged_cached_query
def get_modqueue(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_reported_links)
        queries.append(get_spam_filtered_links)
    if include_comments:
        queries.append(get_reported_comments)
        queries.append(get_spam_filtered_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

@merged_cached_query
def get_unmoderated(sr, user=None):
    sr_ids = moderated_srids(sr, user)
    queries = [get_unmoderated_links]
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

def get_domain_links(domain, sort, time):
    from r2.lib.db import operators
    q = Link._query(operators.domain(Link.c.url) == filters._force_utf8(domain),
                    sort = db_sort(sort),
                    data = True)
    if time != "all":
        q._filter(db_times[time])

    return make_results(q)

def user_query(kind, user_id, sort, time):
    """General profile-page query."""
    q = kind._query(kind.c.author_id == user_id,
                    kind.c._spam == (True, False),
                    sort = db_sort(sort))
    if time != 'all':
        q._filter(db_times[time])
    return make_results(q)

def get_all_comments():
    """the master /comments page"""
    q = Comment._query(sort = desc('_date'))
    return make_results(q)

def get_sr_comments(sr):
    return _get_sr_comments(sr._id)

def _get_sr_comments(sr_id):
    """the subreddit /r/foo/comments page"""
    q = Comment._query(Comment.c.sr_id == sr_id,
                       sort = desc('_date'))
    return make_results(q)

def _get_comments(user_id, sort, time):
    return user_query(Comment, user_id, sort, time)

def get_comments(user, sort, time):
    return _get_comments(user._id, sort, time)

def _get_submitted(user_id, sort, time):
    return user_query(Link, user_id, sort, time)

def get_submitted(user, sort, time):
    return _get_submitted(user._id, sort, time)


def get_user_actions(user, sort, time):
    results = []
    unique_ids = set()

    # Order is important as a listing will only have the action_type of the
    # first occurrance (aka: posts trump comments which trump likes)
    actions_by_type = ((get_submitted(user, sort, time), 'submit'),
                       (get_comments(user, sort, time), 'comment'),
                       (get_liked(user), 'like'))

    for cached_result, action_type in actions_by_type:
        cached_result.fetch()
        for thing in cached_result.data:
            if thing[0] not in unique_ids:
                results.append(thing + (action_type,))
                unique_ids.add(thing[0])

    return sorted(results, key=lambda x: x[1], reverse=True)


def get_overview(user, sort, time):
    return merge_results(get_comments(user, sort, time),
                         get_submitted(user, sort, time))

def rel_query(rel, thing_id, name, filters = []):
    """General relationship query."""

    q = rel._query(rel.c._thing1_id == thing_id,
                   rel.c._t2_deleted == False,
                   rel.c._name == name,
                   sort = desc('_date'),
                   eager_load = True,
                   )
    if filters:
        q._filter(*filters)

    return q

cached_userrel_query = cached_query(UserQueryCache, filter_thing2)
cached_srrel_query = cached_query(SubredditQueryCache, filter_thing2)

@cached_query(UserQueryCache, filter_thing)
def get_liked(user):
    return FakeQuery(sort=[desc("date")])

@cached_query(UserQueryCache, filter_thing)
def get_disliked(user):
    return FakeQuery(sort=[desc("date")])

@cached_query(UserQueryCache)
def get_hidden_links(user_id):
    return FakeQuery(sort=[desc("action_date")])

def get_hidden(user):
    return get_hidden_links(user)

@cached_query(UserQueryCache)
def get_categorized_saved_links(user_id, sr_id, category):
    return FakeQuery(sort=[desc("action_date")])

@cached_query(UserQueryCache)
def get_categorized_saved_comments(user_id, sr_id, category):
    return FakeQuery(sort=[desc("action_date")])

@cached_query(UserQueryCache)
def get_saved_links(user_id, sr_id):
    return FakeQuery(sort=[desc("action_date")])

@cached_query(UserQueryCache)
def get_saved_comments(user_id, sr_id):
    return FakeQuery(sort=[desc("action_date")])

def get_saved(user, sr_id=None, category=None):
    sr_id = sr_id or 'none'
    if not category:
        queries = [get_saved_links(user, sr_id),
                   get_saved_comments(user, sr_id)]
    else:
        queries = [get_categorized_saved_links(user, sr_id, category),
                   get_categorized_saved_comments(user, sr_id, category)]
    return MergedCachedQuery(queries)

@cached_srrel_query
def get_subreddit_messages(sr):
    return rel_query(ModeratorInbox, sr, 'inbox')

@cached_srrel_query
def get_unread_subreddit_messages(sr):
    return rel_query(ModeratorInbox, sr, 'inbox',
                          filters = [ModeratorInbox.c.new == True])

def get_unread_subreddit_messages_multi(srs):
    if not srs:
        return []
    queries = [get_unread_subreddit_messages(sr) for sr in srs]
    return MergedCachedQuery(queries)

inbox_message_rel = Inbox.rel(Account, Message)
@cached_userrel_query
def get_inbox_messages(user):
    return rel_query(inbox_message_rel, user, 'inbox')

@cached_userrel_query
def get_unread_messages(user):
    return rel_query(inbox_message_rel, user, 'inbox',
                          filters = [inbox_message_rel.c.new == True])

inbox_comment_rel = Inbox.rel(Account, Comment)
@cached_userrel_query
def get_inbox_comments(user):
    return rel_query(inbox_comment_rel, user, 'inbox')

@cached_userrel_query
def get_unread_comments(user):
    return rel_query(inbox_comment_rel, user, 'inbox',
                          filters = [inbox_comment_rel.c.new == True])

@cached_userrel_query
def get_inbox_selfreply(user):
    return rel_query(inbox_comment_rel, user, 'selfreply')

@cached_userrel_query
def get_unread_selfreply(user):
    return rel_query(inbox_comment_rel, user, 'selfreply',
                          filters = [inbox_comment_rel.c.new == True])


@cached_userrel_query
def get_inbox_comment_mentions(user):
    return rel_query(inbox_comment_rel, user, "mention")


@cached_userrel_query
def get_unread_comment_mentions(user):
    return rel_query(inbox_comment_rel, user, "mention",
                     filters=[inbox_comment_rel.c.new == True])


def get_inbox(user):
    return merge_results(get_inbox_comments(user),
                         get_inbox_messages(user),
                         get_inbox_comment_mentions(user),
                         get_inbox_selfreply(user))

@cached_query(UserQueryCache)
def get_sent(user_id):
    return Message._query(Message.c.author_id == user_id,
                          Message.c._spam == (True, False),
                          sort = desc('_date'))

def get_unread_inbox(user):
    return merge_results(get_unread_comments(user),
                         get_unread_messages(user),
                         get_unread_comment_mentions(user),
                         get_unread_selfreply(user))

def _user_reported_query(user_id, thing_cls):
    rel_cls = Report.rel(Account, thing_cls)
    return rel_query(rel_cls, user_id, ('-1', '0', '1'))
    # -1: rejected report
    # 0: unactioned report
    # 1: accepted report

@cached_userrel_query
def get_user_reported_links(user_id):
    return _user_reported_query(user_id, Link)

@cached_userrel_query
def get_user_reported_comments(user_id):
    return _user_reported_query(user_id, Comment)

@cached_userrel_query
def get_user_reported_messages(user_id):
    return _user_reported_query(user_id, Message)

@merged_cached_query
def get_user_reported(user_id):
    return [get_user_reported_links(user_id),
            get_user_reported_comments(user_id),
            get_user_reported_messages(user_id)]


def set_promote_status(link, promote_status):
    all_queries = [promote_query(link.author_id) for promote_query in 
                   (get_unpaid_links, get_unapproved_links, 
                    get_rejected_links, get_live_links, get_accepted_links,
                    get_edited_live_links)]
    all_queries.extend([get_all_unpaid_links(), get_all_unapproved_links(),
                        get_all_rejected_links(), get_all_live_links(),
                        get_all_accepted_links(), get_all_edited_live_links()])

    if promote_status == PROMOTE_STATUS.unpaid:
        inserts = [get_unpaid_links(link.author_id), get_all_unpaid_links()]
    elif promote_status == PROMOTE_STATUS.unseen:
        inserts = [get_unapproved_links(link.author_id),
                   get_all_unapproved_links()]
    elif promote_status == PROMOTE_STATUS.rejected:
        inserts = [get_rejected_links(link.author_id), get_all_rejected_links()]
    elif promote_status == PROMOTE_STATUS.promoted:
        inserts = [get_live_links(link.author_id), get_all_live_links()]
    elif promote_status == PROMOTE_STATUS.edited_live:
        inserts = [
            get_edited_live_links(link.author_id),
            get_all_edited_live_links()
        ]
    elif promote_status in (PROMOTE_STATUS.accepted, PROMOTE_STATUS.pending,
                            PROMOTE_STATUS.finished):
        inserts = [get_accepted_links(link.author_id), get_all_accepted_links()]

    deletes = list(set(all_queries) - set(inserts))
    with CachedQueryMutator() as m:
        for q in inserts:
            m.insert(q, [link])
        for q in deletes:
            m.delete(q, [link])

    link.promote_status = promote_status
    link._commit()

    text = "set promote status to '%s'" % PROMOTE_STATUS.name[promote_status]
    PromotionLog.add(link, text)


def _promoted_link_query(user_id, status):
    STATUS_CODES = {'unpaid': PROMOTE_STATUS.unpaid,
                    'unapproved': PROMOTE_STATUS.unseen,
                    'rejected': PROMOTE_STATUS.rejected,
                    'live': PROMOTE_STATUS.promoted,
                    'accepted': (PROMOTE_STATUS.accepted,
                                 PROMOTE_STATUS.pending,
                                 PROMOTE_STATUS.finished),
                    'edited_live': PROMOTE_STATUS.edited_live}

    q = Link._query(Link.c.sr_id == Subreddit.get_promote_srid(),
                    Link.c._spam == (True, False),
                    Link.c._deleted == (True, False),
                    Link.c.promote_status == STATUS_CODES[status],
                    sort=db_sort('new'))
    if user_id:
        q._filter(Link.c.author_id == user_id)
    return q


@cached_query(UserQueryCache)
def get_unpaid_links(user_id):
    return _promoted_link_query(user_id, 'unpaid')


@cached_query(UserQueryCache)
def get_all_unpaid_links():
    return _promoted_link_query(None, 'unpaid')


@cached_query(UserQueryCache)
def get_unapproved_links(user_id):
    return _promoted_link_query(user_id, 'unapproved')


@cached_query(UserQueryCache)
def get_all_unapproved_links():
    return _promoted_link_query(None, 'unapproved')


@cached_query(UserQueryCache)
def get_rejected_links(user_id):
    return _promoted_link_query(user_id, 'rejected')


@cached_query(UserQueryCache)
def get_all_rejected_links():
    return _promoted_link_query(None, 'rejected')


@cached_query(UserQueryCache)
def get_live_links(user_id):
    return _promoted_link_query(user_id, 'live')


@cached_query(UserQueryCache)
def get_all_live_links():
    return _promoted_link_query(None, 'live')


@cached_query(UserQueryCache)
def get_accepted_links(user_id):
    return _promoted_link_query(user_id, 'accepted')


@cached_query(UserQueryCache)
def get_all_accepted_links():
    return _promoted_link_query(None, 'accepted')


@cached_query(UserQueryCache)
def get_edited_live_links(user_id):
    return _promoted_link_query(user_id, 'edited_live')


@cached_query(UserQueryCache)
def get_all_edited_live_links():
    return _promoted_link_query(None, 'edited_live')


@cached_query(UserQueryCache)
def get_payment_flagged_links():
    return FakeQuery(sort=[desc("_date")])


def set_payment_flagged_link(link):
    with CachedQueryMutator() as m:
        q = get_payment_flagged_links()
        m.insert(q, [link])


def unset_payment_flagged_link(link):
    with CachedQueryMutator() as m:
        q = get_payment_flagged_links()
        m.delete(q, [link])


@cached_query(UserQueryCache)
def get_underdelivered_campaigns():
    return FakeQuery(sort=[desc("_date")])


def set_underdelivered_campaigns(campaigns):
    campaigns = tup(campaigns)
    with CachedQueryMutator() as m:
        q = get_underdelivered_campaigns()
        m.insert(q, campaigns)


def unset_underdelivered_campaigns(campaigns):
    campaigns = tup(campaigns)
    with CachedQueryMutator() as m:
        q = get_underdelivered_campaigns()
        m.delete(q, campaigns)


@merged_cached_query
def get_promoted_links(user_id):
    queries = [get_unpaid_links(user_id), get_unapproved_links(user_id),
               get_rejected_links(user_id), get_live_links(user_id),
               get_accepted_links(user_id), get_edited_live_links(user_id)]
    return queries


@merged_cached_query
def get_all_promoted_links():
    queries = [get_all_unpaid_links(), get_all_unapproved_links(),
               get_all_rejected_links(), get_all_live_links(),
               get_all_accepted_links(), get_all_edited_live_links()]
    return queries


@cached_query(SubredditQueryCache, filter_fn=filter_thing)
def get_all_gilded_comments():
    return FakeQuery(sort=[desc("date")])


@cached_query(SubredditQueryCache, filter_fn=filter_thing)
def get_all_gilded_links():
    return FakeQuery(sort=[desc("date")])


@merged_cached_query
def get_all_gilded():
    return [get_all_gilded_comments(), get_all_gilded_links()]


@cached_query(SubredditQueryCache, filter_fn=filter_thing)
def get_gilded_comments(sr_id):
    return FakeQuery(sort=[desc("date")])


@cached_query(SubredditQueryCache, filter_fn=filter_thing)
def get_gilded_links(sr_id):
    return FakeQuery(sort=[desc("date")])


@merged_cached_query
def get_gilded(sr_ids):
    queries = [get_gilded_links, get_gilded_comments]
    return [query(sr_id)
            for sr_id, query in itertools.product(tup(sr_ids), queries)]


@cached_query(UserQueryCache, filter_fn=filter_thing)
def get_gilded_user_comments(user_id):
    return FakeQuery(sort=[desc("date")])


@cached_query(UserQueryCache, filter_fn=filter_thing)
def get_gilded_user_links(user_id):
    return FakeQuery(sort=[desc("date")])


@merged_cached_query
def get_gilded_users(user_ids):
    queries = [get_gilded_user_links, get_gilded_user_comments]
    return [query(user_id)
            for user_id, query in itertools.product(tup(user_ids), queries)]


@cached_query(UserQueryCache, filter_fn=filter_thing)
def get_user_gildings(user_id):
    return FakeQuery(sort=[desc("date")])


@merged_cached_query
def get_gilded_user(user):
    return [get_gilded_user_comments(user), get_gilded_user_links(user)]


def add_queries(queries, insert_items=None, delete_items=None):
    """Adds multiple queries to the query queue. If insert_items or
       delete_items is specified, the query may not need to be
       recomputed against the database."""

    for q in queries:
        if insert_items and q.can_insert():
            g.log.debug("Inserting %s into query %s" % (insert_items, q))
            with g.stats.get_timer('permacache.foreground.insert'):
                q.insert(insert_items)
        elif delete_items and q.can_delete():
            g.log.debug("Deleting %s from query %s" % (delete_items, q))
            with g.stats.get_timer('permacache.foreground.delete'):
                q.delete(delete_items)
        else:
            raise Exception("Cannot update query %r!" % (q,))

    # dual-write any queries that are being migrated to the new query cache
    with CachedQueryMutator() as m:
        new_queries = [getattr(q, 'new_query') for q in queries if hasattr(q, 'new_query')]

        if insert_items:
            for query in new_queries:
                m.insert(query, tup(insert_items))

        if delete_items:
            for query in new_queries:
                m.delete(query, tup(delete_items))

#can be rewritten to be more efficient
def all_queries(fn, obj, *param_lists):
    """Given a fn and a first argument 'obj', calls the fn(obj, *params)
    for every permutation of the parameters in param_lists"""
    results = []
    params = [[obj]]
    for pl in param_lists:
        new_params = []
        for p in pl:
            for c in params:
                new_param = list(c)
                new_param.append(p)
                new_params.append(new_param)
        params = new_params

    results = [fn(*p) for p in params]
    return results

## The following functions should be called after their respective
## actions to update the correct listings.
def new_link(link):
    "Called on the submission and deletion of links"
    sr = Subreddit._byID(link.sr_id)
    author = Account._byID(link.author_id)

    # just update "new" here, new_vote will handle hot/top/controversial
    results = [get_links(sr, 'new', 'all')]
    results.append(get_submitted(author, 'new', 'all'))

    for domain in utils.UrlParser(link.url).domain_permutations():
        results.append(get_domain_links(domain, 'new', "all"))

    with CachedQueryMutator() as m:
        if link._spam:
            m.insert(get_spam_links(sr), [link])
        if not (sr.exclude_banned_modqueue and author._spam):
            m.insert(get_unmoderated_links(sr), [link])

    add_queries(results, insert_items = link)
    amqp.add_item('new_link', link._fullname)


def add_to_commentstree_q(comment):
    if utils.to36(comment.link_id) in g.live_config["fastlane_links"]:
        amqp.add_item('commentstree_fastlane_q', comment._fullname)
    elif g.shard_commentstree_queues:
        amqp.add_item('commentstree_%d_q' % (comment.link_id % 10),
                      comment._fullname)
    else:
        amqp.add_item('commentstree_q', comment._fullname)


def update_comment_notifications(comment, inbox_rels):
    is_visible = not comment._deleted and not comment._spam

    with CachedQueryMutator() as mutator:
        for inbox_rel in tup(inbox_rels):
            inbox_owner = inbox_rel._thing1
            unread = (is_visible and
                getattr(inbox_rel, 'unread_preremoval', True))

            if inbox_rel._name == "inbox":
                query = get_inbox_comments(inbox_owner)
            elif inbox_rel._name == "selfreply":
                query = get_inbox_selfreply(inbox_owner)
            else:
                raise ValueError("wtf is " + inbox_rel._name)

            # mentions happen in butler_q

            if is_visible:
                mutator.insert(query, [inbox_rel])
            else:
                mutator.delete(query, [inbox_rel])

            set_unread(comment, inbox_owner, unread=unread, mutator=mutator)


def new_comment(comment, inbox_rels):
    author = Account._byID(comment.author_id)

    # just update "new" here, new_vote will handle hot/top/controversial
    job = [get_comments(author, 'new', 'all')]

    sr = Subreddit._byID(comment.sr_id)

    if comment._deleted:
        job_key = "delete_items"
        job.append(get_sr_comments(sr))
        job.append(get_all_comments())
    else:
        job_key = "insert_items"
        if comment._spam:
            with CachedQueryMutator() as m:
                m.insert(get_spam_comments(sr), [comment])
                if (was_spam_filtered(comment) and
                        not (sr.exclude_banned_modqueue and author._spam)):
                    m.insert(get_spam_filtered_comments(sr), [comment])

        amqp.add_item('new_comment', comment._fullname)
        add_to_commentstree_q(comment)

    job_dict = { job_key: comment }
    add_queries(job, **job_dict)

    # note that get_all_comments() is updated by the amqp process
    # r2.lib.db.queries.run_new_comments (to minimise lock contention)

    if inbox_rels:
        update_comment_notifications(comment, inbox_rels)


def new_subreddit(sr):
    "no precomputed queries here yet"
    amqp.add_item('new_subreddit', sr._fullname)


def new_message(message, inbox_rels, add_to_sent=True, update_modmail=True):
    from r2.lib.comment_tree import add_message

    from_user = Account._byID(message.author_id)

    # check if the from_user is exempt from ever adding to sent
    if not from_user.update_sent_messages:
        add_to_sent = False

    if message.display_author:
        add_to_sent = False

    modmail_rel_included = False
    update_recipient = False
    add_to_user = None

    with CachedQueryMutator() as m:
        if add_to_sent:
            m.insert(get_sent(from_user), [message])

        for inbox_rel in tup(inbox_rels):
            to = inbox_rel._thing1

            if isinstance(inbox_rel, ModeratorInbox):
                m.insert(get_subreddit_messages(to), [inbox_rel])
                modmail_rel_included = True
                set_sr_unread(message, to, unread=True, mutator=m)
            else:
                m.insert(get_inbox_messages(to), [inbox_rel])
                update_recipient = True
                # make sure we add this message to the user's inbox
                add_to_user = to
                set_unread(message, to, unread=True, mutator=m)

    update_modmail = update_modmail and modmail_rel_included

    amqp.add_item('new_message', message._fullname)
    add_message(message, update_recipient=update_recipient,
                update_modmail=update_modmail, add_to_user=add_to_user)
    
    # light up the modmail icon for all other mods with mail access
    if update_modmail:
        mod_perms = message.subreddit_slow.moderators_with_perms()
        mod_ids = [mod_id for mod_id, perms in mod_perms.iteritems()
            if mod_id != from_user._id and perms.get('mail', False)]
        moderators = Account._byID(mod_ids, data=True, return_dict=False)
        for mod in moderators:
            if not mod.modmsgtime:
                mod.modmsgtime = message._date
                mod._commit()


def set_unread(messages, user, unread, mutator=None):
    messages = tup(messages)

    inbox_rels = Inbox.get_rels(user, messages)
    Inbox.set_unread(inbox_rels, unread)

    update_unread_queries(inbox_rels, insert=unread, mutator=mutator)


def update_unread_queries(inbox_rels, insert=True, mutator=None):
    """Update all the cached queries related to the inbox relations"""
    if not mutator:
        m = CachedQueryMutator()
    else:
        m = mutator

    inbox_rels = tup(inbox_rels)
    for inbox_rel in inbox_rels:
        thing = inbox_rel._thing2
        user = inbox_rel._thing1

        if isinstance(thing, Comment):
            if inbox_rel._name == "inbox":
                query = get_unread_comments(user._id)
            elif inbox_rel._name == "selfreply":
                query = get_unread_selfreply(user._id)
            elif inbox_rel._name == "mention":
                query = get_unread_comment_mentions(user._id)
        elif isinstance(thing, Message):
            query = get_unread_messages(user._id)
        else:
            raise ValueError("can't handle %s" % thing.__class__.__name__)

        if insert:
            m.insert(query, [inbox_rel])
        else:
            m.delete(query, [inbox_rel])

    if not mutator:
        m.send()


def set_sr_unread(messages, sr, unread, mutator=None):
    messages = tup(messages)

    inbox_rels = ModeratorInbox.get_rels(sr, messages)
    ModeratorInbox.set_unread(inbox_rels, unread)

    update_unread_sr_queries(inbox_rels, insert=unread, mutator=mutator)


def update_unread_sr_queries(inbox_rels, insert=True, mutator=None):
    if not mutator:
        m = CachedQueryMutator()
    else:
        m = mutator

    inbox_rels = tup(inbox_rels)
    for inbox_rel in inbox_rels:
        sr = inbox_rel._thing1
        query = get_unread_subreddit_messages(sr)

        if insert:
            m.insert(query, [inbox_rel])
        else:
            m.delete(query, [inbox_rel])

    if not mutator:
        m.send()


def unread_handler(things, user, unread):
    """Given a user and Things of varying types, set their unread state."""
    sr_messages = collections.defaultdict(list)
    comments = []
    messages = []
    # Group things by subreddit or type
    for thing in things:
        if isinstance(thing, Message):
            if getattr(thing, 'sr_id', False):
                sr_messages[thing.sr_id].append(thing)
            else:
                messages.append(thing)
        else:
            comments.append(thing)

    if sr_messages:
        mod_srs = Subreddit.reverse_moderator_ids(user)
        srs = Subreddit._byID(sr_messages.keys())
    else:
        mod_srs = []

    with CachedQueryMutator() as m:
        for sr_id, things in sr_messages.items():
            # Remove the item(s) from the user's inbox
            set_unread(things, user, unread, mutator=m)

            if sr_id in mod_srs:
                # Only moderators can change the read status of that
                # message in the modmail inbox
                sr = srs[sr_id]
                set_sr_unread(things, sr, unread, mutator=m)

        if comments:
            set_unread(comments, user, unread, mutator=m)

        if messages:
            set_unread(messages, user, unread, mutator=m)


def unnotify(thing, possible_recipients=None):
    """Given a Thing, remove any notifications to its possible recipients.

    `possible_recipients` is a list of account IDs to unnotify. If not passed,
    deduce all possible recipients and remove their notifications.
    """
    from r2.lib import butler
    error_message = ("Unable to unnotify thing of type: %r" % thing)
    notification_handler(thing,
        notify_function=butler.remove_mention_notification,
        error_message=error_message,
        possible_recipients=possible_recipients,
    )


def renotify(thing, possible_recipients=None):
    """Given a Thing, reactivate notifications for possible recipients.

    `possible_recipients` is a list of account IDs to renotify. If not passed,
    deduce all possible recipients and add their notifications.
    This is used when unspamming comments.
    """
    from r2.lib import butler
    error_message = ("Unable to renotify thing of type: %r" % thing)
    notification_handler(thing,
        notify_function=butler.readd_mention_notification,
        error_message=error_message,
        possible_recipients=possible_recipients,
    )


def notification_handler(thing, notify_function,
        error_message, possible_recipients=None):
    if not possible_recipients:
        possible_recipients = Inbox.possible_recipients(thing)

    if not possible_recipients:
        return

    accounts = Account._byID(
        possible_recipients,
        return_dict=False,
        ignore_missing=True,
    )

    if isinstance(thing, Comment):
        rels = Inbox._fast_query(
            accounts,
            thing,
            ("inbox", "selfreply", "mention"),
        )

        # if the comment has been spammed, remember the previous
        # new value in case it becomes unspammed
        if thing._spam:
            for (tupl, rel) in rels.iteritems():
                if rel:
                    rel.unread_preremoval = rel.new
                    rel._commit()

        replies, mentions = utils.partition(
            lambda r: r._name == "mention",
            filter(None, rels.values()),
        )

        for mention in mentions:
            notify_function(mention)

        replies = list(replies)
        if replies:
            update_comment_notifications(thing, replies)
    else:
        raise ValueError(error_message)


def _by_srid(things, srs=True):
    """Takes a list of things and returns them in a dict separated by
       sr_id, in addition to the looked-up subreddits"""
    ret = {}

    for thing in tup(things):
        if getattr(thing, 'sr_id', None) is not None:
            ret.setdefault(thing.sr_id, []).append(thing)

    if srs:
        _srs = Subreddit._byID(ret.keys(), return_dict=True) if ret else {}
        return ret, _srs
    else:
        return ret


def _by_author(things, authors=True):
    ret = collections.defaultdict(list)

    for thing in tup(things):
        author_id = getattr(thing, 'author_id')
        if author_id:
            ret[author_id].append(thing)

    if authors:
        _authors = Account._byID(ret.keys(), return_dict=True) if ret else {}
        return ret, _authors
    else:
        return ret

def _by_thing1_id(rels):
    ret = {}
    for rel in tup(rels):
        ret.setdefault(rel._thing1_id, []).append(rel)
    return ret


def was_spam_filtered(thing):
    if (thing._spam and not thing._deleted and
        getattr(thing, 'verdict', None) != 'mod-removed'):
        return True
    else:
        return False


def delete(things):
    query_cache_inserts, query_cache_deletes = _common_del_ban(things)
    by_srid, srs = _by_srid(things)
    by_author, authors = _by_author(things)

    for sr_id, sr_things in by_srid.iteritems():
        sr = srs[sr_id]
        links = [x for x in sr_things if isinstance(x, Link)]
        comments = [x for x in sr_things if isinstance(x, Comment)]

        if links:
            query_cache_deletes.append((get_spam_links(sr), links))
            query_cache_deletes.append((get_spam_filtered_links(sr), links))
            query_cache_deletes.append((get_unmoderated_links(sr_id),
                                            links))
            query_cache_deletes.append((get_edited_links(sr_id), links))
        if comments:
            query_cache_deletes.append((get_spam_comments(sr), comments))
            query_cache_deletes.append((get_spam_filtered_comments(sr),
                                        comments))
            query_cache_deletes.append((get_edited_comments(sr), comments))

    for author_id, a_things in by_author.iteritems():
        author = authors[author_id]
        links = [x for x in a_things if isinstance(x, Link)]
        comments = [x for x in a_things if isinstance(x, Comment)]

        if links:
            results = [get_submitted(author, 'hot', 'all'),
                       get_submitted(author, 'new', 'all')]
            for sort in time_filtered_sorts:
                for time in db_times.keys():
                    results.append(get_submitted(author, sort, time))
            add_queries(results, delete_items=links)
            query_cache_inserts.append((get_deleted_links(author_id), links))
        if comments:
            results = [get_comments(author, 'hot', 'all'),
                       get_comments(author, 'new', 'all')]
            for sort in time_filtered_sorts:
                for time in db_times.keys():
                    results.append(get_comments(author, sort, time))
            add_queries(results, delete_items=comments)
            query_cache_inserts.append((get_deleted_comments(author_id),
                                        comments))

    with CachedQueryMutator() as m:
        for q, inserts in query_cache_inserts:
            m.insert(q, inserts)
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)

    for thing in tup(things):
        thing.update_search_index()


def edit(thing):
    if isinstance(thing, Link):
        query = get_edited_links
    elif isinstance(thing, Comment):
        query = get_edited_comments

    with CachedQueryMutator() as m:
        m.delete(query(thing.sr_id), [thing])
        m.insert(query(thing.sr_id), [thing])


def ban(things, filtered=True):
    query_cache_inserts, query_cache_deletes = _common_del_ban(things)
    by_srid = _by_srid(things, srs=False)

    for sr_id, sr_things in by_srid.iteritems():
        links = []
        modqueue_links = []
        comments = []
        modqueue_comments = []
        for item in sr_things:
            # don't add posts by banned users if subreddit prefs exclude them
            add_to_modqueue = (filtered and
                       not (item.subreddit_slow.exclude_banned_modqueue and
                            item.author_slow._spam))

            if isinstance(item, Link):
                links.append(item)
                if add_to_modqueue:
                    modqueue_links.append(item)
            elif isinstance(item, Comment):
                comments.append(item)
                if add_to_modqueue:
                    modqueue_comments.append(item)

        if links:
            query_cache_inserts.append((get_spam_links(sr_id), links))
            if not filtered:
                query_cache_deletes.append(
                        (get_spam_filtered_links(sr_id), links))
                query_cache_deletes.append(
                        (get_unmoderated_links(sr_id), links))

        if modqueue_links:
            query_cache_inserts.append(
                    (get_spam_filtered_links(sr_id), modqueue_links))

        if comments:
            query_cache_inserts.append((get_spam_comments(sr_id), comments))
            if not filtered:
                query_cache_deletes.append(
                        (get_spam_filtered_comments(sr_id), comments))

        if modqueue_comments:
            query_cache_inserts.append(
                    (get_spam_filtered_comments(sr_id), modqueue_comments))

    with CachedQueryMutator() as m:
        for q, inserts in query_cache_inserts:
            m.insert(q, inserts)
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)

    for thing in tup(things):
        thing.update_search_index()


def _common_del_ban(things):
    query_cache_inserts = []
    query_cache_deletes = []
    by_srid, srs = _by_srid(things)

    for sr_id, sr_things in by_srid.iteritems():
        sr = srs[sr_id]
        links = [x for x in sr_things if isinstance(x, Link)]
        comments = [x for x in sr_things if isinstance(x, Comment)]

        if links:
            results = [get_links(sr, 'hot', 'all'), get_links(sr, 'new', 'all')]
            for sort in time_filtered_sorts:
                for time in db_times.keys():
                    results.append(get_links(sr, sort, time))
            add_queries(results, delete_items=links)
            query_cache_deletes.append([get_reported_links(sr), links])
            query_cache_deletes.append([get_reported_links(None), links])
        if comments:
            query_cache_deletes.append([get_reported_comments(sr), comments])
            query_cache_deletes.append([get_reported_comments(None), comments])

    return query_cache_inserts, query_cache_deletes


def unban(things, insert=True):
    query_cache_deletes = []

    by_srid, srs = _by_srid(things)
    if not by_srid:
        return

    for sr_id, things in by_srid.iteritems():
        sr = srs[sr_id]
        links = [x for x in things if isinstance(x, Link)]
        comments = [x for x in things if isinstance(x, Comment)]

        if insert and links:
            # put it back in the listings
            results = [get_links(sr, 'hot', 'all'),
                       get_links(sr, 'top', 'all'),
                       get_links(sr, 'controversial', 'all'),
                       ]
            # the time-filtered listings will have to wait for the
            # next mr_top run
            add_queries(results, insert_items=links)

            # Check if link is being unbanned and should be put in
            # 'new' with current time
            new_links = []
            for l in links:
                ban_info = l.ban_info
                if ban_info.get('reset_used', True) == False and \
                    ban_info.get('auto', False):
                    l_copy = deepcopy(l)
                    l_copy._date = ban_info['unbanned_at']
                    new_links.append(l_copy)
                else:
                    new_links.append(l)
            add_queries([get_links(sr, 'new', 'all')], insert_items=new_links)
            query_cache_deletes.append([get_spam_links(sr), links])

        if insert and comments:
            add_queries([get_all_comments(), get_sr_comments(sr)],
                        insert_items=comments)
            query_cache_deletes.append([get_spam_comments(sr), comments])

        if links:
            query_cache_deletes.append((get_unmoderated_links(sr), links))
            query_cache_deletes.append([get_spam_filtered_links(sr), links])

        if comments:
            query_cache_deletes.append([get_spam_filtered_comments(sr), comments])

    with CachedQueryMutator() as m:
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)

    for thing in tup(things):
        thing.update_search_index()

def new_report(thing, report_rel):
    reporter_id = report_rel._thing1_id

    # determine if the report is for spam so we can update the global
    # report queue as well as the per-subreddit one
    reason = getattr(report_rel, "reason", None)
    is_spam_report = reason and "spam" in reason.lower()

    with CachedQueryMutator() as m:
        if isinstance(thing, Link):
            m.insert(get_reported_links(thing.sr_id), [thing])
            if is_spam_report:
                m.insert(get_reported_links(None), [thing])
            m.insert(get_user_reported_links(reporter_id), [report_rel])
        elif isinstance(thing, Comment):
            m.insert(get_reported_comments(thing.sr_id), [thing])
            if is_spam_report:
                m.insert(get_reported_comments(None), [thing])
            m.insert(get_user_reported_comments(reporter_id), [report_rel])
        elif isinstance(thing, Message):
            m.insert(get_user_reported_messages(reporter_id), [report_rel])

    amqp.add_item("new_report", thing._fullname)


def clear_reports(things, rels):
    query_cache_deletes = []

    by_srid = _by_srid(things, srs=False)

    for sr_id, sr_things in by_srid.iteritems():
        links = [ x for x in sr_things if isinstance(x, Link) ]
        comments = [ x for x in sr_things if isinstance(x, Comment) ]

        if links:
            query_cache_deletes.append([get_reported_links(sr_id), links])
            query_cache_deletes.append([get_reported_links(None), links])
        if comments:
            query_cache_deletes.append([get_reported_comments(sr_id), comments])
            query_cache_deletes.append([get_reported_comments(None), comments])

    # delete from user_reported if the report was correct
    rels = [r for r in rels if r._name == '1']
    if rels:
        link_rels = [r for r in rels if r._type2 == Link]
        comment_rels = [r for r in rels if r._type2 == Comment]
        message_rels = [r for r in rels if r._type2 == Message]

        rels_to_query = ((link_rels, get_user_reported_links),
                         (comment_rels, get_user_reported_comments),
                         (message_rels, get_user_reported_messages))

        for thing_rels, query in rels_to_query:
            if not thing_rels:
                continue

            by_thing1_id = _by_thing1_id(thing_rels)
            for reporter_id, reporter_rels in by_thing1_id.iteritems():
                query_cache_deletes.append([query(reporter_id), reporter_rels])

    with CachedQueryMutator() as m:
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)


def add_all_srs():
    """Recalculates every listing query for every subreddit. Very,
       very slow."""
    q = Subreddit._query(sort = asc('_date'))
    for sr in fetch_things2(q):
        for q in all_queries(get_links, sr, ('hot', 'new'), ['all']):
            q.update()
        for q in all_queries(get_links, sr, time_filtered_sorts, db_times.keys()):
            q.update()
        get_spam_links(sr).update()
        get_spam_comments(sr).update()
        get_reported_links(sr).update()
        get_reported_comments(sr).update()

def update_user(user):
    if isinstance(user, str):
        user = Account._by_name(user)
    elif isinstance(user, int):
        user = Account._byID(user)

    results = [get_inbox_messages(user),
               get_inbox_comments(user),
               get_inbox_selfreply(user),
               get_sent(user),
               get_liked(user),
               get_disliked(user),
               get_submitted(user, 'new', 'all'),
               get_comments(user, 'new', 'all')]
    for q in results:
        q.update()

def add_all_users():
    q = Account._query(sort = asc('_date'))
    for user in fetch_things2(q):
        update_user(user)

# amqp queue processing functions

def run_new_comments(limit=1000):
    """Add new incoming comments to the /comments page"""
    # this is done as a queue because otherwise the contention for the
    # lock on the query would be very high

    @g.stats.amqp_processor('newcomments_q')
    def _run_new_comments(msgs, chan):
        fnames = [msg.body for msg in msgs]

        comments = Comment._by_fullname(fnames, data=True, return_dict=False)
        add_queries([get_all_comments()],
                    insert_items=comments)

        bysrid = _by_srid(comments, False)
        for srid, sr_comments in bysrid.iteritems():
            add_queries([_get_sr_comments(srid)],
                        insert_items=sr_comments)

    amqp.handle_items('newcomments_q', _run_new_comments, limit=limit)

def run_commentstree(qname="commentstree_q", limit=400):
    """Add new incoming comments to their respective comments trees"""

    @g.stats.amqp_processor(qname)
    def _run_commentstree(msgs, chan):
        comments = Comment._by_fullname([msg.body for msg in msgs],
                                        data = True, return_dict = False)
        print 'Processing %r' % (comments,)

        if comments:
            add_comments(comments)

    # High velocity threads put additional pressure on Cassandra.
    if qname == "commentstree_fastlane_q":
        limit = max(1000, limit)
    amqp.handle_items(qname, _run_commentstree, limit=limit)


def _by_type(items):
    by_type = collections.defaultdict(list)
    for item in items:
        by_type[item.__class__].append(item)
    return by_type


def get_stored_votes(user, things):
    if not user or not things:
        return {}

    results = {}
    things_by_type = _by_type(things)

    for thing_class, items in things_by_type.iteritems():
        if not thing_class.is_votable:
            continue

        rel_class = VotesByAccount.rel(thing_class)
        votes = rel_class.fast_query(user, items)
        for cross, direction in votes.iteritems():
            results[cross] = Vote.deserialize_direction(int(direction))

    return results


def get_likes(user, requested_items):
    if not user or not requested_items:
        return {}

    res = {}

    try:
        last_modified = LastModified._byID(user._fullname)
    except tdb_cassandra.NotFound:
        last_modified = None

    items_in_grace_period = {}
    items_by_type = _by_type(requested_items)
    for type_, items in items_by_type.iteritems():
        if not type_.is_votable:
            # these items can't be voted on. just mark 'em as None and skip.
            for item in items:
                res[(user, item)] = None
            continue

        rel_cls = VotesByAccount.rel(type_)
        last_vote = getattr(last_modified, rel_cls._last_modified_name, None)
        if last_vote:
            time_since_last_vote = datetime.now(pytz.UTC) - last_vote

        # only do prequeued_vote lookups if we've voted within the grace period
        # and therefore might have votes in flight in the queues.
        if last_vote and time_since_last_vote < g.vote_queue_grace_period:
            too_new = 0

            for item in items:
                if item._age > time_since_last_vote:
                    key = prequeued_vote_key(user, item)
                    items_in_grace_period[key] = (user, item)
                else:
                    # the item is newer than our last vote, we can't have
                    # possibly voted on it.
                    res[(user, item)] = None
                    too_new += 1

            if too_new:
                g.stats.simple_event("vote.prequeued.too-new", delta=too_new)
        else:
            g.stats.simple_event("vote.prequeued.graceless", delta=len(items))

    # look up votes in memcache for items that could have been voted on
    # but not processed by a queue processor yet.
    if items_in_grace_period:
        g.stats.simple_event(
            "vote.prequeued.fetch", delta=len(items_in_grace_period))
        r = g.gencache.get_multi(items_in_grace_period.keys())
        for key, v in r.iteritems():
            res[items_in_grace_period[key]] = Vote.deserialize_direction(v)

    cassavotes = get_stored_votes(
        user, [i for i in requested_items if (user, i) not in res])
    res.update(cassavotes)

    return res


def consume_mark_all_read():
    @g.stats.amqp_processor('markread_q')
    def process_mark_all_read(msg):
        user = Account._by_fullname(msg.body)
        inbox_fullnames = get_unread_inbox(user)
        for inbox_chunk in in_chunks(inbox_fullnames, size=100):
            things = Thing._by_fullname(inbox_chunk, return_dict=False)
            unread_handler(things, user, unread=False)

    amqp.consume_items('markread_q', process_mark_all_read)


def consume_deleted_accounts():
    @g.stats.amqp_processor('del_account_q')
    def process_deleted_accounts(msg):
        account = Thing._by_fullname(msg.body)
        assert isinstance(account, Account)

        if account.has_stripe_subscription:
            from r2.controllers.ipn import cancel_stripe_subscription
            cancel_stripe_subscription(account.gold_subscr_id)

        # Mark their link submissions for updating on cloudsearch
        query = LinksByAccount._cf.xget(account._id36)
        for link_id36, unused in query:
            fullname = Link._fullname_from_id36(link_id36)
            msg = pickle.dumps({"fullname": fullname})
            amqp.add_item("search_changes", msg, message_id=fullname,
                delivery_mode=amqp.DELIVERY_TRANSIENT)

    amqp.consume_items('del_account_q', process_deleted_accounts)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class BooleanOp(object):
    def __init__(self, *ops):
        self.ops = ops

    def __repr__(self):
        return '<%s_ %s>' % (self.__class__.__name__, str(self.ops))

class or_(BooleanOp): pass
class and_(BooleanOp): pass
class not_(BooleanOp): pass

class op(object):
    def __init__(self, lval, lval_name, rval):
        self.lval = lval
        self.rval = rval
        self.lval_name = lval_name

    def __repr__(self):
        return '<%s: %s, %s>' % (self.__class__.__name__, self.lval, self.rval)

    # sorts in a consistent order, required for Query._cache_key()
    def __cmp__(self, other):
        return cmp(repr(self), repr(other))

class eq(op): pass
class ne(op): pass
class lt(op): pass
class lte(op): pass
class gt(op): pass
class gte(op): pass
class in_(op): pass

class Slot(object):
    def __init__(self, lval):
        if isinstance(lval, Slot):
            self.name = lval.name
            self.lval = lval
        else:
            self.name = lval

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self.name)

    def __eq__(self, other):
        return eq(self, self.name, other)

    def __ne__(self, other):
        return ne(self, self.name, other)

    def __lt__(self, other):
        return lt(self, self.name, other)

    def __le__(self, other):
        return lte(self, self.name, other)

    def __gt__(self, other):
        return gt(self, self.name, other)

    def __ge__(self, other):
        return gte(self, self.name, other)

    def in_(self, other):
        return in_(self, self.name, other)

class Slots(object):
    def __getattr__(self, attr):
        return Slot(attr)

    def __getitem__(self, attr):
        return Slot(attr)
        
def op_iter(ops):
    for o in ops:
        if isinstance(o, op):
            yield o
        elif isinstance(o, BooleanOp):
            for p in op_iter(o.ops):
                yield p

class query_func(Slot): pass
class lower(query_func): pass
class ip_network(query_func): pass
class base_url(query_func): pass
class domain(query_func): pass
class year_func(query_func): pass

class timeago(object):
    def __init__(self, interval):
        self.interval = interval

    def __repr__(self):
        return '<interval: %s>' % self.interval

class sort(object):
    def __init__(self, col):
        self.col = col

    def __repr__(self):
        return '<sort:%s %s>' % (self.__class__.__name__, str(self.col))

    def __eq__(self, other):
        return self.__class__ == other.__class__ and self.col == other.col

    def __ne__(self, other):
        return not self.__eq__(other)


class asc(sort): pass
class desc(sort):pass
class shuffled(desc): pass
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import copy, deepcopy
import cPickle as pickle
from datetime import datetime, timedelta
import hashlib
import itertools
import new
import sys

from _pylibmc import MemcachedError
from pylons import app_globals as g

from r2.lib import amqp, hooks
from r2.lib.db import tdb_sql as tdb, sorts, operators
from r2.lib.sgm import sgm
from r2.lib.utils import class_property, Results, tup, to36


class NotFound(Exception):
    pass


CreationError = tdb.CreationError

thing_types = {}
rel_types = {}


class SafeSetAttr:
    def __init__(self, cls):
        self.cls = cls

    def __enter__(self):
        self.cls.__safe__ = True

    def __exit__(self, type, value, tb):
        self.cls.__safe__ = False


class TdbTransactionContext(object):
    def __enter__(self):
        tdb.transactions.begin()

    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type:
            tdb.transactions.rollback()
            raise
        else:
            tdb.transactions.commit()


class DataThing(object):
    _base_props = ()
    _int_props = ()
    _data_int_props = ()
    _int_prop_suffix = None
    _defaults = {}
    _essentials = ()
    c = operators.Slots()
    __safe__ = False
    _cache = None
    _cache_ttl = int(timedelta(hours=12).total_seconds())

    def __init__(self):
        safe_set_attr = SafeSetAttr(self)
        with safe_set_attr:
            self.safe_set_attr = safe_set_attr
            self._dirties = {}
            self._t = {}
            self._created = False

    #TODO some protection here?
    def __setattr__(self, attr, val, make_dirty=True):
        if attr.startswith('__') or self.__safe__:
            object.__setattr__(self, attr, val)
            return 

        if attr.startswith('_'):
            #assume baseprops has the attr
            if make_dirty and hasattr(self, attr):
                old_val = getattr(self, attr)
            object.__setattr__(self, attr, val)
            if not attr in self._base_props:
                return
        else:
            old_val = self._t.get(attr, self._defaults.get(attr))
            self._t[attr] = val
        if make_dirty and val != old_val:
            self._dirties[attr] = (old_val, val)

    def __setstate__(self, state):
        # pylibmc's automatic unpicking will call __setstate__ if it exists.
        # if we don't implement __setstate__ the check for existence will fail
        # in an atypical (and not properly handled) way because we override
        # __getattr__. the implementation provided here is identical to what
        # would happen in the default unimplemented case.
        self.__dict__ = state

    def __getattr__(self, attr):
        try:
            return self._t[attr]
        except KeyError:
            try:
                return self._defaults[attr]
            except KeyError:
                # attr didn't exist--continue on to error recovery below
                pass

        try:
            _id = object.__getattribute__(self, "_id")
        except AttributeError:
            _id = "???"

        try:
            class_name = object.__getattribute__(self, "__class__").__name__
        except AttributeError:
            class_name = "???"

        try:
            id_str = "%d" % _id
        except TypeError:
            id_str = "%r" % _id

        error_msg = "{cls}({id}).{attr} not found".format(
            cls=class_name,
            id=_id,
            attr=attr,
        )

        raise AttributeError, error_msg

    @classmethod
    def _cache_prefix(cls):
        return cls.__name__ + '_'

    def _cache_key(self):
        prefix = self._cache_prefix()
        return "{prefix}{id}".format(prefix=prefix, id=self._id)

    @classmethod
    def get_things_from_db(cls, ids):
        """Read props from db and return id->thing dict."""
        raise NotImplementedError

    @classmethod
    def get_things_from_cache(cls, ids, stale=False, allow_local=True):
        """Read things from cache and return id->thing dict."""
        cache = cls._cache
        prefix = cls._cache_prefix()
        things_by_id = cache.get_multi(
            ids, prefix=prefix, stale=stale, allow_local=allow_local,
            stat_subname=cls.__name__)
        return things_by_id

    @classmethod
    def write_things_to_cache(cls, things_by_id):
        """Write id->thing dict to cache.

        Used to populate the cache after a cache miss/db read. To ensure we
        don't clobber a write by another process (we don't have a lock) we use
        add_multi to only set the values that don't exist.

        """

        cache = cls._cache
        prefix = cls._cache_prefix()
        try:
            cache.add_multi(things_by_id, prefix=prefix, time=cls._cache_ttl)
        except MemcachedError as e:
            g.log.warning("write_things_to_cache error: %s", e)

    def get_read_modify_write_lock(self):
        """Return the lock to be used when doing a read-modify-write.

        When modifying a Thing we must read its current version from cache and
        update that to avoid clobbering modifications made by other processes
        after we first read the Thing.

        """

        return g.make_lock("thing_commit", 'commit_' + self._fullname)

    def write_new_thing_to_db(self):
        """Write the new thing to db and return its id."""
        raise NotImplementedError

    def write_props_to_db(self, props, data_props, brand_new_thing):
        """Write the props to db."""
        raise NotImplementedError

    def write_changes_to_db(self, changes, brand_new_thing=False):
        """Write changes to db."""
        if not changes:
            return

        data_props = {}
        props = {}
        for prop, (old_value, new_value) in changes.iteritems():
            if prop.startswith('_'):
                props[prop[1:]] = new_value
            else:
                data_props[prop] = new_value

        self.write_props_to_db(props, data_props, brand_new_thing)

    def write_thing_to_cache(self, lock, brand_new_thing=False):
        """After modifying a thing write the entire object to cache.

        The caller must either pass in the read_modify_write lock or be acting
        for a newly created thing (that has therefore never been cached before).

        """

        assert brand_new_thing or lock.have_lock

        cache = self.__class__._cache
        key = self._cache_key()
        cache.set(key, self, time=self.__class__._cache_ttl)

    def update_from_cache(self, lock):
        """Read the current value of thing from cache and update self.

        To be used before writing cache to avoid clobbering changes made by a
        different process. Must be called under write lock.

        """

        assert lock.have_lock

        # disallow reading from local cache because we want to pull in changes
        # made by other processes since we first read this thing.
        other_selfs = self.__class__.get_things_from_cache(
            [self._id], allow_local=False)
        if not other_selfs:
            return
        other_self = other_selfs[self._id]

        # update base_props
        for base_prop in self._base_props:
            other_self_val = getattr(other_self, base_prop)
            self.__setattr__(base_prop, other_self_val, make_dirty=False)

        # update data_props
        self._t = other_self._t

        # reapply changes made to self
        self_changes = self._dirties
        self._dirties = {}
        for data_prop, (old_val, new_val) in self_changes.iteritems():
            setattr(self, data_prop, new_val)

    @classmethod
    def record_cache_write(cls, event, delta=1):
        raise NotImplementedError

    def _commit(self):
        """Write changes to db and write the full object to cache.

        When writing to postgres we write only the changes. The data in postgres
        is the canonical version.

        For a few reasons (speed, decreased load on postgres, postgres
        replication lag) we want to keep a perfectly consistent copy of the
        thing in cache.

        To achieve this we read the current value of the thing from cache to
        pull in any changes made by other processes, apply our changes to the
        thing, and finally set it in cache. This is done under lock to ensure
        read/write safety.

        If the cached thing is evicted or expires we must read from postgres.

        Failure cases:
        * Write to cache fails. The cache now contains stale/incorrect data. To
          ensure we recover quickly TTLs should be set as low as possible
          without overloading postgres.
        * There is long replication lag and high cache pressure. When an object
          is modified it is written to cache, but quickly evicted, The next
          lookup might read from a postgres secondary before the changes have
          been replicated there. To protect against this replication lag and
          cache pressure should be monitored and kept at acceptable levels.
        * Near simultaneous writes that create a logical inconsistency. Say
          request 1 and request 2 both read state 0 of a Thing. Request 1
          changes Thing.prop from True to False and writes to cache and
          postgres. Request 2 examines the value of Thing.prop, sees that it is
          True, and due to logic in the app sets Thing.prop_is_true to True and
          writes to cache and postgres. Request 2 didn't clobber the change
          made by request 1, but it made a logically incorrect change--the
          resulting state is Thing.prop = False and Thing.prop_is_true = True.
          Logic like this should be identified and avoided wherever possible, or
          protected against using locks.

        """

        if not self._created:
            with TdbTransactionContext():
                _id = self.write_new_thing_to_db()
                self._id = _id
                self._created = True

                changes = self._dirties.copy()
                self.write_changes_to_db(changes, brand_new_thing=True)
                self._dirties.clear()

            self.write_thing_to_cache(lock=None, brand_new_thing=True)
            self.record_cache_write(event="create")
        else:
            with self.get_read_modify_write_lock() as lock:
                self.update_from_cache(lock)
                if not self._dirty:
                    return

                with TdbTransactionContext():
                    changes = self._dirties.copy()
                    self.write_changes_to_db(changes, brand_new_thing=False)
                    self._dirties.clear()

                self.write_thing_to_cache(lock)
                self.record_cache_write(event="modify")

        hooks.get_hook("thing.commit").call(thing=self, changes=changes)

    def _incr(self, prop, amt=1):
        raise NotImplementedError

    @property
    def _id36(self):
        return to36(self._id)

    @class_property
    def _fullname_prefix(cls):
        return cls._type_prefix + to36(cls._type_id)

    @classmethod
    def _fullname_from_id36(cls, id36):
        return cls._fullname_prefix + '_' + id36

    @property
    def _fullname(self):
        return self._fullname_from_id36(self._id36)

    @classmethod
    def _byID(cls, ids, data=True, return_dict=True, stale=False,
              ignore_missing=False):
        # data props are ALWAYS loaded, data keyword is meaningless
        ids, single = tup(ids, ret_is_single=True)

        for x in ids:
            if not isinstance(x, (int, long)):
                raise ValueError('non-integer thing_id in %r' % ids)
            if x > tdb.MAX_THING_ID:
                raise NotFound('huge thing_id in %r' % ids)
            elif x < tdb.MIN_THING_ID:
                raise NotFound('negative thing_id in %r' % ids)

        if not single and not ids:
            if return_dict:
                return {}
            else:
                return []

        things_by_id = cls.get_things_from_cache(ids, stale=stale)
        missing_ids = [_id
            for _id in ids
            if _id not in things_by_id
        ]

        if missing_ids:
            from_db_by_id = cls.get_things_from_db(missing_ids)
        else:
            from_db_by_id = {}

        if from_db_by_id:
            cls.write_things_to_cache(from_db_by_id)
            cls.record_cache_write(event="cache", delta=len(from_db_by_id))

        things_by_id.update(from_db_by_id)

        # Check to see if we found everything we asked for
        missing = [_id for _id in ids if _id not in things_by_id]
        if missing and not ignore_missing:
            raise NotFound, '%s %s' % (cls.__name__, missing)

        if missing:
            ids = [_id for _id in ids if _id not in missing]

        if single:
            return things_by_id[ids[0]] if ids else None
        elif return_dict:
            return things_by_id
        else:
            return filter(None, (things_by_id.get(_id) for _id in ids))

    @classmethod
    def _byID36(cls, id36s, return_dict = True, **kw):

        id36s, single = tup(id36s, True)

        # will fail if it's not a string
        ids = [ int(x, 36) for x in id36s ]

        things = cls._byID(ids, return_dict=True, **kw)
        things = {thing._id36: thing for thing in things.itervalues()}

        if single:
            return things.values()[0]
        elif return_dict:
            return things
        else:
            return filter(None, (things.get(i) for i in id36s))

    @classmethod
    def _by_fullname(cls, names,
                     return_dict = True, 
                     ignore_missing=False,
                     **kw):
        names, single = tup(names, True)

        table = {}
        lookup = {}
        # build id list by type
        for fullname in names:
            try:
                real_type, thing_id = fullname.split('_')
                #distinguish between things and realtions
                if real_type[0] == 't':
                    type_dict = thing_types
                elif real_type[0] == 'r':
                    type_dict = rel_types
                else:
                    raise NotFound
                real_type = type_dict[int(real_type[1:], 36)]
                thing_id = int(thing_id, 36)
                lookup[fullname] = (real_type, thing_id)
                table.setdefault(real_type, []).append(thing_id)
            except (KeyError, ValueError):
                if single:
                    raise NotFound

        # lookup ids for each type
        identified = {}
        for real_type, thing_ids in table.iteritems():
            i = real_type._byID(thing_ids, ignore_missing=ignore_missing, **kw)
            identified[real_type] = i

        # interleave types in original order of the name
        res = []
        for fullname in names:
            if lookup.has_key(fullname):
                real_type, thing_id = lookup[fullname]
                thing = identified.get(real_type, {}).get(thing_id)
                if not thing and ignore_missing:
                    continue
                res.append((fullname, thing))

        if single:
            return res[0][1] if res else None
        elif return_dict:
            return dict(res)
        else:
            return [x for i, x in res]

    @property
    def _dirty(self):
        return bool(len(self._dirties))

    @classmethod
    def _query(cls, *a, **kw):
        raise NotImplementedError()


class ThingMeta(type):
    def __init__(cls, name, bases, dct):
        if name == 'Thing' or hasattr(cls, '_nodb') and cls._nodb: return
        if g.env == 'unit_test':
            return

        #TODO exceptions
        cls._type_name = name.lower()
        try:
            cls._type_id = tdb.types_name[cls._type_name].type_id
        except KeyError:
            raise KeyError, 'is the thing database %s defined?' % name

        global thing_types
        thing_types[cls._type_id] = cls

        super(ThingMeta, cls).__init__(name, bases, dct)
    
    def __repr__(cls):
        return '<thing: %s>' % cls._type_name

class Thing(DataThing):
    __metaclass__ = ThingMeta
    _base_props = ('_ups', '_downs', '_date', '_deleted', '_spam')
    _int_props = ('_ups', '_downs')
    _type_prefix = 't'

    is_votable = False

    def __init__(self, ups = 0, downs = 0, date = None, deleted = False,
                 spam = False, id = None, **attrs):
        DataThing.__init__(self)

        with self.safe_set_attr:
            if id:
                self._id = id
                self._created = True

            if not date:
                date = datetime.now(g.tz)
            else:
                date = date.astimezone(g.tz)

            self._ups = ups
            self._downs = downs
            self._date = date
            self._deleted = deleted
            self._spam = spam

        #new way
        for k, v in attrs.iteritems():
            self.__setattr__(k, v, not self._created)

    @classmethod
    def record_cache_write(cls, event, delta=1):
        name = cls.__name__.lower()
        event_name = "thing.{event}.{name}".format(event=event, name=name)
        g.stats.simple_event(event_name, delta)

    def __repr__(self):
        return '<%s %s>' % (self.__class__.__name__,
                            self._id if self._created else '[unsaved]')

    @classmethod
    def get_things_from_db(cls, ids):
        """Read props from db and return id->thing dict."""
        props_by_id = tdb.get_thing(cls._type_id, ids)
        data_props_by_id = tdb.get_thing_data(cls._type_id, ids)

        things_by_id = {}
        for _id, props in props_by_id.iteritems():
            data_props = data_props_by_id.get(_id, {})
            thing = cls(
                ups=props.ups,
                downs=props.downs,
                date=props.date,
                deleted=props.deleted,
                spam=props.spam,
                id=_id,
            )
            thing._t.update(data_props)

            if not all(data_prop in thing._t for data_prop in cls._essentials):
                # a Thing missing an essential prop is invalid
                # this can happen if a process looks up the Thing as it's
                # created but between when the props and the data props are
                # written
                g.log.error("%s missing essentials, got %s", thing, thing._t)
                g.stats.simple_event("thing.load.missing_essentials")
                continue

            things_by_id[_id] = thing

        return things_by_id

    def write_new_thing_to_db(self):
        """Write the new thing to db and return its id."""
        assert not self._created

        _id = tdb.make_thing(
            type_id=self.__class__._type_id,
            ups=self._ups,
            downs=self._downs,
            date=self._date,
            deleted=self._deleted,
            spam=self._spam,
        )
        return _id

    def write_props_to_db(self, props, data_props, brand_new_thing):
        """Write the props to db."""
        if data_props:
            tdb.set_thing_data(
                type_id=self.__class__._type_id,
                thing_id=self._id,
                brand_new_thing=brand_new_thing,
                **data_props
            )

        if props and not brand_new_thing:
            # if the thing is brand new its props have just been written by
            # write_new_thing_to_db
            tdb.set_thing_props(
                type_id=self.__class__._type_id,
                thing_id=self._id,
                **props
            )

    def _incr(self, prop, amt=1):
        """Increment self.prop."""
        assert not self._dirty

        is_base_prop = prop in self._int_props
        is_data_prop = (prop in self._data_int_props or
            self._int_prop_suffix and prop.endswith(self._int_prop_suffix))
        db_prop = prop[1:] if is_base_prop else prop

        assert is_base_prop or is_data_prop

        with self.get_read_modify_write_lock() as lock:
            self.update_from_cache(lock)
            old_val = getattr(self, prop)
            new_val = old_val + amt

            self.__setattr__(prop, new_val, make_dirty=False)

            with TdbTransactionContext():
                if is_base_prop:
                    # can just incr a base prop because it must have been set
                    # when the object was created
                    tdb.incr_thing_prop(
                        type_id=self.__class__._type_id,
                        thing_id=self._id,
                        prop=db_prop,
                        amount=amt,
                    )
                elif (prop in self.__class__._defaults and
                        self.__class__._defaults[prop] == old_val):
                    # when updating a data prop from the default value assume
                    # the value was never actually set so it's not safe to incr
                    tdb.set_thing_data(
                        type_id=self.__class__._type_id,
                        thing_id=self._id,
                        brand_new_thing=False,
                        **{db_prop: new_val}
                    )
                else:
                    tdb.incr_thing_data(
                        type_id=self.__class__._type_id,
                        thing_id=self._id,
                        prop=db_prop,
                        amount=amt,
                    )

                # write to cache within the transaction context so an exception
                # will cause a transaction rollback
                self.write_thing_to_cache(lock)
        self.record_cache_write(event="incr")

    @property
    def _age(self):
        return datetime.now(g.tz) - self._date

    @property
    def _hot(self):
        return sorts.hot(self._ups, self._downs, self._date)

    @property
    def _score(self):
        return sorts.score(self._ups, self._downs)

    @property
    def _controversy(self):
        return sorts.controversy(self._ups, self._downs)

    @property
    def _confidence(self):
        return sorts.confidence(self._ups, self._downs)

    @property
    def num_votes(self):
        return self._ups + self._downs

    @property
    def is_distinguished(self):
        """Return whether this Thing has a special flag on it (mod, admin, etc).

        Done this way because distinguish is implemented in such a way where it
        does not exist by default, but can also be set to a string of 'no',
        which also means it is not distinguished.
        """
        return getattr(self, 'distinguished', 'no') != 'no'

    @classmethod
    def _query(cls, *all_rules, **kw):
        need_deleted = True
        need_spam = True
        #add default spam/deleted
        rules = []
        optimize_rules = kw.pop('optimize_rules', False)
        for r in all_rules:
            if not isinstance(r, operators.op):
                continue
            if r.lval_name == '_deleted':
                need_deleted = False
                # if the caller is explicitly unfiltering based on this column,
                # we don't need this rule at all. taking this out can save us a
                # join that is very expensive on pg9.
                if optimize_rules and r.rval == (True, False):
                    continue
            elif r.lval_name == '_spam':
                need_spam = False
                # see above for explanation
                if optimize_rules and r.rval == (True, False):
                    continue
            rules.append(r)

        if need_deleted:
            rules.append(cls.c._deleted == False)

        if need_spam:
            rules.append(cls.c._spam == False)

        return Things(cls, *rules, **kw)

    @classmethod
    def sort_ids_by_data_value(cls, thing_ids, value_name,
            limit=None, desc=False):
        return tdb.sort_thing_ids_by_data_value(
            cls._type_id, thing_ids, value_name, limit, desc)

    def update_search_index(self, boost_only=False):
        msg = {'fullname': self._fullname}
        if boost_only:
            msg['boost_only'] = True

        amqp.add_item('search_changes', pickle.dumps(msg),
                      message_id=self._fullname,
                      delivery_mode=amqp.DELIVERY_TRANSIENT)


class RelationMeta(type):
    def __init__(cls, name, bases, dct):
        if name == 'RelationCls': return
        #print "checking relation", name

        if g.env == 'unit_test':
            return

        cls._type_name = name.lower()
        try:
            cls._type_id = tdb.rel_types_name[cls._type_name].type_id
        except KeyError:
            raise KeyError, 'is the relationship database %s defined?' % name

        global rel_types
        rel_types[cls._type_id] = cls

        super(RelationMeta, cls).__init__(name, bases, dct)

    def __repr__(cls):
        return '<relation: %s>' % cls._type_name

def Relation(type1, type2):
    class RelationCls(DataThing):
        __metaclass__ = RelationMeta
        if not (issubclass(type1, Thing) and issubclass(type2, Thing)):
                raise TypeError('Relation types must be subclass of %s' % Thing)

        _type1 = type1
        _type2 = type2

        _base_props = ('_thing1_id', '_thing2_id', '_name', '_date')
        _type_prefix = Relation._type_prefix

        _cache_ttl = int(timedelta(hours=1).total_seconds())

        _enable_fast_query = True
        _rel_cache = g.relcache
        _rel_cache_ttl = int(timedelta(hours=1).total_seconds())

        @classmethod
        def get_things_from_db(cls, ids):
            """Read props from db and return id->rel dict."""
            props_by_id = tdb.get_rel(cls._type_id, ids)
            data_props_by_id = tdb.get_rel_data(cls._type_id, ids)

            rels_by_id = {}
            for _id, props in props_by_id.iteritems():
                data_props = data_props_by_id.get(_id, {})
                rel = cls(
                    thing1=props.thing1_id,
                    thing2=props.thing2_id,
                    name=props.name,
                    date=props.date,
                    id=_id,
                )
                rel._t.update(data_props)

                if not all(data_prop in rel._t for data_prop in cls._essentials):
                    # a Relation missing an essential prop is invalid
                    # this can happen if a process looks up the Relation as it's
                    # created but between when the props and the data props are
                    # written
                    g.log.error("%s missing essentials, got %s", rel, rel._t)
                    g.stats.simple_event("thing.load.missing_essentials")
                    continue

                rels_by_id[_id] = rel

            return rels_by_id

        def write_new_thing_to_db(self):
            """Write the new rel to db and return its id."""
            assert not self._created

            _id = tdb.make_relation(
                rel_type_id=self.__class__._type_id,
                thing1_id=self._thing1_id,
                thing2_id=self._thing2_id,
                name=self._name,
                date=self._date,
            )
            return _id

        def write_props_to_db(self, props, data_props, brand_new_thing):
            """Write the props to db."""
            if data_props:
                tdb.set_rel_data(
                    rel_type_id=self.__class__._type_id,
                    thing_id=self._id,
                    brand_new_thing=brand_new_thing,
                    **data_props
                )

            if props and not brand_new_thing:
                tdb.set_rel_props(
                    rel_type_id=self.__class__._type_id,
                    rel_id=self._id,
                    **props
                )

        # eager_load means, do you load thing1 and thing2 immediately. It calls
        # _byID(xxx).
        @classmethod
        def _byID_rel(cls, ids, data=True, return_dict=True,
                      eager_load=False, thing_data=True, thing_stale=False,
                      ignore_missing=False):
            # data props are ALWAYS loaded, data and thing_data keywords are
            # meaningless

            ids, single = tup(ids, True)

            bases = cls._byID(
                ids, return_dict=True, ignore_missing=ignore_missing)

            values = bases.values()

            if values and eager_load:
                load_things(values, stale=thing_stale)

            if single:
                return bases[ids[0]]
            elif return_dict:
                return bases
            else:
                return filter(None, (bases.get(i) for i in ids))

        def __init__(self, thing1, thing2, name, date = None, id = None, **attrs):
            DataThing.__init__(self)

            def id_and_obj(in_thing):
                if isinstance(in_thing, (int, long)):
                    return in_thing
                else:
                    return in_thing._id

            with self.safe_set_attr:
                if id:
                    self._id = id
                    self._created = True

                if not date:
                    date = datetime.now(g.tz)
                else:
                    date = date.astimezone(g.tz)

                #store the id, and temporarily store the actual object
                #because we may need it later
                self._thing1_id = id_and_obj(thing1)
                self._thing2_id = id_and_obj(thing2)
                self._name = name
                self._date = date

            for k, v in attrs.iteritems():
                self.__setattr__(k, v, not self._created)

        @classmethod
        def record_cache_write(cls, event, delta=1):
            name = cls.__name__.lower()
            event_name = "rel.{event}.{name}".format(event=event, name=name)
            g.stats.simple_event(event_name, delta)

        def __getattr__(self, attr):
            if attr == '_thing1':
                return self._type1._byID(self._thing1_id)
            elif attr == '_thing2':
                return self._type2._byID(self._thing2_id)
            elif attr.startswith('_t1'):
                return getattr(self._thing1, attr[3:])
            elif attr.startswith('_t2'):
                return getattr(self._thing2, attr[3:])
            else:
                return DataThing.__getattr__(self, attr)

        def __repr__(self):
            return ('<%s %s: <%s %s> - <%s %s> %s>' %
                    (self.__class__.__name__, self._name,
                     self._type1.__name__, self._thing1_id,
                     self._type2.__name__,self._thing2_id,
                     '[unsaved]' if not self._created else '\b'))

        @classmethod
        def _rel_cache_prefix(cls):
            return "rel:"

        @classmethod
        def _rel_cache_key_from_parts(cls, thing1_id, thing2_id, name):
            key = "{prefix}{cls}_{t1}_{t2}_{name}".format(
                prefix=cls._rel_cache_prefix(),
                cls=cls.__name__,
                t1=str(thing1_id),
                t2=str(thing2_id),
                name=name,
            )
            return key

        def _rel_cache_key(self):
            return self._rel_cache_key_from_parts(
                self._thing1_id,
                self._thing2_id,
                self._name,
            )

        def _commit(self):
            DataThing._commit(self)

            if self.__class__._enable_fast_query:
                ttl = self.__class__._rel_cache_ttl
                self._rel_cache.set(self._rel_cache_key(), self._id, time=ttl)

        def _delete(self):
            tdb.del_rel(self._type_id, self._id)

            self._cache.delete(self._cache_key())

            if self.__class__._enable_fast_query:
                ttl = self.__class__._rel_cache_ttl
                self._rel_cache.set(self._rel_cache_key(), None, time=ttl)

            # temporarily set this property so the rest of this request
            # knows it's deleted. save -> unsave, hide -> unhide
            self._name = 'un' + self._name

        @classmethod
        def _fast_query(cls, thing1s, thing2s, name, data=True, eager_load=True,
                        thing_data=True, thing_stale=False):
            """looks up all the relationships between thing1_ids and
               thing2_ids and caches them"""

            if not cls._enable_fast_query:
                raise ValueError("%s._fast_query is disabled" % cls.__name__)

            cache_key_lookup = dict()

            # We didn't find these keys in the cache, look them up in the
            # database
            def lookup_rel_ids(uncached_keys):
                rel_ids = {}

                # Lookup thing ids and name from cache key
                t1_ids = set()
                t2_ids = set()
                names = set()
                for cache_key in uncached_keys:
                    (thing1, thing2, name) = cache_key_lookup[cache_key]
                    t1_ids.add(thing1._id)
                    t2_ids.add(thing2._id)
                    names.add(name)

                q = cls._query(
                        cls.c._thing1_id == t1_ids,
                        cls.c._thing2_id == t2_ids,
                        cls.c._name == names)

                for rel in q:
                    rel_cache_key = cls._rel_cache_key_from_parts(
                        rel._thing1_id,
                        rel._thing2_id,
                        str(rel._name),
                    )
                    rel_ids[rel_cache_key] = rel._id

                for cache_key in uncached_keys:
                    if cache_key not in rel_ids:
                        rel_ids[cache_key] = None

                return rel_ids

            # make lookups for thing ids and names
            thing1_dict = dict((t._id, t) for t in tup(thing1s))
            thing2_dict = dict((t._id, t) for t in tup(thing2s))

            names = map(str, tup(name))

            # permute all of the pairs via cartesian product
            rel_tuples = itertools.product(
                thing1_dict.values(),
                thing2_dict.values(),
                names)

            # create cache keys for all permutations and initialize lookup
            for t in rel_tuples:
                thing1, thing2, name = t
                rel_cache_key = cls._rel_cache_key_from_parts(
                    thing1._id,
                    thing2._id,
                    name,
                )
                cache_key_lookup[rel_cache_key] = t

            # get the relation ids from the cache or query the db
            res = sgm(
                cache=cls._rel_cache,
                keys=cache_key_lookup.keys(),
                miss_fn=lookup_rel_ids,
                time=cls._rel_cache_ttl,
                ignore_set_errors=True,
            )

            # get the relation objects
            rel_ids = {rel_id for rel_id in res.itervalues()
                              if rel_id is not None}
            rels = cls._byID_rel(
                rel_ids,
                eager_load=eager_load,
                thing_stale=thing_stale)

            # Takes aggregated results from cache and db (res) and transforms
            # the values from ids to Relations.
            res_obj = {}
            for cache_key, rel_id in res.iteritems():
                t = cache_key_lookup[cache_key]
                rel = rels[rel_id] if rel_id is not None else None
                res_obj[t] = rel

            return res_obj

        @classmethod
        def _query(cls, *a, **kw):
            return Relations(cls, *a, **kw)

        @classmethod
        def _simple_query(cls, props, *rules, **kw):
            """Return only the requested props rather than Relation objects."""
            return RelationsPropsOnly(cls, props, *rules, **kw)

    return RelationCls
Relation._type_prefix = 'r'


class Query(object):
    def __init__(self, kind, *rules, **kw):
        self._rules = []
        self._kind = kind

        self._read_cache = kw.get('read_cache')
        self._write_cache = kw.get('write_cache')
        self._cache_time = kw.get('cache_time', 86400)
        self._limit = kw.get('limit')
        self._offset = kw.get('offset')
        self._stale = kw.get('stale', False)
        self._sort = kw.get('sort', ())
        self._filter_primary_sort_only = kw.get('filter_primary_sort_only', False)

        self._filter(*rules)

    def _setsort(self, sorts):
        sorts = tup(sorts)
        #make sure sorts are wrapped in a Sort obj
        have_date = False
        op_sorts = []
        for s in sorts:
            if not isinstance(s, operators.sort):
                s = operators.asc(s)
            op_sorts.append(s)
            if s.col.endswith('_date'):
                have_date = True
        if op_sorts and not have_date:
            op_sorts.append(operators.desc('_date'))

        self._sort_param = op_sorts
        return self

    def _getsort(self):
        return self._sort_param

    _sort = property(_getsort, _setsort)

    def _reverse(self):
        for s in self._sort:
            if isinstance(s, operators.asc):
                s.__class__ = operators.desc
            else:
                s.__class__ = operators.asc

    def _list(self, data=True):
        return list(self)

    def _dir(self, thing, reverse):
        ors = []

        # this fun hack lets us simplify the query on /r/all 
        # for postgres-9 compatibility. please remove it when
        # /r/all is precomputed.
        sorts = range(len(self._sort))
        if self._filter_primary_sort_only:
            sorts = [0]

        #for each sort add and a comparison operator
        for i in sorts:
            s = self._sort[i]

            if isinstance(s, operators.asc):
                op = operators.gt
            else:
                op = operators.lt

            if reverse:
                op = operators.gt if op == operators.lt else operators.lt

            #remember op takes lval and lval_name
            ands = [op(s.col, s.col, getattr(thing, s.col))]

            #for each sort up to the last add an equals operator
            for j in range(0, i):
                s = self._sort[j]
                ands.append(thing.c[s.col] == getattr(thing, s.col))

            ors.append(operators.and_(*ands))

        return self._filter(operators.or_(*ors))

    def _before(self, thing):
        return self._dir(thing, True)

    def _after(self, thing):
        return self._dir(thing, False)

    def _filter(*a, **kw):
        raise NotImplementedError

    def _cursor(*a, **kw):
        raise NotImplementedError

    def _cache_key(self):
        fingerprint = str(self._sort) + str(self._limit) + str(self._offset)
        if self._rules:
            rules = copy(self._rules)
            rules.sort()
            for rule in rules:
                fingerprint += str(rule)

        cache_key = "query:{kind}.{id}".format(
            kind=self._kind.__name__,
            id=hashlib.sha1(fingerprint).hexdigest()
        )
        return cache_key

    def _get_results(self):
        things = self._cursor().fetchall()
        return things

    def get_from_cache(self, allow_local=True):
        thing_fullnames = g.gencache.get(
            self._cache_key(), allow_local=allow_local)
        if thing_fullnames:
            things = Thing._by_fullname(thing_fullnames, return_dict=False,
                                        stale=self._stale)
            return things

    def set_to_cache(self, things):
        thing_fullnames = [thing._fullname for thing in things]
        g.gencache.set(self._cache_key(), thing_fullnames, self._cache_time)

    def __iter__(self):
        if self._read_cache:
            things = self.get_from_cache()
        else:
            things = None

        if things is None and not self._write_cache:
            things = self._get_results()
        elif things is None:
            # it's not in the cache, and we have the power to
            # update it, which we should do in a lock to prevent
            # concurrent requests for the same data
            with g.make_lock("thing_query", "lock_%s" % self._cache_key()):
                # see if it was set while we were waiting for our
                # lock
                if self._read_cache:
                    things = self.get_from_cache(allow_local=False)
                else:
                    things = None

                if things is None:
                    things = self._get_results()
                    self.set_to_cache(things)

        for thing in things:
            yield thing


class Things(Query):
    def __init__(self, kind, *rules, **kw):
        self._use_data = False
        Query.__init__(self, kind, *rules, **kw)

    def _filter(self, *rules):
        for op in operators.op_iter(rules):
            if not op.lval_name.startswith('_'):
                self._use_data = True

        self._rules += rules
        return self

    def _cursor(self):
        if self._use_data:
            find_fn = tdb.find_data
        else:
            find_fn = tdb.find_things

        cursor = find_fn(
            type_id=self._kind._type_id,
            sort=self._sort,
            limit=self._limit,
            offset=self._offset,
            constraints=self._rules,
        )

        #called on a bunch of rows to fetch their properties in batch
        def row_fn(ids):
            return self._kind._byID(ids, return_dict=False, stale=self._stale)

        return Results(cursor, row_fn, do_batch=True)

def load_things(rels, stale=False):
    rels = tup(rels)
    kind = rels[0].__class__

    t1_ids = set()
    if kind._type1 == kind._type2:
        t2_ids = t1_ids
    else:
        t2_ids = set()

    for rel in rels:
        t1_ids.add(rel._thing1_id)
        t2_ids.add(rel._thing2_id)

    kind._type1._byID(t1_ids, stale=stale)
    if kind._type1 != kind._type2:
        t2_items = kind._type2._byID(t2_ids, stale=stale)

class Relations(Query):
    def __init__(self, kind, *rules, **kw):
        self._eager_load = kw.get('eager_load')
        self._thing_stale = kw.get('thing_stale')
        Query.__init__(self, kind, *rules, **kw)

    def _filter(self, *rules):
        self._rules += rules
        return self

    def _eager(self, eager):
        #load the things (id, ups, down, etc.)
        self._eager_load = eager
        return self

    def _make_rel(self, rows):
        rel_ids = [row._rel_id for row in rows]
        rels = self._kind._byID(rel_ids, return_dict=False)
        if rels and self._eager_load:
            load_things(rels, stale=self._thing_stale)
        return rels

    def _cursor(self):
        c = tdb.find_rels(
            ret_props=["_rel_id"],
            rel_type_id=self._kind._type_id,
            sort=self._sort,
            limit=self._limit,
            offset=self._offset,
            constraints=self._rules,
        )
        return Results(c, self._make_rel, do_batch=True)


class RelationsPropsOnly(Relations):
    def __init__(self, kind, props, *rules, **kw):
        self.props = props
        Relations.__init__(self, kind, *rules, **kw)

    def _cursor(self):
        c = tdb.find_rels(
            ret_props=self.props,
            rel_type_id=self._kind._type_id,
            sort=self._sort,
            limit=self._limit,
            offset=self._offset,
            constraints=self._rules,
        )
        return c

    def _cache_key(self):
        fingerprint = str(self._sort) + str(self._limit) + str(self._offset)
        if self._rules:
            rules = copy(self._rules)
            rules.sort()
            for rule in rules:
                fingerprint += str(rule)
        fingerprint += '|'.join(sorted(self.props))

        cache_key = "query:{kind}.{id}".format(
            kind=self._kind.__name__,
            id=hashlib.sha1(fingerprint).hexdigest()
        )
        return cache_key

    def _get_results(self):
        rows = self._cursor().fetchall()
        return rows

    def get_from_cache(self, allow_local=True):
        return g.gencache.get(self._cache_key(), allow_local=allow_local)

    def set_to_cache(self, rows):
        g.gencache.set(self._cache_key(), rows, self._cache_time)


class MultiCursor(object):
    def __init__(self, *execute_params):
        self._execute_params = execute_params
        self._cursor = None

    def fetchone(self):
        if not self._cursor:
            self._cursor = self._execute(*self._execute_params)
            
        return self._cursor.next()
                
    def fetchall(self):
        if not self._cursor:
            self._cursor = self._execute(*self._execute_params)

        return [i for i in self._cursor]

class MergeCursor(MultiCursor):
    def _execute(self, cursors, sorts):
        #a "pair" is a (cursor, item, done) tuple
        def safe_next(c):
            try:
                #hack to keep searching even if fetching a thing returns notfound
                while True:
                    try:
                        return [c, c.fetchone(), False]
                    except NotFound:
                        #skips the broken item
                        pass
            except StopIteration:
                return c, None, True

        def undone(pairs):
            return [p for p in pairs if not p[2]]

        pairs = undone(safe_next(c) for c in cursors)

        while pairs:
            #only one query left, just dump it
            if len(pairs) == 1:
                c, item, done = pair = pairs[0]
                while not done:
                    yield item
                    c, item, done = safe_next(c)
                    pair[:] = c, item, done
            else:
                #by default, yield the first item
                yield_pair = pairs[0]
                for s in sorts:
                    col = s.col
                    #sort direction?
                    max_fn = min if isinstance(s, operators.asc) else max

                    #find the max (or min) val
                    vals = [(getattr(i[1], col), i) for i in pairs]
                    max_pair = vals[0]
                    all_equal = True
                    for pair in vals[1:]:
                        if all_equal and pair[0] != max_pair[0]:
                            all_equal = False
                        max_pair = max_fn(max_pair, pair, key=lambda x: x[0])

                    if not all_equal:
                        yield_pair = max_pair[1]
                        break

                c, item, done = yield_pair
                yield item
                yield_pair[:] = safe_next(c)

            pairs = undone(pairs)
        raise StopIteration


class MultiQuery(Query):
    def __init__(self, queries, *rules, **kw):
        self._queries = queries
        Query.__init__(self, None, *rules, **kw)

        assert not self._read_cache
        assert not self._write_cache

    def get_from_cache(self):
        raise NotImplementedError()

    def set_to_cache(self):
        raise NotImplementedError()

    def _cursor(self):
        raise NotImplementedError()

    def _reverse(self):
        for q in self._queries:
            q._reverse()

    def _setdata(self, data):
        return

    def _getdata(self):
        return True

    _data = property(_getdata, _setdata)

    def _setsort(self, sorts):
        for q in self._queries:
            q._sort = deepcopy(sorts)

    def _getsort(self):
        if self._queries:
            return self._queries[0]._sort

    _sort = property(_getsort, _setsort)

    def _filter(self, *rules):
        for q in self._queries:
            q._filter(*rules)

    def _getrules(self):
        return [q._rules for q in self._queries]

    def _setrules(self, rules):
        for q,r in zip(self._queries, rules):
            q._rules = r

    _rules = property(_getrules, _setrules)

    def _getlimit(self):
        return self._queries[0]._limit

    def _setlimit(self, limit):
        for q in self._queries:
            q._limit = limit

    _limit = property(_getlimit, _setlimit)


class Merge(MultiQuery):
    def _cursor(self):
        if (any(q._sort for q in self._queries) and
            not reduce(lambda x,y: (x == y) and x,
                      (q._sort for q in self._queries))):
            raise "The sorts should be the same"

        return MergeCursor((q._cursor() for q in self._queries),
                           self._sort)

def MultiRelation(name, *relations):
    rels_tmp = {}
    for rel in relations:
        t1, t2 = rel._type1, rel._type2
        clsname = name + '_' + t1.__name__.lower() + '_' + t2.__name__.lower()
        cls = new.classobj(clsname, (rel,), {'__module__':t1.__module__})
        setattr(sys.modules[t1.__module__], clsname, cls)
        rels_tmp[(t1, t2)] = cls

    class MultiRelationCls(object):
        c = operators.Slots()
        rels = rels_tmp

        def __init__(self, thing1, thing2, *a, **kw):
            r = self.rel(thing1, thing2)
            self.__class__ = r
            self.__init__(thing1, thing2, *a, **kw)

        @classmethod
        def rel(cls, thing1, thing2):
            t1 = thing1 if isinstance(thing1, ThingMeta) else thing1.__class__
            t2 = thing2 if isinstance(thing2, ThingMeta) else thing2.__class__
            return cls.rels[(t1, t2)]

        @classmethod
        def _query(cls, *rules, **kw):
            #TODO it should be possible to send the rules and kw to
            #the merge constructor
            queries = [r._query(*rules, **kw) for r in cls.rels.values()]
            if "sort" in kw:
                print "sorting MultiRelations is not supported"
            return Merge(queries)

        @classmethod
        def _fast_query(cls, sub, obj, name, data=True, eager_load=True,
                        thing_data=True):
            #divide into types
            def type_dict(items):
                types = {}
                for i in items:
                    types.setdefault(i.__class__, []).append(i)
                return types

            sub_dict = type_dict(tup(sub))
            obj_dict = type_dict(tup(obj))

            #for each pair of types, see if we have a query to send
            res = {}
            for types, rel in cls.rels.iteritems():
                t1, t2 = types
                if sub_dict.has_key(t1) and obj_dict.has_key(t2):
                    res.update(rel._fast_query(
                        sub_dict[t1], obj_dict[t2], name, eager_load=eager_load))

            return res

    return MultiRelationCls
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import functools
import types

from r2.lib.db.thing import CreationError
from r2.lib.memoize import memoize


class UserRelManager(object):
    """Manages access to a relation between a type of thing and users."""

    def __init__(self, name, relation, permission_class):
        self.name = name
        self.relation = relation
        self.permission_class = permission_class

    def get(self, thing, user):
        if user:
            q = self.relation._fast_query([thing], [user], self.name)
            rel = q.get((thing, user, self.name))
            if rel:
                rel._permission_class = self.permission_class
            return rel

    def add(self, thing, user, permissions=None, **attrs):
        if self.get(thing, user):
            return None
        r = self.relation(thing, user, self.name, **attrs)
        if permissions is not None:
            r.set_permissions(permissions)

        try:
            r._commit()
        except CreationError:
            return None

        r._permission_class = self.permission_class
        return r

    def remove(self, thing, user):
        r = self.get(thing, user)
        if r:
            r._delete()
            return True
        return False

    def mutate(self, thing, user, **attrs):
        r = self.get(thing, user)
        if r:
            for k, v in attrs.iteritems():
                setattr(r, k, v)
            r._commit()
            r._permission_class = self.permission_class
            return r
        else:
            return self.add(thing, user, **attrs)

    def ids(self, thing):
        q = self.relation._simple_query(
            ["_thing2_id"],
            self.relation.c._thing1_id == thing._id,
            self.relation.c._name == self.name,
            sort='_date',
        )
        return [r._thing2_id for r in q]

    def reverse_ids(self, user):
        q = self.relation._simple_query(
            ["_thing1_id"],
            self.relation.c._thing2_id == user._id,
            self.relation.c._name == self.name,
        )
        return [r._thing1_id for r in q]

    def by_thing(self, thing):
        q = self.relation._query(
            self.relation.c._thing1_id == thing._id,
            self.relation.c._name == self.name,
            sort='_date',
            data=True,
        )

        for r in q:
            r._permission_class = self.permission_class
            yield r


class MemoizedUserRelManager(UserRelManager):
    """Memoized manager for a relation to users."""

    def __init__(self, name, relation, permission_class,
                 disable_ids_fn=False, disable_reverse_ids_fn=False):
        super(MemoizedUserRelManager, self).__init__(
            name, relation, permission_class)

        self.disable_ids_fn = disable_ids_fn
        self.disable_reverse_ids_fn = disable_reverse_ids_fn
        self.ids_fn_name = self.name + '_ids'
        self.reverse_ids_fn_name = 'reverse_' + self.name + '_ids'

        sup = super(MemoizedUserRelManager, self)
        self.ids = memoize(self.ids_fn_name)(sup.ids)
        self.reverse_ids = memoize(self.reverse_ids_fn_name)(sup.reverse_ids)
        self.add = self._update_caches_on_success(sup.add)
        self.remove = self._update_caches_on_success(sup.remove)

    def _update_caches(self, thing, user):
        if not self.disable_ids_fn:
            self.ids(thing, _update=True)
        if not self.disable_reverse_ids_fn:
            self.reverse_ids(user, _update=True)

    def _update_caches_on_success(self, method):
        @functools.wraps(method)
        def wrapper(thing, user, *args, **kwargs):
            try:
                result = method(thing, user, *args, **kwargs)
            except:
                raise
            else:
                self._update_caches(thing, user)
            return result
        return wrapper


def UserRel(name, relation, disable_ids_fn=False, disable_reverse_ids_fn=False,
            permission_class=None):
    """Mixin for Thing subclasses for managing a relation to users.

    Provides the following suite of methods for a relation named "<relation>":

      - is_<relation>(self, user) - whether user is related to self
      - add_<relation>(self, user) - relates user to self
      - remove_<relation>(self, user) - dissolves relation of user to self

    This suite also usually includes (unless explicitly disabled):

      - <relation>_ids(self) - list of user IDs related to self
      - (static) reverse_<relation>_ids(user) - list of thing IDs user is
          related to
    """
    mgr = MemoizedUserRelManager(
        name, relation, permission_class,
        disable_ids_fn, disable_reverse_ids_fn)

    class UR:
        @classmethod
        def _bind(cls, fn):
            return types.UnboundMethodType(fn, None, cls)

    setattr(UR, 'is_' + name, UR._bind(mgr.get))
    setattr(UR, 'get_' + name, UR._bind(mgr.get))
    setattr(UR, 'add_' + name, UR._bind(mgr.add))
    setattr(UR, 'remove_' + name, UR._bind(mgr.remove))
    setattr(UR, 'each_' + name, UR._bind(mgr.by_thing))
    setattr(UR, name + '_permission_class', permission_class)
    if not disable_ids_fn:
        setattr(UR, mgr.ids_fn_name, UR._bind(mgr.ids))
    if not disable_reverse_ids_fn:
        setattr(UR, mgr.reverse_ids_fn_name, staticmethod(mgr.reverse_ids))

    return UR


def MigratingUserRel(name, relation, disable_ids_fn=False,
                     disable_reverse_ids_fn=False, permission_class=None):
    """
    Replacement for UserRel to be used during migrations away from the system.

    The resulting "UserRel" classes generated are to be used as standalones and
    not included in Subreddit.__bases__.

    """

    mgr = MemoizedUserRelManager(
        name, relation, permission_class,
        disable_ids_fn, disable_reverse_ids_fn)

    class URM: pass

    setattr(URM, 'is_' + name, mgr.get)
    setattr(URM, 'get_' + name, mgr.get)
    setattr(URM, 'add_' + name, staticmethod(mgr.add))
    setattr(URM, 'remove_' + name, staticmethod(mgr.remove))
    setattr(URM, 'each_' + name, mgr.by_thing)
    setattr(URM, name + '_permission_class', permission_class)

    if not disable_ids_fn:
        setattr(URM, mgr.ids_fn_name, mgr.ids)

    if not disable_reverse_ids_fn:
        setattr(URM, mgr.reverse_ids_fn_name, staticmethod(mgr.reverse_ids))

    return URM
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import tdb_sql
import sqlalchemy as sa

def thing_tables():
    for type in tdb_sql.types_id.values():
        yield type.thing_table

    for table in tdb_sql.extra_thing_tables.values():
        yield table

def rel_tables():
    for type in tdb_sql.rel_types_id.values():
        yield type.rel_table[0]

def dtables():
    for type in tdb_sql.types_id.values():
        yield type.data_table[0]

    for type in tdb_sql.rel_types_id.values():
        yield type.rel_table[3]

def exec_all(command, data=False, rel = False, print_only = False):
    if data:
        tables = dtables()
    elif rel:
        tables = rel_tables()
    else:
        tables = thing_tables()

    for tt in tables:
        #print tt
        engine = tt.bind
        if print_only:
            print command % dict(type=tt.name)
        else:
            try:
                engine.execute(command % dict(type=tt.name))
            except:
                print "FAILED!"

"alter table %(type)s add primary key (thing_id, key)"
"drop index idx_thing_id_%(type)s"

"create index concurrently idx_thing1_name_date_%(type)s on %(type)s (thing1_id, name, date);"
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import logging
import os
import random
import socket
import sqlalchemy
import time
import traceback


logger = logging.getLogger('dm_manager')
logger.addHandler(logging.StreamHandler())
APPLICATION_NAME = "reddit@%s:%d" % (socket.gethostname(), os.getpid())


def get_engine(name, db_host='', db_user='', db_pass='', db_port='5432',
               pool_size=5, max_overflow=5, g_override=None):
    db_port = int(db_port)

    arguments = {
        "dbname": name,
        "host": db_host,
        "port": db_port,
        "application_name": APPLICATION_NAME,
    }
    if db_user:
        arguments["user"] = db_user
    if db_pass:
        arguments["password"] = db_pass
    dsn = "%20".join("%s=%s" % x for x in arguments.iteritems())

    engine = sqlalchemy.create_engine(
        'postgresql:///?dsn=' + dsn,
        strategy='threadlocal',
        pool_size=int(pool_size),
        max_overflow=int(max_overflow),
        # our code isn't ready for unicode to appear
        # in place of strings yet
        use_native_unicode=False,
    )

    if g_override:
        sqlalchemy.event.listens_for(engine, 'before_cursor_execute')(
            g_override.stats.pg_before_cursor_execute)
        sqlalchemy.event.listens_for(engine, 'after_cursor_execute')(
            g_override.stats.pg_after_cursor_execute)

    return engine


class db_manager:
    def __init__(self):
        self.type_db = None
        self.relation_type_db = None
        self._things = {}
        self._relations = {}
        self._engines = {}
        self.avoid_master_reads = {}
        self.dead = {}

    def add_thing(self, name, thing_dbs, avoid_master=False, **kw):
        """thing_dbs is a list of database engines. the first in the
        list is assumed to be the master, the rest are slaves."""
        self._things[name] = thing_dbs
        self.avoid_master_reads[name] = avoid_master

    def add_relation(self, name, type1, type2, relation_dbs,
                     avoid_master=False, **kw):
        self._relations[name] = (type1, type2, relation_dbs)
        self.avoid_master_reads[name] = avoid_master

    def setup_db(self, db_name, g_override=None, **params):
        engine = get_engine(g_override=g_override, **params)
        self._engines[db_name] = engine

        if db_name not in ("email", "authorize", "hc", "traffic"):
            # test_engine creates a connection to the database, for some less
            # important and less used databases we will skip this and only
            # create the connection if it's needed
            self.test_engine(engine, g_override)

    def things_iter(self):
        for name, engines in self._things.iteritems():
            # ensure we ALWAYS return the actual master as the first,
            # regardless of if we think it's dead or not.
            yield name, [engines[0]] + [e for e in engines[1:]
                                        if e not in self.dead]

    def rels_iter(self):
        for name, (t1_name, t2_name, engines) in self._relations.iteritems():
            engines = [engines[0]] + [e for e in engines[1:]
                                      if e not in self.dead]
            yield name, (t1_name, t2_name, engines)

    def mark_dead(self, engine, g_override=None):
        logger.error("db_manager: marking connection dead: %r", engine)
        self.dead[engine] = time.time()

    def test_engine(self, engine, g_override=None):
        try:
            list(engine.execute("select 1"))
            if engine in self.dead:
                logger.error("db_manager: marking connection alive: %r",
                             engine)
                del self.dead[engine]
            return True
        except Exception:
            logger.error(traceback.format_exc())
            logger.error("connection failure: %r" % engine)
            self.mark_dead(engine, g_override)
            return False

    def get_engine(self, name):
        return self._engines[name]

    def get_engines(self, names):
        return [self._engines[name] for name in names if name in self._engines]

    def get_read_table(self, tables):
        if len(tables) == 1:
            return tables[0]
        return  random.choice(list(tables))
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


import hashlib
import inspect

from mako.exceptions import TemplateLookupException
from mako.template import Template as mTemplate
from pylons import app_globals as g


NULL_TEMPLATE = mTemplate("")
NULL_TEMPLATE.is_null = True

class tp_manager:
    def __init__(self, template_cls=mTemplate):
        self.templates = {}
        self.Template = template_cls
        self.cache_override_styles = set()

    def add_handler(self, name, style, handler):
        key = (name.lower(), style.lower())
        self.templates[key] = handler

        # a template has been manually specified for this style so record that
        # we should override g.reload_templates when retrieving templates
        self.cache_override_styles.add(style.lower())

    def cache_template(self, cls, style, template):
        use_cache = not g.reload_templates
        if use_cache:
            if (not hasattr(template, "hash") and
                    getattr(template, "filename", None)):
                with open(template.filename, 'r') as handle:
                    template.hash = hashlib.sha1(handle.read()).hexdigest()
            key = (cls.__name__.lower(), style)
            self.templates[key] = template

    def get_template(self, cls, style):
        name = cls.__name__.lower()
        use_cache = not g.reload_templates

        if use_cache or style.lower() in self.cache_override_styles:
            key = (name, style)
            template = self.templates.get(key)
            if template:
                return template

        filename = "/%s.%s" % (name, style)
        try:
            template = g.mako_lookup.get_template(filename)
        except TemplateLookupException:
            return

        self.cache_template(cls, style, template)

        return template

    def get(self, thing, style):
        if not isinstance(thing, type(object)):
            thing = thing.__class__

        style = style.lower()
        template = self.get_template(thing, style)
        if template:
            return template

        # walk back through base classes to find a template
        for cls in inspect.getmro(thing)[1:]:
            template = self.get_template(cls, style)
            if template:
                break
        else:
            # didn't find a template, use the null template
            template = NULL_TEMPLATE

        # cache template for thing so we don't need to introspect on subsequent
        # calls
        self.cache_template(thing, style, template)

        return template

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from api import *
from interaction import *
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
For talking to authorize.net credit card payments via their XML api.

This file consists mostly of wrapper classes for dealing with their
API, while the actual useful functions live in interaction.py

NOTE: This is using the Customer Information Manager (CIM) API
http://developer.authorize.net/api/cim/
"""

import re
from httplib import HTTPSConnection
from urlparse import urlparse

from BeautifulSoup import BeautifulStoneSoup
from pylons import app_globals as g
from xml.sax.saxutils import escape

from r2.lib.export import export
from r2.lib.utils import iters, Storage

__all__ = ["PROFILE_LIMIT", "TRANSACTION_NOT_FOUND"]

TRANSACTION_NOT_FOUND = 16

# response codes http://www.authorize.net/support/ReportingGuide_XML.pdf
TRANSACTION_APPROVED = 1
TRANSACTION_DECLINED = 2
TRANSACTION_ERROR = 3
TRANSACTION_IN_REVIEW = 4

# response reason codes
TRANSACTION_DUPLICATE = 11
# transactions with identical amount, credit card, and invoice submitted within
# some time window will raise an error
# https://support.authorize.net/authkb/index?page=content&id=A425


# list of the most common errors.
Errors = Storage(
    TRANSACTION_FAIL="E00027",
    DUPLICATE_RECORD="E00039", 
    RECORD_NOT_FOUND="E00040",
    TOO_MANY_PAY_PROFILES="E00042",
    TOO_MANY_SHIP_ADDRESSES="E00043",
)

PROFILE_LIMIT = 10 # max payment profiles per user allowed by authorize.net


@export
class AuthorizeNetException(Exception):
    def __init__(self, msg, code=None):
        # don't let CC info show up in logs
        msg = re.sub("<cardNumber>\d+(\d{4})</cardNumber>", 
                     "<cardNumber>...\g<1></cardNumber>",
                     msg)
        msg = re.sub("<cardCode>\d+</cardCode>",
                     "<cardCode>omitted</cardCode>",
                     msg)
        self.code = code
        super(AuthorizeNetException, self).__init__(msg)


class TransactionError(Exception):
    def __init__(self, message):
        self.message = message


class DuplicateTransactionError(TransactionError):
    def __init__(self, transaction_id):
        self.transaction_id = transaction_id
        message = ('DuplicateTransactionError with transaction_id %d' %
                   transaction_id)
        super(DuplicateTransactionError, self).__init__(message)


class AuthorizationHoldNotFound(Exception): pass


# xml tags whose content shouldn't be escaped 
_no_escape_list = ["extraOptions"]


class SimpleXMLObject(object):
    """
    All API transactions are done with authorize.net using XML, so
    here's a class for generating and extracting structured data from
    XML.
    """
    _keys = []
    def __init__(self, **kw):
        self._used_keys = self._keys if self._keys else kw.keys()
        for k in self._used_keys:
            if not hasattr(self, k):
                setattr(self, k, kw.get(k, ""))

    @staticmethod
    def simple_tag(name, content, **attrs):
        attrs = " ".join('%s="%s"' % (k, v) for k, v in attrs.iteritems())
        if attrs:
            attrs = " " + attrs
        return ("<%(name)s%(attrs)s>%(content)s</%(name)s>" %
                dict(name=name, content=content, attrs=attrs))

    def toXML(self):
        content = []
        def process(k, v):
            if isinstance(v, SimpleXMLObject):
                v = v.toXML()
            elif v is not None:
                v = unicode(v)
                if k not in _no_escape_list:
                    v = escape(v) # escape &, <, and >
            if v is not None:
                content.append(self.simple_tag(k, v))

        for k in self._used_keys:
            v = getattr(self, k)
            if isinstance(v, iters):
                for val in v:
                    process(k, val)
            else:
                process(k, v)
        return self._wrapper("".join(content))

    @classmethod
    def fromXML(cls, data):
        kw = {}
        for k in cls._keys:
            d = data.find(k.lower())
            if d and d.contents:
                kw[k] = unicode(d.contents[0])
        return cls(**kw)


    def __repr__(self):
        return "<%s {%s}>" % (self.__class__.__name__,
                              ",".join("%s=%s" % (k, repr(getattr(self, k)))
                                       for k in self._used_keys))

    def _name(self):
        name = self.__class__.__name__
        return name[0].lower() + name[1:]
    
    def _wrapper(self, content):
        return content


class Auth(SimpleXMLObject):
    _keys = ["name", "transactionKey"]


@export
class Address(SimpleXMLObject):
    _keys = ["firstName", "lastName", "company", "address",
             "city", "state", "zip", "country", "phoneNumber",
             "faxNumber",
             "customerPaymentProfileId",
             "customerAddressId" ]
    def __init__(self, **kw):
        kw['customerPaymentProfileId'] = kw.get("customerPaymentProfileId",
                                                 None)
        kw['customerAddressId'] = kw.get("customerAddressId", None)
        SimpleXMLObject.__init__(self, **kw)


@export
class CreditCard(SimpleXMLObject):
    _keys = ["cardNumber", "expirationDate", "cardCode"]


class Profile(SimpleXMLObject):
    _keys = ["merchantCustomerId", "description",
             "email", "customerProfileId", "paymentProfiles", "validationMode"]

    def __init__(self, description, merchantCustomerId, customerProfileId,
                 paymentProfiles, validationMode=None):
        SimpleXMLObject.__init__(
            self,
            merchantCustomerId=merchantCustomerId,
            description=description,
            email="",
            paymentProfiles=paymentProfiles,
            validationMode=validationMode,
            customerProfileId=customerProfileId,
        )


class PaymentProfile(SimpleXMLObject):
    _keys = ["billTo", "payment", "customerPaymentProfileId", "validationMode"]
    def __init__(self, billTo, card, customerPaymentProfileId=None,
                 validationMode=None):
        SimpleXMLObject.__init__(
            self,
            billTo=billTo,
            customerPaymentProfileId=customerPaymentProfileId,
            payment=SimpleXMLObject(creditCard=card),
            validationMode=validationMode,
        )

    @classmethod
    def fromXML(cls, res):
        paymentId = int(res.customerpaymentprofileid.contents[0])
        billTo = Address.fromXML(res.billto)
        card = CreditCard.fromXML(res.payment)
        return cls(billTo, card, paymentId)


@export
class Order(SimpleXMLObject):
    _keys = ["invoiceNumber", "description", "purchaseOrderNumber"]


class Transaction(SimpleXMLObject):
    _keys = ["amount", "customerProfileId", "customerPaymentProfileId",
             "transId", "order"]

    def __init__(self, amount, customerProfileId, customerPaymentProfileId,
                 transId=None, order=None):
        SimpleXMLObject.__init__(
            self, amount=amount, customerProfileId=customerProfileId,
            customerPaymentProfileId=customerPaymentProfileId, transId=transId,
            order=order)

    def _wrapper(self, content):
        return self.simple_tag(self._name(), content)


# only authorize (no charge is made)
@export
class ProfileTransAuthOnly(Transaction): pass


# charge only (requires previous auth_only)
@export
class ProfileTransPriorAuthCapture(Transaction): pass


# refund a transaction
@export
class ProfileTransRefund(Transaction): pass


# void a transaction
@export
class ProfileTransVoid(Transaction): pass


#-----
class AuthorizeNetRequest(SimpleXMLObject):
    _keys = ["merchantAuthentication"]

    @property
    def merchantAuthentication(self):
        return Auth(name=g.secrets['authorizenetname'],
                    transactionKey=g.secrets['authorizenetkey'])

    def _wrapper(self, content):
        return ('<?xml version="1.0" encoding="utf-8"?>' +
                self.simple_tag(self._name(), content,
                             xmlns="AnetApi/xml/v1/schema/AnetApiSchema.xsd"))

    def make_request(self):
        u = urlparse(g.authorizenetapi)
        conn = HTTPSConnection(u.hostname, u.port)
        conn.request("POST", u.path, self.toXML().encode('utf-8'),
                     {"Content-type": "text/xml"})
        res = conn.getresponse()
        res = self.handle_response(res.read())
        conn.close()
        return res

    def is_error_code(self, res, code):
        return (res.message.code and res.message.code.contents and
                res.message.code.contents[0] == code)

    _autoclose_re = re.compile("<([^/]+)/>")
    def _autoclose_handler(self, m):
        return "<%(m)s></%(m)s>" % dict(m=m.groups()[0])

    def handle_response(self, res):
        res = self._autoclose_re.sub(self._autoclose_handler, res)
        res = BeautifulStoneSoup(res, 
                                 markupMassage=False, 
                                 convertEntities=BeautifulStoneSoup.XML_ENTITIES)
        if res.resultcode.contents[0] == u"Ok":
            return self.process_response(res)
        else:
            return self.process_error(res)

    def process_response(self, res):
        raise NotImplementedError

    def process_error(self, res):
        raise NotImplementedError


# --- real request classes below

class CreateCustomerProfileRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["profile", "validationMode"]

    def __init__(self, profile, validationMode=None):
        AuthorizeNetRequest.__init__(
            self, profile=profile, validationMode=validationMode)

    def process_response(self, res):
        customer_id = int(res.customerprofileid.contents[0])
        return customer_id

    def process_error(self, res):
        message_text = res.find("text").contents[0]

        if self.is_error_code(res, Errors.DUPLICATE_RECORD):
            # authorize.net has a record for this user but we don't. get the id
            # from the error message
            matches = re.match(
                "A duplicate record with ID (\d+) already exists", message_text)
            if matches:
                match_groups = matches.groups()
                customer_id = match_groups[0]
                return customer_id

        raise AuthorizeNetException(message_text)


class CreateCustomerPaymentProfileRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["customerProfileId", "paymentProfile",
        "validationMode"]

    def __init__(self, customerProfileId, paymentProfile, validationMode=None):
        AuthorizeNetRequest.__init__(
            self, customerProfileId=customerProfileId,
            paymentProfile=paymentProfile, validationMode=validationMode)

    def process_response(self, res):
        pay_id = int(res.customerpaymentprofileid.contents[0])
        return pay_id

    def process_error(self, res):
        message_text = res.find("text").contents[0]
        raise AuthorizeNetException(message_text)


class GetCustomerProfileRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["customerProfileId"]

    def __init__(self, customerProfileId):
        AuthorizeNetRequest.__init__(
            self, customerProfileId=customerProfileId)

    def process_response(self, res):
        merchantCustomerId = res.merchantcustomerid.contents[0]
        description = res.description.contents[0]
        profile_id = int(res.customerprofileid.contents[0])

        payment_profiles = []
        for profile in res.findAll("paymentprofiles"):
            address = Address.fromXML(profile)
            credit_card = CreditCard.fromXML(profile.payment)
            customerPaymentProfileId = int(address.customerPaymentProfileId)

            payment_profile = PaymentProfile(
                billTo=address,
                card=credit_card,
                customerPaymentProfileId=customerPaymentProfileId,
            )
            payment_profiles.append(payment_profile)

        profile = Profile(
            description=description,
            merchantCustomerId=merchantCustomerId,
            customerProfileId=profile_id,
            paymentProfiles=payment_profiles,
        )
        return profile

    def process_error(self, res):
        message_text = res.find("text").contents[0]
        code = res.find('code').contents[0]
        raise AuthorizeNetException(message_text, code=code)


# TODO: implement
class DeleteCustomerPaymentProfileRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["customerProfileId",
        "customerPaymentProfileId"]

    def __init__(self, customerProfileId, customerPaymentProfileId):
        AuthorizeNetRequest.__init__(
            self, customerProfileId=customerProfileId,
            customerPaymentProfileId=customerPaymentProfileId)

    def process_response(self, res):
        return True

    def process_error(self, res):
        if self.is_error_code(res, Errors.RECORD_NOT_FOUND):
            return True

        message_text = res.find("text").contents[0]
        raise AuthorizeNetException(message_text)


class UpdateCustomerPaymentProfileRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["customerProfileId", "paymentProfile",
        "validationMode"]

    def __init__(self, customerProfileId, paymentProfile, validationMode=None):
        AuthorizeNetRequest.__init__(
            self, customerProfileId=customerProfileId,
            paymentProfile=paymentProfile, validationMode=validationMode)

    def process_response(self, res):
        return self.paymentProfile.customerPaymentProfileId

    def process_error(self, res):
        message_text = res.find("text").contents[0]
        raise AuthorizeNetException(message_text)


class CreateCustomerProfileTransactionRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["transaction", "extraOptions"]

    # unlike every other response we get back, this api function
    # returns CSV data of the response with no field labels.  these
    # are used in package_response to zip this data into a usable
    # storage.
    response_keys = ("response_code",
                     "response_subcode",
                     "response_reason_code",
                     "response_reason_text",
                     "authorization_code",
                     "avs_response",
                     "trans_id",
                     "invoice_number",
                     "description",
                     "amount", "method",
                     "transaction_type",
                     "customerID",
                     "firstName", "lastName",
                     "company", "address", "city", "state",
                     "zip", "country", 
                     "phoneNumber", "faxNumber", "email",
                     "shipTo_firstName", "shipTo_lastName",
                     "shipTo_company", "shipTo_address",
                     "shipTo_city", "shipTo_state",
                     "shipTo_zip", "shipTo_country",
                     "tax", "duty", "freight",
                     "tax_exempt", "po_number", "md5",
                     "cav_response")

    # list of casts for the response fields given above
    response_types = dict(response_code=int,
                          response_subcode=int,
                          response_reason_code=int,
                          trans_id=int)

    def __init__(self, **kw):
        self._extra = kw.get("extraOptions", {})
        AuthorizeNetRequest.__init__(self, **kw)

    @property
    def extraOptions(self):
        return "<![CDATA[%s]]>" % "&".join("%s=%s" % x
                                            for x in self._extra.iteritems())

    def process_response(self, res):
        return (True, self.package_response(res))

    def process_error(self, res):
        return (False, self.package_response(res))

    def package_response(self, res):
        content = res.directresponse.contents[0]
        s = Storage(zip(self.response_keys, content.split(',')))
        for name, cast in self.response_types.iteritems():
            try:
                s[name] = cast(s[name])
            except ValueError:
                pass
        return s


class GetSettledBatchListRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["includeStatistics", 
                                         "firstSettlementDate", 
                                         "lastSettlementDate"]
    def __init__(self, start_date, end_date, **kw):
        AuthorizeNetRequest.__init__(self, 
                                     includeStatistics=1,
                                     firstSettlementDate=start_date.isoformat(),
                                     lastSettlementDate=end_date.isoformat(),
                                     **kw)

    def process_response(self, res):
        return res

    def process_error(self, res):
        message_text = res.find("text").contents[0]
        raise AuthorizeNetException(message_text)


def create_customer_profile(merchant_customer_id, description):
    profile = Profile(
        description=description,
        merchantCustomerId=merchant_customer_id,
        paymentProfiles=None,
        customerProfileId=None,
    )

    request = CreateCustomerProfileRequest(profile=profile)

    try:
        customer_id = request.make_request()
    except AuthorizeNetException:
        return None

    return customer_id


def get_customer_profile(customer_id):
    request = GetCustomerProfileRequest(customerProfileId=customer_id)

    try:
        profile = request.make_request()
    except AuthorizeNetException:
        return None

    return profile


def create_payment_profile(customer_id, address, credit_card, validate=False):
    payment_profile = PaymentProfile(billTo=address, card=credit_card)

    request = CreateCustomerPaymentProfileRequest(
        customerProfileId=customer_id,
        paymentProfile=payment_profile,
        validationMode="liveMode" if validate else None,
    )

    payment_profile_id = request.make_request()

    return payment_profile_id


def update_payment_profile(customer_id, payment_profile_id, address,
                           credit_card, validate=False):
    payment_profile = PaymentProfile(
        billTo=address,
        card=credit_card,
        customerPaymentProfileId=payment_profile_id,
    )

    request = UpdateCustomerPaymentProfileRequest(
        customerProfileId=customer_id,
        paymentProfile=payment_profile,
        validationMode="liveMode" if validate else None,
    )

    payment_profile_id = request.make_request()

    return payment_profile_id


# TODO: implement
def delete_payment_profile(customer_id, payment_profile_id):
    request = DeleteCustomerPaymentProfileRequest(
        customerProfileId=customer_id,
        customerPaymentProfileId=payment_profile_id,
    )

    try:
        success = request.make_request()
    except AuthorizeNetException:
        return False
    else:
        return True


def create_authorization_hold(customer_id, payment_profile_id, amount, invoice,
                              customer_ip=None):
    order = Order(invoiceNumber=invoice)
    transaction = ProfileTransAuthOnly(
        amount="%.2f" % amount,
        customerProfileId=customer_id,
        customerPaymentProfileId=payment_profile_id,
        transId=None,
        order=order,
    )
    if customer_ip:
        extra = {"x_customer_ip": customer_ip}
    else:
        extra = {}

    request = CreateCustomerProfileTransactionRequest(
        transaction=transaction, extraOptions=extra)
    success, res = request.make_request()

    if (res.trans_id and
            res.response_code == TRANSACTION_ERROR and
            res.response_reason_code == TRANSACTION_DUPLICATE):
        raise DuplicateTransactionError(res.trans_id)

    if success:
        return res.trans_id
    else:
        raise TransactionError(res.response_reason_text)


def capture_authorization_hold(customer_id, payment_profile_id, amount,
                               transaction_id):
    transaction = ProfileTransPriorAuthCapture(
        amount="%.2f" % amount,
        customerProfileId=customer_id,
        customerPaymentProfileId=payment_profile_id,
        transId=transaction_id,
    )

    request = CreateCustomerProfileTransactionRequest(
        transaction=transaction)
    success, res = request.make_request()
    response_reason_code = res.get("response_reason_code")

    if success:
        return
    elif response_reason_code == TRANSACTION_NOT_FOUND:
        raise AuthorizationHoldNotFound()
    else:
        raise TransactionError(res.response_reason_text)


def void_authorization_hold(customer_id, payment_profile_id, transaction_id):
    transaction = ProfileTransVoid(
        amount=None,
        customerProfileId=customer_id,
        customerPaymentProfileId=payment_profile_id,
        transId=transaction_id,
    )

    request = CreateCustomerProfileTransactionRequest(
        transaction=transaction)
    success, res = request.make_request()

    if success:
        return res.trans_id
    else:
        raise TransactionError(res.response_reason_text)


def refund_transaction(customer_id, payment_profile_id, amount, transaction_id):
    transaction = ProfileTransRefund(
        amount="%.2f" % amount,
        customerProfileId=customer_id,
        customerPaymentProfileId=payment_profile_id,
        transId=transaction_id,
    )
    request = CreateCustomerProfileTransactionRequest(
        transaction=transaction)
    success, res = request.make_request()

    if not success:
        raise TransactionError(res.response_reason_text)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from sqlalchemy.orm.exc import MultipleResultsFound

from pylons import request
from pylons import app_globals as g

from r2.lib.db.thing import NotFound
from r2.lib.utils import Storage
from r2.lib.export import export
from r2.models.bidding import Bid, CustomerID, PayID
from r2.lib.authorize import api


__all__ = []


FREEBIE_PAYMENT_METHOD_ID = -1


@export
def get_or_create_customer_profile(user):
    profile_id = CustomerID.get_id(user._id)
    if not profile_id:
        profile_id = api.create_customer_profile(
            merchant_customer_id=user._fullname, description=user.name)
        CustomerID.set(user, profile_id)

    profile = api.get_customer_profile(profile_id)

    if not profile or profile.merchantCustomerId != user._fullname:
        raise ValueError("error getting customer profile")

    for payment_profile in profile.paymentProfiles:
        PayID.add(user, payment_profile.customerPaymentProfileId)

    return profile


def add_payment_method(user, address, credit_card, validate=False):
    profile_id = CustomerID.get_id(user._id)
    payment_method_id = api.create_payment_profile(
        profile_id, address, credit_card, validate)

    if payment_method_id:
        PayID.add(user, payment_method_id)
        return payment_method_id


def update_payment_method(user, payment_method_id, address, credit_card,
                          validate=False):
    profile_id = CustomerID.get_id(user._id)
    payment_method_id = api.update_payment_profile(
        profile_id, payment_method_id, address, credit_card, validate)
    return payment_method_id


@export
def delete_payment_method(user, payment_method_id):
    profile_id = CustomerID.get_id(user._id)
    success = api.delete_payment_profile(profile_id, payment_method_id)
    if success:
        PayID.delete(user, payment_method_id)


@export
def add_or_update_payment_method(user, address, credit_card, pay_id=None):
    if pay_id:
        return update_payment_method(user, pay_id, address, credit_card,
                                     validate=True)
    else:
        return add_payment_method(user, address, credit_card, validate=True)


@export
def is_charged_transaction(trans_id, campaign):
    if not trans_id: return False # trans_id == 0 means no bid
    try:
        bid = Bid.one(transaction=trans_id, campaign=campaign)
    except NotFound:
        return False
    except MultipleResultsFound:
        g.log.error('Multiple bids for trans_id %s' % trans_id)
        return False

    return bid.is_charged() or bid.is_refund()


@export
def auth_freebie_transaction(amount, user, link, campaign_id):
    transaction_id = -link._id

    try:
        # attempt to update existing freebie transaction
        bid = Bid.one(thing_id=link._id, transaction=transaction_id,
                      campaign=campaign_id)
    except NotFound:
        bid = Bid._new(transaction_id, user, FREEBIE_PAYMENT_METHOD_ID,
                       link._id, amount, campaign_id)
    else:
        bid.bid = amount
        bid.auth()

    return transaction_id, ""


@export
def auth_transaction(amount, user, payment_method_id, link, campaign_id):
    if payment_method_id not in PayID.get_ids(user._id):
        return None, "invalid payment method"

    profile_id = CustomerID.get_id(user._id)
    invoice = "T%dC%d" % (link._id, campaign_id)

    try:
        transaction_id = api.create_authorization_hold(
            profile_id, payment_method_id, amount, invoice, request.ip)
    except api.DuplicateTransactionError as e:
        transaction_id = e.transaction_id
        try:
            bid = Bid.one(transaction_id, campaign=campaign_id)
        except NotFound:
            bid = Bid._new(transaction_id, user, payment_method_id, link._id,
                           amount, campaign_id)
        g.log.error("%s on campaign %d" % (e.message, campaign_id))
        return transaction_id, None
    except api.TransactionError as e:
        return None, e.message

    bid = Bid._new(transaction_id, user, payment_method_id, link._id, amount,
                   campaign_id)
    return transaction_id, None


@export
def charge_transaction(user, transaction_id, campaign_id):
    bid = Bid.one(transaction=transaction_id, campaign=campaign_id)
    if bid.is_charged():
        return True, None

    if transaction_id < 0:
        bid.charged()
        return True, None

    profile_id = CustomerID.get_id(user._id)

    try:
        api.capture_authorization_hold(
            customer_id=profile_id,
            payment_profile_id=bid.pay_id,
            amount=bid.bid,
            transaction_id=transaction_id,
        )
    except api.AuthorizationHoldNotFound:
        # authorization hold has expired
        bid.void()
        return False, api.TRANSACTION_NOT_FOUND
    except api.TransactionError as e:
        return False, e.message

    bid.charged()
    return True, None


@export
def void_transaction(user, transaction_id, campaign_id):
    bid = Bid.one(transaction=transaction_id, campaign=campaign_id)

    if transaction_id <= 0:
        bid.void()
        return True, None

    profile_id = CustomerID.get_id(user._id)
    try:
        api.void_authorization_hold(profile_id, bid.pay_id, transaction_id)
    except api.TransactionError as e:
        return False, e.message

    bid.void()
    return True, None


@export
def refund_transaction(user, transaction_id, campaign_id, amount):
    bid =  Bid.one(transaction=transaction_id, campaign=campaign_id)
    if transaction_id < 0:
        bid.refund(amount)
        return True, None

    profile_id = CustomerID.get_id(user._id)
    try:
        api.refund_transaction(
            customer_id=profile_id,
            payment_profile_id=bid.pay_id,
            amount=amount,
            transaction_id=transaction_id,
        )
    except api.TransactionError as e:
        return False, e.message

    bid.refund(amount)
    return True, None
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import base64
import codecs
import ConfigParser
import cPickle as pickle
import functools
import itertools
import math
import os
import random
import re
import signal
import time
import traceback

from collections import OrderedDict
from copy import deepcopy
from datetime import date, datetime, timedelta
from decimal import Decimal
from urllib import unquote_plus, unquote
from urllib2 import urlopen, Request
from urlparse import urlparse, urlunparse

import pytz
import snudown
import unidecode
from r2.lib.utils import reddit_agent_parser

from babel.dates import TIMEDELTA_UNITS
from BeautifulSoup import BeautifulSoup, SoupStrainer
from mako.filters import url_escape
from pylons import request, config
from pylons import tmpl_context as c
from pylons import app_globals as g
from pylons.i18n import ungettext, _

from r2.lib.contrib import ipaddress
from r2.lib.filters import _force_unicode, _force_utf8
from r2.lib.require import require, require_split, RequirementException
from r2.lib.utils._utils import *

iters = (list, tuple, set)

def randstr(length,
            alphabet='abcdefghijklmnopqrstuvwxyz0123456789'):
    """Return a string made up of random chars from alphabet."""
    return ''.join(random.choice(alphabet) for _ in xrange(length))


class Storage(dict):
    """
    A Storage object is like a dictionary except `obj.foo` can be used
    in addition to `obj['foo']`.

        >>> o = storage(a=1)
        >>> o.a
        1
        >>> o['a']
        1
        >>> o.a = 2
        >>> o['a']
        2
        >>> del o.a
        >>> o.a
        Traceback (most recent call last):
            ...
        AttributeError: 'a'

    """
    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError, k:
            raise AttributeError, k

    def __setattr__(self, key, value):
        self[key] = value

    def __delattr__(self, key):
        try:
            del self[key]
        except KeyError, k:
            raise AttributeError, k

    def __repr__(self):
        return '<Storage ' + dict.__repr__(self) + '>'

storage = Storage


class Enum(Storage):
    def __init__(self, *a):
        self.name = tuple(a)
        Storage.__init__(self, ((e, i) for i, e in enumerate(a)))
    def __contains__(self, item):
        if isinstance(item, int):
            return item in self.values()
        else:
            return Storage.__contains__(self, item)


class class_property(object):
    """A decorator that combines @classmethod and @property.

    http://stackoverflow.com/a/8198300/120999
    """
    def __init__(self, function):
        self.function = function
    def __get__(self, instance, cls):
        return self.function(cls)


class Results():
    def __init__(self, sa_ResultProxy, build_fn, do_batch=False):
        self.rp = sa_ResultProxy
        self.fn = build_fn
        self.do_batch = do_batch

    @property
    def rowcount(self):
        return self.rp.rowcount

    def _fetch(self, res):
        if self.do_batch:
            return self.fn(res)
        else:
            return [self.fn(row) for row in res]

    def fetchall(self):
        return self._fetch(self.rp.fetchall())

    def fetchmany(self, n):
        rows = self._fetch(self.rp.fetchmany(n))
        if rows:
            return rows
        else:
            raise StopIteration

    def fetchone(self):
        row = self.rp.fetchone()
        if row:
            if self.do_batch:
                if isinstance(row, Storage):
                    rows = (row,)
                else:
                    rows = tup(row)
                return self.fn(rows)[0]
            else:
                return self.fn(row)
        else:
            raise StopIteration

r_base_url = re.compile("(?i)(?:.+?://)?([^#]*[^#/])/?")
r_domain = re.compile("(?i)(?:.+?://)?([^/:#?]*)")
r_domain_prefix = re.compile('^www\d*\.')


def strip_www(domain):
    stripped = domain
    if domain.count('.') > 1:
        prefix = r_domain_prefix.findall(domain)
        if domain.startswith("www") and len(prefix):
            stripped = '.'.join(domain.split('.')[1:])
    return stripped


def is_subdomain(subdomain, base):
    """Check if a domain is equal to or a subdomain of a base domain."""
    return subdomain == base or (
        subdomain is not None and subdomain.endswith('.' + base))


lang_re = re.compile(r"\A\w\w(-\w\w)?\Z")


def is_language_subdomain(subdomain):
    return lang_re.match(subdomain)


def base_url(url):
    res = r_base_url.findall(url)
    if res and res[0]:
        base = strip_www(res[0])
    else:
        base = url
    return base.lower()


def domain(url):
    """
        Takes a URL and returns the domain part, minus www., if
        present
    """
    match = r_domain.search(url)
    if match:
        domain = strip_www(match.group(1))
    else:
        domain = url
    return domain.lower()


def extract_subdomain(host=None, base_domain=None):
    """Try to extract a subdomain from the request, as compared to g.domain.

    host and base_domain exist as arguments primarily for the sake of unit
    tests, although their usage should not be considered restrained to that.
    """
    # These would be the argument defaults, but we need them evaluated at
    # run-time, not definition-time.
    if host is None:
        host = request.host
    if base_domain is None:
        base_domain = g.domain

    if not host:
        return ''

    end_index = host.find(base_domain) - 1 # For the conjoining dot.
    # Is either the requested domain the same as the base domain, or the
    # base is not a substring?
    if end_index < 0:
        return ''
    return host[:end_index]

r_path_component = re.compile(".*?/(.*)")
def path_component(s):
    """
        takes a url http://www.foo.com/i/like/cheese and returns
        i/like/cheese
    """
    res = r_path_component.findall(base_url(s))
    return (res and res[0]) or s

def get_title(url):
    """Fetch the contents of url and try to extract the page's title."""
    if not url or not url.startswith(('http://', 'https://')):
        return None

    try:
        req = Request(url)
        if g.useragent:
            req.add_header('User-Agent', g.useragent)
        opener = urlopen(req, timeout=15)

        # determine the encoding of the response
        for param in opener.info().getplist():
            if param.startswith("charset="):
                param_name, sep, charset = param.partition("=")
                codec = codecs.getreader(charset)
                break
        else:
            codec = codecs.getreader("utf-8")

        with codec(opener, "ignore") as reader:
            # Attempt to find the title in the first 1kb
            data = reader.read(1024)
            title = extract_title(data)

            # Title not found in the first kb, try searching an additional 10kb
            if not title:
                data += reader.read(10240)
                title = extract_title(data)

        return title

    except:
        return None

def extract_title(data):
    """Try to extract the page title from a string of HTML.

    An og:title meta tag is preferred, but will fall back to using
    the <title> tag instead if one is not found. If using <title>,
    also attempts to trim off the site's name from the end.
    """
    bs = BeautifulSoup(data, convertEntities=BeautifulSoup.HTML_ENTITIES)
    if not bs or not bs.html.head:
        return
    head_soup = bs.html.head

    title = None

    # try to find an og:title meta tag to use
    og_title = (head_soup.find("meta", attrs={"property": "og:title"}) or
                head_soup.find("meta", attrs={"name": "og:title"}))
    if og_title:
        title = og_title.get("content")

    # if that failed, look for a <title> tag to use instead
    if not title and head_soup.title and head_soup.title.string:
        title = head_soup.title.string

        # remove end part that's likely to be the site's name
        # looks for last delimiter char between spaces in strings
        # delimiters: |, -, emdash, endash,
        #             left- and right-pointing double angle quotation marks
        reverse_title = title[::-1]
        to_trim = re.search(u'\s[\u00ab\u00bb\u2013\u2014|-]\s',
                            reverse_title,
                            flags=re.UNICODE)

        # only trim if it won't take off over half the title
        if to_trim and to_trim.end() < len(title) / 2:
            title = title[:-(to_trim.end())]

    if not title:
        return

    # get rid of extraneous whitespace in the title
    title = re.sub(r'\s+', ' ', title, flags=re.UNICODE)

    return title.encode('utf-8').strip()

VALID_SCHEMES = ('http', 'https', 'ftp', 'mailto')
valid_dns = re.compile('\A[-a-zA-Z0-9_]+\Z')
def sanitize_url(url, require_scheme=False, valid_schemes=VALID_SCHEMES):
    """Validates that the url is of the form

    scheme://domain/path/to/content#anchor?cruft

    using the python built-in urlparse.  If the url fails to validate,
    returns None.  If no scheme is provided and 'require_scheme =
    False' is set, the url is returned with scheme 'http', provided it
    otherwise validates"""

    if not url:
        return None

    url = url.strip()
    if url.lower() == 'self':
        return url

    try:
        u = urlparse(url)
        # first pass: make sure a scheme has been specified
        if not require_scheme and not u.scheme:
            # "//example.com/"
            if u.hostname:
                prepend = "https:" if c.secure else "http:"
            # "example.com/"
            else:
                prepend = "http://"
            url = prepend + url
            u = urlparse(url)
    except ValueError:
        return None

    if not u.scheme:
        return None
    if valid_schemes is not None and u.scheme not in valid_schemes:
        return None

    # if there is a scheme and no hostname, it is a bad url.
    if not u.hostname:
        return None
    # work around CRBUG-464270
    if len(u.hostname) > 255:
        return None
    # work around for Chrome crash with "%%30%30" - Sep 2015
    if "%00" in unquote(u.path):
        return None
    if u.username is not None or u.password is not None:
        return None

    try:
        idna_hostname = u.hostname.encode('idna')
    except TypeError as e:
        g.log.warning("Bad hostname given [%r]: %s", u.hostname, e)
        raise
    except UnicodeError:
        return None

    # Make sure FQDNs like google.com. (with trailing dot) are allowed. This
    # is necessary to support linking to bare TLDs.
    if idna_hostname.endswith('.'):
        idna_hostname = idna_hostname[:-1]

    for label in idna_hostname.split('.'):
        if not re.match(valid_dns, label):
            return None

    if idna_hostname != u.hostname:
        url = urlunparse((u[0], idna_hostname, u[2], u[3], u[4], u[5]))
    return url

def trunc_string(text, max_length, suffix='...'):
    """Truncate a string, attempting to split on a word-break.

    If the first word is longer than max_length, then truncate within the word.

    Adapted from http://stackoverflow.com/a/250406/120999 .
    """
    if len(text) <= max_length:
        return text
    else:
        hard_truncated = text[:(max_length - len(suffix))]
        word_truncated = hard_truncated.rsplit(' ', 1)[0]
        return word_truncated + suffix

# Truncate a time to a certain number of minutes
# e.g, trunc_time(5:52, 30) == 5:30
def trunc_time(time, mins, hours=None):
    if hours is not None:
        if hours < 1 or hours > 60:
            raise ValueError("Hours %d is weird" % mins)
        time = time.replace(hour = hours * (time.hour / hours))

    if mins < 1 or mins > 60:
        raise ValueError("Mins %d is weird" % mins)

    return time.replace(minute = mins * (time.minute / mins),
                        second = 0,
                        microsecond = 0)

def long_datetime(datetime):
    return datetime.astimezone(g.tz).ctime() + " " + str(g.tz)

def median(l):
    if l:
        s = sorted(l)
        i = len(s) / 2
        return s[i]

def query_string(dict):
    pairs = []
    for k,v in dict.iteritems():
        if v is not None:
            try:
                k = url_escape(_force_unicode(k))
                v = url_escape(_force_unicode(v))
                pairs.append(k + '=' + v)
            except UnicodeDecodeError:
                continue
    if pairs:
        return '?' + '&'.join(pairs)
    else:
        return ''


# Characters that might cause parsing differences in different implementations
# Spaces only seem to cause parsing differences when occurring directly before
# the scheme
URL_PROBLEMATIC_RE = re.compile(
    ur'(\A\x20|[\x00-\x19\xA0\u1680\u180E\u2000-\u2029\u205f\u3000\\])',
    re.UNICODE
)


def paranoid_urlparser_method(check):
    """
    Decorator for checks on `UrlParser` instances that need to be paranoid
    """
    def check_wrapper(parser, *args, **kwargs):
        return UrlParser.perform_paranoid_check(parser, check, *args, **kwargs)

    return check_wrapper


class UrlParser(object):
    """
    Wrapper for urlparse and urlunparse for making changes to urls.

    All attributes present on the tuple-like object returned by
    urlparse are present on this class, and are setable, with the
    exception of netloc, which is instead treated via a getter method
    as a concatenation of hostname and port.

    Unlike urlparse, this class allows the query parameters to be
    converted to a dictionary via the query_dict method (and
    correspondingly updated via update_query).  The extension of the
    path can also be set and queried.

    The class also contains reddit-specific functions for setting,
    checking, and getting a path's subreddit.
    """

    __slots__ = ['scheme', 'path', 'params', 'query',
                 'fragment', 'username', 'password', 'hostname', 'port',
                 '_orig_url', '_orig_netloc', '_query_dict']

    valid_schemes = ('http', 'https', 'ftp', 'mailto')

    def __init__(self, url):
        u = urlparse(url)
        for s in self.__slots__:
            if hasattr(u, s):
                setattr(self, s, getattr(u, s))
        self._orig_url    = url
        self._orig_netloc = getattr(u, 'netloc', '')
        self._query_dict  = None

    def __eq__(self, other):
        """A loose equality method for UrlParsers.

        In particular, this returns true for UrlParsers whose resultant urls
        have the same query parameters, but in a different order.  These are
        treated the same most of the time, but if you need strict equality,
        compare the string results of unparse().
        """
        if not isinstance(other, UrlParser):
            return False

        (s_scheme, s_netloc, s_path, s_params, s_query, s_fragment) = self._unparse()
        (o_scheme, o_netloc, o_path, o_params, o_query, o_fragment) = other._unparse()
        # Check all the parsed components for equality, except the query, which
        # is easier to check in its pure-dictionary form.
        if (s_scheme != o_scheme or
                s_netloc != o_netloc or
                s_path != o_path or
                s_params != o_params or
                s_fragment != o_fragment):
            return False
        # Coerce query dicts from OrderedDicts to standard dicts to avoid an
        # order-sensitive comparison.
        if dict(self.query_dict) != dict(other.query_dict):
            return False

        return True

    def update_query(self, **updates):
        """Add or change query parameters."""
        # Since in HTTP everything's a string, coercing values to strings now
        # makes equality testing easier.  Python will throw an error if you try
        # to pass in a non-string key, so that's already taken care of for us.
        updates = {k: _force_unicode(v) for k, v in updates.iteritems()}
        self.query_dict.update(updates)

    @property
    def query_dict(self):
        """A dictionary of the current query parameters.

        Keys and values pulled from the original url are un-url-escaped.

        Modifying this function's return value will result in changes to the
        unparse()-d url, but it's recommended instead to make any changes via
        `update_query()`.
        """
        if self._query_dict is None:
            def _split(param):
                p = param.split('=')
                return (unquote_plus(p[0]),
                        unquote_plus('='.join(p[1:])))
            self._query_dict = OrderedDict(
                                 _split(p) for p in self.query.split('&') if p)
        return self._query_dict

    def path_extension(self):
        """Fetches the current extension of the path.

        If the url does not end in a file or the file has no extension, returns
        an empty string.
        """
        filename = self.path.split('/')[-1]
        filename_parts = filename.split('.')
        if len(filename_parts) == 1:
            return ''

        return filename_parts[-1]

    def has_image_extension(self):
        """Guess if the url leads to an image."""
        extension = self.path_extension().lower()
        return extension in {'gif', 'jpeg', 'jpg', 'png', 'tiff'}

    def has_static_image_extension(self):
        """Guess if the url leads to a non-animated image."""
        extension = self.path_extension().lower()
        return extension in {'jpeg', 'jpg', 'png', 'tiff'}

    def set_extension(self, extension):
        """
        Changes the extension of the path to the provided value (the
        "." should not be included in the extension as a "." is
        provided)
        """
        pieces = self.path.split('/')
        dirs = pieces[:-1]
        base = pieces[-1].split('.')
        base = '.'.join(base[:-1] if len(base) > 1 else base)
        if extension:
            base += '.' + extension
        dirs.append(base)
        self.path =  '/'.join(dirs)
        return self

    def canonicalize(self):
        subdomain = extract_subdomain(self.hostname)
        if subdomain == '' or is_language_subdomain(subdomain):
            self.hostname = 'www.{0}'.format(g.domain)
        if not self.path.endswith('/'):
            self.path += '/'
        self.scheme = 'https'

    def switch_subdomain_by_extension(self, extension=None):
        """Change the subdomain to the one that fits an extension.

        This should only be used on reddit URLs.

        Arguments:

        * extension: the template extension to which the middleware hints when
          parsing the subdomain resulting from this function.

        >>> u = UrlParser('http://www.reddit.com/r/redditdev')
        >>> u.switch_subdomain_by_extension('compact')
        >>> u.unparse()
        'http://i.reddit.com/r/redditdev'

        If `extension` is not provided or does not match any known extensions,
        the default subdomain (`g.domain_prefix`) will be used.

        Note that this will not remove any existing extensions; if you want to
        ensure the explicit extension does not override the subdomain hint, you
        should call `set_extension('')` first.
        """
        new_subdomain = g.domain_prefix
        for subdomain, subdomain_extension in g.extension_subdomains.iteritems():
            if extension == subdomain_extension:
                new_subdomain = subdomain
                break
        self.hostname = '%s.%s' % (new_subdomain, g.domain)

    def unparse(self):
        """
        Converts the url back to a string, applying all updates made
        to the fields thereof.

        Note: if a host name has been added and none was present
        before, will enforce scheme -> "http" unless otherwise
        specified.  Double-slashes are removed from the resultant
        path, and the query string is reconstructed only if the
        query_dict has been modified/updated.
        """
        return urlunparse(self._unparse())

    def _unparse(self):
        q = query_string(self.query_dict).lstrip('?')

        # make sure the port is not doubly specified
        if getattr(self, 'port', None) and ":" in self.hostname:
            self.hostname = self.hostname.split(':')[0]

        # if there is a netloc, there had better be a scheme
        if self.netloc and not self.scheme:
            self.scheme = "http"

        return (self.scheme, self.netloc,
                self.path.replace('//', '/'),
                self.params, q, self.fragment)

    def path_has_subreddit(self):
        """
        utility method for checking if the path starts with a
        subreddit specifier (namely /r/ or /subreddits/).
        """
        return self.path.startswith(('/r/', '/subreddits/', '/reddits/'))

    def get_subreddit(self):
        """checks if the current url refers to a subreddit and returns
        that subreddit object.  The cases here are:

          * the hostname is unset or is g.domain, in which case it
            looks for /r/XXXX or /subreddits.  The default in this case
            is Default.
          * the hostname is a cname to a known subreddit.

        On failure to find a subreddit, returns None.
        """
        from r2.models import Subreddit, NotFound, DefaultSR
        try:
            if (not self.hostname or
                    is_subdomain(self.hostname, g.domain) or
                    self.hostname.startswith(g.domain)):
                if self.path.startswith('/r/'):
                    return Subreddit._by_name(self.path.split('/')[2])
                else:
                    return DefaultSR()
            elif self.hostname:
                return Subreddit._by_domain(self.hostname)
        except NotFound:
            pass
        return None

    def perform_paranoid_check(self, check, *args, **kwargs):
        """
        Perform a check on a URL that needs to account for bugs in `unparse()`

        If you need to account for quirks in browser URL parsers, you should
        use this along with `is_web_safe_url()`. Trying to parse URLs like
        a browser would just makes things really hairy.
        """
        variants_to_check = (
            self,
            UrlParser(self.unparse())
        )
        # If the check doesn't pass on *every* variant, it's a fail.
        return all(
            check(variant, *args, **kwargs) for variant in variants_to_check
        )

    @paranoid_urlparser_method
    def is_web_safe_url(self):
        """Determine if this URL could cause issues with different parsers"""

        # There's no valid reason for this, and just serves to confuse UAs.
        # and urllib2.
        if self._orig_url.startswith("///"):
            return False

        # Double-checking the above
        if not self.hostname and self.path.startswith('//'):
            return False

        # A host-relative link with a scheme like `https:/baz` or `https:?quux`
        if self.scheme and not self.hostname:
            return False

        # Credentials in the netloc? Not on reddit!
        if "@" in self._orig_netloc:
            return False

        # `javascript://www.reddit.com/%0D%Aalert(1)` is not safe, obviously
        if self.scheme and self.scheme.lower() not in self.valid_schemes:
            return False

        # Reject any URLs that contain characters known to cause parsing
        # differences between parser implementations
        for match in re.finditer(URL_PROBLEMATIC_RE, self._orig_url):
            # XXX: Yuck. We have non-breaking spaces in title slugs! They
            # should be safe enough to allow after three slashes. Opera 12's the
            # only browser that trips over them, and it doesn't fall for
            # `http:///foo.com/`.
            # Check both in case unicode promotion fails
            if match.group(0) in {u'\xa0', '\xa0'}:
                if match.string[0:match.start(0)].count('/') < 3:
                    return False
            else:
                return False

        return True

    def is_reddit_url(self, subreddit=None):
        """utility method for seeing if the url is associated with
        reddit as we don't necessarily want to mangle non-reddit
        domains

        returns true only if hostname is nonexistant, a subdomain of
        g.domain, or a subdomain of the provided subreddit's cname.
        """

        valid_subdomain = (
            not self.hostname or
            is_subdomain(self.hostname, g.domain) or
            (subreddit and subreddit.domain and
                is_subdomain(self.hostname, subreddit.domain))
        )

        if not valid_subdomain or not self.hostname or not g.offsite_subdomains:
            return valid_subdomain
        return not any(
            is_subdomain(self.hostname, "%s.%s" % (subdomain, g.domain))
            for subdomain in g.offsite_subdomains
        )

    def path_add_subreddit(self, subreddit):
        """
        Adds the subreddit's path to the path if another subreddit's
        prefix is not already present.
        """
        if not (self.path_has_subreddit()
                or self.path.startswith(subreddit.user_path)):
            self.path = (subreddit.user_path + self.path)
        return self

    @property
    def netloc(self):
        """
        Getter method which returns the hostname:port, or empty string
        if no hostname is present.
        """
        if not self.hostname:
            return ""
        elif getattr(self, "port", None):
            return self.hostname + ":" + str(self.port)
        return self.hostname

    def __repr__(self):
        return "<URL %s>" % repr(self.unparse())

    def domain_permutations(self, fragments=False, subdomains=True):
        """
          Takes a domain like `www.reddit.com`, and returns a list of ways
          that a user might search for it, like:
          * www
          * reddit
          * com
          * www.reddit.com
          * reddit.com
          * com
        """
        ret = set()
        if self.hostname:
            r = self.hostname.split('.')

            if subdomains:
                for x in xrange(len(r)-1):
                    ret.add('.'.join(r[x:len(r)]))

            if fragments:
                for x in r:
                    ret.add(x)

        return ret

    @classmethod
    def base_url(cls, url):
        u = cls(url)

        # strip off any www and lowercase the hostname:
        netloc = strip_www(u.netloc.lower())

        # http://code.google.com/web/ajaxcrawling/docs/specification.html
        fragment = u.fragment if u.fragment.startswith("!") else ""

        return urlunparse((u.scheme.lower(), netloc,
                           u.path, u.params, u.query, fragment))


def coerce_url_to_protocol(url, protocol='http'):
    '''Given an absolute (but potentially protocol-relative) url, coerce it to
    a protocol.'''
    parsed_url = UrlParser(url)
    parsed_url.scheme = protocol
    return parsed_url.unparse()

def url_is_embeddable_image(url):
    """The url is on an oembed-friendly domain and looks like an image."""
    parsed_url = UrlParser(url)

    if parsed_url.path_extension().lower() in {"jpg", "gif", "png", "jpeg"}:
        if parsed_url.hostname not in g.known_image_domains:
            return False
        return True

    return False


def url_to_thing(url):
    """Given a reddit URL, return the Thing to which it associates.

    Examples:
        /r/somesr - Subreddit
        /r/somesr/comments/j2jx - Link
        /r/somesr/comments/j2jx/slug/k2js - Comment
    """
    from r2.models import Comment, Link, Message, NotFound, Subreddit, Thing
    from r2.config.middleware import SubredditMiddleware
    sr_pattern = SubredditMiddleware.sr_pattern

    urlparser = UrlParser(_force_utf8(url))
    if not urlparser.is_reddit_url():
        return None

    try:
        sr_name = sr_pattern.match(urlparser.path).group(1)
    except AttributeError:
        sr_name = None

    path = sr_pattern.sub('', urlparser.path)
    if not path or path == '/':
        if not sr_name:
            return None

        try:
            return Subreddit._by_name(sr_name, data=True)
        except NotFound:
            return None

    # potential TypeError raised here because of environ being None
    # when calling outside of app context
    try:
        route_dict = config['routes.map'].match(path)
    except TypeError:
        return None

    if not route_dict:
        return None

    try:
        comment = route_dict.get('comment')
        if comment:
            return Comment._byID36(comment, data=True)

        article = route_dict.get('article')
        if article:
            return Link._byID36(article, data=True)

        msg = route_dict.get('mid')
        if msg:
            return Message._byID36(msg, data=True)
    except (NotFound, ValueError):
        return None

    return None


def pload(fname, default = None):
    "Load a pickled object from a file"
    try:
        f = file(fname, 'r')
        d = pickle.load(f)
    except IOError:
        d = default
    else:
        f.close()
    return d

def psave(fname, d):
    "Save a pickled object into a file"
    f = file(fname, 'w')
    pickle.dump(d, f)
    f.close()

def unicode_safe(res):
    try:
        return str(res)
    except UnicodeEncodeError:
        try:
            return unicode(res).encode('utf-8')
        except UnicodeEncodeError:
            return res.decode('utf-8').encode('utf-8')

def decompose_fullname(fullname):
    """
        decompose_fullname("t3_e4fa") ->
            (Thing, 3, 658918)
    """
    from r2.lib.db.thing import Thing,Relation
    if fullname[0] == 't':
        type_class = Thing
    elif fullname[0] == 'r':
        type_class = Relation

    type_id36, thing_id36 = fullname[1:].split('_')

    type_id = int(type_id36,36)
    id      = int(thing_id36,36)

    return (type_class, type_id, id)

def cols(lst, ncols):
    """divides a list into columns, and returns the
    rows. e.g. cols('abcdef', 2) returns (('a', 'd'), ('b', 'e'), ('c',
    'f'))"""
    nrows = int(math.ceil(1.*len(lst) / ncols))
    lst = lst + [None for i in range(len(lst), nrows*ncols)]
    cols = [lst[i:i+nrows] for i in range(0, nrows*ncols, nrows)]
    rows = zip(*cols)
    rows = [filter(lambda x: x is not None, r) for r in rows]
    return rows

def fetch_things(t_class,since,until,batch_fn=None,
                 *query_params, **extra_query_dict):
    """
        Simple utility function to fetch all Things of class t_class
        (spam or not, but not deleted) that were created from 'since'
        to 'until'
    """

    from r2.lib.db.operators import asc

    if not batch_fn:
        batch_fn = lambda x: x

    query_params = ([t_class.c._date >= since,
                     t_class.c._date <  until,
                     t_class.c._spam == (True,False)]
                    + list(query_params))
    query_dict   = {'sort':  asc('_date'),
                    'limit': 100,
                    'data':  True}
    query_dict.update(extra_query_dict)

    q = t_class._query(*query_params,
                        **query_dict)

    orig_rules = deepcopy(q._rules)

    things = list(q)
    while things:
        things = batch_fn(things)
        for t in things:
            yield t
        q._rules = deepcopy(orig_rules)
        q._after(t)
        things = list(q)


def fetch_things2(query, chunk_size = 100, batch_fn = None, chunks = False):
    """Incrementally run query with a limit of chunk_size until there are
    no results left. batch_fn transforms the results for each chunk
    before returning."""

    assert query._sort, "you must specify the sort order in your query!"

    orig_rules = deepcopy(query._rules)
    query._limit = chunk_size
    items = list(query)
    done = False
    while items and not done:
        #don't need to query again at the bottom if we didn't get enough
        if len(items) < chunk_size:
            done = True

        after = items[-1]

        if batch_fn:
            items = batch_fn(items)

        if chunks:
            yield items
        else:
            for i in items:
                yield i

        if not done:
            query._rules = deepcopy(orig_rules)
            query._after(after)
            items = list(query)


def exponential_retrier(func_to_retry,
                        exception_filter=lambda *args, **kw: True,
                        retry_min_wait_ms=500,
                        max_retries=5):
    """Call func_to_retry and return it's results.
    If func_to_retry throws an exception, retry.

    :param Function func_to_retry: Function to execute
        and possibly retry.
    :param exception_filter:  Only retry exceptions for
        which this function returns True.  Always returns True by default.
    :param int retry_min_wait_ms: Initial wait period
        if an exception happens in milliseconds.
        After each retry this value will be multiplied by 2
        thus achieving exponential backoff algorithm.
    :param int max_retries:  How many times to wait before
        just re-throwing last exception.
        Value of zero would result in no retry attempts.
    """
    sleep_time = retry_min_wait_ms
    num_retried = 0
    while True:
        try:
            return func_to_retry()
        # StopIteration should never be retried as its part of regular logic.
        except StopIteration:
            raise
        except Exception as e:
            g.log.exception("%d number retried" % num_retried)
            num_retried += 1
            # if we ran out of retries or this Exception
            # shouldnt be retried then raise the exception instead of sleeping
            if num_retried > max_retries or not exception_filter(e):
                raise

            # convert to ms.  Use floating point literal for int -> float
            time.sleep(sleep_time / 1000.0)
            sleep_time *= 2


def fetch_things_with_retry(query,
                            chunk_size=100,
                            batch_fn=None,
                            chunks=False,
                            retry_min_wait_ms=500,
                            max_retries=0):
    """Incrementally run query with a limit of chunk_size until there are
    no results left. batch_fn transforms the results for each chunk
    before returning.

    If a query at some point generates an exception
    retry it using exponential backoff.

    By default retrying is turned off."""

    assert query._sort, "you must specify the sort order in your query!"

    retrier = functools.partial(exponential_retrier,
                                retry_min_wait_ms=retry_min_wait_ms,
                                max_retries=max_retries)

    orig_rules = deepcopy(query._rules)
    query._limit = chunk_size
    items = retrier(lambda: list(query))

    done = False
    while items and not done:
        # don't need to query again at the bottom if we didn't get enough
        if len(items) < chunk_size:
            done = True

        after = items[-1]

        if batch_fn:
                items = batch_fn(items)

        if chunks:
            yield items
        else:
            for i in items:
                yield i

        if not done:
            query._rules = deepcopy(orig_rules)
            query._after(after)
            items = retrier(lambda: list(query))


def fix_if_broken(thing, delete = True, fudge_links = False):
    from r2.models import Link, Comment, Subreddit, Message

    # the minimum set of attributes that are required
    attrs = dict((cls, cls._essentials)
                 for cls
                 in (Link, Comment, Subreddit, Message))

    if thing.__class__ not in attrs:
        raise TypeError

    for attr in attrs[thing.__class__]:
        try:
            # try to retrieve the attribute
            getattr(thing, attr)
        except AttributeError:
            if not delete:
                raise

            if isinstance(thing, Link) and fudge_links:
                if attr == "sr_id":
                    thing.sr_id = 6
                    print "Fudging %s.sr_id to %d" % (thing._fullname,
                                                      thing.sr_id)
                elif attr == "author_id":
                    thing.author_id = 8244672
                    print "Fudging %s.author_id to %d" % (thing._fullname,
                                                          thing.author_id)
                else:
                    print "Got weird attr %s; can't fudge" % attr

            if not thing._deleted:
                print "%s is missing %r, deleting" % (thing._fullname, attr)
                thing._deleted = True

            thing._commit()

            if not fudge_links:
                break


def find_recent_broken_things(from_time = None, to_time = None,
                              delete = False):
    """
        Occasionally (usually during app-server crashes), Things will
        be partially written out to the database. Things missing data
        attributes break the contract for these things, which often
        breaks various pages. This function hunts for and destroys
        them as appropriate.
    """
    from r2.models import Link, Comment
    from r2.lib.db.operators import desc

    from_time = from_time or timeago('1 hour')
    to_time = to_time or datetime.now(g.tz)

    for cls in (Link, Comment):
        q = cls._query(cls.c._date > from_time,
                       cls.c._date < to_time,
                       data=True,
                       sort=desc('_date'))
        for thing in fetch_things2(q):
            fix_if_broken(thing, delete = delete)


def timeit(func):
    "Run some function, and return (RunTimeInSeconds,Result)"
    before=time.time()
    res=func()
    return (time.time()-before,res)
def lineno():
    "Returns the current line number in our program."
    import inspect
    print "%s\t%s" % (datetime.now(),inspect.currentframe().f_back.f_lineno)

def IteratorFilter(iterator, fn):
    for x in iterator:
        if fn(x):
            yield x

def UniqueIterator(iterator, key = lambda x: x):
    """
    Takes an iterator and returns an iterator that returns only the
    first occurence of each entry
    """
    so_far = set()
    def no_dups(x):
        k = key(x)
        if k in so_far:
            return False
        else:
            so_far.add(k)
            return True

    return IteratorFilter(iterator, no_dups)

def safe_eval_str(unsafe_str):
    return unsafe_str.replace('\\x3d', '=').replace('\\x26', '&')

rx_whitespace = re.compile('\s+', re.UNICODE)
rx_notsafe = re.compile('\W+', re.UNICODE)
rx_underscore = re.compile('_+', re.UNICODE)
def title_to_url(title, max_length = 50):
    """Takes a string and makes it suitable for use in URLs"""
    title = _force_unicode(title)           #make sure the title is unicode
    title = rx_whitespace.sub('_', title)   #remove whitespace
    title = rx_notsafe.sub('', title)       #remove non-printables
    title = rx_underscore.sub('_', title)   #remove double underscores
    title = title.strip('_')                #remove trailing underscores
    title = title.lower()                   #lowercase the title

    if len(title) > max_length:
        #truncate to nearest word
        title = title[:max_length]
        last_word = title.rfind('_')
        if (last_word > 0):
            title = title[:last_word]
    return title or "_"


def unicode_title_to_ascii(title, max_length=50):
    title = _force_unicode(title)
    title = unidecode.unidecode(title)
    return title_to_url(title, max_length)


def dbg(s):
    import sys
    sys.stderr.write('%s\n' % (s,))

def trace(fn):
    def new_fn(*a,**kw):
        ret = fn(*a,**kw)
        dbg("Fn: %s; a=%s; kw=%s\nRet: %s"
            % (fn,a,kw,ret))
        return ret
    return new_fn

def common_subdomain(domain1, domain2):
    if not domain1 or not domain2:
        return ""
    domain1 = domain1.split(":")[0]
    domain2 = domain2.split(":")[0]
    if len(domain1) > len(domain2):
        domain1, domain2 = domain2, domain1

    if domain1 == domain2:
        return domain1
    else:
        dom = domain1.split(".")
        for i in range(len(dom), 1, -1):
            d = '.'.join(dom[-i:])
            if domain2.endswith(d):
                return d
    return ""


def url_links_builder(url, exclude=None, num=None, after=None, reverse=None,
                      count=None, public_srs_only=False):
    from r2.lib.template_helpers import add_sr
    from r2.models import IDBuilder, Link, NotFound, Subreddit
    from operator import attrgetter

    if url.startswith('/'):
        url = add_sr(url, force_hostname=True)

    try:
        links = Link._by_url(url, None)
    except NotFound:
        links = []

    links = [ link for link in links
                   if link._fullname != exclude ]

    if public_srs_only and not c.user_is_admin:
        subreddits = Subreddit._byID([link.sr_id for link in links], data=True)
        links = [link for link in links
                 if subreddits[link.sr_id].type not in Subreddit.private_types]

    links.sort(key=attrgetter('num_comments'), reverse=True)

    # don't show removed links in duplicates unless admin or mod
    # or unless it's your own post
    def include_link(link):
        return (not link._spam or
                (c.user_is_loggedin and
                    (link.author_id == c.user._id or
                        c.user_is_admin or
                        link.subreddit.is_moderator(c.user))))

    builder = IDBuilder([link._fullname for link in links], skip=True,
                        keep_fn=include_link, num=num, after=after,
                        reverse=reverse, count=count)

    return builder

class TimeoutFunctionException(Exception):
    pass

class TimeoutFunction:
    """Force an operation to timeout after N seconds. Works with POSIX
       signals, so it's not safe to use in a multi-treaded environment"""
    def __init__(self, function, timeout):
        self.timeout = timeout
        self.function = function

    def handle_timeout(self, signum, frame):
        raise TimeoutFunctionException()

    def __call__(self, *args, **kwargs):
        # can only be called from the main thread
        old = signal.signal(signal.SIGALRM, self.handle_timeout)
        signal.alarm(self.timeout)
        try:
            result = self.function(*args, **kwargs)
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old)
        return result


def to_date(d):
    if isinstance(d, datetime):
        return d.date()
    return d

def to_datetime(d):
    if type(d) == date:
        return datetime(d.year, d.month, d.day)
    return d

def in_chunks(it, size=25):
    chunk = []
    it = iter(it)
    try:
        while True:
            chunk.append(it.next())
            if len(chunk) >= size:
                yield chunk
                chunk = []
    except StopIteration:
        if chunk:
            yield chunk


def progress(it, verbosity=100, key=repr, estimate=None, persec=True):
    """An iterator that yields everything from `it', but prints progress
       information along the way, including time-estimates if
       possible"""
    from itertools import islice
    from datetime import datetime
    import sys

    now = start = datetime.now()
    elapsed = start - start

    # try to guess at the estimate if we can
    if estimate is None:
        try:
            estimate = len(it)
        except:
            pass

    def timedelta_to_seconds(td):
        return td.days * (24*60*60) + td.seconds + (float(td.microseconds) / 1000000)
    def format_timedelta(td, sep=''):
        ret = []
        s = timedelta_to_seconds(td)
        if s < 0:
            neg = True
            s *= -1
        else:
            neg = False

        if s >= (24*60*60):
            days = int(s//(24*60*60))
            ret.append('%dd' % days)
            s -= days*(24*60*60)
        if s >= 60*60:
            hours = int(s//(60*60))
            ret.append('%dh' % hours)
            s -= hours*(60*60)
        if s >= 60:
            minutes = int(s//60)
            ret.append('%dm' % minutes)
            s -= minutes*60
        if s >= 1:
            seconds = int(s)
            ret.append('%ds' % seconds)
            s -= seconds

        if not ret:
            return '0s'

        return ('-' if neg else '') + sep.join(ret)
    def format_datetime(dt, show_date=False):
        if show_date:
            return dt.strftime('%Y-%m-%d %H:%M')
        else:
            return dt.strftime('%H:%M:%S')
    def deq(dt1, dt2):
        "Indicates whether the two datetimes' dates describe the same (day,month,year)"
        d1, d2 = dt1.date(), dt2.date()
        return (    d1.day   == d2.day
                and d1.month == d2.month
                and d1.year  == d2.year)

    sys.stderr.write('Starting at %s\n' % (start,))

    # we're going to islice it so we need to start an iterator
    it = iter(it)

    seen = 0
    while True:
        this_chunk = 0
        thischunk_started = datetime.now()

        # the simple bit: just iterate and yield
        for item in islice(it, verbosity):
            this_chunk += 1
            seen += 1
            yield item

        if this_chunk < verbosity:
            # we're done, the iterator is empty
            break

        now = datetime.now()
        elapsed = now - start
        thischunk_seconds = timedelta_to_seconds(now - thischunk_started)

        if estimate:
            # the estimate is based on the total number of items that
            # we've processed in the total amount of time that's
            # passed, so it should smooth over momentary spikes in
            # speed (but will take a while to adjust to long-term
            # changes in speed)
            remaining = ((elapsed/seen)*estimate)-elapsed
            completion = now + remaining
            count_str = ('%d/%d %.2f%%'
                         % (seen, estimate, float(seen)/estimate*100))
            completion_str = format_datetime(completion, not deq(completion,now))
            estimate_str = (' (%s remaining; completion %s)'
                            % (format_timedelta(remaining),
                               completion_str))
        else:
            count_str = '%d' % seen
            estimate_str = ''

        if key:
            key_str = ': %s' % key(item)
        else:
            key_str = ''

        # unlike the estimate, the persec count is the number per
        # second for *this* batch only, without smoothing
        if persec and thischunk_seconds > 0:
            persec_str = ' (%.1f/s)' % (float(this_chunk)/thischunk_seconds,)
        else:
            persec_str = ''

        sys.stderr.write('%s%s, %s%s%s\n'
                         % (count_str, persec_str,
                            format_timedelta(elapsed), estimate_str, key_str))

    now = datetime.now()
    elapsed = now - start
    elapsed_seconds = timedelta_to_seconds(elapsed)
    if persec and seen > 0 and elapsed_seconds > 0:
        persec_str = ' (@%.1f/sec)' % (float(seen)/elapsed_seconds)
    else:
        persec_str = ''
    sys.stderr.write('Processed %d%s items in %s..%s (%s)\n'
                     % (seen,
                        persec_str,
                        format_datetime(start, not deq(start, now)),
                        format_datetime(now, not deq(start, now)),
                        format_timedelta(elapsed)))

class Hell(object):
    def __str__(self):
        return "boom!"

class Bomb(object):
    @classmethod
    def __getattr__(cls, key):
        raise Hell()

    @classmethod
    def __setattr__(cls, key, val):
        raise Hell()

    @classmethod
    def __repr__(cls):
        raise Hell()


class SimpleSillyStub(object):
    """A simple stub object that does nothing when you call its methods."""
    def __nonzero__(self):
        return False

    def __getattr__(self, name):
        return self.stub

    def stub(self, *args, **kwargs):
        pass

    __exit__ = __enter__ = stub


def strordict_fullname(item, key='fullname'):
    """Sometimes we migrate AMQP queues from simple strings to pickled
    dictionaries. During the migratory period there may be items in
    the queue of both types, so this function tries to detect which
    the item is. It shouldn't really be used on a given queue for more
    than a few hours or days"""
    try:
        d = pickle.loads(item)
    except:
        d = {key: item}

    if (not isinstance(d, dict)
        or key not in d
        or not isinstance(d[key], str)):
        raise ValueError('Error trying to migrate %r (%r)'
                         % (item, d))

    return d

def thread_dump(*a):
    import sys, traceback
    from datetime import datetime

    sys.stderr.write('%(t)s Thread Dump @%(d)s %(t)s\n' % dict(t='*'*15,
                                                               d=datetime.now()))

    for thread_id, stack in sys._current_frames().items():
        sys.stderr.write('\t-- Thread ID: %s--\n' %  (thread_id,))

        for filename, lineno, fnname, line in traceback.extract_stack(stack):
            sys.stderr.write('\t\t%(filename)s(%(lineno)d): %(fnname)s\n'
                             % dict(filename=filename, lineno=lineno, fnname=fnname))
            sys.stderr.write('\t\t\t%(line)s\n' % dict(line=line))


def constant_time_compare(actual, expected):
    """
    Returns True if the two strings are equal, False otherwise

    The time taken is dependent on the number of characters provided
    instead of the number of characters that match.

    When we upgrade to Python 2.7.7 or newer, we should use hmac.compare_digest
    instead.
    """
    actual_len   = len(actual)
    expected_len = len(expected)
    result = actual_len ^ expected_len
    if expected_len > 0:
        for i in xrange(actual_len):
            result |= ord(actual[i]) ^ ord(expected[i % expected_len])
    return result == 0


def extract_urls_from_markdown(md):
    "Extract URLs that will be hot links from a piece of raw Markdown."

    html = snudown.markdown(_force_utf8(md))
    links = SoupStrainer("a")

    for link in BeautifulSoup(html, parseOnlyThese=links):
        url = link.get('href')
        if url:
            yield url


def extract_user_mentions(text):
    """Return a set of all usernames (lowercased) mentioned in Markdown text.

    This function works by processing the Markdown, and then looking through
    all links in the resulting HTML. Any links that start with /u/ (as a
    relative link) are considered to be a "mention", so this will mostly just
    catch the links created by our auto-linking of /u/ and u/.

    Note that the usernames are converted to lowercase and added to a set,
    so only unique mentions will be returned.
    """
    from r2.lib.validator import chkuser
    usernames = set()

    for url in extract_urls_from_markdown(text):
        if not url.startswith("/u/"):
            continue

        username = url[len("/u/"):]
        if not chkuser(username):
            continue

        usernames.add(username.lower())

    return usernames


def summarize_markdown(md):
    """Get the first paragraph of some Markdown text, potentially truncated."""

    first_graf, sep, rest = md.partition("\n\n")
    return first_graf[:500]


def blockquote_text(text):
    """Wrap a chunk of Markdown text into a blockquote."""
    return "\n".join("> " + line for line in text.splitlines())


def find_containing_network(ip_ranges, address):
    """Find an IP network that contains the given address."""
    addr = ipaddress.ip_address(address)
    for network in ip_ranges:
        if addr in network:
            return network
    return None


def is_throttled(address):
    """Determine if an IP address is in a throttled range."""
    return bool(find_containing_network(g.throttles, address))


def parse_http_basic(authorization_header):
    """Parse the username/credentials out of an HTTP Basic Auth header.

    Raises RequirementException if anything is uncool.
    """
    auth_scheme, auth_token = require_split(authorization_header, 2)
    require(auth_scheme.lower() == "basic")
    try:
        auth_data = base64.b64decode(auth_token)
    except TypeError:
        raise RequirementException
    return require_split(auth_data, 2, ":")


def simple_traceback(limit):
    """Generate a pared-down traceback that's human readable but small.

    `limit` is how many frames of the stack to put in the traceback.

    """

    stack_trace = traceback.extract_stack(limit=limit)[:-2]
    return "\n".join("-".join((os.path.basename(filename),
                               function_name,
                               str(line_number),
                              ))
                     for filename, line_number, function_name, text
                     in stack_trace)


def weighted_lottery(weights, _random=random.random):
    """Randomly choose a key from a dict where values are weights.

    Weights should be non-negative numbers, and at least one weight must be
    non-zero. The probability that a key will be selected is proportional to
    its weight relative to the sum of all weights. Keys with zero weight will
    be ignored.

    Raises ValueError if weights is empty or contains a negative weight.
    """

    total = sum(weights.itervalues())
    if total <= 0:
        raise ValueError("total weight must be positive")

    r = _random() * total
    t = 0
    for key, weight in weights.iteritems():
        if weight < 0:
            raise ValueError("weight for %r must be non-negative" % key)
        t += weight
        if t > r:
            return key

    # this point should never be reached
    raise ValueError(
        "weighted_lottery messed up: r=%r, t=%r, total=%r" % (r, t, total))


class GoldPrice(object):
    """Simple price math / formatting type.

    Prices are assumed to be USD at the moment.

    """
    def __init__(self, decimal):
        self.decimal = Decimal(decimal)

    def __mul__(self, other):
        return type(self)(self.decimal * other)

    def __div__(self, other):
        return type(self)(self.decimal / other)

    def __str__(self):
        return "$%s" % self.decimal.quantize(Decimal("1.00"))

    def __repr__(self):
        return "%s(%s)" % (type(self).__name__, self)

    @property
    def pennies(self):
        return int(self.decimal * 100)


def config_gold_price(v, key=None, data=None):
    return GoldPrice(v)


def canonicalize_email(email):
    """Return the given email address without various localpart manglings.

    a.s.d.f+something@gmail.com --> asdf@gmail.com

    This is not at all RFC-compliant or correct. It's only intended to be a
    quick heuristic to remove commonly used mangling techniques.

    """

    if not email:
        return ""

    email = _force_utf8(email.lower())

    localpart, at, domain = email.partition("@")
    if not at or "@" in domain:
        return ""

    localpart = localpart.replace(".", "")
    localpart = localpart.partition("+")[0]

    return localpart + "@" + domain


def precise_format_timedelta(delta, locale, threshold=.85, decimals=2):
    """Like babel.dates.format_datetime but with adjustable precision"""
    seconds = delta.total_seconds()

    for unit, secs_per_unit in TIMEDELTA_UNITS:
        value = abs(seconds) / secs_per_unit
        if value >= threshold:
            plural_form = locale.plural_form(value)
            pattern = None
            for choice in (unit + ':medium', unit):
                patterns = locale._data['unit_patterns'].get(choice)
                if patterns is not None:
                    pattern = patterns[plural_form]
                    break
            if pattern is None:
                return u''
            decimals = int(decimals)
            format_string = "%." + str(decimals) + "f"
            return pattern.replace('{0}', format_string % value)
    return u''


def parse_ini_file(config_file):
    """Given an open file, read and parse it like an ini file."""

    parser = ConfigParser.RawConfigParser()
    parser.optionxform = str  # ensure keys are case-sensitive as expected
    parser.readfp(config_file)
    return parser

def fuzz_activity(count):
    """Add some jitter to an activity metric to maintain privacy."""
    # decay constant is e**(-x / 60)
    decay = math.exp(float(-count) / 60)
    jitter = round(5 * decay)
    return count + random.randint(0, jitter)


def shuffle_slice(x, start, stop=None):
    """Given a list, shuffle a portion of the list in-place, returning None.

    This uses a knuth shuffle borrowed from http://stackoverflow.com/a/11706463
    which is a slightly tweaked version of shuffle from the `random` stdlib:
    https://hg.python.org/cpython/file/8962d1c442a6/Lib/random.py#l256
    """
    if stop is None:
        stop = len(x)

    for i in reversed(xrange(start + 1, stop)):
        j = random.randint(start, i)
        x[i], x[j] = x[j], x[i]


# port of https://docs.python.org/dev/library/itertools.html#itertools-recipes
def partition(pred, iterable):
    "Use a predicate to partition entries into false entries and true entries"
    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
    t1, t2 = itertools.tee(iterable)
    return itertools.ifilterfalse(pred, t1), itertools.ifilter(pred, t2)

# http://docs.python.org/2/library/itertools.html#recipes
def roundrobin(*iterables):
    "roundrobin('ABC', 'D', 'EF') --> A D E B F C"
    # Recipe credited to George Sakkis
    pending = len(iterables)
    nexts = itertools.cycle(iter(it).next for it in iterables)
    while pending:
        try:
            for next in nexts:
                yield next()
        except StopIteration:
            pending -= 1
            nexts = itertools.cycle(itertools.islice(nexts, pending))


def lowercase_keys_recursively(subject):
    """Return a dict with all keys lowercased (recursively)."""
    lowercased = dict()
    for key, val in subject.iteritems():
        if isinstance(val, dict):
            val = lowercase_keys_recursively(val)
        lowercased[key.lower()] = val

    return lowercased


def sampled(live_config_var):
    """Wrap a function that should only actually run occasionally

    The wrapped function will only actually execute at the rate
    specified by the live_config sample rate given.

    Example:

    @sampled("foobar_sample_rate")
    def foobar():
        ...

    If g.live_config["foobar_sample_rate"] is set to 0.5, foobar()
    will only execute 50% of the time when it is called.

    """
    def sampled_decorator(fn):
        @functools.wraps(fn)
        def sampled_fn(*a, **kw):
            if random.random() > g.live_config[live_config_var]:
                return None
            else:
                return fn(*a, **kw)
        return sampled_fn
    return sampled_decorator


def squelch_exceptions(fn):
    """Wrap a function to log and suppress all internal exceptions

    When running in debug mode, the exception will be propagated, but
    in production environments, the function exception will be logged,
    then suppressed.

    Use of this decorator is not an excuse to not handle exceptions

    """
    @functools.wraps(fn)
    def squelched_fn(*a, **kw):
        try:
            return fn(*a, **kw)
        except BaseException:
            if g.debug:
                raise
            else:
                # log.exception will send a stack trace as well
                g.log.exception("squelching exception")
    return squelched_fn


EPOCH = datetime(1970, 1, 1, tzinfo=pytz.UTC)


def epoch_timestamp(dt):
    """Returns the number of seconds from the epoch to date.

    :param datetime dt: datetime (with time zone)
    :rtype: float
    """
    return (dt - EPOCH).total_seconds()


def to_epoch_milliseconds(dt):
    """Returns the number of milliseconds from the epoch to date.

    :param datetime dt: datetime (with time zone)
    :rtype: int
    """
    return int(math.floor(1000. * epoch_timestamp(dt)))


def from_epoch_milliseconds(ms):
    """Convert milliseconds from the epoch to UTC datetime.

    :param int ms: milliseconds since the epoch
    :rtype: :py:class:`datetime.datetime`
    """
    seconds = int(ms / 1000.)
    microseconds = (ms - 1000 * seconds) * 1000.
    return EPOCH + timedelta(seconds=seconds, microseconds=microseconds)


def rate_limiter(max_per_second):
    """Limit number of calls to returned closure per second to max_per_second
    algorithm adapted from here:
        http://blog.gregburek.com/2011/12/05/Rate-limiting-with-decorators/
    """
    min_interval = 1.0 / float(max_per_second)
    # last_time_called needs to be a list so we can do a closure on it
    last_time_called = [0.0]

    def throttler():
        elapsed = time.clock() - last_time_called[0]
        left_to_wait = min_interval - elapsed
        if left_to_wait > 0:
            time.sleep(left_to_wait)
        last_time_called[0] = time.clock()
    return throttler


def rate_limited_generator(rate_limit_per_second, iterable):
    """Yield from iterable without going over rate limit"""
    throttler = rate_limiter(rate_limit_per_second)
    for i in iterable:
        throttler()
        yield i
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from utils import *
from http_utils import *
from reddit_agent_parser import Agent
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import app_globals as g
from pylons import request
from pylons import tmpl_context as c

from r2.config import feature


def is_tracking_link_enabled(link=None, element_name=None):
    if c.user_is_admin:
        return False  # Less noise while admin mode enabled, esp. in usernotes
    if element_name and element_name.startswith('trending_sr'):
        return True
    if feature.is_enabled('utm_comment_links'):
        return True
    return False

<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import pytz
from datetime import datetime

DATE_RFC822 = '%a, %d %b %Y %H:%M:%S %Z'
DATE_RFC850 = '%A, %d-%b-%y %H:%M:%S %Z'
DATE_ANSI = '%a %b %d %H:%M:%S %Y'


def read_http_date(date_str):
    try:
        date = datetime.strptime(date_str, DATE_RFC822)
    except ValueError:
        try:
            date = datetime.strptime(date_str, DATE_RFC850)
        except ValueError:
            try:
                date = datetime.strptime(date_str, DATE_ANSI)
            except ValueError:
                return None
    date = date.replace(tzinfo = pytz.timezone('GMT'))
    return date


def http_date_str(date):
    date = date.astimezone(pytz.timezone('GMT'))
    return date.strftime(DATE_RFC822)


def get_requests_resp_json(resp):
    """Kludge so we can use `requests` versions below or above 1.x"""
    if callable(resp.json):
        return resp.json()
    return resp.json
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2016 reddit
# Inc. All Rights Reserved.
###############################################################################

from httpagentparser import (
    AndroidBrowser,
    Browser,
    detect as de,
    DetectorBase,
    detectorshub)
import re
from inspect import isclass


def register_detector(cls):
    """Collector of all the reddit detectors."""
    detectorshub.register(cls())
    return cls


class RedditDetectorBase(DetectorBase):
    agent_string = None
    version_string = '(\.?\d+)*'

    def __init__(self):
        if self.agent_string:
            self.agent_regex = re.compile(self.agent_string.format(
                look_for=self.look_for, version_string=self.version_string))
        else:
            self.agent_regex = None

        self.version_regex = re.compile('(?P<version>{})'.format(
            self.version_string))

    def getVersion(self, agent, word):
        match = None
        if self.agent_regex:
            match = self.agent_regex.search(agent)

        if not match:
            match = self.version_regex.search(agent)

        if match and 'version' in match.groupdict().keys():
            return match.group('version')

    def detect(self, agent, result):
        detected = super(RedditDetectorBase, self).detect(agent, result)

        if not detected or not self.agent_regex:
            return detected

        match = self.agent_regex.search(agent)
        groups = match.groupdict()
        platform_name = groups.get('platform')
        version = groups.get('pversion')

        if platform_name:
            platform = {}
            platform['name'] = platform_name
            if version:
                platform['version'] = version
            result['platform'] = platform

        if self.is_app:
            result['app_name'] = result['browser']['name']

        return True


class RedditBrowser(RedditDetectorBase, Browser):
    """Base class for all reddit specific browsers."""
    # is_app denotes a client that is a native mobile application, but not a
    # browser.
    is_app = False


@register_detector
class RedditIsFunDetector(RedditBrowser):
    is_app = True
    look_for = 'reddit is fun'
    name = 'reddit is fun'
    agent_string = ('^{look_for} \((?P<platform>.*?)\) '
                    '(?P<version>{version_string})$')
    override = [AndroidBrowser]


@register_detector
class RedditAndroidDetector(RedditBrowser):
    is_app = True
    look_for = 'RedditAndroid'
    name = 'Reddit: The Official App'
    agent_string = '{look_for} (?P<version>{version_string})$'


@register_detector
class RedditIOSDetector(RedditBrowser):
    is_app = True
    look_for = 'Reddit'
    name = 'reddit iOS'
    skip_if_found = ['Android']
    agent_string = (
        '{look_for}\/Version (?P<version>{version_string})\/Build '
        '(?P<b_number>\d+)\/(?P<platform>.*?) Version '
        '(?P<pversion>{version_string}) \(Build .*?\)')


@register_detector
class AlienBlueDetector(RedditBrowser):
    is_app = True
    look_for = 'AlienBlue'
    name = 'Alien Blue'
    agent_string = (
        '{look_for}\/(?P<version>{version_string}) CFNetwork\/'
        '{version_string} (?P<platform>.*?)\/(?P<pversion>{version_string})')


@register_detector
class RelayForRedditDetector(RedditBrowser):
    is_app = True
    look_for = 'Relay by /u/DBrady'
    name = 'relay for reddit'
    agent_string = '{look_for} v(?P<version>{version_string})'


@register_detector
class RedditSyncDetector(RedditBrowser):
    is_app = True
    look_for = 'reddit_sync'
    name = 'Sync for reddit'
    agent_string = (
        'android:com\.laurencedawson\.{look_for}'
        ':v(?P<version>{version_string}) \(by /u/ljdawson\)')


@register_detector
class NarwhalForRedditDetector(RedditBrowser):
    is_app = True
    look_for = 'narwhal'
    name = 'narwhal for reddit'
    agent_string = '{look_for}-(?P<platform>.*?)\/\d+ by det0ur'


@register_detector
class McRedditDetector(RedditBrowser):
    is_app = True
    look_for = 'McReddit'
    name = 'McReddit'
    agent_string = '{look_for} - Reddit Client for (?P<platform>.*?)$'


@register_detector
class ReaditDetector(RedditBrowser):
    look_for = 'Readit'
    name = 'Readit'
    agent_string = '(\({look_for} for WP /u/MessageAcrossStudios\) ?){{1,2}}'


@register_detector
class BaconReaderDetector(RedditBrowser):
    is_app = True
    look_for = 'BaconReader'
    name = 'Bacon Reader'
    agent_string = (
        '{look_for}\/(?P<version>{version_string}) \([a-zA-Z]+; '
        '(?P<platform>.*?) (?P<pversion>{version_string}); '
        'Scale\/{version_string}\)')


def detect(*args, **kw):
    return de(*args, **kw)


class Agent(object):
    __slots__ = (
        "agent_string",
        "browser_name",
        "browser_version",
        "os_name",
        "os_version",
        "platform_name",
        "platform_version",
        "sub_platform_name",
        "bot",
        "app_name",
        "is_mobile_browser",
    )

    MOBILE_PLATFORMS = {'iOS', 'Windows', 'Android', 'BlackBerry'}

    def __init__(self, **kw):
        kw.setdefault("is_mobile_browser", False)
        for k in self.__slots__:
            setattr(self, k, kw.get(k))

    @classmethod
    def parse(cls, ua):
        agent = cls(agent_string=ua)
        parsed = detect(ua)
        for attr in ("browser", "os", "platform"):
            d = parsed.get(attr)
            if d:
                for subattr in ("name", "version"):
                    if subattr in d:
                        key = "%s_%s" % (attr, subattr)
                        setattr(agent, key, d[subattr])

        agent.bot = parsed.get('bot')
        dist = parsed.get('dist')
        if dist:
            agent.sub_platform_name = dist.get('name')

        # if this is a known app, extract the app_name
        agent.app_name = parsed.get('app_name')
        agent.is_mobile_browser = agent.determine_mobile_browser()
        return agent

    def determine_mobile_browser(self):
        if self.platform_name in self.MOBILE_PLATFORMS:
            if self.sub_platform_name == 'IPad':
                return False

            if (
                self.platform_name == 'Android' and
                not (
                    'Mobile' in self.agent_string or
                    self.browser_name == 'Opera Mobile'
                )
            ):
                return False

            if (
                self.platform_name == 'Windows' and
                self.sub_platform_name != 'Windows Phone'
            ):
                return False

            if 'Opera Mini' in self.agent_string:
                return False

            return True
        return False

    def to_dict(self):
        d = {}
        for k in self.__slots__:
            if k != "agent_string":
                v = getattr(self, k, None)
                if v:
                    d[k] = v
        return d
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

def select_provider(config_parser, working_set, type, name):
    """Given a type and name return an instantiated provider.

    Providers are objects that abstract away an external service. They are
    looked up via the pkg_resources system which means that they may not even
    be implemented in the main reddit code.

    A provider must implement the expected interface, be registered via
    setuptools with the right entry point, and have a no-argument constructor.

    If a provider class has an attribute `config` which is a ConfigParse-style
    spec dictionary, the spec will be added to the main configuration parser
    similarly to the plugin system. This allows providers to declare extra
    config options for parsing.

    """

    try:
        entry_point = working_set.iter_entry_points(type, name).next()
    except StopIteration:
        raise Exception("unknown %s provider: %r" % (type, name))
    else:
        provider_cls = entry_point.load()

    if hasattr(provider_cls, "config"):
        config_parser.add_spec(provider_cls.config)

    return provider_cls()
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class ImageResizingProvider(object):
    """Provider for generating resizable image urls.

    """
    def resize_image(self, image, width=None, censor_nsfw=False, max_ratio=None):
        """Turn a url of an image in storage into one that will produce a
        resized image.

        `image` must be a dictionary with keys:
            url: the storage url given by the media provider after uploading
            width: in pixels
            height: in pixels

        `width` is optionally a number of pixels wide for the resultant image;
        if not specified, the dimensions will be the same as the source image.

        `censor_nsfw` is a boolean indicating whether the resizer should
        attempt to censor the image (e.g. by blurring it) due to it being NSFW.

        `max_ratio` is the maximum value of the height of the resultant image
        divided by the width; if not specified, the aspect ratio will be the
        same as the source image.

        The return value should be an absolute URL with the `https` scheme if
        supported, but should also work if accessed with `http`.

        Throws NotLargeEnough if the source image is smaller than the requested
        width.
        """
        raise NotImplementedError

    def purge_url(self, url):
        """Purge an image (by url) from the provider.

        Providers should override and implement this method if they do
        something like keep a cache of resized versions that are
        requested. This will allow the cached version to be deleted, in
        cases where it needs to be re-generated or removed entirely.
        """
        pass


class NotLargeEnough(Exception): pass
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib

from pylons import app_globals as g
import requests

from r2.lib.configparse import ConfigValue
from r2.lib.providers.image_resizing import (
    ImageResizingProvider,
    NotLargeEnough,
)
from r2.lib.utils import UrlParser, query_string

class ImgixImageResizingProvider(ImageResizingProvider):
    """A provider that uses imgix to create on-the-fly resizings."""
    config = {
        ConfigValue.str: [
            'imgix_domain',
        ],
    }

    def resize_image(self, image, width=None, censor_nsfw=False, max_ratio=None):
        url = UrlParser(image['url'])
        url.hostname = g.imgix_domain
        # Let's encourage HTTPS; it's cool, works just fine on HTTP pages, and
        # will prevent insecure content warnings on HTTPS pages.
        url.scheme = 'https'

        if max_ratio:
            url.update_query(fit='crop')
            # http://www.imgix.com/docs/reference/size#param-crop
            url.update_query(crop='faces,entropy')
            url.update_query(arh=max_ratio)

        if width:
            if width > image['width']:
                raise NotLargeEnough()
            # http://www.imgix.com/docs/reference/size#param-w
            url.update_query(w=width)
        if censor_nsfw:
            # Do an initial blur to make sure we're getting rid of icky
            # details.
            #
            # http://www.imgix.com/docs/reference/stylize#param-blur
            url.update_query(blur=600)
            # And then add pixellation to help the image compress well.
            #
            # http://www.imgix.com/docs/reference/stylize#param-px
            url.update_query(px=32)
        if g.imgix_signing:
            url = self._sign_url(url, g.secrets['imgix_signing_token'])
        return url.unparse()

    def _sign_url(self, url, token):
        """Sign a url for imgix's secured sources.

        Based very heavily on the example code in the docs:
            http://www.imgix.com/docs/tutorials/securing-images

        Arguments:

        * url -- a UrlParser instance of the url to sign.  This object may be
                 modified by the function, so make a copy beforehand if that is
                 a concern.
        * token -- a string token provided by imgix for request signing

        Returns a UrlParser instance with signing parameters.
        """
        # Build the signing value
        signvalue = token + url.path
        if url.query_dict:
          signvalue += query_string(url.query_dict)

        # Calculate MD5 of the signing value.
        signature = hashlib.md5(signvalue).hexdigest()

        url.update_query(s=signature)
        return url

    def purge_url(self, url):
        """Purge an image (by url) from imgix.

        Reference: http://www.imgix.com/docs/tutorials/purging-images

        Note that as mentioned in the imgix docs, in order to remove
        an image, this function should be used *after* already
        removing the image from our source, or imgix will just re-fetch
        and replace the image with a new copy even after purging.
        """
        requests.post(
            "https://api.imgix.com/v2/image/purger",
            auth=(g.secrets["imgix_api_key"], ""),
            data={"url": url},
        )
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.providers.image_resizing import ImageResizingProvider

class NoOpImageResizingProvider(ImageResizingProvider):
    """A passthrough solution that won't actually resize any images.
    
    Combines well with the filesystem media provider for an entirely local
    setup.
    """
    def resize_image(self, image, width=None, censor_nsfw=False, max_ratio=None):
        # The simplest solution: just pass it on through.
        return image['url']
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.providers.image_resizing import ImageResizingProvider

class UnsplashitImageResizingProvider(ImageResizingProvider):
    """A simple resizer that provides correctly-sized kitten images.

    Useful if you don't want the external dependencies of imgix, but need
    correctly-sized images for testing a UI.
    """
    def resize_image(self, image, width=None, censor_nsfw=False, max_ratio=None):
        if width is None:
            width = image['width']
        height = width * 2

        return 'https://unsplash.it/%d/%d' % (width, height)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class TicketProvider(object):
    """Provider for handling support tickets interactions.

    """
    
    def build_ticket_url_from_id(self, ticket_id):
        """
        Creates a URL to access a ticket based on URL

        `ticket_id` the unique identifier for the ticket

        Returns a URL to the ticket
        """
        raise NotImplementedError
        
    def create(self):
        """Creates a new support ticket with the provider. 

        Parameters may vary depending on the provider you use.

        The return value should be a JSON object of the created ticket.
        """
        raise NotImplementedError

    def get(self, ticket_id):
        """Gets an existing ticket from the provider.

        `ticket_id` is a unique identifier dor the ticket.

        The return value should be a JSON object of the created ticket.
        """
        raise NotImplementedError

    def update(self):
        """Updates the ticket with the provider

        Parameters may vary depending on the provider you use.

        The return value should be a JSON object of the updated ticket.
        """
        raise NotImplementedError
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import re
import requests

from pylons import app_globals as g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.support import TicketProvider

class ZenDeskProvider(TicketProvider):
    """A provider that interfaces with ZenDesk for managing tickets."""

    def build_ticket_url_from_id(self, ticket_id):
        return '%sagent/tickets/%s' % (
            g.live_config['ticket_base_url'], 
            ticket_id,
        )

    def create(self, 
               subject, 
               group_id,
               comment_body, 
               comment_is_public=False,
               requester_id=None, 
               custom_fields=[],
               ):
        """Creates a new support ticket on ZenDesk. 

        `subject` (self explanatory)
        `group_id` The group ID to assign to
        `comment_body` (self explanatory)
        `comment_is_public` Whether the comment is public or internal
        `requester_id` The external user ID for the user submitting
        `custom_fields` If custom feilds are defined and need to be set

        Returns a JSON object of the newly created ticket.
        """
        zd_ticket_create_params = {
            'ticket': {
                "requester_id": requester_id,
                "subject": subject,
                "comment": { 
                    "body": comment_body,
                    "public": comment_is_public,
                },
                "group_id": group_id,
                "custom_fields": custom_fields,
            }
        }
        
        timer = g.stats.get_timer("providers.zendesk.ticket_create")
        timer.start()
        response = requests.post(
            '%s/api/v2/tickets.json' % g.live_config['ticket_base_url'], 
            auth=(
                '%s/token' % g.secrets['zendesk_user'], 
                g.secrets["zendesk_api_key"]
            ),
            headers={'content-type': 'application/json'},
            data=json.dumps(zd_ticket_create_params))
        timer.stop
        
        if response.status_code != 201:
            g.log.error(
                'ZENDESK_CREATE_ERROR: code: %s msg: %s' % 
                (response.status_code, response.text)
            )
            return None

        return json.loads(response.content)['ticket']
    
    def get_ticket_id_from_url(self, ticket_url):
        """Extracts the ticket ID from a URL."""
        r = re.compile('(\d).*')
        numbers_in_url = r.findall(ticket_url)
        if not numbers_in_url:
            raise Exception('No digits in request_url')
        else:
            return numbers_in_url[0]

    def get(self, ticket_id):        
        """Gets a ticket from ZenDesk as a JSON object"""
        timer = g.stats.get_timer("providers.zendesk.ticket_get")
        timer.start()
        response = requests.get(
            '%s/api/v2/tickets/%s.json' % (
                g.live_config['ticket_base_url'], 
                ticket_id
            ),
            auth=(
                '%s/token' % g.secrets['zendesk_user'], 
                g.secrets["zendesk_api_key"]
            ),
            headers={'content-type': 'application/json'},
        )
        timer.stop()
        
        if response.status_code != 200:
            g.log.error(
                'ZENDESK_GET_ERROR: code: %s msg: %s' % 
                (response.status_code, response.text)
            )
            return None
            
        return json.loads(response.content)['ticket']

    def update(self, ticket, status=None,
               comment_body='', comment_is_public=False, 
               tag_list=None):
        """Updates the ticket on ZenDesk."""
        
        if comment_body:
            comment_json = {
                    "public": comment_is_public, 
                    "body": comment_body,
                }
        else:
            comment_json = {}
        
        ticket_updated_json = {
            'ticket': {
                "comment": comment_json,
                "status": status or ticket['status'],
                "tags": tag_list or ticket['tags'],
            }
        }
        
        timer = g.stats.get_timer("providers.zendesk.ticket_update")
        timer.start()
        response = requests.put(
            '%s/api/v2/tickets/%s.json' % (
                g.live_config['ticket_base_url'], 
                ticket['id']
            ),
            auth=(
                '%s/token' % g.secrets['zendesk_user'], 
                g.secrets["zendesk_api_key"]
            ),
            headers={'content-type': 'application/json'},
            data=json.dumps(ticket_updated_json)
        )
        timer.stop()
        
        if response.status_code != 200:
            g.log.error(
                'ZENDESK_UPDATE_ERROR: code: %s msg: %s' % 
                (response.status_code, response.text)
            )
            return None
            
        return json.loads(response.content)['ticket']
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import urllib

from pylons import request
from pylons import app_globals as g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.auth import AuthenticationProvider
from r2.lib.utils import constant_time_compare


class CookieAuthenticationProvider(AuthenticationProvider):
    """An authentication provider that uses standard HTTP cookies.

    """

    config = {
        ConfigValue.str: [
            "login_cookie",
        ],
    }

    def is_logout_allowed(self):
        return True

    def get_authenticated_account(self):
        from r2.models import Account, NotFound

        quoted_session_cookie = request.cookies.get(g.login_cookie)
        if not quoted_session_cookie:
            return None
        session_cookie = urllib.unquote(quoted_session_cookie)

        try:
            uid, timestr, hash = session_cookie.split(",")
            uid = int(uid)
        except:
            return None

        try:
            account = Account._byID(uid, data=True)
        except NotFound:
            return None

        expected_cookie = account.make_cookie(timestr)
        if not constant_time_compare(session_cookie, expected_cookie):
            return None
        return account
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


class AuthenticationProvider(object):
    """Provider for authenticating web requests.

    Authentication providers should look at the request environment and
    determine if a particular user is logged in to the site.  This may take the
    form of cookies like on reddit.com or perhaps a web server provided HTTP
    header in intranet environments.

    Authentication systems may allow users to select their login, or may force
    them to a particular one and disallow logout.

    Note: this is NOT intended for API authentication, see instead: OAuth.

    """

    def is_logout_allowed(self):
        """Return if the user allowed to log out.

        Some authentication systems, such as single sign-on on an intranet,
        pick up the authenticated user from an external system and logging out
        of reddit would be meaningless.  If disallowed, some UI elements can be
        disabled to reduce confusion.

        """
        raise NotImplementedError

    def get_authenticated_account(self):
        """Return the authenticated user, or None if logged out.

        """
        raise NotImplementedError
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import crypt

import bcrypt

from pylons import request
from pylons import app_globals as g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.auth import AuthenticationProvider
from r2.lib.require import RequirementException
from r2.lib.utils import constant_time_compare, parse_http_basic


class HttpAuthenticationProvider(AuthenticationProvider):
    """An authentication provider based on HTTP basic authentication.

    This provider uses HTTP basic authentication (via the Authorization header)
    to authenticate the user. Logout is disallowed.

    If auth_trust_http_authorization is set to true, the Authorization header
    is fully trusted. The password will not be checked and accounts will be
    automatically registered when not already present.

    """

    config = {
        ConfigValue.bool: [
            "auth_trust_http_authorization",
        ],
    }

    def is_logout_allowed(self):
        return False

    def get_authenticated_account(self):
        from r2.models import Account, NotFound, register

        try:
            authorization = request.environ.get("HTTP_AUTHORIZATION")
            username, password = parse_http_basic(authorization)
        except RequirementException:
            return None

        try:
            account = Account._by_name(username)
        except NotFound:
            if g.auth_trust_http_authorization:
                # note: we're explicitly allowing automatic re-registration of
                # _deleted accounts and login of _banned accounts here because
                # we're trusting you know what you're doing in an SSO situation
                account = register(username, password, request.ip)
            else:
                return None

        # if we're to trust the authorization headers, don't check passwords
        if g.auth_trust_http_authorization:
            return account

        # not all systems support bcrypt in the standard crypt
        if account.password.startswith("$2a$"):
            expected_hash = bcrypt.hashpw(password, account.password)
        else:
            expected_hash = crypt.crypt(password, account.password)

        if not constant_time_compare(expected_hash, account.password):
            return None
        return account
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class MediaProvider(object):
    """Provider for storing media objects.

    Media objects are thumbnails, subreddit images/stylesheets, and app icons.
    A media provider must allow new objects to be added to the system and for
    users to be able to view those objects over HTTP.

    """
    def make_inaccessible(self, url):
        """Make the content unavaiable, but do not remove. Content could
        be recovered at a later time.

        `url` must be a url linking to the content

        The return value should be a 
        """
        raise NotImplementedError

    def put(self, category, name, contents, headers=None):
        """Put a media object on the media server and return its HTTP URL.

        `name` must be a local filename including an extension.

        `contents` is a byte string of the contents of the file or a file-like
                   object the contents of which will be read.

        `headers` an optional dict of additional headers to attach to the media
                  object. the provider MAY ignore these.

        The return value should be an absolute URL with the `http` scheme but
        should also work if accessed with `https`.

        """
        raise NotImplementedError

    def purge(self, url):
        """Remove the content. Content can not be recovered."""
        raise NotImplementedError
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import mimetypes
import os
import re

import boto

from pylons import app_globals as g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.media import MediaProvider


_NEVER = "Thu, 31 Dec 2037 23:59:59 GMT"


class S3MediaProvider(MediaProvider):
    """A media provider using Amazon S3.

    Credentials for uploading objects can be provided via `S3KEY_ID` and
    `S3SECRET_KEY`. If not provided, boto will search for credentials in
    alternate venues including environment variables and EC2 instance roles if
    on Amazon EC2.

    The `s3_media_direct` option configures how URLs are generated. When true,
    URLs will use Amazon's domain name meaning a zero-DNS configuration. If
    false, the bucket name will be assumed to be a valid domain name that is
    appropriately CNAME'd to S3 and URLs will be generated accordingly.

    If more than one bucket is provided in `s3_media_buckets`, items will be
    sharded out to the various buckets based on their filename. This allows for
    hostname parallelization in the non-direct HTTP case.

    """
    config = {
        ConfigValue.str: [
            "S3KEY_ID",
            "S3SECRET_KEY",
            "s3_media_domain",
        ],
        ConfigValue.bool: [
            "s3_media_direct",
        ],
        ConfigValue.tuple: [
            "s3_media_buckets",
            "s3_image_buckets",
        ],
    }

    buckets = {
        'thumbs': 's3_media_buckets',
        'stylesheets': 's3_media_buckets',
        'icons': 's3_media_buckets',
        'previews': 's3_image_buckets',
    }
 
    def _get_bucket(self, bucket_name, validate=False):
     
        s3 = boto.connect_s3(g.S3KEY_ID or None, g.S3SECRET_KEY or None)
        bucket = s3.get_bucket(bucket_name, validate=validate)

        return bucket

    def _get_bucket_key_from_url(self, url):
        if g.s3_media_domain in url:
            r_bucket = re.compile('.*\://(?:%s.)?([^\/]+)' % g.s3_media_domain)
        else:
            r_bucket = re.compile('.*\://?([^\/]+)')

        bucket_name = r_bucket.findall(url)[0]
        key_name = url.split('/')[-1]

        return bucket_name, key_name
     
    def make_inaccessible(self, url):
        """Make the content unavailable, but do not remove."""
        bucket_name, key_name = self._get_bucket_key_from_url(url)

        timer = g.stats.get_timer("providers.s3.key_set_private")
        timer.start()

        bucket = self._get_bucket(bucket_name, validate=False)

        key = bucket.get_key(key_name)
        if key:
            # set the file as private, but don't delete it, if it exists
            key.set_acl('private')

        timer.stop()

        return True

    def put(self, category, name, contents, headers=None):
        buckets = getattr(g, self.buckets[category])
        # choose a bucket based on the filename
        name_without_extension = os.path.splitext(name)[0]
        index = ord(name_without_extension[-1]) % len(buckets)
        bucket_name = buckets[index]

        # guess the mime type
        mime_type, encoding = mimetypes.guess_type(name)

        # build up the headers
        s3_headers = {
            "Content-Type": mime_type,
            "Expires": _NEVER,
        }
        if headers:
            s3_headers.update(headers)

        # send the key
        bucket = self._get_bucket(bucket_name, validate=False)
        key = bucket.new_key(name)

        if isinstance(contents, basestring):
            set_fn = key.set_contents_from_string
        else:
            set_fn = key.set_contents_from_file

        set_fn(
            contents,
            headers=s3_headers,
            policy="public-read",
            reduced_redundancy=True,
            replace=True,
        )

        if g.s3_media_direct:
            return "http://%s/%s/%s" % (g.s3_media_domain, bucket_name, name)
        else:
            return "http://%s/%s" % (bucket_name, name)

    def purge(self, url):
        """Deletes the key as specified by the url"""
        bucket_name, key_name = self._get_bucket_key_from_url(url)

        timer = g.stats.get_timer("providers.s3.key_set_private")
        timer.start()

        bucket = self._get_bucket(bucket_name, validate=False)

        key_name = url.split('/')[-1]
        key = bucket.get_key(key_name)
        if key:
            # delete the key if it exists
            key.delete()

        timer.stop()

        return True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import shutil
import urlparse

from pylons import app_globals as g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.media import MediaProvider


class FileSystemMediaProvider(MediaProvider):
    """A simple media provider that writes to the filesystem.

    It is assumed that an external HTTP server will take care of serving the
    media objects once written.

    `media_fs_root` is the root directory on the filesystem to write the objects
    into.

    `media_fs_base_url_http` is the base URL on which to find the media
    objects. It should be an absolute URL to the root directory of the media
    object server that is accessible via both HTTP and HTTPS.

    """
    config = {
        ConfigValue.str: [
            "media_fs_root",
            "media_fs_base_url_http",
        ],
    }

    def make_inaccessible(self, url):
        # When it comes to file system, there isn't really the concept of
        # "making a file inaccessible" separate from deletion without
        # losing track of it. For the sake of not creating orphaned files, 
        # not implementing this method
        g.log.warning(
            'FileSystemMediaProvider.make_inaccessible is consciously '
            'not implemented and does not raise an error.'
        )
        return True

    def put(self, category, name, contents, headers=None):
        assert os.path.dirname(name) == ""
        path = os.path.join(g.media_fs_root, name)
        with open(path, "w") as f:
            if isinstance(contents, basestring):
                f.write(contents)
            else:
                shutil.copyfileobj(contents, f)
        return urlparse.urljoin(g.media_fs_base_url_http, name)
        
    def purge(self, url):
        """Remove the content from disk. Content can not be recovered."""

        name = url.split('/')[-1]
        path = os.path.join(g.media_fs_root, name)
        os.remove(path)
        return True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################
import cPickle as pickle
from datetime import datetime, timedelta
import functools
import httplib
import json
from lxml import etree
from pylons import tmpl_context as c
from pylons import app_globals as g
import socket
import time
import urllib

import l2cs

from r2.lib import amqp, filters
from r2.lib.db.operators import desc
from r2.lib.db.sorts import epoch_seconds
from r2.lib.filters import _force_unicode
from r2.lib.providers.search import SearchProvider
from r2.lib.providers.search.common import (
    InvalidQuery,
    LinkFields,
    Results,
    safe_get,
    safe_xml_str,
    SearchError,
    SearchHTTPError,
    SubredditFields,
)
import r2.lib.utils as r2utils
from r2.models import (
    Account,
    AllMinus,
    DomainSR,
    FakeSubreddit,
    FriendsSR,
    Link,
    MultiReddit,
    NotFound,
    Subreddit,
    Thing,
)

_TIMEOUT = 5 # seconds for http requests to cloudsearch
_CHUNK_SIZE = 4000000 # Approx. 4 MB, to stay under the 5MB limit
_VERSION_OFFSET = 13257906857


class CloudSearchUploader(object):
    use_safe_get = False
    types = ()

    def __init__(self, doc_api, fullnames=None, version_offset=_VERSION_OFFSET):
        self.doc_api = doc_api
        self._version_offset = version_offset
        self.fullnames = fullnames

    @classmethod
    def desired_fullnames(cls, items):
        '''Pull fullnames that represent instances of 'types' out of items'''
        fullnames = set()
        type_ids = [type_._type_id for type_ in cls.types]
        for item in items:
            item_type = r2utils.decompose_fullname(item['fullname'])[1]
            if item_type in type_ids:
                fullnames.add(item['fullname'])
        return fullnames

    def _version_tenths(self):
        '''Cloudsearch documents don't update unless the sent "version" field
        is higher than the one currently indexed. As our documents don't have
        "versions" and could in theory be updated multiple times in one second,
        for now, use "tenths of a second since 12:00:00.00 1/1/2012" as the
        "version" - this will last approximately 13 years until bumping up against
        the version max of 2^32 for cloudsearch docs'''
        return int(time.time() * 10) - self._version_offset

    def _version_seconds(self):
        return int(time.time()) - int(self._version_offset / 10)

    _version = _version_tenths

    def add_xml(self, thing, version):
        add = etree.Element("add", id=thing._fullname, version=str(version),
                            lang="en")

        for field_name, value in self.fields(thing).iteritems():
            field = etree.SubElement(add, "field", name=field_name)
            field.text = safe_xml_str(value)

        return add

    def delete_xml(self, thing, version=None):
        '''Return the cloudsearch XML representation of
        "delete this from the index"
        
        '''
        version = str(version or self._version())
        delete = etree.Element("delete", id=thing._fullname, version=version)
        return delete

    def delete_ids(self, ids):
        '''Delete documents from the index.
        'ids' should be a list of fullnames
        
        '''
        version = self._version()
        deletes = [etree.Element("delete", id=id_, version=str(version))
                   for id_ in ids]
        batch = etree.Element("batch")
        batch.extend(deletes)
        return self.send_documents(batch)

    def xml_from_things(self):
        '''Generate a <batch> XML tree to send to cloudsearch for
        adding/updating/deleting the given things
        
        '''
        batch = etree.Element("batch")
        self.batch_lookups()
        version = self._version()
        for thing in self.things:
            try:
                if thing._spam or thing._deleted:
                    delete_node = self.delete_xml(thing, version)
                    batch.append(delete_node)
                elif self.should_index(thing):
                    add_node = self.add_xml(thing, version)
                    batch.append(add_node)
            except (AttributeError, KeyError) as e:
                # Problem! Bail out, which means these items won't get
                # "consumed" from the queue. If the problem is from DB
                # lag or a transient issue, then the queue consumer
                # will succeed eventually. If it's something else,
                # then manually run a consumer with 'use_safe_get'
                # on to get past the bad Thing in the queue
                if not self.use_safe_get:
                    raise
                else:
                    g.log.warning("Ignoring problem on thing %r.\n\n%r",
                                  thing, e)
        return batch

    def should_index(self, thing):
        raise NotImplementedError

    def batch_lookups(self):
        try:
            self.things = Thing._by_fullname(self.fullnames, data=True,
                                             return_dict=False)
        except NotFound:
            if self.use_safe_get:
                self.things = safe_get(Thing._by_fullname, self.fullnames,
                                       data=True, return_dict=False)
            else:
                raise

    def fields(self, thing):
        raise NotImplementedError

    def inject(self, quiet=False):
        '''Send things to cloudsearch. Return value is time elapsed, in seconds,
        of the communication with the cloudsearch endpoint
        
        '''
        xml_things = self.xml_from_things()

        if not len(xml_things):
            return 0

        cs_start = datetime.now(g.tz)
        sent = self.send_documents(xml_things)
        cs_time = (datetime.now(g.tz) - cs_start).total_seconds()

        adds, deletes, warnings = 0, 0, []
        for record in sent:
            response = etree.fromstring(record)
            adds += int(response.get("adds", 0))
            deletes += int(response.get("deletes", 0))
            if response.get("warnings"):
                warnings.append(response.get("warnings"))

        g.stats.simple_event("cloudsearch.uploads.adds", delta=adds)
        g.stats.simple_event("cloudsearch.uploads.deletes", delta=deletes)
        g.stats.simple_event("cloudsearch.uploads.warnings",
                delta=len(warnings))

        if not quiet:
            print "%s Changes: +%i -%i" % (self.__class__.__name__,
                                           adds, deletes)
            if len(warnings):
                print "%s Warnings: %s" % (self.__class__.__name__,
                                           "; ".join(warnings))

        return cs_time

    def send_documents(self, docs):
        '''Open a connection to the cloudsearch endpoint, and send the documents
        for indexing. Multiple requests are sent if a large number of documents
        are being sent (see chunk_xml())
        
        Raises SearchHTTPError if the endpoint indicates a failure
        '''
        responses = []
        connection = httplib.HTTPConnection(
            self.doc_api, port=80, timeout=_TIMEOUT)
        chunker = chunk_xml(docs)
        try:
            for data in chunker:
                headers = {}
                headers['Content-Type'] = 'application/xml'
                # HTTPLib calculates Content-Length header automatically
                connection.request('POST', "/2011-02-01/documents/batch",
                                   data, headers)
                response = connection.getresponse()
                if 200 <= response.status < 300:
                    responses.append(response.read())
                else:
                    raise SearchHTTPError(response.status,
                                               response.reason,
                                               response.read())
        finally:
            connection.close()
        return responses


class LinkUploader(CloudSearchUploader):
    types = (Link,)

    def __init__(self, doc_api, fullnames=None, version_offset=_VERSION_OFFSET):
        super(LinkUploader, self).__init__(doc_api, fullnames, version_offset)
        self.accounts = {}
        self.srs = {}

    def fields(self, thing):
        '''Return fields relevant to a Link search index'''
        account = self.accounts[thing.author_id]
        sr = self.srs[thing.sr_id]
        return LinkFields(thing, account, sr).fields()

    def batch_lookups(self):
        super(LinkUploader, self).batch_lookups()
        author_ids = [thing.author_id for thing in self.things
                      if hasattr(thing, 'author_id')]
        try:
            self.accounts = Account._byID(author_ids, data=True,
                                          return_dict=True)
        except NotFound:
            if self.use_safe_get:
                self.accounts = safe_get(Account._byID, author_ids, data=True,
                                         return_dict=True)
            else:
                raise

        sr_ids = [thing.sr_id for thing in self.things
                  if hasattr(thing, 'sr_id')]
        try:
            self.srs = Subreddit._byID(sr_ids, data=True, return_dict=True)
        except NotFound:
            if self.use_safe_get:
                self.srs = safe_get(Subreddit._byID, sr_ids, data=True,
                                    return_dict=True)
            else:
                raise

    def should_index(self, thing):
        return (thing.promoted is None and getattr(thing, "sr_id", None) != -1)


class SubredditUploader(CloudSearchUploader):
    types = (Subreddit,)
    _version = CloudSearchUploader._version_seconds

    def fields(self, thing):
        return SubredditFields(thing).fields()

    def should_index(self, thing):
        return thing._id != Subreddit.get_promote_srid()


def chunk_xml(xml, depth=0):
    '''Chunk POST data into pieces that are smaller than the 20 MB limit.
    
    Ideally, this never happens (if chunking is necessary, would be better
    to avoid xml'ifying before testing content_length)'''
    data = etree.tostring(xml)
    content_length = len(data)
    if content_length < _CHUNK_SIZE:
        yield data
    else:
        depth += 1
        print "WARNING: Chunking (depth=%s)" % depth
        half = len(xml) / 2
        left_half = xml # for ease of reading
        right_half = etree.Element("batch")
        # etree magic simultaneously removes the elements from one tree
        # when they are appended to a different tree
        right_half.extend(xml[half:])
        for chunk in chunk_xml(left_half, depth=depth):
            yield chunk
        for chunk in chunk_xml(right_half, depth=depth):
            yield chunk


@g.stats.amqp_processor('cloudsearch_changes')
def _run_changed(msgs, chan):
    '''Consume the cloudsearch_changes queue, and print reporting information
    on how long it took and how many remain
    
    '''
    start = datetime.now(g.tz)

    changed = [pickle.loads(msg.body) for msg in msgs]

    link_fns = LinkUploader.desired_fullnames(changed)
    sr_fns = SubredditUploader.desired_fullnames(changed)

    link_uploader = LinkUploader(g.CLOUDSEARCH_DOC_API, fullnames=link_fns)
    subreddit_uploader = SubredditUploader(g.CLOUDSEARCH_SUBREDDIT_DOC_API,
                                           fullnames=sr_fns)

    link_time = link_uploader.inject()
    subreddit_time = subreddit_uploader.inject()
    cloudsearch_time = link_time + subreddit_time

    totaltime = (datetime.now(g.tz) - start).total_seconds()

    print ("%s: %d messages in %.2fs seconds (%.2fs secs waiting on "
           "cloudsearch); %d duplicates, %s remaining)" %
           (start, len(changed), totaltime, cloudsearch_time,
            len(changed) - len(link_fns | sr_fns),
            msgs[-1].delivery_info.get('message_count', 'unknown')))


def run_changed(drain=False, min_size=500, limit=1000, sleep_time=10,
                use_safe_get=False, verbose=False):
    '''Run by `cron` (through `paster run`) on a schedule to send Things to
        Amazon CloudSearch
    
    '''
    if use_safe_get:
        CloudSearchUploader.use_safe_get = True
    amqp.handle_items('cloudsearch_changes', _run_changed, min_size=min_size,
                      limit=limit, drain=drain, sleep_time=sleep_time,
                      verbose=verbose)


def _progress_key(item):
    return "%s/%s" % (item._id, item._date)


def rebuild_link_index(start_at=None, sleeptime=1, cls=Link,
                       uploader=LinkUploader, doc_api='CLOUDSEARCH_DOC_API',
                       estimate=50000000, chunk_size=1000):
    doc_api = getattr(g, doc_api)
    uploader = uploader(doc_api)

    q = cls._query(cls.c._deleted == (True, False), sort=desc('_date'))

    if start_at:
        after = cls._by_fullname(start_at)
        assert isinstance(after, cls)
        q._after(after)

    q = r2utils.fetch_things2(q, chunk_size=chunk_size)
    q = r2utils.progress(q, verbosity=1000, estimate=estimate, persec=True,
                         key=_progress_key)
    for chunk in r2utils.in_chunks(q, size=chunk_size):
        uploader.things = chunk
        for x in range(5):
            try:
                uploader.inject()
            except httplib.HTTPException as err:
                print "Got %s, sleeping %s secs" % (err, x)
                time.sleep(x)
                continue
            else:
                break
        else:
            raise err
        last_update = chunk[-1]
        print "last updated %s" % last_update._fullname
        time.sleep(sleeptime)


rebuild_subreddit_index = functools.partial(rebuild_link_index,
                                            cls=Subreddit,
                                            uploader=SubredditUploader,
                                            doc_api='CLOUDSEARCH_SUBREDDIT_DOC_API',
                                            estimate=200000,
                                            chunk_size=1000)


def test_run_link(start_link, count=1000):
    '''Inject `count` number of links, starting with `start_link`'''
    if isinstance(start_link, basestring):
        start_link = int(start_link, 36)
    links = Link._byID(range(start_link - count, start_link), data=True,
                       return_dict=False)
    uploader = LinkUploader(g.CLOUDSEARCH_DOC_API, things=links)
    return uploader.inject()


def test_run_srs(*sr_names):
    '''Inject Subreddits by name into the index'''
    srs = Subreddit._by_name(sr_names).values()
    uploader = SubredditUploader(g.CLOUDSEARCH_SUBREDDIT_DOC_API, things=srs)
    return uploader.inject()


### Query Code ###
_SEARCH = "/2011-02-01/search?"
INVALID_QUERY_CODES = ('CS-UnknownFieldInMatchExpression',
                       'CS-IncorrectFieldTypeInMatchExpression',
                       'CS-InvalidMatchSetExpression',)
DEFAULT_FACETS = {"reddit": {"count":20}}
def basic_query(query=None, bq=None, faceting=None, size=1000,
                start=0, rank=None, rank_expressions=None,
                return_fields=None, record_stats=False, search_api=None):
    if search_api is None:
        search_api = g.CLOUDSEARCH_SEARCH_API
    if faceting is None:
        faceting = DEFAULT_FACETS
    path = _encode_query(query, bq, faceting, size, start, rank,
                         rank_expressions, return_fields)
    timer = None
    if record_stats:
        timer = g.stats.get_timer("providers.cloudsearch")
        timer.start()
    connection = httplib.HTTPConnection(search_api, port=80, timeout=_TIMEOUT)
    try:
        connection.request('GET', path)
        resp = connection.getresponse()
        response = resp.read()
        if record_stats:
            g.stats.action_count("event.search_query", resp.status)
        if resp.status >= 300:
            try:
                reasons = json.loads(response)
            except ValueError:
                pass
            else:
                messages = reasons.get("messages", [])
                for message in messages:
                    if message['code'] in INVALID_QUERY_CODES:
                        raise InvalidQuery(resp.status, resp.reason, message,
                                           search_api, path, reasons)
            raise SearchHTTPError(resp.status, resp.reason,
                                  search_api, path, response)
    except socket.timeout as e:
        g.stats.simple_event('cloudsearch.error.timeout')
        raise SearchError(e, search_api, path)
    except socket.error as e:
        g.stats.simple_event('cloudsearch.error.socket')
        raise SearchError(e, search_api, path)
    finally:
        connection.close()
        if timer is not None:
            timer.stop()

    return json.loads(response)


basic_link = functools.partial(basic_query, size=10, start=0,
                               rank="-relevance",
                               return_fields=['title', 'reddit',
                                              'author_fullname'],
                               record_stats=False,
                               search_api=g.CLOUDSEARCH_SEARCH_API)


basic_subreddit = functools.partial(basic_query,
                                    faceting=None,
                                    size=10, start=0,
                                    rank="-activity",
                                    return_fields=['title', 'reddit',
                                                   'author_fullname'],
                                    record_stats=False,
                                    search_api=g.CLOUDSEARCH_SUBREDDIT_SEARCH_API)


def _encode_query(query, bq, faceting, size, start, rank, rank_expressions,
                  return_fields):
    if not (query or bq):
        raise ValueError("Need query or bq")
    params = {}
    if bq:
        params["bq"] = bq
    if query:
        params["q"] = query
    params["results-type"] = "json"
    params["size"] = size
    params["start"] = start
    if rank:
        params["rank"] = rank
    if rank_expressions:
        for rank, expression in rank_expressions.iteritems():
            params['rank-%s' % rank] = expression
    if faceting:
        params["facet"] = ",".join(faceting.iterkeys())
        for facet, options in faceting.iteritems():
            params["facet-%s-top-n" % facet] = options.get("count", 20)
            if "sort" in options:
                params["facet-%s-sort" % facet] = options["sort"]
    if return_fields:
        params["return-fields"] = ",".join(return_fields)
    encoded_query = urllib.urlencode(params)
    path = _SEARCH + encoded_query
    return path


class CloudSearchQuery(object):
    '''Represents a search query sent to cloudsearch'''
    search_api = None
    sorts = {}
    recents = {None: None}
    default_syntax = "plain"
    lucene_parser = None

    def __init__(self, query, sr=None, sort=None, syntax=None, raw_sort=None,
                 faceting=None, recent=None, include_over18=True,
                 rank_expressions=None, start=0, num=1000):
        if syntax is None:
            syntax = self.default_syntax
        elif syntax not in self.known_syntaxes:
            raise ValueError("Unknown search syntax: %s" % syntax)
        self.syntax = syntax

        self.query = filters._force_unicode(query or u'')

        # parsed query
        self.converted_data = None
        self.q = u''
        self.bq = u''

        # filters
        self.sr = sr
        self._recent = recent
        self.recent = self.recents[recent]
        self.include_over18 = include_over18

        # rank / rank expressions
        self._sort = sort
        if raw_sort:
            self.sort = raw_sort
        else:
            self.sort = self.sorts.get(sort)
        self.rank_expressions = rank_expressions

        # pagination
        self.start = start
        self.num = num

        # facets
        self.faceting = faceting

        self.results = None

    def run(self, _update=False):
        results = self._run(_update=_update)
        self.results = Results(results.docs, results.hits, results._facets)
        return self.results

    def _parse(self):
        query = self.preprocess_query(self.query)

        if self.syntax == "cloudsearch":
            self.bq = self.customize_query(query)
        elif self.syntax == "lucene":
            bq = l2cs.convert(query, self.lucene_parser)
            self.converted_data = {"syntax": "cloudsearch", "converted": bq}
            self.bq = self.customize_query(bq)
        elif self.syntax == "plain":
            self.q = query.encode('utf-8')
            self.bq = self.customize_query()

        if not self.q and not self.bq:
            raise InvalidQuery

    def _run(self, _update=False):
        '''Run the search against self.query'''
        try:
            self._parse()
        except InvalidQuery:
            return Results([], 0, {})

        if g.sqlprinting:
            g.log.info("%s", self)

        return self._run_cached(self.q, self.bq.encode('utf-8'), self.sort,
                                self.rank_expressions, self.faceting,
                                start=self.start, num=self.num, _update=_update)

    def preprocess_query(self, query):
        return query

    def customize_query(self, bq=u''):
        return bq

    @classmethod
    def create_boolean_query(cls, queries):
        '''Return an AND clause combining all queries'''
        if len(queries) > 1:
            return '(and ' + ' '.join(queries) + ')'
        elif queries:
            return queries[0]
        return u''

    def __repr__(self):
        '''Return a string representation of this query'''
        result = ["<", self.__class__.__name__, "> query:",
                  repr(self.query), " "]
        if self.bq:
            result.append(" bq:")
            result.append(repr(self.bq))
            result.append(" ")
        if self.sort:
            result.append("sort:")
            result.append(self.sort)
        return ''.join(result)

    @classmethod
    def _run_cached(cls, query, bq, sort="relevance", rank_expressions=None,
                    faceting=None, start=0, num=1000, _update=False):
        '''Query the cloudsearch API. _update parameter allows for supposed
        easy memoization at later date.
        
        Example result set:
        
        {u'facets': {u'reddit': {u'constraints':
                                    [{u'count': 114, u'value': u'politics'},
                                    {u'count': 42, u'value': u'atheism'},
                                    {u'count': 27, u'value': u'wtf'},
                                    {u'count': 19, u'value': u'gaming'},
                                    {u'count': 12, u'value': u'bestof'},
                                    {u'count': 12, u'value': u'tf2'},
                                    {u'count': 11, u'value': u'AdviceAnimals'},
                                    {u'count': 9, u'value': u'todayilearned'},
                                    {u'count': 9, u'value': u'pics'},
                                    {u'count': 9, u'value': u'funny'}]}},
         u'hits': {u'found': 399,
                   u'hit': [{u'id': u't3_11111'},
                            {u'id': u't3_22222'},
                            {u'id': u't3_33333'},
                            {u'id': u't3_44444'},
                            ...
                            ],
                   u'start': 0},
         u'info': {u'cpu-time-ms': 10,
                   u'messages': [{u'code': u'CS-InvalidFieldOrRankAliasInRankParameter',
                                  u'message': u"Unable to create score object for rank '-hot'",
                                  u'severity': u'warning'}],
                   u'rid': u'<hash>',
                   u'time-ms': 9},
                   u'match-expr': u"(label 'my query')",
                   u'rank': u'-text_relevance'}
        
        '''
        try:
            response = basic_query(query=query, bq=bq, size=num, start=start,
                                   rank=sort, rank_expressions=rank_expressions,
                                   search_api=cls.search_api,
                                   faceting=faceting, record_stats=True)
        except (SearchHTTPError, SearchError) as e:
            g.log.error("Search Error: %r", e)
            raise

        warnings = response['info'].get('messages', [])
        for warning in warnings:
            g.log.warning("%(code)s (%(severity)s): %(message)s" % warning)

        hits = response['hits']['found']
        docs = [doc['id'] for doc in response['hits']['hit']]
        facets = response.get('facets', {})
        for facet in facets.keys():
            values = facets[facet]['constraints']
            facets[facet] = values

        results = Results(docs, hits, facets)
        return results


class LinkSearchQuery(CloudSearchQuery):
    search_api = g.CLOUDSEARCH_SEARCH_API
    sorts = {
        'relevance': '-relevance',
        'relevance2': '-relevance2',
        'hot': '-hot2',
        'top': '-top',
        'new': '-timestamp',
        'comments': '-num_comments',
    }
    recents = {
        'hour': timedelta(hours=1),
        'day': timedelta(days=1),
        'week': timedelta(days=7),
        'month': timedelta(days=31),
        'year': timedelta(days=366),
        'all': None,
        None: None,
    }
    schema = l2cs.make_schema(LinkFields.lucene_fieldnames())
    lucene_parser = l2cs.make_parser(
             int_fields=LinkFields.lucene_fieldnames(type_=int),
             yesno_fields=LinkFields.lucene_fieldnames(type_="yesno"),
             schema=schema)
    known_syntaxes = g.search_syntaxes
    default_syntax = "lucene"

    def customize_query(self, bq=u''):
        queries = []
        if bq:
            queries = [bq]
        if self.sr:
            subreddit_query = self._restrict_sr(self.sr)
            if subreddit_query:
                queries.append(subreddit_query)
        if self.recent:
            recent_query = self._restrict_recent(self.recent)
            queries.append(recent_query)
        if not self.include_over18:
            queries.append('over18:0')
        return self.create_boolean_query(queries)

    @staticmethod
    def _restrict_recent(recent):
        now = datetime.now(g.tz)
        since = epoch_seconds(now - recent)
        return 'timestamp:%i..' % since

    @staticmethod
    def _restrict_sr(sr):
        '''Return a cloudsearch appropriate query string that restricts
        results to only contain results from sr
        
        '''
        if isinstance(sr, MultiReddit):
            if not sr.sr_ids:
                raise InvalidQuery
            srs = ["sr_id:%s" % sr_id for sr_id in sr.sr_ids]
            return "(or %s)" % ' '.join(srs)
        elif isinstance(sr, DomainSR):
            return "site:'\"%s\"'" % sr.domain
        elif isinstance(sr, FriendsSR):
            if not c.user_is_loggedin or not c.user.friends:
                raise InvalidQuery
            # The query limit is roughly 8k bytes. Limit to 200 friends to
            # avoid getting too close to that limit
            friend_ids = c.user.friends[:200]
            friends = ["author_fullname:'%s'" %
                       Account._fullname_from_id36(r2utils.to36(id_))
                       for id_ in friend_ids]
            return "(or %s)" % ' '.join(friends)
        elif isinstance(sr, AllMinus):
            if not sr.exclude_sr_ids:
                raise InvalidQuery
            exclude_srs = ["sr_id:%s" % sr_id for sr_id in sr.exclude_sr_ids]
            return "(not (or %s))" % ' '.join(exclude_srs)
        elif not isinstance(sr, FakeSubreddit):
            return "sr_id:%s" % sr._id

        return None


class CloudSearchSubredditSearchQuery(CloudSearchQuery):
    search_api = g.CLOUDSEARCH_SUBREDDIT_SEARCH_API
    sorts = {
        'relevance': '-relevance',
        'activity': '-activity',
    }
    known_syntaxes = ("plain",)
    default_syntax = "plain"

    def preprocess_query(self, query):
        # Expand search for /r/subreddit to include subreddit name.
        sr = query.strip('/').split('/')
        if len(sr) == 2 and sr[0] == 'r' and Subreddit.is_valid_name(sr[1]):
            query = '"%s" | %s' % (query, sr[1])
        return query

    def customize_query(self, bq=u''):
        queries = []
        if bq:
            queries = [bq]
        if not self.include_over18:
            queries.append('over18:0')
        return self.create_boolean_query(queries)


class CloudSearchProvider(SearchProvider):
    '''Provider implementation: wrap it all up as a SearchProvider'''
    InvalidQuery = (InvalidQuery,)
    SearchException = (SearchHTTPError, SearchError)

    SearchQuery = LinkSearchQuery

    SubredditSearchQuery = CloudSearchSubredditSearchQuery

    def run_changed(self, drain=False, min_size=int(getattr(g, 'SOLR_MIN_BATCH', 500)), limit=1000, sleep_time=10, 
            use_safe_get=False, verbose=False):
        '''Run by `cron` (through `paster run`) on a schedule to send Things to Cloud
        '''
        if use_safe_get:
            CloudSearchUploader.use_safe_get = True
        amqp.handle_items('cloudsearch_changes', _run_changed, min_size=min_size,
                          limit=limit, drain=drain, sleep_time=sleep_time,
                          verbose=verbose)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import cPickle as pickle
from datetime import datetime, timedelta
import functools
import httplib
import json
import socket
import time
import urllib

from lxml import etree
from pylons import tmpl_context as c
from pylons import app_globals as g

from r2.lib import amqp, filters
from r2.lib.configparse import ConfigValue
from r2.lib.db.operators import desc
from r2.lib.db.sorts import epoch_seconds
from r2.lib.providers.search import SearchProvider
from r2.lib.providers.search.common import (
        InvalidQuery, 
        LinkFields, 
        Results,
        safe_get, 
        safe_xml_str,
        SearchError,
        SearchHTTPError, 
        SubredditFields, 
    )
import r2.lib.utils as r2utils
from r2.models import (
        Account, 
        All, 
        AllMinus,
        DefaultSR,
        DomainSR, 
        FakeSubreddit, 
        Friends, 
        Link, 
        MultiReddit, 
        NotFound,
        Subreddit, 
        Thing,
    )


_CHUNK_SIZE = 4000000 # Approx. 4 MB, to stay under the 5MB limit
DEFAULT_FACETS = {"reddit": {"count":20}}

WARNING_XPATH = ".//lst[@name='error']/str[@name='warning']"
STATUS_XPATH = ".//lst/int[@name='status']"

SORTS_DICT = {'text_relevance': 'score',
              'relevance': 'score'}

def basic_query(query=None, bq=None, faceting=None, size=1000,
                start=0, rank="", return_fields=None, record_stats=False,
                search_api=None):
    if search_api is None:
        search_api = g.solr_search_host
    if faceting is None:
        faceting = DEFAULT_FACETS
    path = _encode_query(query, faceting, size, start, rank, return_fields)
    timer = None
    if record_stats:
        timer = g.stats.get_timer("solrsearch_timer")
        timer.start()
    connection = httplib.HTTPConnection(search_api, g.solr_port)
    try:
        connection.request('GET', path)
        resp = connection.getresponse()
        response = resp.read()
        if record_stats:
            g.stats.action_count("event.search_query", resp.status)
        if resp.status >= 300:
            try:
                response_json = json.loads(response)
            except ValueError:
                pass
            else:
                if 'error' in response_json:
                    message = response_json['error'].get('msg', 'Unknown error')
                    raise InvalidQuery(resp.status, resp.reason, message,
                                       search_api, path, reasons)
            raise SearchHTTPError(resp.status, resp.reason,
                                  search_api, path, response)
    except socket.error as e:
        raise SearchError(e, search_api, path)
    finally:
        connection.close()
        if timer is not None:
            timer.stop()

    return json.loads(response)


basic_link = functools.partial(basic_query, size=10, start=0,
                               rank="",
                               return_fields=['title', 'reddit',
                                              'author_fullname'],
                               record_stats=False,
                               search_api=g.solr_search_host)


basic_subreddit = functools.partial(basic_query,
                                    faceting=None,
                                    size=10, start=0,
                                    rank="activity",
                                    return_fields=['title', 'reddit',
                                                   'author_fullname'],
                                    record_stats=False,
                                    search_api=g.solr_subreddit_search_host)


class SolrSearchQuery(object):
    '''Represents a search query sent to solr'''
    search_api = None
    recents = {None: None}
    default_syntax = "solr"

    def __init__(self, query, sr=None, sort=None, syntax=None, raw_sort=None,
                 faceting=None, recent=None, include_over18=True,
                 rank_expressions=None, start=0, num=1000):
        if syntax is None:
            syntax = self.default_syntax
        elif syntax not in self.known_syntaxes:
            raise ValueError("Unknown search syntax: %s" % syntax)
        self.bq = None
        self.query = filters._force_unicode(query or u'')
        self.converted_data = None
        self.syntax = syntax

        # filters
        self.sr = sr
        self._recent = recent
        self.recent = self.recents[recent]
        self.include_over18 = include_over18
        
        # rank / rank expressions
        self._sort = sort
        if raw_sort:
            self.sort = _translate_raw_sort(raw_sort)
        elif sort:
            self.sort = self.sorts.get(sort)
        else:
            self.sort = 'score'
        self.rank_expressions = rank_expressions

        # pagination
        self.start = start
        self.num = num

        # facets
        self.faceting = faceting
        
        self.results = None

    def run(self, after=None, reverse=False, num=1000, _update=False):
        self.bq = u''
        results = self._run(_update=_update)

        docs, hits, facets = results.docs, results.hits, results._facets

        after_docs = r2utils.get_after(docs, after, num, reverse=reverse)

        self.results = Results(after_docs, hits, facets)
        return self.results

    def _run(self, start=0, num=1000, _update=False):
        '''Run the search against self.query'''
        
        q = self.query.encode('utf-8')
        if g.sqlprinting:
            g.log.info("%s", self)
        q = self.customize_query(self.query)
        return self._run_cached(q, self.bq.encode('utf-8'), self.sort,
                                self.faceting, start=start, num=num,
                                _update=_update)

    def customize_query(self, q):
        return q

    def __repr__(self):
        '''Return a string representation of this query'''
        result = ["<", self.__class__.__name__, "> query:",
                  repr(self.query), " "]
        if self.bq:
            result.append(" bq:")
            result.append(repr(self.bq))
            result.append(" ")
        result.append("sort:")
        result.append(self.sort)
        return ''.join(result)

    @classmethod
    def _run_cached(cls, query, bq, sort="score", faceting=None, start=0,
                    num=1000, _update=False):
        '''Query the solr HOST. _update parameter allows for supposed
        easy memoization at later date.
        
        Example result set:
        {
            u'responseHeader':{
                u'status':0,
                u'QTime':2,
                u'params':{
                    u'sort':u'activity desc',
                    u'defType':u'edismax',
                    u'q':u'coffee',
                    u'start':u'0',
                    u'wt':u'json',
                    u'size':u'1000'
                }
            },
            u'response':{
                u'start':0,
                u'numFound':1,
                u'docs':[
                    {
                        u'_version_':1496564637825499136,
                        u'type_id':5,
                        u'reddit':u'coffee',
                        u'fullname':u't5_3',
                        u'author':u'grandpa',
                        u'url':u'http://hamsandwich.com/sideoffries/?attachment_id=44',
                        u'num_comments':0,
                        u'downs':1,
                        u'title':u'013',
                        u'site':u"[u'reddit.com',u'hamsandwich.reddit.com']", 
                        u'author_s': u'grandpa', 
                        u'over18': False, 
                        u'timestamp': 1427180669, 
                        u'sr_id': 2, 
                        u'author_fullname': u't2_1', 
                        u'is_self': False, 
                        u'subreddit': u'coffee', 
                        u'ups': 0, u'id': u't5_3'}, 
                    {
                ]
            }
        }
        '''
        if not query:
            return Results([], 0, {})
        try:
            response = basic_query(query=query, bq=bq, size=num, start=start,
                               rank=sort, search_api=cls.search_api,
                               faceting=faceting, record_stats=True)
        except (SearchHTTPError, SearchError) as e:
            g.log.error("Search Error: %r", e)
            raise

        hits = response['response']['numFound']
        docs = [doc['id'] for doc in response['response']['docs']]
        facets = {}
        if hits and faceting:
            facet_fields = response['facet_counts'].get('facet_fields', {})
            for field in facet_fields:
                facets[field] = []
                while facet_fields[field]:
                    value = facet_fields[field].pop(0)
                    count = facet_fields[field].pop(0)
                    facets[field].append(dict(value=value, count=count)) 
            

        results = Results(docs, hits, facets)
        return results


class LinkSearchQuery(SolrSearchQuery):
    search_api = g.solr_search_host
    sorts = {
        'relevance': 'score desc',
        'hot': 'max(hot/45000.0, 1.0) desc',
        'top': 'top desc',
        'new': 'timestamp desc',
        'comments': 'num_comments desc',
    }
    recents = {
        'hour': timedelta(hours=1),
        'day': timedelta(days=1),
        'week': timedelta(days=7),
        'month': timedelta(days=31),
        'year': timedelta(days=366),
        'all': None,
        None: None,
    }
    known_syntaxes = g.search_syntaxes
    default_syntax = 'solr'

    def customize_query(self, bq):
        queries = [bq]
        subreddit_query = self._get_sr_restriction(self.sr)
        if subreddit_query:
            queries.append(subreddit_query)
        if self.recent:
            recent_query = self._restrict_recent(self.recent)
            queries.append(recent_query)
        return self.create_boolean_query(queries)

    @classmethod
    def create_boolean_query(cls, queries):
        '''Return an AND clause combining all queries'''
        if len(queries) > 1:
            bq = ' AND  '.join(['(%s)' %q for q in queries]) 
        else:
            bq = queries[0]
        return bq

    @staticmethod
    def _restrict_recent(recent):
        now = datetime.now(g.tz)
        since = epoch_seconds(now - recent)
        return 'timestamp:%i..' % since

    @staticmethod
    def _get_sr_restriction(sr):
        '''Return a solr-appropriate query string that restricts
        results to only contain results from sr
        
        '''
        bq = []
        if (not sr) or sr == All or isinstance(sr, DefaultSR):
            return None
        elif isinstance(sr, MultiReddit):
            for sr_id in sr.sr_ids:
                bq.append("sr_id:%s" % sr_id)
        elif isinstance(sr, DomainSR):
            bq = ["site:'%s'" % sr.domain]
        elif sr == Friends:
            if not c.user_is_loggedin or not c.user.friends:
                return None
            friend_ids = c.user.friends
            friends = ["author_fullname:'%s'" %
                       Account._fullname_from_id36(r2utils.to36(id_))
                       for id_ in friend_ids]
            bq.extend(friends)
        elif isinstance(sr, AllMinus):
            for sr_id in sr.exclude_sr_ids:
                bq.append("-sr_id:%s" % sr_id)
        elif not isinstance(sr, FakeSubreddit):
            bq = ["sr_id:%s" % sr._id]
        return ' OR '.join(bq)


class SolrSubredditSearchQuery(SolrSearchQuery):
    search_api = g.solr_subreddit_search_host
    sorts = {
        'relevance': 'activity desc',
    }
    known_syntaxes = ("plain", "solr")
    default_syntax = "plain"


def _encode_query(query, faceting, size, start, rank, return_fields):
    if not query:
        raise ValueError("Need query")
    params = {}
    params["q"] = query
    params["wt"] = "json"
    #params["defType"] = "edismax"
    params["size"] = size
    params["start"] = start
    if rank: 
        params['sort'] = rank.strip().lower()
        if not params['sort'].split()[-1] in ['asc', 'desc']:
            params['sort'] = '%s desc' % params['sort']
    facet_limit = []
    facet_sort = []
    if faceting:
        params["facet"] = "true"
        params["facet.field"] = ",".join(faceting.iterkeys())
        for facet, options in faceting.iteritems():
            facet_limit.append(options.get("count", 20))
            if "sort" in options:
                if not options['sort'].split()[-1] in ['asc', 'desc']:
                    options['sort'] = '%s desc' % options['sort']
                facet_sort.append(options["sort"])
        params["facet.limit"] = ",".join([str(l) for l in facet_limit])
        params["facet.sort"] = ",".join(facet_sort)
        params["facet.sort"] = params["facet.sort"] or 'score desc' 
    if return_fields:
        params["qf"] = ",".join(return_fields)
    encoded_query = urllib.urlencode(params)
    if getattr(g, 'solr_version', '1').startswith('4'):
        path = '/solr/%s/select?%s' % \
            (getattr(g, 'solr_core', 'collection1'), encoded_query)
    else:
        path = '/solr/select?%s' %  encoded_query
    return path    


class SolrSearchUploader(object):
    
    def __init__(self, solr_host=None, solr_port=None, fullnames=None):
        self.solr_host = solr_host or g.solr_doc_host
        self.solr_port = solr_port or g.solr_port
        self.fullnames = fullnames    

    @classmethod
    def desired_fullnames(cls, items):
        '''Pull fullnames that represent instances of 'types' out of items'''

        fullnames = set()
        type_ids = [type_._type_id for type_ in cls.types]
        for item in items:
            item_type = r2utils.decompose_fullname(item['fullname'])[1]
            if item_type in type_ids:
                fullnames.add(item['fullname'])
        return fullnames

    def add_xml(self, thing):

        doc = etree.Element("doc")
        field = etree.SubElement(doc, "field", name='id')
        field.text = thing._fullname

        for field_name, value in self.fields(thing).iteritems():
            field = etree.SubElement(doc, "field", name=field_name)
            field.text = safe_xml_str(value)

        return doc

    def delete_xml(self, thing):
        '''Return the solr XML representation of
        "delete this from the index"
        
        '''
        delete = etree.fromstring('<id>%s</id>' % thing._id)
        return delete

    def delete_ids(self, ids):
        '''Delete documents from the index.
        'ids' should be a list of fullnames
        
        '''
        deletes = [etree.fromstring('<id>%s</id>' % id_) \
                for id_ in ids]


        batch = etree.Element("delete")
        batch.extend(deletes)
        return self.send_documents(batch)

    def batch_lookups(self):                                                                                            
        try:                                                                                                            
            self.things = Thing._by_fullname(self.fullnames, data=True,                                                 
                                             return_dict=False)                                                         
        except NotFound:                                                                                                
            if self.use_safe_get:                                                                                       
                self.things = safe_get(Thing._by_fullname, self.fullnames,                                              
                                       data=True, return_dict=False)                                                    
            else:                                                                                                       
                raise 

    def xml_from_things(self):
        '''Generate a <batch> XML tree to send to solr for
        adding/updating/deleting the given things
        
        '''
        add = etree.Element("add")
        delete = etree.Element("delete")
        commit = etree.Element("commit")
        commit.attrib["waitSearcher"] = "false"

        self.batch_lookups()
        for thing in self.things:
            try:
                if thing._spam or thing._deleted:
                    delete_node = self.delete_xml(thing)
                    delete.append(delete_node)
                elif self.should_index(thing):
                    add_node = self.add_xml(thing)
                    add.append(add_node)
            except (AttributeError, KeyError) as e:
                # Problem! Bail out, which means these items won't get
                # "consumed" from the queue. If the problem is from DB
                # lag or a transient issue, then the queue consumer
                # will succeed eventually. If it's something else,
                # then manually run a consumer with 'use_safe_get'
                # on to get past the bad Thing in the queue
                if not self.use_safe_get:
                    raise
                else:
                    g.log.warning("Ignoring problem on thing %r.\n\n%r",
                                  thing, e)

        elems = []
        if len(add):
            elems.append(add)
        if len(delete):
            elems.append(delete)
        if elems:
            # Only need to commit if something is sent
            elems.append(commit)
        return elems


    def inject(self, quiet=False):
        '''Send things to solr. Return value is time elapsed, in seconds,
        of the communication with the solr endpoint
        
        '''

        xml_things = self.xml_from_things()

        cs_time = 0
        for batch in xml_things:

            cs_start = datetime.now(g.tz)
            sent = self.send_documents(batch)
            cs_time = cs_time + (datetime.now(g.tz) - cs_start).total_seconds()

            adds, deletes, warnings = 0, 0, []
            for record in sent:
                response = etree.fromstring(record)
                status = response.find(STATUS_XPATH).text
                if status == '0':
                    # success! 
                    adds += len(batch.findall('doc'))
                    deletes += len(batch.findall('delete'))
                    for w in response.find(WARNING_XPATH) or []:
                        warnings.append(w.text)

            g.stats.simple_event("solrsearch.uploads.adds", delta=adds)
            g.stats.simple_event("solrsearch.uploads.deletes", delta=deletes)
            g.stats.simple_event("solrsearch.uploads.warnings",
                    delta=len(warnings))

            if not quiet:
                print "%s Changes: +%i -%i" % (self.__class__.__name__,
                                               adds, deletes)
                if len(warnings):
                    print "%s Warnings: %s" % (self.__class__.__name__,
                                               "; ".join(warnings))

        return cs_time    

    def send_documents(self, docs):
        '''Open a connection to the Solr endpoint, and send the documents
        for indexing. Multiple requests are sent if a large number of documents
        are being sent (see chunk_xml())
        
        Raises SearchHTTPError if the endpoint indicates a failure
        '''
        core = getattr(g, 'solr_core', 'collection1') 
        responses = []
        connection = httplib.HTTPConnection(self.solr_host, self.solr_port)
        chunker = chunk_xml(docs)
        headers = {}
        headers['Content-Type'] = 'application/xml'
        try:
            for data in chunker:
                # HTTPLib calculates Content-Length header automatically
                if getattr(g, 'solr_version', '1').startswith('4'):
                    connection.request('POST', "/solr/%s/update/" % core,
                                       data, headers)
                else:     
                    connection.request('POST', "/solr/update/",
                                       data, headers)
                response = connection.getresponse()
                if 200 <= response.status < 300:
                    responses.append(response.read())
                else:
                    raise SearchHTTPError(response.status,
                                               response.reason,
                                               response.read())
        finally:
            connection.close()
        return responses


def chunk_xml(xml, depth=0):
    '''Chunk POST data into pieces that are smaller than the 20 MB limit.
    
    Ideally, this never happens (if chunking is necessary, would be better
    to avoid xml'ifying before testing content_length)'''
    data = etree.tostring(xml)
    root = xml.findall('add') and 'add' or 'delete'
    content_length = len(data)
    if content_length < _CHUNK_SIZE:
        yield data
    else:
        depth += 1
        print "WARNING: Chunking (depth=%s)" % depth
        half = len(xml) / 2
        left_half = xml # for ease of reading
        right_half = etree.Element(root)
        # etree magic simultaneously removes the elements from one tree
        # when they are appended to a different tree
        right_half.append(xml[half:])
        for chunk in chunk_xml(left_half, depth=depth):
            yield chunk
        for chunk in chunk_xml(right_half, depth=depth):
            yield chunk


@g.stats.amqp_processor('solrsearch_q')
def _run_changed(msgs, chan):
    '''Consume the cloudsearch_changes queue, and print reporting information
    on how long it took and how many remain
    
    '''
    start = datetime.now(g.tz)

    changed = [pickle.loads(msg.body) for msg in msgs]

    link_fns = SolrLinkUploader.desired_fullnames(changed)
    sr_fns = SolrSubredditUploader.desired_fullnames(changed)

    link_uploader = SolrLinkUploader(g.solr_doc_host, fullnames=link_fns)
    subreddit_uploader = SolrSubredditUploader(g.solr_subreddit_doc_host,
                                           fullnames=sr_fns)

    link_time = link_uploader.inject()
    subreddit_time = subreddit_uploader.inject()
    solrsearch_time = link_time + subreddit_time

    totaltime = (datetime.now(g.tz) - start).total_seconds()

    print ("%s: %d messages in %.2fs seconds (%.2fs secs waiting on "
           "solr); %d duplicates, %s remaining)" %
           (start, len(changed), totaltime, solrsearch_time,
            len(changed) - len(link_fns | sr_fns),
            msgs[-1].delivery_info.get('message_count', 'unknown')))


class SolrLinkUploader(SolrSearchUploader):
    types = (Link,)

    def __init__(self, solr_host=None, solr_port=None, fullnames=None):
        super(SolrLinkUploader, self).__init__(fullnames=fullnames)
        self.accounts = {}
        self.srs = {}

    def fields(self, thing):
        '''Return fields relevant to a Link search index'''
        account = self.accounts[thing.author_id]
        sr = self.srs[thing.sr_id]
        return LinkFields(thing, account, sr).fields()

    def batch_lookups(self):
        super(SolrLinkUploader, self).batch_lookups()
        author_ids = [thing.author_id for thing in self.things
                      if hasattr(thing, 'author_id')]
        try:
            self.accounts = Account._byID(author_ids, data=True,
                                          return_dict=True)
        except NotFound:
            if self.use_safe_get:
                self.accounts = safe_get(Account._byID, author_ids, data=True,
                                         return_dict=True)
            else:
                raise

        sr_ids = [thing.sr_id for thing in self.things
                  if hasattr(thing, 'sr_id')]
        try:
            self.srs = Subreddit._byID(sr_ids, data=True, return_dict=True)
        except NotFound:
            if self.use_safe_get:
                self.srs = safe_get(Subreddit._byID, sr_ids, data=True,
                                    return_dict=True)
            else:
                raise

    def should_index(self, thing):
        return (thing.promoted is None and getattr(thing, "sr_id", None) != -1)
    

class SolrSubredditUploader(SolrSearchUploader):
    types = (Subreddit,)

    def fields(self, thing):
        return SubredditFields(thing).fields()

    def should_index(self, thing):
        return thing._id != Subreddit.get_promote_srid()

 
def _progress_key(item):
    return "%s/%s" % (item._id, item._date)


def _rebuild_link_index(start_at=None, sleeptime=1, cls=Link,
                       uploader=SolrLinkUploader, estimate=50000000, 
                       chunk_size=1000):
    uploader = uploader()

    q = cls._query(cls.c._deleted == (True, False), sort=desc('_date'))

    if start_at:
        after = cls._by_fullname(start_at)
        assert isinstance(after, cls)
        q._after(after)

    q = r2utils.fetch_things2(q, chunk_size=chunk_size)
    q = r2utils.progress(q, verbosity=1000, estimate=estimate, persec=True,
                         key=_progress_key)
    for chunk in r2utils.in_chunks(q, size=chunk_size):
        uploader.things = chunk
        uploader.fullnames = [c._fullname for c in chunk] 
        for x in range(5):
            try:
                uploader.inject()
            except httplib.HTTPException as err:
                print "Got %s, sleeping %s secs" % (err, x)
                time.sleep(x)
                continue
            else:
                break
        else:
            raise err
        last_update = chunk[-1]
        print "last updated %s" % last_update._fullname
        time.sleep(sleeptime)


rebuild_subreddit_index = functools.partial(_rebuild_link_index,
                                            cls=Subreddit,
                                            uploader=SolrSubredditUploader,
                                            estimate=200000,
                                            chunk_size=1000)


def test_run_link(start_link, count=1000):
    '''Inject `count` number of links, starting with `start_link`'''
    if isinstance(start_link, basestring):
        start_link = int(start_link, 36)
    links = Link._byID(range(start_link - count, start_link), data=True,
                       return_dict=False)
    uploader = SolrLinkUploader(things=links)
    return uploader.inject()


def test_run_srs(*sr_names):
    '''Inject Subreddits by name into the index'''
    srs = Subreddit._by_name(sr_names).values()
    uploader = SolrSubredditUploader(things=srs)
    return uploader.inject()


def _translate_raw_sort(sort):
    '''translate from cloudsearch syntax'''
    sort_dir = ''
    if sort.startswith('-'):
        sort = sort[1:]
        sort_dir = ' desc'
    sort = SORTS_DICT.get(sort, sort) 
    return '%s%s' % (sort, sort_dir)

class SolrSearchProvider(SearchProvider):
    '''Provider implementation: wrap it all up as a SearchProvider
    
    example config:
    # version of solr service--versions 1.x and 4.x have been tested. 
    # only the major version number matters here
    solr_version = 1
    # solr search service hostname or IP
    solr_search_host = 127.0.0.1
    # hostname or IP for link upload
    solr_doc_host = 127.0.0.1
    # hostname or IP for subreddit search
    solr_subreddit_search_host = 127.0.0.1
    # hostname or IP subreddit upload
    solr_subreddit_doc_host = 127.0.0.1
    # solr port (assumed same on all hosts)
    solr_port = 8080
    # solr4 core name (not used with Solr 1.x)
    solr_core = collection1
    # default batch size 
    # limit is hard-coded to 1000
    # set to 1 for testing
    solr_min_batch = 500
    # optionally, you may select your solr query parser here
    # see documentation for your version of Solr
    solr_query_parser = 
    '''

    SOLR_VERSION = 1
  
    config = {
        ConfigValue.int: [
            "solr_port",
            "solr_min_batch",
        ],
        ConfigValue.str: [
            "solr_search_host",
            "solr_doc_host",
            "solr_subreddit_search_host",
            "solr_subreddit_doc_host",
            "solr_core",
            "solr_version",
        ],
    }    

    InvalidQuery = (InvalidQuery,)
    SearchException = (SearchHTTPError, SearchError)

    SearchQuery = LinkSearchQuery

    SubredditSearchQuery = SolrSubredditSearchQuery
    
    def run_changed(self, drain=False, min_size=int(getattr(g, 'solr_min_batch', 500)), limit=1000, sleep_time=10, 
            use_safe_get=False, verbose=False):
        '''Run by `cron` (through `paster run`) on a schedule to send Things to Solr
        '''
        if use_safe_get:
            SolrSearchUploader.use_safe_get = True
        amqp.handle_items('cloudsearch_changes', _run_changed, min_size=min_size,
                          limit=limit, drain=drain, sleep_time=sleep_time,
                          verbose=verbose)

    def rebuild_link_index(self, start_at=None, sleeptime=1, cls=Link,
                           uploader=SolrLinkUploader, estimate=50000000, 
                           chunk_size=1000):
         _rebuild_link_index(start_at, sleeptime, cls, uploader, estimate,  
                            chunk_size)
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


class SearchProvider(object):
    """Provider for search.
    """

    def InvalidQuery(self):
        raise NotImplementedError

    def SearchException(self):
        raise NotImplementedError

    def Query(self):
        raise NotImplementedError

    def SubredditSearchQuery(self):
        raise NotImplementedError

    def sorts(self):
        raise NotImplementedError

    def run_changed(self):
        raise NotImplementedError
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
from pylons import tmpl_context as c
from pylons import app_globals as g
import collections
import httplib
import time
import re

import r2.lib.utils as r2utils
from r2.models import (Link, NotFound, Subreddit)


class InvalidQuery(Exception):
    pass


class SearchError(Exception):
    pass


class SearchHTTPError(httplib.HTTPException):
    pass


def safe_xml_str(s, use_encoding="utf-8"):
    '''Replace invalid-in-XML unicode control characters with '\uFFFD'.
    Also, coerces result to unicode
    
    '''
    illegal_xml = re.compile(u'[\x00-\x08\x0b\x0c\x0e-\x1F\uD800-\uDFFF\uFFFE\uFFFF]')

    if not isinstance(s, unicode):
        if isinstance(s, str):
            s = unicode(s, use_encoding, errors="replace")
        else:
            # ints will raise TypeError if the "errors" kwarg
            # is passed, but since it's not a str no problem
            s = unicode(s)
    s = illegal_xml.sub(u"\uFFFD", s)
    return s


def safe_get(get_fn, ids, return_dict=True, **kw):
    items = {}
    for i in ids:
        try:
            item = get_fn(i, **kw)
        except NotFound:
            g.log.info("%s failed for %r", get_fn.__name__, i)
        else:
            items[i] = item
    if return_dict:
        return items
    else:
        return items.values()



SAME_AS_CLOUDSEARCH = object()
FIELD_TYPES = (int, str, datetime, SAME_AS_CLOUDSEARCH, "yesno")

def field(name=None, cloudsearch_type=str, lucene_type=SAME_AS_CLOUDSEARCH):
    Field = collections.namedtuple("Field", "name cloudsearch_type "
                                   "lucene_type function")
    if lucene_type is SAME_AS_CLOUDSEARCH:
        lucene_type = cloudsearch_type
    if cloudsearch_type not in FIELD_TYPES + (None,):
        raise ValueError("cloudsearch_type %r not in %r" %
                         (cloudsearch_type, FIELD_TYPES))
    if lucene_type not in FIELD_TYPES + (None,):
        raise ValueError("lucene_type %r not in %r" %
                         (lucene_type, FIELD_TYPES))
    if callable(name):
        # Simple case; decorated as '@field'; act as a decorator instead
        # of a decorator factory
        function = name
        name = None
    else:
        function = None

    def field_inner(fn):
        fn.field = Field(name or fn.func_name, cloudsearch_type,
                         lucene_type, fn)
        return fn

    if function:
        return field_inner(function)
    else:
        return field_inner


class FieldsMeta(type):
    def __init__(cls, name, bases, attrs):
        type.__init__(cls, name, bases, attrs)
        fields = []
        for attr in attrs.itervalues():
            if hasattr(attr, "field"):
                fields.append(attr.field)
        cls._fields = tuple(fields)


class FieldsBase(object):
    __metaclass__ = FieldsMeta

    def fields(self):
        data = {}
        for field in self._fields:
            if field.cloudsearch_type is None:
                continue
            val = field.function(self)
            if val is not None:
                data[field.name] = val
        return data

    @classmethod
    def all_fields(cls):
        return cls._fields

    @classmethod
    def cloudsearch_fields(cls, type_=None, types=FIELD_TYPES):
        types = (type_,) if type_ else types
        return [f for f in cls._fields if f.cloudsearch_type in types]

    @classmethod
    def lucene_fields(cls, type_=None, types=FIELD_TYPES):
        types = (type_,) if type_ else types
        return [f for f in cls._fields if f.lucene_type in types]

    @classmethod
    def cloudsearch_fieldnames(cls, type_=None, types=FIELD_TYPES):
        return [f.name for f in cls.cloudsearch_fields(type_=type_,
                                                       types=types)]

    @classmethod
    def lucene_fieldnames(cls, type_=None, types=FIELD_TYPES):
        return [f.name for f in cls.lucene_fields(type_=type_, types=types)]


class LinkFields(FieldsBase):
    def __init__(self, link, author, sr):
        self.link = link
        self.author = author
        self.sr = sr

    @field(cloudsearch_type=int, lucene_type=None)
    def ups(self):
        return max(0, self.link._ups)

    @field(cloudsearch_type=int, lucene_type=None)
    def downs(self):
        return max(0, self.link._downs)

    @field(cloudsearch_type=int, lucene_type=None)
    def num_comments(self):
        return max(0, getattr(self.link, 'num_comments', 0))

    @field
    def fullname(self):
        return self.link._fullname

    @field
    def subreddit(self):
        return self.sr.name

    @field
    def reddit(self):
        return self.sr.name

    @field
    def title(self):
        return self.link.title

    @field(cloudsearch_type=int)
    def sr_id(self):
        return self.link.sr_id

    @field(cloudsearch_type=int, lucene_type=datetime)
    def timestamp(self):
        return int(time.mktime(self.link._date.utctimetuple()))

    @field(cloudsearch_type=int, lucene_type="yesno")
    def over18(self):
        nsfw = self.sr.over_18 or self.link.is_nsfw
        return (1 if nsfw else 0)

    @field(cloudsearch_type=None, lucene_type="yesno")
    def nsfw(self):
        return NotImplemented

    @field(cloudsearch_type=int, lucene_type="yesno")
    def is_self(self):
        return (1 if self.link.is_self else 0)

    @field(name="self", cloudsearch_type=None, lucene_type="yesno")
    def self_(self):
        return NotImplemented

    @field
    def author_fullname(self):
        return None if self.author._deleted else self.author._fullname

    @field(name="author")
    def author_field(self):
        return None if self.author._deleted else self.author.name

    @field(cloudsearch_type=int)
    def type_id(self):
        return self.link._type_id

    @field
    def site(self):
        if self.link.is_self:
            return g.domain
        else:
            try:
                url = r2utils.UrlParser(self.link.url)
                return list(url.domain_permutations())
            except ValueError:
                return None

    @field
    def selftext(self):
        if self.link.is_self and self.link.selftext:
            return self.link.selftext
        else:
            return None

    @field
    def url(self):
        if not self.link.is_self:
            return self.link.url
        else:
            return None

    @field
    def flair_css_class(self):
        return self.link.flair_css_class

    @field
    def flair_text(self):
        return self.link.flair_text

    @field(cloudsearch_type=None, lucene_type=str)
    def flair(self):
        return NotImplemented


class SubredditFields(FieldsBase):
    def __init__(self, sr):
        self.sr = sr

    @field
    def name(self):
        return self.sr.name

    @field
    def title(self):
        return self.sr.title

    @field(name="type")
    def type_(self):
        return self.sr.type

    @field
    def language(self):
        return self.sr.lang

    @field
    def header_title(self):
        return None if self.sr.type == 'private' else self.sr.header_title

    @field
    def description(self):
        return self.sr.public_description

    @field
    def sidebar(self):
        return None if self.sr.type == 'private' else self.sr.description

    @field(cloudsearch_type=int)
    def over18(self):
        return 1 if self.sr.over_18 else 0

    @field
    def link_type(self):
        return self.sr.link_type

    @field
    def activity(self):
        return self.sr._downs

    @field
    def subscribers(self):
        return self.sr._ups

    @field
    def type_id(self):
        return self.sr._type_id


class Results(object):
    def __init__(self, docs, hits, facets):
        self.docs = docs
        self.hits = hits
        self._facets = facets
        self._subreddits = []

    def __repr__(self):
        return '%s(%r, %r, %r)' % (self.__class__.__name__,
                                   self.docs,
                                   self.hits,
                                   self._facets)

    @property
    def subreddit_facets(self):
        '''Filter out subreddits that the user isn't allowed to see'''
        if not self._subreddits and 'reddit' in self._facets:
            sr_facets = [(sr['value'], sr['count']) for sr in
                         self._facets['reddit']]

            # look up subreddits
            srs_by_name = Subreddit._by_name([name for name, count
                                              in sr_facets])

            sr_facets = [(srs_by_name[name], count) for name, count
                         in sr_facets if name in srs_by_name]

            # filter by can_view
            self._subreddits = [(sr, count) for sr, count in sr_facets
                                if sr.can_view(c.user)]

        return self._subreddits
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class EmailProvider(object):
    """Provider for sending emails.

    """
    def send_email(self, to_address, from_address, subject, text, reply_to,
                       parent_email_id=None, other_email_ids=None):
        """Send an email.

        `to_address` is a string or list of string email addresses.

        `from_address` is a email address.

        `subject` is the email's subject.

        `text` is the text of the email.

        `reply_to` is the reply-to address, which will be sent as the "Reply-To"
        email header.

        `parent_email_id` is the Message-Id of the email this is a reply to (if
        any). This is sent as the "In-Reply-To" email header.

        `other_email_ids` is a list of Message-Ids of emails in the conversation
        and including `parent_email_id`. This will be converted to a space
        delimited string and sent as the "References" email header.

        The return value is the Message-Id of the sent email.

        """
        raise NotImplementedError


class EmailSendError(Exception): pass
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.providers.email import EmailProvider


class NullEmailProvider(EmailProvider):
    """A no-op email provider.

    """

    def send_email(self, to_address, from_address, subject, text, reply_to,
                       parent_email_id=None, other_email_ids=None):
        return None
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################


import json
import re

import requests

from r2.lib.configparse import ConfigValue
from r2.lib.providers.email import (
    EmailProvider,
    EmailSendError,
)
from r2.lib.utils import tup


class MailgunEmailProvider(EmailProvider):
    """A provider that uses mailgun to send emails."""

    config = {
        ConfigValue.str: [
            "mailgun_api_base_url",
        ],
        # This also requires the secret "mailgun_api_key"
    }

    def send_email(self, to_address, from_address, subject, text, reply_to,
                   parent_email_id=None, other_email_ids=None):
        from pylons import app_globals as g

        if not text and not html:
            msg = "must provide either text or html in email body"
            raise TypeError(msg)

        # pick out the mailgun domain from the from_address field domain
        from_domain_match = re.search("@([\w.]+)", from_address)

        if from_domain_match is None:
            raise ValueError("from address is malformed")

        mailgun_domain = from_domain_match.group(1)

        if mailgun_domain not in g.mailgun_domains:
            raise ValueError("from address must be from an approved domain")

        message_post_url = "/".join((
            g.mailgun_api_base_url, mailgun_domain, "messages"))

        to_address = tup(to_address)
        parent_email_id = parent_email_id or ''
        other_email_ids = other_email_ids or []

        response = requests.post(
            message_post_url,
            auth=("api", g.secrets['mailgun_api_key']),
            data={
                "from": from_address,
                "to": to_address,
                "subject": subject,
                "text": text,
                "o:tracking": False,    # disable link rewriting
                "h:Reply-To": reply_to,
                "h:In-Reply-To": parent_email_id,
                "h:References": " ".join(other_email_ids),
            },
        )

        if response.status_code != 200:
            msg = "mailgun sending email failed {status}: {text}".format(
                status=response.status_code, text=response.text)
            raise EmailSendError(msg)

        try:
            body = json.loads(response.text)
        except ValueError:
            msg = "mailgun sending email bad response {status}: {text}".format(
                status=response.status_code, text=response.text)
            g.stats.simple_event("mailgun.outgoing.failure")
            raise EmailSendError(msg)

        g.stats.simple_event("mailgun.outgoing.success")
        email_id = body["id"]
        return email_id
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import json
import requests

from pylons import app_globals as g

from r2.lib.providers.cdn import CdnProvider
from r2.lib.utils import constant_time_compare

class CloudFlareCdnProvider(CdnProvider):
    """A provider for reddit's configuration of CloudFlare.

    """
 
    def _do_content_purge(self, url):  
        """Does the purge of the content from CloudFlare."""      
        data = {
            'files': [
                url,
            ]
        }

        timer = g.stats.get_timer("providers.cloudflare.content_purge")
        timer.start()
        response = requests.delete(
            g.secrets['cloudflare_purge_key_url'],
            headers={
                'X-Auth-Email': g.secrets['cloudflare_email_address'],
                'X-Auth-Key': g.secrets['cloudflare_api_key'],
                'content-type': 'application/json',
            },
            data=json.dumps(data),
        )
        timer.stop()

    def get_client_ip(self, environ):
        try:
            client_ip = environ["HTTP_CF_CONNECTING_IP"]
            provided_hash = environ["HTTP_CF_CIP_TAG"].lower()
        except KeyError:
            return None

        secret = g.secrets["cdn_ip_verification"]
        expected_hash = hashlib.sha1(client_ip + secret).hexdigest()

        if not constant_time_compare(expected_hash, provided_hash):
            return None

        return client_ip

    def get_client_location(self, environ):
        return environ.get("HTTP_CF_IPCOUNTRY", None)

    def purge_content(self, url):
        """Purges the content specified by url from the cache."""

        # per the CloudFlare docs:
        #    https://www.cloudflare.com/docs/client-api.html#s4.5
        #    The full URL of the file that needs to be purged from 
        #    CloudFlare's  cache. Keep in mind, that if an HTTP and 
        #    an HTTPS version of the file exists, then both versions 
        #    will need to be purged independently
        # create the "alternate" URL for http or https
        if 'https://' in url:
            url_altered = url.replace('https://', 'http://')
        else:
            url_altered = url.replace('http://', 'https://')

        self._do_content_purge(url)
        self._do_content_purge(url_altered)

        return True
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

class CdnProvider(object):
    """Provider for handling Content Delivery Network (CDN) interactions.

    """

    def get_client_ip(self, environ):
        """Verify and return the CDN-provided remote IP address.

        For requests coming through the CDN, the value of REMOTE_ADDR will be
        the IP address of one of the CDN's edge nodes rather than that of the
        client itself.  The CDN can provide the remote client's true IP as well
        as verification that the provided IP is indeed accurate via extra
        headers, the details of which vary with each CDN.

        This function should analyze the request environment and return the
        remote client's true IP address if it's present and validates. If it is
        not present or does not validate, it should return None.

        """
        raise NotImplementedError

    def get_client_location(self, environ):
        """Return CDN-provided geo location data for the requester.

        The return value is an ISO 3166-1 Alpha 2 format country code or None.

        This function is only defined when get_client_ip returns a non-None
        value, i.e. when the request has been validated as being from the CDN.

        """
        raise NotImplementedError

    def purge_content(self, url):
        """Purge content from the CDN by URL"""
        raise NotImplementedError
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.providers.cdn import CdnProvider


class NullCdnProvider(CdnProvider):
    """A no-op provider for when no CDN is present.

    """

    def get_client_ip(self, environ):
        return None

    def get_client_location(self, environ):
        return None
<EOF>
<BOF>
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2015 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import requests
from pylons import app_globals as g

from r2.lib.providers.cdn import CdnProvider
from r2.lib.utils import constant_time_compare


class FastlyCdnProvider(CdnProvider):
    """A provider for reddit's configuration of Fastly."""

    def get_client_ip(self, environ):
        try:
            client_ip = environ["HTTP_CF_CONNECTING_IP"]
            provided_hash = environ["HTTP_CF_CIP_TAG"].lower()
        except KeyError:
            return None

        secret = g.secrets["cdn_ip_verification"]
        expected_hash = hashlib.sha1(client_ip + secret).hexdigest()

        if not constant_time_compare(expected_hash, provided_hash):
            return None

        return client_ip

    def get_client_location(self, environ):
        return environ.get("HTTP_CF_IPCOUNTRY", None)

    def purge_content(self, url):
        """Purge the content specified by url from the cache.

        https://docs.fastly.com/api/purge#purge

        """

        with g.stats.get_timer('providers.fastly.content_purge'):
            response = requests.request('PURGE', url)
<EOF>
