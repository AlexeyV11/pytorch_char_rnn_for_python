<BOF>
#!/usr/bin/env python
"""
CLI to manage redash.
"""

from redash.cli import manager

if __name__ == '__main__':
    manager()
<EOF>
<BOF>
from unittest import TestCase

from jsonschema import ValidationError

from redash.utils.configuration import ConfigurationContainer


configuration_schema = {
    "type": "object",
    "properties": {
        "a": {
            "type": "integer"
        },
        "e": {
            "type": "integer"
        },
        "b": {
            "type": "string"
        }
    },
    "required": ["a"],
    "secret": ["b"]
}


class TestConfigurationToJson(TestCase):
    def setUp(self):
        self.config = {'a': 1, 'b': 'test'}
        self.container = ConfigurationContainer(self.config, configuration_schema)

    def test_returns_plain_dict(self):
        self.assertDictEqual(self.config, self.container.to_dict())

    def test_raises_exception_when_no_schema_set(self):
        self.container.set_schema(None)
        self.assertRaises(RuntimeError, lambda: self.container.to_dict(mask_secrets=True))

    def test_returns_dict_with_masked_secrets(self):
        d = self.container.to_dict(mask_secrets=True)

        self.assertEqual(d['a'], self.config['a'])
        self.assertNotEqual(d['b'], self.config['b'])

        self.assertEqual(self.config['b'], self.container['b'])


class TestConfigurationUpdate(TestCase):
    def setUp(self):
        self.config = {'a': 1, 'b': 'test'}
        self.container = ConfigurationContainer(self.config, configuration_schema)

    def test_rejects_invalid_new_config(self):
        self.assertRaises(ValidationError, lambda: self.container.update({'c': 3}))

    def test_fails_if_no_schema_set(self):
        self.container.set_schema(None)
        self.assertRaises(RuntimeError, lambda: self.container.update({'c': 3}))

    def test_ignores_secret_placehodler(self):
        self.container.update(self.container.to_dict(mask_secrets=True))
        self.assertEqual(self.container['b'], self.config['b'])

    def test_updates_secret(self):
        new_config = {'a': 2, 'b': 'new'}
        self.container.update(new_config)
        self.assertDictEqual(self.container._config, new_config)

    def test_doesnt_leave_leftovers(self):
        container = ConfigurationContainer({'a': 1, 'b': 'test', 'e': 3}, configuration_schema)
        new_config = container.to_dict(mask_secrets=True)
        new_config.pop('e')
        container.update(new_config)

        self.assertEqual(container['a'], 1)
        self.assertEqual('test', container['b'])
        self.assertNotIn('e', container)

    def test_works_for_schema_without_secret(self):
        secretless = configuration_schema.copy()
        secretless.pop('secret')
        container = ConfigurationContainer({'a': 1, 'b': 'test', 'e': 3}, secretless)
        container.update({'a': 2})
        self.assertEqual(container['a'], 2)
<EOF>
<BOF>
import os
import time

from flask import request
from mock import patch
from six.moves import reload_module
from sqlalchemy.orm.exc import NoResultFound
from tests import BaseTestCase

from redash import models, settings
from redash.authentication import (api_key_load_user_from_request,
                                   get_login_url, hmac_load_user_from_request,
                                   sign)
from redash.authentication.google_oauth import (create_and_login_user,
                                                verify_profile)


class TestApiKeyAuthentication(BaseTestCase):
    #
    # This is a bad way to write these tests, but the way Flask works doesn't make it easy to write them properly...
    #
    def setUp(self):
        super(TestApiKeyAuthentication, self).setUp()
        self.api_key = '10'
        self.query = self.factory.create_query(api_key=self.api_key)
        models.db.session.flush()
        self.query_url = '/{}/api/queries/{}'.format(self.factory.org.slug, self.query.id)
        self.queries_url = '/{}/api/queries'.format(self.factory.org.slug)

    def test_no_api_key(self):
        with self.app.test_client() as c:
            rv = c.get(self.query_url)
            self.assertIsNone(api_key_load_user_from_request(request))

    def test_wrong_api_key(self):
        with self.app.test_client() as c:
            rv = c.get(self.query_url, query_string={'api_key': 'whatever'})
            self.assertIsNone(api_key_load_user_from_request(request))

    def test_correct_api_key(self):
        with self.app.test_client() as c:
            rv = c.get(self.query_url, query_string={'api_key': self.api_key})
            self.assertIsNotNone(api_key_load_user_from_request(request))

    def test_no_query_id(self):
        with self.app.test_client() as c:
            rv = c.get(self.queries_url, query_string={'api_key': self.api_key})
            self.assertIsNone(api_key_load_user_from_request(request))

    def test_user_api_key(self):
        user = self.factory.create_user(api_key="user_key")
        models.db.session.flush()
        with self.app.test_client() as c:
            rv = c.get(self.queries_url, query_string={'api_key': user.api_key})
            self.assertEqual(user.id, api_key_load_user_from_request(request).id)

    def test_api_key_header(self):
        with self.app.test_client() as c:
            rv = c.get(self.query_url, headers={'Authorization': "Key {}".format(self.api_key)})
            self.assertIsNotNone(api_key_load_user_from_request(request))

    def test_api_key_header_with_wrong_key(self):
        with self.app.test_client() as c:
            rv = c.get(self.query_url, headers={'Authorization': "Key oops"})
            self.assertIsNone(api_key_load_user_from_request(request))

    def test_api_key_for_wrong_org(self):
        other_user = self.factory.create_admin(org=self.factory.create_org())

        with self.app.test_client() as c:
            rv = c.get(self.query_url, headers={'Authorization': "Key {}".format(other_user.api_key)})
            self.assertEqual(404, rv.status_code)


class TestHMACAuthentication(BaseTestCase):
    #
    # This is a bad way to write these tests, but the way Flask works doesn't make it easy to write them properly...
    #
    def setUp(self):
        super(TestHMACAuthentication, self).setUp()
        self.api_key = '10'
        self.query = self.factory.create_query(api_key=self.api_key)
        models.db.session.flush()
        self.path = '/{}/api/queries/{}'.format(self.query.org.slug, self.query.id)
        self.expires = time.time() + 1800

    def signature(self, expires):
        return sign(self.query.api_key, self.path, expires)

    def test_no_signature(self):
        with self.app.test_client() as c:
            rv = c.get(self.path)
            self.assertIsNone(hmac_load_user_from_request(request))

    def test_wrong_signature(self):
        with self.app.test_client() as c:
            rv = c.get(self.path, query_string={'signature': 'whatever', 'expires': self.expires})
            self.assertIsNone(hmac_load_user_from_request(request))

    def test_correct_signature(self):
        with self.app.test_client() as c:
            rv = c.get(self.path, query_string={'signature': self.signature(self.expires), 'expires': self.expires})
            self.assertIsNotNone(hmac_load_user_from_request(request))

    def test_no_query_id(self):
        with self.app.test_client() as c:
            rv = c.get('/{}/api/queries'.format(self.query.org.slug), query_string={'api_key': self.api_key})
            self.assertIsNone(hmac_load_user_from_request(request))

    def test_user_api_key(self):
        user = self.factory.create_user(api_key="user_key")
        path = '/api/queries/'
        models.db.session.flush()

        signature = sign(user.api_key, path, self.expires)
        with self.app.test_client() as c:
            rv = c.get(path, query_string={'signature': signature, 'expires': self.expires, 'user_id': user.id})
            self.assertEqual(user.id, hmac_load_user_from_request(request).id)


class TestCreateAndLoginUser(BaseTestCase):
    def test_logins_valid_user(self):
        user = self.factory.create_user(email=u'test@example.com')

        with patch('redash.authentication.login_user') as login_user_mock:
            create_and_login_user(self.factory.org, user.name, user.email)
            login_user_mock.assert_called_once_with(user, remember=True)

    def test_creates_vaild_new_user(self):
        email = u'test@example.com'
        name = 'Test User'

        with patch('redash.authentication.login_user') as login_user_mock:
            create_and_login_user(self.factory.org, name, email)

            self.assertTrue(login_user_mock.called)
            user = models.User.query.filter(models.User.email == email).one()
            self.assertEqual(user.email, email)

    def test_updates_user_name(self):
        user = self.factory.create_user(email=u'test@example.com')

        with patch('redash.authentication.login_user') as login_user_mock:
            create_and_login_user(self.factory.org, "New Name", user.email)
            login_user_mock.assert_called_once_with(user, remember=True)


class TestVerifyProfile(BaseTestCase):
    def test_no_domain_allowed_for_org(self):
        profile = dict(email=u'arik@example.com')
        self.assertFalse(verify_profile(self.factory.org, profile))

    def test_domain_not_in_org_domains_list(self):
        profile = dict(email=u'arik@example.com')
        self.factory.org.settings[models.Organization.SETTING_GOOGLE_APPS_DOMAINS] = ['example.org']
        self.assertFalse(verify_profile(self.factory.org, profile))

    def test_domain_in_org_domains_list(self):
        profile = dict(email=u'arik@example.com')
        self.factory.org.settings[models.Organization.SETTING_GOOGLE_APPS_DOMAINS] = ['example.com']
        self.assertTrue(verify_profile(self.factory.org, profile))

        self.factory.org.settings[models.Organization.SETTING_GOOGLE_APPS_DOMAINS] = ['example.org', 'example.com']
        self.assertTrue(verify_profile(self.factory.org, profile))

    def test_org_in_public_mode_accepts_any_domain(self):
        profile = dict(email=u'arik@example.com')
        self.factory.org.settings[models.Organization.SETTING_IS_PUBLIC] = True
        self.factory.org.settings[models.Organization.SETTING_GOOGLE_APPS_DOMAINS] = []
        self.assertTrue(verify_profile(self.factory.org, profile))

    def test_user_not_in_domain_but_account_exists(self):
        profile = dict(email=u'arik@example.com')
        self.factory.create_user(email=u'arik@example.com')
        self.factory.org.settings[models.Organization.SETTING_GOOGLE_APPS_DOMAINS] = ['example.org']
        self.assertTrue(verify_profile(self.factory.org, profile))


class TestGetLoginUrl(BaseTestCase):
    def test_when_multi_org_enabled_and_org_exists(self):
        with self.app.test_request_context('/{}/'.format(self.factory.org.slug)):
            self.assertEqual(get_login_url(next=None), '/{}/login'.format(self.factory.org.slug))

    def test_when_multi_org_enabled_and_org_doesnt_exist(self):
        with self.app.test_request_context('/{}_notexists/'.format(self.factory.org.slug)):
            self.assertEqual(get_login_url(next=None), '/')


class TestRedirectToUrlAfterLoggingIn(BaseTestCase):
    def setUp(self):
        super(TestRedirectToUrlAfterLoggingIn, self).setUp()
        self.user = self.factory.user
        self.password = 'test1234'

    def test_no_next_param(self):
        response = self.post_request('/login', data={'email': self.user.email, 'password': self.password}, org=self.factory.org)
        self.assertEqual(response.location, 'http://localhost/{}/'.format(self.user.org.slug))

    def test_simple_path_in_next_param(self):
        response = self.post_request('/login?next=queries', data={'email': self.user.email, 'password': self.password}, org=self.factory.org)
        self.assertEqual(response.location, 'http://localhost/queries')

    def test_starts_scheme_url_in_next_param(self):
        response = self.post_request('/login?next=https://redash.io', data={'email': self.user.email, 'password': self.password}, org=self.factory.org)
        self.assertEqual(response.location, 'http://localhost/')

    def test_without_scheme_url_in_next_param(self):
        response = self.post_request('/login?next=//redash.io', data={'email': self.user.email, 'password': self.password}, org=self.factory.org)
        self.assertEqual(response.location, 'http://localhost/')

    def test_without_scheme_with_path_url_in_next_param(self):
        response = self.post_request('/login?next=//localhost/queries', data={'email': self.user.email, 'password': self.password}, org=self.factory.org)
        self.assertEqual(response.location, 'http://localhost/queries')


class TestRemoteUserAuth(BaseTestCase):
    DEFAULT_SETTING_OVERRIDES = {
        'REDASH_REMOTE_USER_LOGIN_ENABLED': 'true'
    }

    def setUp(self):
        # Apply default setting overrides to every test
        self.override_settings(None)

        super(TestRemoteUserAuth, self).setUp()

    def override_settings(self, overrides):
        """Override settings for testing purposes.

        This helper method can be used to override specific environmental
        variables to enable / disable Re:Dash features for the duration
        of the test.

        Note that these overrides only affect code that checks the value of
        the setting at runtime. It doesn't affect code that only checks the
        value during program initialization.

        :param dict overrides: a dict of environmental variables to override
            when the settings are reloaded
        """
        variables = self.DEFAULT_SETTING_OVERRIDES.copy()
        variables.update(overrides or {})
        with patch.dict(os.environ, variables):
            reload_module(settings)

        # Queue a cleanup routine that reloads the settings without overrides
        # once the test ends
        self.addCleanup(lambda: reload_module(settings))

    def assert_correct_user_attributes(self, user, email='test@example.com', name='test@example.com', groups=None, org=None):
        """Helper to assert that the user attributes are correct."""
        groups = groups or []
        if self.factory.org.default_group.id not in groups:
            groups.append(self.factory.org.default_group.id)

        self.assertIsNotNone(user)
        self.assertEqual(user.email, email)
        self.assertEqual(user.name, name)
        self.assertEqual(user.org, org or self.factory.org)
        self.assertItemsEqual(user.group_ids, groups)

    def get_test_user(self, email='test@example.com', org=None):
        """Helper to fetch an user from the database."""

        # Expire all cached objects to ensure these values are read directly
        # from the database.
        models.db.session.expire_all()

        return models.User.get_by_email_and_org(email, org or self.factory.org)

    def test_remote_login_disabled(self):
        self.override_settings({
            'REDASH_REMOTE_USER_LOGIN_ENABLED': 'false'
        })

        self.get_request('/remote_user/login', org=self.factory.org, headers={
            'X-Forwarded-Remote-User': 'test@example.com'
        })

        with self.assertRaises(NoResultFound):
            self.get_test_user()

    def test_remote_login_default_header(self):
        self.get_request('/remote_user/login', org=self.factory.org, headers={
            'X-Forwarded-Remote-User': 'test@example.com'
        })

        self.assert_correct_user_attributes(self.get_test_user())

    def test_remote_login_custom_header(self):
        self.override_settings({
            'REDASH_REMOTE_USER_HEADER': 'X-Custom-User'
        })

        self.get_request('/remote_user/login', org=self.factory.org, headers={
            'X-Custom-User': 'test@example.com'
        })

        self.assert_correct_user_attributes(self.get_test_user())
<EOF>
<BOF>
import os
import datetime
import logging
from unittest import TestCase
from contextlib import contextmanager

os.environ['REDASH_REDIS_URL'] = os.environ.get('REDASH_REDIS_URL', "redis://localhost:6379/0").replace("/0", "/5")
# Use different url for Celery to avoid DB being cleaned up:
os.environ['REDASH_CELERY_BROKER'] = os.environ.get('REDASH_REDIS_URL', "redis://localhost:6379/0").replace("/5", "/6")

# Dummy values for oauth login
os.environ['REDASH_GOOGLE_CLIENT_ID'] = "dummy"
os.environ['REDASH_GOOGLE_CLIENT_SECRET'] = "dummy"
os.environ['REDASH_MULTI_ORG'] = "true"

from redash import create_app
from redash import redis_connection
from redash.models import db
from redash.utils import json_dumps, json_loads
from tests.factories import Factory, user_factory


logging.disable("INFO")
logging.getLogger("metrics").setLevel("ERROR")


def authenticate_request(c, user):
    with c.session_transaction() as sess:
        sess['user_id'] = user.id


@contextmanager
def authenticated_user(c, user=None):
    if not user:
        user = user_factory.create()
        db.session.commit()
    authenticate_request(c, user)

    yield user


class BaseTestCase(TestCase):
    def setUp(self):
        self.app = create_app()
        self.db = db
        self.app.config['TESTING'] = True
        self.app.config['SERVER_NAME'] = 'localhost'
        self.app_ctx = self.app.app_context()
        self.app_ctx.push()
        db.session.close()
        db.drop_all()
        db.create_all()
        self.factory = Factory()
        self.client = self.app.test_client()

    def tearDown(self):
        db.session.remove()
        db.get_engine(self.app).dispose()
        self.app_ctx.pop()
        redis_connection.flushdb()

    def make_request(self, method, path, org=None, user=None, data=None,
                     is_json=True, follow_redirects=False):
        if user is None:
            user = self.factory.user

        if org is None:
            org = self.factory.org

        if org is not False:
            path = "/{}{}".format(org.slug, path)

        if user:
            authenticate_request(self.client, user)

        method_fn = getattr(self.client, method.lower())
        headers = {}

        if data and is_json:
            data = json_dumps(data)

        if is_json:
            content_type = 'application/json'
        else:
            content_type = None

        response = method_fn(
            path,
            data=data,
            headers=headers,
            content_type=content_type,
            follow_redirects=follow_redirects,
        )

        if response.data and is_json:
            response.json = json_loads(response.data)

        return response

    def get_request(self, path, org=None, headers=None):
        if org:
            path = "/{}{}".format(org.slug, path)

        return self.client.get(path, headers=headers)

    def post_request(self, path, data=None, org=None, headers=None):
        if org:
            path = "/{}{}".format(org.slug, path)

        return self.client.post(path, data=data, headers=headers)

    def assertResponseEqual(self, expected, actual):
        for k, v in expected.iteritems():
            if isinstance(v, datetime.datetime) or isinstance(actual[k],
                    datetime.datetime):
                continue

            if isinstance(v, list):
                continue

            if isinstance(v, dict):
                self.assertResponseEqual(v, actual[k])
                continue

            self.assertEqual(v, actual[k], "{} not equal (expected: {}, actual: {}).".format(k, v, actual[k]))
<EOF>
<BOF>
from collections import namedtuple
from unittest import TestCase
from redash.permissions import has_access


MockUser = namedtuple('MockUser', ['permissions', 'group_ids'])
view_only = True


class TestHasAccess(TestCase):
    def test_allows_admin_regardless_of_groups(self):
        user = MockUser(['admin'], [])

        self.assertTrue(has_access({}, user, view_only))
        self.assertTrue(has_access({}, user, not view_only))

    def test_allows_if_user_member_in_group_with_view_access(self):
        user = MockUser([], [1])

        self.assertTrue(has_access({1: view_only}, user, view_only))

    def test_allows_if_user_member_in_group_with_full_access(self):
        user = MockUser([], [1])

        self.assertTrue(has_access({1: not view_only}, user, not view_only))

    def test_allows_if_user_member_in_multiple_groups(self):
        user = MockUser([], [1, 2, 3])

        self.assertTrue(has_access({1: not view_only, 2: view_only}, user, not view_only))
        self.assertFalse(has_access({1: view_only, 2: view_only}, user, not view_only))
        self.assertTrue(has_access({1: view_only, 2: view_only}, user, view_only))
        self.assertTrue(has_access({1: not view_only, 2: not view_only}, user, view_only))

    def test_not_allows_if_not_enough_permission(self):
        user = MockUser([], [1])

        self.assertFalse(has_access({1: view_only}, user, not view_only))
        self.assertFalse(has_access({2: view_only}, user, not view_only))
        self.assertFalse(has_access({2: view_only}, user, view_only))
        self.assertFalse(has_access({2: not view_only, 1: view_only}, user, not view_only))
<EOF>
<BOF>
import mock
import textwrap
from click.testing import CliRunner

from tests import BaseTestCase
from redash.utils.configuration import ConfigurationContainer
from redash.query_runner import query_runners
from redash.cli import manager
from redash.models import DataSource, Group, Organization, User, db


class DataSourceCommandTests(BaseTestCase):
    def test_interactive_new(self):
        runner = CliRunner()
        pg_i = query_runners.keys().index('pg') + 1
        result = runner.invoke(
            manager,
            ['ds', 'new'],
            input="test\n%s\n\n\nexample.com\n\n\ntestdb\n" % (pg_i,))
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertEqual(DataSource.query.count(), 1)
        ds = DataSource.query.first()
        self.assertEqual(ds.name, 'test')
        self.assertEqual(ds.type, 'pg')
        self.assertEqual(ds.options['dbname'], 'testdb')

    def test_options_new(self):
        runner = CliRunner()
        result = runner.invoke(
            manager,
            ['ds', 'new',
             'test',
             '--options', '{"host": "example.com", "dbname": "testdb"}',
             '--type', 'pg'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertEqual(DataSource.query.count(), 1)
        ds = DataSource.query.first()
        self.assertEqual(ds.name, 'test')
        self.assertEqual(ds.type, 'pg')
        self.assertEqual(ds.options['host'], 'example.com')
        self.assertEqual(ds.options['dbname'], 'testdb')

    def test_bad_type_new(self):
        runner = CliRunner()
        result = runner.invoke(
            manager, ['ds', 'new', 'test', '--type', 'wrong'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('not supported', result.output)
        self.assertEqual(DataSource.query.count(), 0)

    def test_bad_options_new(self):
        runner = CliRunner()
        result = runner.invoke(
            manager, ['ds', 'new', 'test', '--options',
                      '{"host": 12345, "dbname": "testdb"}',
                      '--type', 'pg'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('invalid configuration', result.output)
        self.assertEqual(DataSource.query.count(), 0)

    def test_list(self):
        self.factory.create_data_source(
            name='test1', type='pg',
            options=ConfigurationContainer({"host": "example.com",
                                            "dbname": "testdb1"}))
        self.factory.create_data_source(
            name='test2', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))

        self.factory.create_data_source(
            name='Atest', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(manager, ['ds', 'list'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        expected_output = """
        Id: 3
        Name: Atest
        Type: sqlite
        Options: {"dbpath": "/tmp/test.db"}
        --------------------
        Id: 1
        Name: test1
        Type: pg
        Options: {"dbname": "testdb1", "host": "example.com"}
        --------------------
        Id: 2
        Name: test2
        Type: sqlite
        Options: {"dbpath": "/tmp/test.db"}
        """
        self.assertMultiLineEqual(result.output,
                                  textwrap.dedent(expected_output).lstrip())

    def test_connection_test(self):
        self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(manager, ['ds', 'test', 'test1'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertIn('Success', result.output)

    def test_connection_bad_test(self):
        self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": __file__}))
        runner = CliRunner()
        result = runner.invoke(manager, ['ds', 'test', 'test1'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('Failure', result.output)

    def test_connection_delete(self):
        self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(manager, ['ds', 'delete', 'test1'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertIn('Deleting', result.output)
        self.assertEqual(DataSource.query.count(), 0)

    def test_connection_bad_delete(self):
        self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(manager, ['ds', 'delete', 'wrong'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn("Couldn't find", result.output)
        self.assertEqual(DataSource.query.count(), 1)

    def test_options_edit(self):
        self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(
            manager, ['ds', 'edit', 'test1', '--options',
                      '{"host": "example.com", "dbname": "testdb"}',
                      '--name', 'test2',
                      '--type', 'pg'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertEqual(DataSource.query.count(), 1)
        ds = DataSource.query.first()
        self.assertEqual(ds.name, 'test2')
        self.assertEqual(ds.type, 'pg')
        self.assertEqual(ds.options['host'], 'example.com')
        self.assertEqual(ds.options['dbname'], 'testdb')

    def test_bad_type_edit(self):
        self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(
            manager, ['ds', 'edit', 'test', '--type', 'wrong'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('not supported', result.output)
        ds = DataSource.query.first()
        self.assertEqual(ds.type, 'sqlite')

    def test_bad_options_edit(self):
        ds = self.factory.create_data_source(
            name='test1', type='sqlite',
            options=ConfigurationContainer({"dbpath": "/tmp/test.db"}))
        runner = CliRunner()
        result = runner.invoke(
            manager, ['ds', 'new', 'test', '--options',
                      '{"host": 12345, "dbname": "testdb"}',
                      '--type', 'pg'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('invalid configuration', result.output)
        ds = DataSource.query.first()
        self.assertEqual(ds.type, 'sqlite')
        self.assertEqual(ds.options._config, {"dbpath": "/tmp/test.db"})


class GroupCommandTests(BaseTestCase):
    def test_create(self):
        gcount = Group.query.count()
        perms = ['create_query', 'edit_query', 'view_query']
        runner = CliRunner()
        result = runner.invoke(manager, ['groups', 'create', 'test', '--permissions', ','.join(perms)])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertEqual(Group.query.count(), gcount + 1)
        g = Group.query.order_by(Group.id.desc()).first()
        db.session.add(self.factory.org)
        self.assertEqual(g.org_id, self.factory.org.id)
        self.assertEqual(g.permissions, perms)

    def test_change_permissions(self):
        g = self.factory.create_group(permissions=['list_dashboards'])
        db.session.flush()
        g_id = g.id
        perms = ['create_query', 'edit_query', 'view_query']
        runner = CliRunner()
        result = runner.invoke(
            manager, ['groups', 'change_permissions', str(g_id), '--permissions', ','.join(perms)])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        g = Group.query.filter(Group.id == g_id).first()
        self.assertEqual(g.permissions, perms)

    def test_list(self):
        self.factory.create_group(name='test', permissions=['list_dashboards'])
        self.factory.create_group(name='Agroup', permissions=['list_dashboards'])
        self.factory.create_group(name='Bgroup', permissions=['list_dashboards'])

        self.factory.create_user(name='Fred Foobar',
                         email=u'foobar@example.com',
                         org=self.factory.org,
                         group_ids=[self.factory.default_group.id])

        runner = CliRunner()
        result = runner.invoke(manager, ['groups', 'list'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        output = """
        Id: 4
        Name: Agroup
        Type: regular
        Organization: default
        Permissions: [list_dashboards]
        Users: 
        --------------------
        Id: 5
        Name: Bgroup
        Type: regular
        Organization: default
        Permissions: [list_dashboards]
        Users: 
        --------------------
        Id: 1
        Name: admin
        Type: builtin
        Organization: default
        Permissions: [admin,super_admin]
        Users: 
        --------------------
        Id: 2
        Name: default
        Type: builtin
        Organization: default
        Permissions: [create_dashboard,create_query,edit_dashboard,edit_query,view_query,view_source,execute_query,list_users,schedule_query,list_dashboards,list_alerts,list_data_sources]
        Users: Fred Foobar
        --------------------
        Id: 3
        Name: test
        Type: regular
        Organization: default
        Permissions: [list_dashboards]
        Users: 
        """
        self.assertMultiLineEqual(result.output,
                                  textwrap.dedent(output).lstrip())


class OrganizationCommandTests(BaseTestCase):
    def test_set_google_apps_domains(self):
        domains = ['example.org', 'example.com']
        runner = CliRunner()
        result = runner.invoke(manager, ['org', 'set_google_apps_domains', ','.join(domains)])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        db.session.add(self.factory.org)
        self.assertEqual(self.factory.org.google_apps_domains, domains)

    def test_show_google_apps_domains(self):
        self.factory.org.settings[Organization.SETTING_GOOGLE_APPS_DOMAINS] = [
            'example.org', 'example.com']
        db.session.add(self.factory.org)
        db.session.commit()
        runner = CliRunner()
        result = runner.invoke(manager, ['org', 'show_google_apps_domains'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        output = """
        Current list of Google Apps domains: example.org, example.com
        """
        self.assertMultiLineEqual(result.output,
                                  textwrap.dedent(output).lstrip())

    def test_list(self):
        self.factory.create_org(name='test', slug='test_org')
        self.factory.create_org(name='Borg', slug='B_org')
        self.factory.create_org(name='Aorg', slug='A_org')
        runner = CliRunner()
        result = runner.invoke(manager, ['org', 'list'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        output = """
        Id: 4
        Name: Aorg
        Slug: A_org
        --------------------
        Id: 3
        Name: Borg
        Slug: B_org
        --------------------
        Id: 1
        Name: Default
        Slug: default
        --------------------
        Id: 2
        Name: test
        Slug: test_org
        """
        self.assertMultiLineEqual(result.output,
                                  textwrap.dedent(output).lstrip())


class UserCommandTests(BaseTestCase):
    def test_create_basic(self):
        runner = CliRunner()
        result = runner.invoke(
            manager, ['users', 'create', 'foobar@example.com', 'Fred Foobar'],
            input="password1\npassword1\n")
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        u = User.query.filter(User.email == u"foobar@example.com").first()
        self.assertEqual(u.name, "Fred Foobar")
        self.assertTrue(u.verify_password('password1'))
        self.assertEqual(u.group_ids, [u.org.default_group.id])

    def test_create_admin(self):
        runner = CliRunner()
        result = runner.invoke(
            manager, ['users', 'create', 'foobar@example.com', 'Fred Foobar',
                      '--password', 'password1', '--admin'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        u = User.query.filter(User.email == u"foobar@example.com").first()
        self.assertEqual(u.name, "Fred Foobar")
        self.assertTrue(u.verify_password('password1'))
        self.assertEqual(u.group_ids, [u.org.default_group.id,
                                       u.org.admin_group.id])

    def test_create_googleauth(self):
        runner = CliRunner()
        result = runner.invoke(
            manager, ['users', 'create', 'foobar@example.com', 'Fred Foobar', '--google'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        u = User.query.filter(User.email == u"foobar@example.com").first()
        self.assertEqual(u.name, "Fred Foobar")
        self.assertIsNone(u.password_hash)
        self.assertEqual(u.group_ids, [u.org.default_group.id])

    def test_create_bad(self):
        self.factory.create_user(email=u'foobar@example.com')
        runner = CliRunner()
        result = runner.invoke(
            manager, ['users', 'create', u'foobar@example.com', 'Fred Foobar'],
            input="password1\npassword1\n")
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('Failed', result.output)

    def test_delete(self):
        self.factory.create_user(email=u'foobar@example.com')
        ucount = User.query.count()
        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'delete', 'foobar@example.com'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        self.assertEqual(User.query.filter(User.email ==
                                           u"foobar@example.com").count(), 0)
        self.assertEqual(User.query.count(), ucount - 1)

    def test_delete_bad(self):
        ucount = User.query.count()
        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'delete', u'foobar@example.com'])
        self.assertIn('Deleted 0 users', result.output)
        self.assertEqual(User.query.count(), ucount)

    def test_password(self):
        self.factory.create_user(email=u'foobar@example.com')
        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'password', u'foobar@example.com', 'xyzzy'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        u = User.query.filter(User.email == u"foobar@example.com").first()
        self.assertTrue(u.verify_password('xyzzy'))

    def test_password_bad(self):
        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'password', u'foobar@example.com', 'xyzzy'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('not found', result.output)

    def test_password_bad_org(self):
        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'password', u'foobar@example.com', 'xyzzy', '--org', 'default'])
        self.assertTrue(result.exception)
        self.assertEqual(result.exit_code, 1)
        self.assertIn('not found', result.output)

    def test_invite(self):
        admin = self.factory.create_user(email=u'redash-admin@example.com')
        runner = CliRunner()
        with mock.patch('redash.cli.users.invite_user') as iu:
            result = runner.invoke(manager, ['users', 'invite', u'foobar@example.com', 'Fred Foobar', u'redash-admin@example.com'])
            self.assertFalse(result.exception)
            self.assertEqual(result.exit_code, 0)
            self.assertTrue(iu.called)
            c = iu.call_args[0]
            db.session.add_all(c)
            self.assertEqual(c[0].id, self.factory.org.id)
            self.assertEqual(c[1].id, admin.id)
            self.assertEqual(c[2].email, 'foobar@example.com')

    def test_list(self):
        self.factory.create_user(name='Fred Foobar',
                                 email=u'foobar@example.com',
                                 org=self.factory.org)

        self.factory.create_user(name='William Foobar',
                                 email=u'william@example.com',
                                 org=self.factory.org)

        self.factory.create_user(name='Andrew Foobar',
                                 email=u'andrew@example.com',
                                 org=self.factory.org)

        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'list'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        output = """
        Id: 3
        Name: Andrew Foobar
        Email: andrew@example.com
        Organization: Default
        Active: True
        Groups: default
        --------------------
        Id: 1
        Name: Fred Foobar
        Email: foobar@example.com
        Organization: Default
        Active: True
        Groups: default
        --------------------
        Id: 2
        Name: William Foobar
        Email: william@example.com
        Organization: Default
        Active: True
        Groups: default
        """
        self.assertMultiLineEqual(result.output,
                                  textwrap.dedent(output).lstrip())

    def test_grant_admin(self):
        u = self.factory.create_user(name='Fred Foobar',
                                     email=u'foobar@example.com',
                                     org=self.factory.org,
                                     group_ids=[self.factory.default_group.id])
        runner = CliRunner()
        result = runner.invoke(manager, ['users', 'grant_admin', u'foobar@example.com'])
        self.assertFalse(result.exception)
        self.assertEqual(result.exit_code, 0)
        db.session.add(u)
        self.assertEqual(u.group_ids, [u.org.default_group.id,
                                       u.org.admin_group.id])
<EOF>
<BOF>
from collections import namedtuple
from unittest import TestCase

from redash.utils import (build_url, collect_parameters_from_request,
                          collect_query_parameters, filter_none)

DummyRequest = namedtuple('DummyRequest', ['host', 'scheme'])


class TestBuildUrl(TestCase):
    def test_simple_case(self):
        self.assertEqual("http://example.com/test", build_url(DummyRequest("", "http"), "example.com", "/test"))

    def test_uses_current_request_port(self):
        self.assertEqual("http://example.com:5000/test", build_url(DummyRequest("example.com:5000", "http"), "example.com", "/test"))

    def test_uses_current_request_schema(self):
        self.assertEqual("https://example.com/test", build_url(DummyRequest("example.com", "https"), "example.com", "/test"))

    def test_skips_port_for_default_ports(self):
        self.assertEqual("https://example.com/test", build_url(DummyRequest("example.com:443", "https"), "example.com", "/test"))
        self.assertEqual("http://example.com/test", build_url(DummyRequest("example.com:80", "http"), "example.com", "/test"))
        self.assertEqual("https://example.com:80/test", build_url(DummyRequest("example.com:80", "https"), "example.com", "/test"))
        self.assertEqual("http://example.com:443/test", build_url(DummyRequest("example.com:443", "http"), "example.com", "/test"))


class TestCollectParametersFromQuery(TestCase):
    def test_returns_empty_list_for_regular_query(self):
        query = u"SELECT 1"
        self.assertEqual([], collect_query_parameters(query))

    def test_finds_all_params(self):
        query = u"SELECT {{param}} FROM {{table}}"
        params = ['param', 'table']
        self.assertEqual(params, collect_query_parameters(query))

    def test_deduplicates_params(self):
        query = u"SELECT {{param}}, {{param}} FROM {{table}}"
        params = ['param', 'table']
        self.assertEqual(params, collect_query_parameters(query))

    def test_handles_nested_params(self):
        query = u"SELECT {{param}}, {{param}} FROM {{table}} -- {{#test}} {{nested_param}} {{/test}}"
        params = ['param', 'table', 'test', 'nested_param']
        self.assertEqual(params, collect_query_parameters(query))


class TestCollectParametersFromRequest(TestCase):
    def test_ignores_non_prefixed_values(self):
        self.assertEqual({}, collect_parameters_from_request({'test': 1}))

    def test_takes_prefixed_values(self):
        self.assertDictEqual({'test': 1, 'something_else': 'test'}, collect_parameters_from_request({'p_test': 1, 'p_something_else': 'test'}))


class TestSkipNones(TestCase):
    def test_skips_nones(self):
        d = {
            'a': 1,
            'b': None
        }

        self.assertDictEqual(filter_none(d), {'a': 1})
<EOF>
<BOF>
from flask import url_for
from flask_login import current_user
from funcy import project
from mock import patch
from tests import BaseTestCase, authenticated_user

from redash import models, settings


class AuthenticationTestMixin(object):
    def test_returns_404_when_not_unauthenticated(self):
        for path in self.paths:
            rv = self.client.get(path)
            self.assertEquals(404, rv.status_code)

    def test_returns_content_when_authenticated(self):
        for path in self.paths:
            rv = self.make_request('get', path, is_json=False)
            self.assertEquals(200, rv.status_code)


class TestAuthentication(BaseTestCase):
    def test_redirects_for_nonsigned_in_user(self):
        rv = self.client.get("/default/")
        self.assertEquals(302, rv.status_code)


class PingTest(BaseTestCase):
    def test_ping(self):
        rv = self.client.get('/ping')
        self.assertEquals(200, rv.status_code)
        self.assertEquals('PONG.', rv.data)


class IndexTest(BaseTestCase):
    def setUp(self):
        self.paths = ['/default/', '/default/dashboard/example', '/default/queries/1', '/default/admin/status']
        super(IndexTest, self).setUp()

    def test_redirect_to_login_when_not_authenticated(self):
        for path in self.paths:
            rv = self.client.get(path)
            self.assertEquals(302, rv.status_code)

    def test_returns_content_when_authenticated(self):
        for path in self.paths:
            rv = self.make_request('get', path, org=False, is_json=False)
            self.assertEquals(200, rv.status_code)


class StatusTest(BaseTestCase):
    def test_returns_data_for_super_admin(self):
        admin = self.factory.create_admin()
        models.db.session.commit()
        rv = self.make_request('get', '/status.json', org=False, user=admin, is_json=False)
        self.assertEqual(rv.status_code, 200)

    def test_returns_403_for_non_admin(self):
        rv = self.make_request('get', '/status.json', org=False, is_json=False)
        self.assertEqual(rv.status_code, 403)

    def test_redirects_non_authenticated_user(self):
        rv = self.client.get('/status.json')
        self.assertEqual(rv.status_code, 302)


class JobAPITest(BaseTestCase, AuthenticationTestMixin):
    def setUp(self):
        self.paths = []
        super(JobAPITest, self).setUp()


class TestLogin(BaseTestCase):
    def setUp(self):
        super(TestLogin, self).setUp()
        self.factory.org.set_setting('auth_password_login_enabled', True)

    @classmethod
    def setUpClass(cls):
        settings.ORG_RESOLVING = "single_org"

    @classmethod
    def tearDownClass(cls):
        settings.ORG_RESOLVING = "multi_org"

    def test_get_login_form(self):
        rv = self.client.get('/default/login')
        self.assertEquals(rv.status_code, 200)

    def test_submit_non_existing_user(self):
        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={'email': 'arik', 'password': 'password'})
            self.assertEquals(rv.status_code, 200)
            self.assertFalse(login_user_mock.called)

    def test_submit_correct_user_and_password(self):
        user = self.factory.user
        user.hash_password('password')

        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={'email': user.email, 'password': 'password'})
            self.assertEquals(rv.status_code, 302)
            login_user_mock.assert_called_with(user, remember=False)

    def test_submit_case_insensitive_user_and_password(self):
        user = self.factory.user
        user.hash_password('password')

        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={'email': user.email.upper(), 'password': 'password'})
            self.assertEquals(rv.status_code, 302)
            login_user_mock.assert_called_with(user, remember=False)

    def test_submit_correct_user_and_password_and_remember_me(self):
        user = self.factory.user
        user.hash_password('password')

        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={'email': user.email, 'password': 'password', 'remember': True})
            self.assertEquals(rv.status_code, 302)
            login_user_mock.assert_called_with(user, remember=True)

    def test_submit_correct_user_and_password_with_next(self):
        user = self.factory.user
        user.hash_password('password')

        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login?next=/test',
                                  data={'email': user.email, 'password': 'password'})
            self.assertEquals(rv.status_code, 302)
            self.assertEquals(rv.location, 'http://localhost/test')
            login_user_mock.assert_called_with(user, remember=False)

    def test_submit_incorrect_user(self):
        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={'email': 'non-existing', 'password': 'password'})
            self.assertEquals(rv.status_code, 200)
            self.assertFalse(login_user_mock.called)

    def test_submit_incorrect_password(self):
        user = self.factory.user
        user.hash_password('password')

        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={
                'email': user.email, 'password': 'badbadpassword'})
            self.assertEquals(rv.status_code, 200)
            self.assertFalse(login_user_mock.called)

    def test_submit_empty_password(self):
        user = self.factory.user

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/default/login', data={'email': user.email, 'password': ''})
            self.assertEquals(rv.status_code, 200)
            self.assertFalse(login_user_mock.called)

    def test_user_already_loggedin(self):
        with authenticated_user(self.client), patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.get('/default/login')
            self.assertEquals(rv.status_code, 302)
            self.assertFalse(login_user_mock.called)


class TestLogout(BaseTestCase):
    def test_logout_when_not_loggedin(self):
        with self.app.test_client() as c:
            rv = c.get('/default/logout')
            self.assertEquals(rv.status_code, 302)
            self.assertFalse(current_user.is_authenticated)

    def test_logout_when_loggedin(self):
        with self.app.test_client() as c, authenticated_user(c, user=self.factory.user):
            rv = c.get('/default/')
            self.assertTrue(current_user.is_authenticated)
            rv = c.get('/default/logout')
            self.assertEquals(rv.status_code, 302)
            self.assertFalse(current_user.is_authenticated)


class TestQuerySnippet(BaseTestCase):
    def test_create(self):
        res = self.make_request(
            'post',
            '/api/query_snippets',
            data={'trigger': 'x', 'description': 'y', 'snippet': 'z'},
            user=self.factory.user)
        self.assertEqual(
            project(res.json, ['id', 'trigger', 'description', 'snippet']), {
                'id': 1,
                'trigger': 'x',
                'description': 'y',
                'snippet': 'z',
            })
        qs = models.QuerySnippet.query.one()
        self.assertEqual(qs.trigger, 'x')
        self.assertEqual(qs.description, 'y')
        self.assertEqual(qs.snippet, 'z')

    def test_edit(self):
        qs = models.QuerySnippet(
            trigger='a',
            description='b',
            snippet='c',
            user=self.factory.user,
            org=self.factory.org
        )
        models.db.session.add(qs)
        models.db.session.commit()
        res = self.make_request(
            'post',
            '/api/query_snippets/1',
            data={'trigger': 'x', 'description': 'y', 'snippet': 'z'},
            user=self.factory.user)
        self.assertEqual(
            project(res.json, ['id', 'trigger', 'description', 'snippet']), {
                'id': 1,
                'trigger': 'x',
                'description': 'y',
                'snippet': 'z',
            })
        self.assertEqual(qs.trigger, 'x')
        self.assertEqual(qs.description, 'y')
        self.assertEqual(qs.snippet, 'z')

    def test_list(self):
        qs = models.QuerySnippet(
            trigger='x',
            description='y',
            snippet='z',
            user=self.factory.user,
            org=self.factory.org
        )
        models.db.session.add(qs)
        models.db.session.commit()
        res = self.make_request(
            'get',
            '/api/query_snippets',
            user=self.factory.user)
        self.assertEqual(res.status_code, 200)
        data = res.json
        self.assertEqual(len(data), 1)
        self.assertEqual(
            project(data[0], ['id', 'trigger', 'description', 'snippet']), {
                'id': 1,
                'trigger': 'x',
                'description': 'y',
                'snippet': 'z',
            })
        self.assertEqual(qs.trigger, 'x')
        self.assertEqual(qs.description, 'y')
        self.assertEqual(qs.snippet, 'z')

    def test_delete(self):
        qs = models.QuerySnippet(
            trigger='a',
            description='b',
            snippet='c',
            user=self.factory.user,
            org=self.factory.org
        )
        models.db.session.add(qs)
        models.db.session.commit()
        self.make_request(
            'delete',
            '/api/query_snippets/1',
            user=self.factory.user)
        self.assertEqual(models.QuerySnippet.query.count(), 0)
<EOF>
<BOF>
#encoding: utf8
import datetime
import json
from unittest import TestCase

import mock
from dateutil.parser import parse as date_parse
from tests import BaseTestCase

from redash import models
from redash.models import db
from redash.utils import gen_query_hash, utcnow


class DashboardTest(BaseTestCase):
    def test_appends_suffix_to_slug_when_duplicate(self):
        d1 = self.factory.create_dashboard()
        db.session.flush()
        self.assertEquals(d1.slug, 'test')

        d2 = self.factory.create_dashboard(user=d1.user)
        db.session.flush()
        self.assertNotEquals(d1.slug, d2.slug)

        d3 = self.factory.create_dashboard(user=d1.user)
        db.session.flush()
        self.assertNotEquals(d1.slug, d3.slug)
        self.assertNotEquals(d2.slug, d3.slug)


class ShouldScheduleNextTest(TestCase):
    def test_interval_schedule_that_needs_reschedule(self):
        now = utcnow()
        two_hours_ago = now - datetime.timedelta(hours=2)
        self.assertTrue(models.should_schedule_next(two_hours_ago, now, "3600",
                                                    0))

    def test_interval_schedule_that_doesnt_need_reschedule(self):
        now = utcnow()
        half_an_hour_ago = now - datetime.timedelta(minutes=30)
        self.assertFalse(models.should_schedule_next(half_an_hour_ago, now,
                                                     "3600", 0))

    def test_exact_time_that_needs_reschedule(self):
        now = utcnow()
        yesterday = now - datetime.timedelta(days=1)
        scheduled_datetime = now - datetime.timedelta(hours=3)
        scheduled_time = "{:02d}:00".format(scheduled_datetime.hour)
        self.assertTrue(models.should_schedule_next(yesterday, now,
                                                    scheduled_time, 0))

    def test_exact_time_that_doesnt_need_reschedule(self):
        now = date_parse("2015-10-16 20:10")
        yesterday = date_parse("2015-10-15 23:07")
        schedule = "23:00"
        self.assertFalse(models.should_schedule_next(yesterday, now, schedule,
                                                     0))

    def test_exact_time_with_day_change(self):
        now = utcnow().replace(hour=0, minute=1)
        previous = (now - datetime.timedelta(days=2)).replace(hour=23,
                                                              minute=59)
        schedule = "23:59".format(now.hour + 3)
        self.assertTrue(models.should_schedule_next(previous, now, schedule,
                                                    0))

    def test_backoff(self):
        now = utcnow()
        two_hours_ago = now - datetime.timedelta(hours=2)
        self.assertTrue(models.should_schedule_next(two_hours_ago, now, "3600",
                                                    5))
        self.assertFalse(models.should_schedule_next(two_hours_ago, now,
                                                     "3600", 10))


class QueryOutdatedQueriesTest(BaseTestCase):
    # TODO: this test can be refactored to use mock version of should_schedule_next to simplify it.
    def test_outdated_queries_skips_unscheduled_queries(self):
        query = self.factory.create_query(schedule=None)
        queries = models.Query.outdated_queries()

        self.assertNotIn(query, queries)

    def test_outdated_queries_works_with_ttl_based_schedule(self):
        two_hours_ago = utcnow() - datetime.timedelta(hours=2)
        query = self.factory.create_query(schedule="3600")
        query_result = self.factory.create_query_result(query=query.query_text, retrieved_at=two_hours_ago)
        query.latest_query_data = query_result

        queries = models.Query.outdated_queries()
        self.assertIn(query, queries)

    def test_outdated_queries_works_scheduled_queries_tracker(self):
        two_hours_ago = datetime.datetime.now() - datetime.timedelta(hours=2)
        query = self.factory.create_query(schedule="3600")
        query_result = self.factory.create_query_result(query=query, retrieved_at=two_hours_ago)
        query.latest_query_data = query_result

        models.scheduled_queries_executions.update(query.id)

        queries = models.Query.outdated_queries()
        self.assertNotIn(query, queries)

    def test_skips_fresh_queries(self):
        half_an_hour_ago = utcnow() - datetime.timedelta(minutes=30)
        query = self.factory.create_query(schedule="3600")
        query_result = self.factory.create_query_result(query=query.query_text, retrieved_at=half_an_hour_ago)
        query.latest_query_data = query_result

        queries = models.Query.outdated_queries()
        self.assertNotIn(query, queries)

    def test_outdated_queries_works_with_specific_time_schedule(self):
        half_an_hour_ago = utcnow() - datetime.timedelta(minutes=30)
        query = self.factory.create_query(schedule=half_an_hour_ago.strftime('%H:%M'))
        query_result = self.factory.create_query_result(query=query.query_text, retrieved_at=half_an_hour_ago - datetime.timedelta(days=1))
        query.latest_query_data = query_result

        queries = models.Query.outdated_queries()
        self.assertIn(query, queries)

    def test_enqueues_query_only_once(self):
        """
        Only one query per data source with the same text will be reported by
        Query.outdated_queries().
        """
        query = self.factory.create_query(schedule="60")
        query2 = self.factory.create_query(
            schedule="60", query_text=query.query_text,
            query_hash=query.query_hash)
        retrieved_at = utcnow() - datetime.timedelta(minutes=10)
        query_result = self.factory.create_query_result(
            retrieved_at=retrieved_at, query_text=query.query_text,
            query_hash=query.query_hash)
        query.latest_query_data = query_result
        query2.latest_query_data = query_result

        self.assertEqual(list(models.Query.outdated_queries()), [query2])

    def test_enqueues_query_with_correct_data_source(self):
        """
        Queries from different data sources will be reported by
        Query.outdated_queries() even if they have the same query text.
        """
        query = self.factory.create_query(
            schedule="60", data_source=self.factory.create_data_source())
        query2 = self.factory.create_query(
            schedule="60", query_text=query.query_text,
            query_hash=query.query_hash)
        retrieved_at = utcnow() - datetime.timedelta(minutes=10)
        query_result = self.factory.create_query_result(
            retrieved_at=retrieved_at, query_text=query.query_text,
            query_hash=query.query_hash)
        query.latest_query_data = query_result
        query2.latest_query_data = query_result

        self.assertEqual(list(models.Query.outdated_queries()),
                         [query2, query])

    def test_enqueues_only_for_relevant_data_source(self):
        """
        If multiple queries with the same text exist, only ones that are
        scheduled to be refreshed are reported by Query.outdated_queries().
        """
        query = self.factory.create_query(schedule="60")
        query2 = self.factory.create_query(
            schedule="3600", query_text=query.query_text,
            query_hash=query.query_hash)
        retrieved_at = utcnow() - datetime.timedelta(minutes=10)
        query_result = self.factory.create_query_result(
            retrieved_at=retrieved_at, query_text=query.query_text,
            query_hash=query.query_hash)
        query.latest_query_data = query_result
        query2.latest_query_data = query_result

        self.assertEqual(list(models.Query.outdated_queries()), [query])

    def test_failure_extends_schedule(self):
        """
        Execution failures recorded for a query result in exponential backoff
        for scheduling future execution.
        """
        query = self.factory.create_query(schedule="60", schedule_failures=4)
        retrieved_at = utcnow() - datetime.timedelta(minutes=16)
        query_result = self.factory.create_query_result(
            retrieved_at=retrieved_at, query_text=query.query_text,
            query_hash=query.query_hash)
        query.latest_query_data = query_result

        self.assertEqual(list(models.Query.outdated_queries()), [])

        query_result.retrieved_at = utcnow() - datetime.timedelta(minutes=17)
        self.assertEqual(list(models.Query.outdated_queries()), [query])


class QueryArchiveTest(BaseTestCase):
    def setUp(self):
        super(QueryArchiveTest, self).setUp()

    def test_archive_query_sets_flag(self):
        query = self.factory.create_query()
        db.session.flush()
        query.archive()

        self.assertEquals(query.is_archived, True)

    def test_archived_query_doesnt_return_in_all(self):
        query = self.factory.create_query(schedule="1")
        yesterday = utcnow() - datetime.timedelta(days=1)
        query_result, _ = models.QueryResult.store_result(
            query.org_id, query.data_source, query.query_hash, query.query_text,
            "1", 123, yesterday)

        query.latest_query_data = query_result
        groups = list(models.Group.query.filter(models.Group.id.in_(query.groups)))
        self.assertIn(query, list(models.Query.all_queries([g.id for g in groups])))
        self.assertIn(query, models.Query.outdated_queries())
        db.session.flush()
        query.archive()

        self.assertNotIn(query, list(models.Query.all_queries([g.id for g in groups])))
        self.assertNotIn(query, models.Query.outdated_queries())

    def test_removes_associated_widgets_from_dashboards(self):
        widget = self.factory.create_widget()
        query = widget.visualization.query_rel
        db.session.commit()
        query.archive()
        db.session.flush()
        self.assertEqual(db.session.query(models.Widget).get(widget.id), None)

    def test_removes_scheduling(self):
        query = self.factory.create_query(schedule="1")

        query.archive()

        self.assertEqual(None, query.schedule)

    def test_deletes_alerts(self):
        subscription = self.factory.create_alert_subscription()
        query = subscription.alert.query_rel
        db.session.commit()
        query.archive()
        db.session.flush()
        self.assertEqual(db.session.query(models.Alert).get(subscription.alert.id), None)
        self.assertEqual(db.session.query(models.AlertSubscription).get(subscription.id), None)


class TestUnusedQueryResults(BaseTestCase):
    def test_returns_only_unused_query_results(self):
        two_weeks_ago = utcnow() - datetime.timedelta(days=14)
        qr = self.factory.create_query_result()
        query = self.factory.create_query(latest_query_data=qr)
        db.session.flush()
        unused_qr = self.factory.create_query_result(retrieved_at=two_weeks_ago)
        self.assertIn((unused_qr.id,), models.QueryResult.unused())
        self.assertNotIn((qr.id,), list(models.QueryResult.unused()))

    def test_returns_only_over_a_week_old_results(self):
        two_weeks_ago = utcnow() - datetime.timedelta(days=14)
        unused_qr = self.factory.create_query_result(retrieved_at=two_weeks_ago)
        db.session.flush()
        new_unused_qr = self.factory.create_query_result()

        self.assertIn((unused_qr.id,), models.QueryResult.unused())
        self.assertNotIn((new_unused_qr.id,), models.QueryResult.unused())


class TestQueryAll(BaseTestCase):
    def test_returns_only_queries_in_given_groups(self):
        ds1 = self.factory.create_data_source()
        ds2 = self.factory.create_data_source()

        group1 = models.Group(name="g1", org=ds1.org, permissions=['create', 'view'])
        group2 = models.Group(name="g2", org=ds1.org, permissions=['create', 'view'])

        q1 = self.factory.create_query(data_source=ds1)
        q2 = self.factory.create_query(data_source=ds2)

        db.session.add_all([
            ds1, ds2,
            group1, group2,
            q1, q2,
            models.DataSourceGroup(
                group=group1, data_source=ds1),
            models.DataSourceGroup(group=group2, data_source=ds2)
            ])
        db.session.flush()
        self.assertIn(q1, list(models.Query.all_queries([group1.id])))
        self.assertNotIn(q2, list(models.Query.all_queries([group1.id])))
        self.assertIn(q1, list(models.Query.all_queries([group1.id, group2.id])))
        self.assertIn(q2, list(models.Query.all_queries([group1.id, group2.id])))

    def test_skips_drafts(self):
        q = self.factory.create_query(is_draft=True)
        self.assertNotIn(q, models.Query.all_queries([self.factory.default_group.id]))

    def test_includes_drafts_of_given_user(self):
        q = self.factory.create_query(is_draft=True)
        self.assertIn(q, models.Query.all_queries([self.factory.default_group.id], user_id=q.user_id))

    def test_order_by_relationship(self):
        u1 = self.factory.create_user(name='alice')
        u2 = self.factory.create_user(name='bob')
        self.factory.create_query(user=u1)
        self.factory.create_query(user=u2)
        db.session.commit()
        # have to reset the order here with None since all_queries orders by
        # created_at by default
        base = models.Query.all_queries([self.factory.default_group.id]).order_by(None)
        qs1 = base.order_by(models.User.name)
        self.assertEqual(['alice', 'bob'], [q.user.name for q in qs1])
        qs2 = base.order_by(models.User.name.desc())
        self.assertEqual(['bob', 'alice'], [q.user.name for q in qs2])


class TestGroup(BaseTestCase):
    def test_returns_groups_with_specified_names(self):
        org1 = self.factory.create_org()
        org2 = self.factory.create_org()

        matching_group1 = models.Group(id=999, name="g1", org=org1)
        matching_group2 = models.Group(id=888, name="g2", org=org1)
        non_matching_group = models.Group(id=777, name="g1", org=org2)

        groups = models.Group.find_by_name(org1, ["g1", "g2"])
        self.assertIn(matching_group1, groups)
        self.assertIn(matching_group2, groups)
        self.assertNotIn(non_matching_group, groups)

    def test_returns_no_groups(self):
        org1 = self.factory.create_org()

        models.Group(id=999, name="g1", org=org1)
        self.assertEqual([], models.Group.find_by_name(org1, ["non-existing"]))


class TestQueryResultStoreResult(BaseTestCase):
    def setUp(self):
        super(TestQueryResultStoreResult, self).setUp()
        self.data_source = self.factory.data_source
        self.query = "SELECT 1"
        self.query_hash = gen_query_hash(self.query)
        self.runtime = 123
        self.utcnow = utcnow()
        self.data = "data"

    def test_stores_the_result(self):
        query_result, _ = models.QueryResult.store_result(
            self.data_source.org_id, self.data_source, self.query_hash,
            self.query, self.data, self.runtime, self.utcnow)

        self.assertEqual(query_result.data, self.data)
        self.assertEqual(query_result.runtime, self.runtime)
        self.assertEqual(query_result.retrieved_at, self.utcnow)
        self.assertEqual(query_result.query_text, self.query)
        self.assertEqual(query_result.query_hash, self.query_hash)
        self.assertEqual(query_result.data_source, self.data_source)

    def test_updates_existing_queries(self):
        query1 = self.factory.create_query(query_text=self.query)
        query2 = self.factory.create_query(query_text=self.query)
        query3 = self.factory.create_query(query_text=self.query)

        query_result, _ = models.QueryResult.store_result(
            self.data_source.org_id, self.data_source, self.query_hash,
            self.query, self.data, self.runtime, self.utcnow)

        self.assertEqual(query1.latest_query_data, query_result)
        self.assertEqual(query2.latest_query_data, query_result)
        self.assertEqual(query3.latest_query_data, query_result)

    def test_doesnt_update_queries_with_different_hash(self):
        query1 = self.factory.create_query(query_text=self.query)
        query2 = self.factory.create_query(query_text=self.query)
        query3 = self.factory.create_query(query_text=self.query + "123")

        query_result, _ = models.QueryResult.store_result(
            self.data_source.org_id, self.data_source, self.query_hash,
            self.query, self.data, self.runtime, self.utcnow)

        self.assertEqual(query1.latest_query_data, query_result)
        self.assertEqual(query2.latest_query_data, query_result)
        self.assertNotEqual(query3.latest_query_data, query_result)

    def test_doesnt_update_queries_with_different_data_source(self):
        query1 = self.factory.create_query(query_text=self.query)
        query2 = self.factory.create_query(query_text=self.query)
        query3 = self.factory.create_query(query_text=self.query, data_source=self.factory.create_data_source())

        query_result, _ = models.QueryResult.store_result(
            self.data_source.org_id, self.data_source, self.query_hash,
            self.query, self.data, self.runtime, self.utcnow)

        self.assertEqual(query1.latest_query_data, query_result)
        self.assertEqual(query2.latest_query_data, query_result)
        self.assertNotEqual(query3.latest_query_data, query_result)


class TestEvents(BaseTestCase):
    def raw_event(self):
        timestamp = 1411778709.791
        user = self.factory.user
        created_at = datetime.datetime.utcfromtimestamp(timestamp)
        db.session.flush()
        raw_event = {"action": "view",
                      "timestamp": timestamp,
                      "object_type": "dashboard",
                      "user_id": user.id,
                      "object_id": 1,
                      "org_id": 1}

        return raw_event, user, created_at

    def test_records_event(self):
        raw_event, user, created_at = self.raw_event()

        event = models.Event.record(raw_event)
        db.session.flush()
        self.assertEqual(event.user, user)
        self.assertEqual(event.action, "view")
        self.assertEqual(event.object_type, "dashboard")
        self.assertEqual(event.object_id, 1)
        self.assertEqual(event.created_at, created_at)

    def test_records_additional_properties(self):
        raw_event, _, _ = self.raw_event()
        additional_properties = {'test': 1, 'test2': 2, 'whatever': "abc"}
        raw_event.update(additional_properties)

        event = models.Event.record(raw_event)

        self.assertDictEqual(event.additional_properties, additional_properties)


def _set_up_dashboard_test(d):
    d.g1 = d.factory.create_group(name='First', permissions=['create', 'view'])
    d.g2 = d.factory.create_group(name='Second',  permissions=['create', 'view'])
    d.ds1 = d.factory.create_data_source()
    d.ds2 = d.factory.create_data_source()
    db.session.flush()
    d.u1 = d.factory.create_user(group_ids=[d.g1.id])
    d.u2 = d.factory.create_user(group_ids=[d.g2.id])
    db.session.add_all([
        models.DataSourceGroup(group=d.g1, data_source=d.ds1),
        models.DataSourceGroup(group=d.g2, data_source=d.ds2)
    ])
    d.q1 = d.factory.create_query(data_source=d.ds1)
    d.q2 = d.factory.create_query(data_source=d.ds2)
    d.v1 = d.factory.create_visualization(query_rel=d.q1)
    d.v2 = d.factory.create_visualization(query_rel=d.q2)
    d.w1 = d.factory.create_widget(visualization=d.v1)
    d.w2 = d.factory.create_widget(visualization=d.v2)
    d.w3 = d.factory.create_widget(visualization=d.v2, dashboard=d.w2.dashboard)
    d.w4 = d.factory.create_widget(visualization=d.v2)
    d.w5 = d.factory.create_widget(visualization=d.v1, dashboard=d.w4.dashboard)
    d.w1.dashboard.is_draft = False
    d.w2.dashboard.is_draft = False
    d.w4.dashboard.is_draft = False

class TestDashboardAll(BaseTestCase):
    def setUp(self):
        super(TestDashboardAll, self).setUp()
        _set_up_dashboard_test(self)

    def test_requires_group_or_user_id(self):
        d1 = self.factory.create_dashboard()
        self.assertNotIn(d1, list(models.Dashboard.all(
           d1.user.org, d1.user.group_ids, None)))
        l2 = list(models.Dashboard.all(
            d1.user.org, [0], d1.user.id))
        self.assertIn(d1, l2)

    def test_returns_dashboards_based_on_groups(self):
        self.assertIn(self.w1.dashboard, list(models.Dashboard.all(
            self.u1.org, self.u1.group_ids, None)))
        self.assertIn(self.w2.dashboard, list(models.Dashboard.all(
            self.u2.org, self.u2.group_ids, None)))
        self.assertNotIn(self.w1.dashboard, list(models.Dashboard.all(
            self.u2.org, self.u2.group_ids, None)))
        self.assertNotIn(self.w2.dashboard, list(models.Dashboard.all(
            self.u1.org, self.u1.group_ids, None)))

    def test_returns_each_dashboard_once(self):
        dashboards = list(models.Dashboard.all(self.u2.org, self.u2.group_ids, None))
        self.assertEqual(len(dashboards), 2)

    def test_returns_dashboard_you_have_partial_access_to(self):
        self.assertIn(self.w5.dashboard, models.Dashboard.all(self.u1.org, self.u1.group_ids, None))

    def test_returns_dashboards_created_by_user(self):
        d1 = self.factory.create_dashboard(user=self.u1)
        db.session.flush()
        self.assertIn(d1, list(models.Dashboard.all(self.u1.org, self.u1.group_ids, self.u1.id)))
        self.assertIn(d1, list(models.Dashboard.all(self.u1.org, [0], self.u1.id)))
        self.assertNotIn(d1, list(models.Dashboard.all(self.u2.org, self.u2.group_ids, self.u2.id)))

    def test_returns_dashboards_with_text_widgets(self):
        w1 = self.factory.create_widget(visualization=None)

        self.assertIn(w1.dashboard, models.Dashboard.all(self.u1.org, self.u1.group_ids, None))
        self.assertIn(w1.dashboard, models.Dashboard.all(self.u2.org, self.u2.group_ids, None))

    def test_returns_dashboards_from_current_org_only(self):
        w1 = self.factory.create_widget(visualization=None)

        user = self.factory.create_user(org=self.factory.create_org())

        self.assertIn(w1.dashboard, models.Dashboard.all(self.u1.org, self.u1.group_ids, None))
        self.assertNotIn(w1.dashboard, models.Dashboard.all(user.org, user.group_ids, None))
<EOF>
<BOF>
from passlib.apps import custom_app_context as pwd_context
import redash.models
from redash.models import db
from redash.permissions import ACCESS_TYPE_MODIFY
from redash.utils import gen_query_hash, utcnow
from redash.utils.configuration import ConfigurationContainer


class ModelFactory(object):
    def __init__(self, model, **kwargs):
        self.model = model
        self.kwargs = kwargs

    def _get_kwargs(self, override_kwargs):
        kwargs = self.kwargs.copy()
        kwargs.update(override_kwargs)

        for key, arg in kwargs.items():
            if callable(arg):
                kwargs[key] = arg()

        return kwargs

    def create(self, **override_kwargs):
        kwargs = self._get_kwargs(override_kwargs)
        obj = self.model(**kwargs)
        db.session.add(obj)
        db.session.commit()
        return obj


class Sequence(object):
    def __init__(self, string):
        self.sequence = 0
        self.string = string

    def __call__(self):
        self.sequence += 1

        return self.string.format(self.sequence)


user_factory = ModelFactory(redash.models.User,
                            name='John Doe', email=Sequence(u'test{}@example.com'),
                            password_hash=pwd_context.encrypt('test1234'),
                            group_ids=[2],
                            org_id=1)

org_factory = ModelFactory(redash.models.Organization,
                           name=Sequence("Org {}"),
                           slug=Sequence("org{}.example.com"),
                           settings={})

data_source_factory = ModelFactory(redash.models.DataSource,
                                   name=Sequence('Test {}'),
                                   type='pg',
                                   # If we don't use lambda here it will reuse the same options between tests:
                                   options=lambda: ConfigurationContainer.from_json('{"dbname": "test"}'),
                                   org_id=1)

dashboard_factory = ModelFactory(redash.models.Dashboard,
                                 name='test',
                                 user=user_factory.create,
                                 layout='[]',
                                 is_draft=False,
                                 org=1)

api_key_factory = ModelFactory(redash.models.ApiKey,
                               object=dashboard_factory.create)

query_factory = ModelFactory(redash.models.Query,
                             name='Query',
                             description='',
                             query_text=u'SELECT 1',
                             user=user_factory.create,
                             is_archived=False,
                             is_draft=False,
                             schedule=None,
                             data_source=data_source_factory.create,
                             org_id=1)

query_with_params_factory = ModelFactory(redash.models.Query,
                             name='New Query with Params',
                             description='',
                             query_text='SELECT {{param1}}',
                             user=user_factory.create,
                             is_archived=False,
                             is_draft=False,
                             schedule=None,
                             data_source=data_source_factory.create,
                             org_id=1)

access_permission_factory = ModelFactory(redash.models.AccessPermission,
                             object_id=query_factory.create,
                             object_type=redash.models.Query.__name__,
                             access_type=ACCESS_TYPE_MODIFY,
                             grantor=user_factory.create,
                             grantee=user_factory.create)

alert_factory = ModelFactory(redash.models.Alert,
                             name=Sequence('Alert {}'),
                             query_rel=query_factory.create,
                             user=user_factory.create,
                             options={})

query_result_factory = ModelFactory(redash.models.QueryResult,
                                    data='{"columns":{}, "rows":[]}',
                                    runtime=1,
                                    retrieved_at=utcnow,
                                    query_text="SELECT 1",
                                    query_hash=gen_query_hash('SELECT 1'),
                                    data_source=data_source_factory.create,
                                    org_id=1)

visualization_factory = ModelFactory(redash.models.Visualization,
                                     type='CHART',
                                     query_rel=query_factory.create,
                                     name='Chart',
                                     description='',
                                     options='{}')

widget_factory = ModelFactory(redash.models.Widget,
                              width=1,
                              options='{}',
                              dashboard=dashboard_factory.create,
                              visualization=visualization_factory.create)

destination_factory = ModelFactory(redash.models.NotificationDestination,
                                   org_id=1,
                                   user=user_factory.create,
                                   name=Sequence('Destination {}'),
                                   type='slack',
                                   options=ConfigurationContainer.from_json('{"url": "https://www.slack.com"}'))

alert_subscription_factory = ModelFactory(redash.models.AlertSubscription,
                                   user=user_factory.create,
                                   destination=destination_factory.create,
                                   alert=alert_factory.create)

query_snippet_factory = ModelFactory(redash.models.QuerySnippet,
                                     trigger=Sequence('trigger {}'),
                                     description='description',
                                     snippet='snippet')


class Factory(object):
    def __init__(self):
        self.org, self.admin_group, self.default_group = redash.models.init_db()
        self._data_source = None
        self._user = None

    @property
    def user(self):
        if self._user is None:
            self._user = self.create_user()
            # Test setup creates users, they need to be in the db by the time
            # the handler's db transaction starts.
            db.session.commit()
        return self._user

    @property
    def data_source(self):
        if self._data_source is None:
            self._data_source = data_source_factory.create(org=self.org)
            db.session.add(redash.models.DataSourceGroup(
                group=self.default_group,
                data_source=self._data_source))

        return self._data_source

    def create_org(self, **kwargs):
        org = org_factory.create(**kwargs)
        self.create_group(org=org, type=redash.models.Group.BUILTIN_GROUP, name="default")
        self.create_group(org=org, type=redash.models.Group.BUILTIN_GROUP, name="admin",
                          permissions=["admin"])

        return org

    def create_user(self, **kwargs):
        args = {
            'org': self.org,
            'group_ids': [self.default_group.id]
        }

        if 'org' in kwargs:
            args['group_ids'] = [kwargs['org'].default_group.id]

        args.update(kwargs)
        return user_factory.create(**args)

    def create_admin(self, **kwargs):
        args = {
            'org': self.org,
            'group_ids': [self.admin_group.id, self.default_group.id]
        }

        if 'org' in kwargs:
            args['group_ids'] = [kwargs['org'].default_group.id, kwargs['org'].admin_group.id]

        args.update(kwargs)
        return user_factory.create(**args)

    def create_group(self, **kwargs):
        args = {
            'name': 'Group',
            'org': self.org
        }

        args.update(kwargs)

        g = redash.models.Group(**args)
        return g

    def create_alert(self, **kwargs):
        args = {
            'user': self.user,
            'query_rel': self.create_query()
        }

        args.update(**kwargs)
        return alert_factory.create(**args)

    def create_alert_subscription(self, **kwargs):
        args = {
            'user': self.user,
            'alert': self.create_alert()
        }

        args.update(**kwargs)
        return alert_subscription_factory.create(**args)

    def create_data_source(self, **kwargs):
        group = None
        if 'group' in kwargs:
            group = kwargs.pop('group')
        args = {
            'org': self.org
        }
        args.update(kwargs)

        if group and 'org' not in kwargs:
            args['org'] = group.org

        view_only = args.pop('view_only', False)
        data_source = data_source_factory.create(**args)

        if group:
            db.session.add(redash.models.DataSourceGroup(
                group=group,
                data_source=data_source,
                view_only=view_only))

        return data_source

    def create_dashboard(self, **kwargs):
        args = {
            'user': self.user,
            'org': self.org
        }
        args.update(kwargs)
        return dashboard_factory.create(**args)

    def create_query(self, **kwargs):
        args = {
            'user': self.user,
            'data_source': self.data_source,
            'org': self.org
        }
        args.update(kwargs)
        return query_factory.create(**args)

    def create_query_with_params(self, **kwargs):
        args = {
            'user': self.user,
            'data_source': self.data_source,
            'org': self.org
        }
        args.update(kwargs)
        return query_with_params_factory.create(**args)

    def create_access_permission(self, **kwargs):
        args = {
            'grantor': self.user
        }
        args.update(kwargs)
        return access_permission_factory.create(**args)

    def create_query_result(self, **kwargs):
        args = {
            'data_source': self.data_source,
        }

        args.update(kwargs)

        if 'data_source' in args and 'org' not in args:
            args['org'] = args['data_source'].org

        return query_result_factory.create(**args)

    def create_visualization(self, **kwargs):
        args = {
            'query_rel': self.create_query()
        }
        args.update(kwargs)
        return visualization_factory.create(**args)

    def create_visualization_with_params(self, **kwargs):
        args = {
            'query_rel': self.create_query_with_params()
        }
        args.update(kwargs)
        return visualization_factory.create(**args)

    def create_widget(self, **kwargs):
        args = {
            'dashboard': self.create_dashboard(),
            'visualization': self.create_visualization()
        }
        args.update(kwargs)
        return widget_factory.create(**args)

    def create_api_key(self, **kwargs):
        args = {
            'org': self.org
        }
        args.update(kwargs)
        return api_key_factory.create(**args)

    def create_destination(self, **kwargs):
        args = {
            'org': self.org
        }
        args.update(kwargs)
        return destination_factory.create(**args)

    def create_query_snippet(self, **kwargs):
        args = {
            'user': self.user,
            'org': self.org
        }
        args.update(kwargs)
        return query_snippet_factory.create(**args)
<EOF>
<BOF>
"""
Some test cases around the Glue catalog.
"""
from unittest import TestCase

import botocore
import mock
from botocore.stub import Stubber

from redash.query_runner.athena import Athena


class TestGlueSchema(TestCase):
    def setUp(self):

        client = botocore.session.get_session().create_client(
            'glue', region_name='mars-east-1', aws_access_key_id='foo', aws_secret_access_key='bar'
        )
        self.stubber = Stubber(client)

        self.patcher = mock.patch('boto3.client')
        mocked_client = self.patcher.start()
        mocked_client.return_value = client

    def tearDown(self):
        self.patcher.stop()

    def test_external_table(self):
        """Unpartitioned table crawled through a JDBC connection"""
        query_runner = Athena({'glue': True, 'region': 'mars-east-1'})

        self.stubber.add_response('get_databases', {'DatabaseList': [{'Name': 'test1'}]}, {})
        self.stubber.add_response(
            'get_tables',
            {
                'TableList': [
                    {
                        'Name': 'jdbc_table',
                        'StorageDescriptor': {
                            'Columns': [{'Name': 'row_id', 'Type': 'int'}],
                            'Location': 'Database.Schema.Table',
                            'Compressed': False,
                            'NumberOfBuckets': -1,
                            'SerdeInfo': {'Parameters': {}},
                            'BucketColumns': [],
                            'SortColumns': [],
                            'Parameters': {
                                'CrawlerSchemaDeserializerVersion': '1.0',
                                'CrawlerSchemaSerializerVersion': '1.0',
                                'UPDATED_BY_CRAWLER': 'jdbc',
                                'classification': 'sqlserver',
                                'compressionType': 'none',
                                'connectionName': 'jdbctest',
                                'typeOfData': 'view',
                            },
                            'StoredAsSubDirectories': False,
                        },
                        'PartitionKeys': [],
                        'TableType': 'EXTERNAL_TABLE',
                        'Parameters': {
                            'CrawlerSchemaDeserializerVersion': '1.0',
                            'CrawlerSchemaSerializerVersion': '1.0',
                            'UPDATED_BY_CRAWLER': 'jdbc',
                            'classification': 'sqlserver',
                            'compressionType': 'none',
                            'connectionName': 'jdbctest',
                            'typeOfData': 'view',
                        },
                    }
                ]
            },
            {'DatabaseName': 'test1'},
        )
        with self.stubber:
            assert query_runner.get_schema() == [{'columns': ['row_id'], 'name': 'test1.jdbc_table'}]

    def test_partitioned_table(self):
        """
        Partitioned table as created by a GlueContext
        """

        query_runner = Athena({'glue': True, 'region': 'mars-east-1'})

        self.stubber.add_response('get_databases', {'DatabaseList': [{'Name': 'test1'}]}, {})
        self.stubber.add_response(
            'get_tables',
            {
                'TableList': [
                    {
                        'Name': 'partitioned_table',
                        'StorageDescriptor': {
                            'Columns': [{'Name': 'sk', 'Type': 'int'}],
                            'Location': 's3://bucket/prefix',
                            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
                            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
                            'Compressed': False,
                            'NumberOfBuckets': -1,
                            'SerdeInfo': {
                                'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',
                                'Parameters': {'serialization.format': '1'},
                            },
                            'BucketColumns': [],
                            'SortColumns': [],
                            'Parameters': {},
                            'SkewedInfo': {
                                'SkewedColumnNames': [],
                                'SkewedColumnValues': [],
                                'SkewedColumnValueLocationMaps': {},
                            },
                            'StoredAsSubDirectories': False,
                        },
                        'PartitionKeys': [{'Name': 'category', 'Type': 'int'}],
                        'TableType': 'EXTERNAL_TABLE',
                        'Parameters': {'EXTERNAL': 'TRUE', 'transient_lastDdlTime': '1537505313'},
                    }
                ]
            },
            {'DatabaseName': 'test1'},
        )
        with self.stubber:
            assert query_runner.get_schema() == [{'columns': ['sk', 'category'], 'name': 'test1.partitioned_table'}]

    def test_view(self):
        query_runner = Athena({'glue': True, 'region': 'mars-east-1'})

        self.stubber.add_response('get_databases', {'DatabaseList': [{'Name': 'test1'}]}, {})
        self.stubber.add_response(
            'get_tables',
            {
                'TableList': [
                    {
                        'Name': 'view',
                        'StorageDescriptor': {
                            'Columns': [{'Name': 'sk', 'Type': 'int'}],
                            'Location': '',
                            'Compressed': False,
                            'NumberOfBuckets': 0,
                            'SerdeInfo': {},
                            'SortColumns': [],
                            'StoredAsSubDirectories': False,
                        },
                        'PartitionKeys': [],
                        'ViewOriginalText': '/* Presto View: ... */',
                        'ViewExpandedText': '/* Presto View */',
                        'TableType': 'VIRTUAL_VIEW',
                        'Parameters': {'comment': 'Presto View', 'presto_view': 'true'},
                    }
                ]
            },
            {'DatabaseName': 'test1'},
        )
        with self.stubber:
            assert query_runner.get_schema() == [{'columns': ['sk'], 'name': 'test1.view'}]

    def test_dodgy_table_does_not_break_schema_listing(self):
        """
        For some reason, not all Glue tables contain a "PartitionKeys" entry.

        This may be a Athena Catalog to Glue catalog migration issue.
        """
        query_runner = Athena({'glue': True, 'region': 'mars-east-1'})

        self.stubber.add_response('get_databases', {'DatabaseList': [{'Name': 'test1'}]}, {})
        self.stubber.add_response(
            'get_tables',
            {
                'TableList': [
                    {
                        'Name': 'csv',
                        'StorageDescriptor': {
                            'Columns': [{'Name': 'region', 'Type': 'string'}],
                            'Location': 's3://bucket/files/',
                            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
                            'Compressed': False,
                            'NumberOfBuckets': 0,
                            'SerdeInfo': {
                                'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe',
                                'Parameters': {'field.delim': '|', 'skip.header.line.count': '1'},
                            },
                            'SortColumns': [],
                            'StoredAsSubDirectories': False,
                        },
                        'Parameters': {'classification': 'csv'},
                    }
                ]
            },
            {'DatabaseName': 'test1'},
        )
        with self.stubber:
            assert query_runner.get_schema() == [{'columns': ['region'], 'name': 'test1.csv'}]
<EOF>
<BOF>
import datetime
from unittest import TestCase

from redash.query_runner.prometheus import get_instant_rows, get_range_rows


class TestPrometheus(TestCase):
    def setUp(self):
        self.instant_query_result = [
            {
                "metric": {
                    "name": "example_metric_name",
                    "foo_bar": "foo",
                },
                "value": [1516937400.781, "7400_foo"]
            },
            {
                "metric": {
                    "name": "example_metric_name",
                    "foo_bar": "bar",
                },
                "value": [1516937400.781, "7400_bar"]
            }
        ]

        self.range_query_result = [
            {
                "metric": {
                    "name": "example_metric_name",
                    "foo_bar": "foo",
                },
                "values": [
                    [1516937400.781, "7400_foo"],
                    [1516938000.781, "8000_foo"],
                ]
            },
            {
                "metric": {
                    "name": "example_metric_name",
                    "foo_bar": "bar",
                },
                "values": [
                    [1516937400.781, "7400_bar"],
                    [1516938000.781, "8000_bar"],
                ]
            }
        ]

    def test_get_instant_rows(self):
        instant_rows = [
            {
                "name": "example_metric_name",
                "foo_bar": "foo",
                "timestamp": datetime.datetime.fromtimestamp(1516937400.781),
                "value": "7400_foo"
            },
            {
                "name": "example_metric_name",
                "foo_bar": "bar",
                "timestamp": datetime.datetime.fromtimestamp(1516937400.781),
                "value": "7400_bar"
            },
        ]

        rows = get_instant_rows(self.instant_query_result)
        self.assertEqual(instant_rows, rows)

    def test_get_range_rows(self):

        range_rows = [
            {
                "name": "example_metric_name",
                "foo_bar": "foo",
                "timestamp": datetime.datetime.fromtimestamp(1516937400.781),
                "value": "7400_foo"
            },
            {
                "name": "example_metric_name",
                "foo_bar": "foo",
                "timestamp": datetime.datetime.fromtimestamp(1516938000.781),
                "value": "8000_foo"
            },
            {
                "name": "example_metric_name",
                "foo_bar": "bar",
                "timestamp": datetime.datetime.fromtimestamp(1516937400.781),
                "value": "7400_bar"
            },
            {
                "name": "example_metric_name",
                "foo_bar": "bar",
                "timestamp": datetime.datetime.fromtimestamp(1516938000.781),
                "value": "8000_bar"
            },
        ]

        rows = get_range_rows(self.range_query_result)
        self.assertEqual(range_rows, rows)
<EOF>
<BOF>
import mock
from unittest import TestCase

import requests
from redash.query_runner import BaseHTTPQueryRunner


class RequiresAuthQueryRunner(BaseHTTPQueryRunner):
    requires_authentication = True


class TestBaseHTTPQueryRunner(TestCase):

    def test_requires_authentication_default(self):
        self.assertFalse(BaseHTTPQueryRunner.requires_authentication)
        schema = BaseHTTPQueryRunner.configuration_schema()
        self.assertNotIn('username', schema['required'])
        self.assertNotIn('password', schema['required'])

    def test_requires_authentication_true(self):
        schema = RequiresAuthQueryRunner.configuration_schema()
        self.assertIn('username', schema['required'])
        self.assertIn('password', schema['required'])

    def test_get_auth_with_values(self):
        query_runner = BaseHTTPQueryRunner({
            'username': 'username',
            'password': 'password'
        })
        self.assertEqual(query_runner.get_auth(), ('username', 'password'))

    def test_get_auth_empty(self):
        query_runner = BaseHTTPQueryRunner({})
        self.assertIsNone(query_runner.get_auth())

    def test_get_auth_empty_requires_authentication(self):
        query_runner = RequiresAuthQueryRunner({})
        self.assertRaisesRegexp(
            ValueError,
            "Username and Password required",
            query_runner.get_auth
        )

    @mock.patch('requests.get')
    def test_get_response_success(self, mock_get):
        mock_response = mock.Mock()
        mock_response.status_code = 200
        mock_response.text = "Success"
        mock_get.return_value = mock_response

        url = 'https://example.com/'
        query_runner = BaseHTTPQueryRunner({})
        response, error = query_runner.get_response(url)
        mock_get.assert_called_once_with(url, auth=None)
        self.assertEqual(response.status_code, 200)
        self.assertIsNone(error)

    @mock.patch('requests.get')
    def test_get_response_success_custom_auth(self, mock_get):
        mock_response = mock.Mock()
        mock_response.status_code = 200
        mock_response.text = "Success"
        mock_get.return_value = mock_response

        url = 'https://example.com/'
        query_runner = BaseHTTPQueryRunner({})
        auth = ('username', 'password')
        response, error = query_runner.get_response(url, auth=auth)
        mock_get.assert_called_once_with(url, auth=auth)
        self.assertEqual(response.status_code, 200)
        self.assertIsNone(error)

    @mock.patch('requests.get')
    def test_get_response_failure(self, mock_get):
        mock_response = mock.Mock()
        mock_response.status_code = 301
        mock_response.text = "Redirect"
        mock_get.return_value = mock_response

        url = 'https://example.com/'
        query_runner = BaseHTTPQueryRunner({})
        response, error = query_runner.get_response(url)
        mock_get.assert_called_once_with(url, auth=None)
        self.assertIn(query_runner.response_error, error)

    @mock.patch('requests.get')
    def test_get_response_httperror_exception(self, mock_get):
        mock_response = mock.Mock()
        mock_response.status_code = 500
        mock_response.text = "Server Error"
        http_error = requests.HTTPError()
        mock_response.raise_for_status.side_effect = http_error
        mock_get.return_value = mock_response

        url = 'https://example.com/'
        query_runner = BaseHTTPQueryRunner({})
        response, error = query_runner.get_response(url)
        mock_get.assert_called_once_with(url, auth=None)
        self.assertIsNotNone(error)
        self.assertIn("Failed to execute query", error)

    @mock.patch('requests.get')
    def test_get_response_requests_exception(self, mock_get):
        mock_response = mock.Mock()
        mock_response.status_code = 500
        mock_response.text = "Server Error"
        exception_message = "Some requests exception"
        requests_exception = requests.RequestException(exception_message)
        mock_response.raise_for_status.side_effect = requests_exception
        mock_get.return_value = mock_response

        url = 'https://example.com/'
        query_runner = BaseHTTPQueryRunner({})
        response, error = query_runner.get_response(url)
        mock_get.assert_called_once_with(url, auth=None)
        self.assertIsNotNone(error)
        self.assertEqual(exception_message, error)

    @mock.patch('requests.get')
    def test_get_response_generic_exception(self, mock_get):
        mock_response = mock.Mock()
        mock_response.status_code = 500
        mock_response.text = "Server Error"
        exception_message = "Some generic exception"
        exception = ValueError(exception_message)
        mock_response.raise_for_status.side_effect = exception
        mock_get.return_value = mock_response

        url = 'https://example.com/'
        query_runner = BaseHTTPQueryRunner({})
        self.assertRaisesRegexp(
            ValueError,
            exception_message,
            query_runner.get_response,
            url
        )
<EOF>
<BOF>
import sqlite3
from unittest import TestCase

import pytest

from redash.query_runner import TYPE_BOOLEAN, TYPE_DATETIME, TYPE_FLOAT, TYPE_INTEGER, TYPE_STRING
from redash.query_runner.query_results import (CreateTableError, PermissionError, _guess_type, _load_query, create_table, extract_cached_query_ids, extract_query_ids, fix_column_name)
from tests import BaseTestCase


class TestExtractQueryIds(TestCase):
    def test_works_with_simple_query(self):
        query = "SELECT 1"
        self.assertEquals([], extract_query_ids(query))

    def test_finds_queries_to_load(self):
        query = "SELECT * FROM query_123"
        self.assertEquals([123], extract_query_ids(query))

    def test_finds_queries_in_joins(self):
        query = "SELECT * FROM query_123 JOIN query_4566"
        self.assertEquals([123, 4566], extract_query_ids(query))

    def test_finds_queries_with_whitespace_characters(self):
        query = "SELECT * FROM    query_123 a JOIN\tquery_4566 b ON a.id=b.parent_id JOIN\r\nquery_78 c ON b.id=c.parent_id"
        self.assertEquals([123, 4566, 78], extract_query_ids(query))


class TestCreateTable(TestCase):
    def test_creates_table_with_colons_in_column_name(self):
        connection = sqlite3.connect(':memory:')
        results = {'columns': [{'name': 'ga:newUsers'}, {
            'name': 'test2'}], 'rows': [{'ga:newUsers': 123, 'test2': 2}]}
        table_name = 'query_123'
        create_table(connection, table_name, results)
        connection.execute('SELECT 1 FROM query_123')

    def test_creates_table(self):
        connection = sqlite3.connect(':memory:')
        results = {'columns': [{'name': 'test1'},
                               {'name': 'test2'}], 'rows': []}
        table_name = 'query_123'
        create_table(connection, table_name, results)
        connection.execute('SELECT 1 FROM query_123')

    def test_creates_table_with_missing_columns(self):
        connection = sqlite3.connect(':memory:')
        results = {'columns': [{'name': 'test1'}, {'name': 'test2'}], 'rows': [
            {'test1': 1, 'test2': 2}, {'test1': 3}]}
        table_name = 'query_123'
        create_table(connection, table_name, results)
        connection.execute('SELECT 1 FROM query_123')

    def test_creates_table_with_spaces_in_column_name(self):
        connection = sqlite3.connect(':memory:')
        results = {'columns': [{'name': 'two words'}, {'name': 'test2'}], 'rows': [
            {'two words': 1, 'test2': 2}, {'test1': 3}]}
        table_name = 'query_123'
        create_table(connection, table_name, results)
        connection.execute('SELECT 1 FROM query_123')

    def test_creates_table_with_dashes_in_column_name(self):
        connection = sqlite3.connect(':memory:')
        results = {
            'columns': [{'name': 'two-words'}, {'name': 'test2'}],
            'rows': [{'two-words': 1, 'test2': 2}]
        }
        table_name = 'query_123'
        create_table(connection, table_name, results)
        connection.execute('SELECT 1 FROM query_123')
        connection.execute('SELECT "two-words" FROM query_123')

    def test_creates_table_with_non_ascii_in_column_name(self):
        connection = sqlite3.connect(':memory:')
        results = {'columns': [{'name': u'\xe4'}, {'name': 'test2'}], 'rows': [
            {u'\xe4': 1, 'test2': 2}]}
        table_name = 'query_123'
        create_table(connection, table_name, results)
        connection.execute('SELECT 1 FROM query_123')

    def test_shows_meaningful_error_on_failure_to_create_table(self):
        connection = sqlite3.connect(':memory:')
        results = {'columns': [], 'rows': []}
        table_name = 'query_123'
        with pytest.raises(CreateTableError):
            create_table(connection, table_name, results)

    def test_loads_results(self):
        connection = sqlite3.connect(':memory:')
        rows = [{'test1': 1, 'test2': 'test'}, {'test1': 2, 'test2': 'test2'}]
        results = {'columns': [{'name': 'test1'},
                               {'name': 'test2'}], 'rows': rows}
        table_name = 'query_123'
        create_table(connection, table_name, results)
        self.assertEquals(
            len(list(connection.execute('SELECT * FROM query_123'))), 2)


class TestGetQuery(BaseTestCase):
    # test query from different account
    def test_raises_exception_for_query_from_different_account(self):
        query = self.factory.create_query()
        user = self.factory.create_user(org=self.factory.create_org())

        self.assertRaises(PermissionError, lambda: _load_query(user, query.id))

    def test_raises_exception_for_query_with_different_groups(self):
        ds = self.factory.create_data_source(group=self.factory.create_group())
        query = self.factory.create_query(data_source=ds)
        user = self.factory.create_user()

        self.assertRaises(PermissionError, lambda: _load_query(user, query.id))

    def test_returns_query(self):
        query = self.factory.create_query()
        user = self.factory.create_user()

        loaded = _load_query(user, query.id)
        self.assertEquals(query, loaded)


class TestGuessType(TestCase):
    def test_string(self):
        self.assertEqual(TYPE_STRING, _guess_type(''))
        self.assertEqual(TYPE_STRING, _guess_type(None))
        self.assertEqual(TYPE_STRING, _guess_type('redash'))

    def test_integer(self):
        self.assertEqual(TYPE_INTEGER, _guess_type(42))

    def test_float(self):
        self.assertEqual(TYPE_FLOAT, _guess_type(3.14))

    def test_boolean(self):
        self.assertEqual(TYPE_BOOLEAN, _guess_type('true'))
        self.assertEqual(TYPE_BOOLEAN, _guess_type('false'))

    def test_date(self):
        self.assertEqual(TYPE_DATETIME, _guess_type('2018-06-28'))


class TestExtractCachedQueryIds(TestCase):
    def test_works_with_simple_query(self):
        query = "SELECT 1"
        self.assertEquals([], extract_cached_query_ids(query))

    def test_finds_queries_to_load(self):
        query = "SELECT * FROM cached_query_123"
        self.assertEquals([123], extract_cached_query_ids(query))

    def test_finds_queries_in_joins(self):
        query = "SELECT * FROM cached_query_123 JOIN cached_query_4566"
        self.assertEquals([123, 4566], extract_cached_query_ids(query))

    def test_finds_queries_with_whitespace_characters(self):
        query = "SELECT * FROM    cached_query_123 a JOIN\tcached_query_4566 b ON a.id=b.parent_id JOIN\r\ncached_query_78 c ON b.id=c.parent_id"
        self.assertEquals([123, 4566, 78], extract_cached_query_ids(query))


class TestFixColumnName(TestCase):
    def test_fix_column_name(self):
        self.assertEquals(u'"a_b_c_d"', fix_column_name("a:b.c d"))
<EOF>
<BOF>
import os
import subprocess

from _pytest.monkeypatch import MonkeyPatch

from redash.query_runner.script import query_to_script_path, run_script
from tests import BaseTestCase


class TestQueryToScript(BaseTestCase):
    monkeypatch = MonkeyPatch()

    def test_unspecified(self):
        self.assertEqual("/foo/bar/baz.sh", query_to_script_path("*", "/foo/bar/baz.sh"))

    def test_specified(self):
        self.assertRaises(IOError, lambda: query_to_script_path("/foo/bar", "baz.sh"))

        self.monkeypatch.setattr(os.path, "exists", lambda x: True)
        self.assertEqual(["/foo/bar/baz.sh"], query_to_script_path("/foo/bar", "baz.sh"))


class TestRunScript(BaseTestCase):
    monkeypatch = MonkeyPatch()

    def test_success(self):
        self.monkeypatch.setattr(subprocess, "check_output", lambda script, shell: "test")
        self.assertEqual(("test", None), run_script("/foo/bar/baz.sh", True))

    def test_failure(self):
        self.monkeypatch.setattr(subprocess, "check_output", lambda script, shell: None)
        self.assertEqual((None, "Error reading output"), run_script("/foo/bar/baz.sh", True))
        self.monkeypatch.setattr(subprocess, "check_output", lambda script, shell: "")
        self.assertEqual((None, "Empty output from script"), run_script("/foo/bar/baz.sh", True))
        self.monkeypatch.setattr(subprocess, "check_output", lambda script, shell: " ")
        self.assertEqual((None, "Empty output from script"), run_script("/foo/bar/baz.sh", True))
<EOF>
<BOF>
from unittest import TestCase
from redash.query_runner.jql import FieldMapping, parse_issue


class TestFieldMapping(TestCase):

    def test_empty(self):
        field_mapping = FieldMapping({})

        self.assertEqual(field_mapping.get_output_field_name('field1'), 'field1')
        self.assertEqual(field_mapping.get_dict_output_field_name('field1','member1'), None)
        self.assertEqual(field_mapping.get_dict_members('field1'), [])

    def test_with_mappings(self):
        field_mapping = FieldMapping({
            'field1': 'output_name_1',
            'field2.member1': 'output_name_2',
            'field2.member2': 'output_name_3'
            })

        self.assertEqual(field_mapping.get_output_field_name('field1'), 'output_name_1')
        self.assertEqual(field_mapping.get_dict_output_field_name('field1','member1'), None)
        self.assertEqual(field_mapping.get_dict_members('field1'), [])

        self.assertEqual(field_mapping.get_output_field_name('field2'), 'field2')
        self.assertEqual(field_mapping.get_dict_output_field_name('field2','member1'), 'output_name_2')
        self.assertEqual(field_mapping.get_dict_output_field_name('field2','member2'), 'output_name_3')
        self.assertEqual(field_mapping.get_dict_output_field_name('field2','member3'), None)
        self.assertEqual(field_mapping.get_dict_members('field2'), ['member1','member2'])


class TestParseIssue(TestCase):
    issue = {
        'key': 'KEY-1',
        'fields': {
            'string_field': 'value1',
            'int_field': 123,
            'string_list_field': ['value1','value2'],
            'dict_field': {'member1':'value1','member2': 'value2'},
            'dict_list_field': [
                {'member1':'value1a','member2': 'value2a'},
                {'member1':'value1b','member2': 'value2b'}
            ],
            'dict_legacy': {'key':'legacyKey','name':'legacyName','dict_legacy':'legacyValue'},
            'watchers': {'watchCount':10}
        }
    }

    def test_no_mapping(self):
        result = parse_issue(self.issue, FieldMapping({}))

        self.assertEqual(result['key'], 'KEY-1')
        self.assertEqual(result['string_field'], 'value1')
        self.assertEqual(result['int_field'], 123)
        self.assertEqual(result['string_list_field'], 'value1,value2')
        self.assertEqual('dict_field' in result, False)
        self.assertEqual('dict_list_field' in result, False)
        self.assertEqual(result['dict_legacy'], 'legacyValue')
        self.assertEqual(result['dict_legacy_key'], 'legacyKey')
        self.assertEqual(result['dict_legacy_name'], 'legacyName')
        self.assertEqual(result['watchers'], 10)

    def test_mapping(self):
        result = parse_issue(self.issue, FieldMapping({
            'string_field': 'string_output_field',
            'string_list_field': 'string_output_list_field',
            'dict_field.member1': 'dict_field_1',
            'dict_field.member2': 'dict_field_2',
            'dict_list_field.member1': 'dict_list_field_1',
            'dict_legacy.key': 'dict_legacy',
            'watchers.watchCount': 'watchCount',
        }))

        self.assertEqual(result['key'], 'KEY-1')
        self.assertEqual(result['string_output_field'], 'value1')
        self.assertEqual(result['int_field'], 123)
        self.assertEqual(result['string_output_list_field'], 'value1,value2')
        self.assertEqual(result['dict_field_1'], 'value1')
        self.assertEqual(result['dict_field_2'], 'value2')
        self.assertEqual(result['dict_list_field_1'], 'value1a,value1b')
        self.assertEqual(result['dict_legacy'], 'legacyKey')
        self.assertEqual('dict_legacy_key' in result, False)
        self.assertEqual('dict_legacy_name' in result, False)
        self.assertEqual('watchers' in result, False)
        self.assertEqual(result['watchCount'], 10)


    def test_mapping_nonexisting_field(self):
        result = parse_issue(self.issue, FieldMapping({
            'non_existing_field': 'output_name1',
            'dict_field.non_existing_member': 'output_name2',
            'dict_list_field.non_existing_member': 'output_name3'
        }))

        self.assertEqual(result['key'], 'KEY-1')
        self.assertEqual(result['string_field'], 'value1')
        self.assertEqual(result['int_field'], 123)
        self.assertEqual(result['string_list_field'], 'value1,value2')
        self.assertEqual('dict_field' in result, False)
        self.assertEqual('dict_list_field' in result, False)
        self.assertEqual(result['dict_legacy'], 'legacyValue')
        self.assertEqual(result['dict_legacy_key'], 'legacyKey')
        self.assertEqual(result['dict_legacy_name'], 'legacyName')
        self.assertEqual(result['watchers'], 10)
<EOF>
<BOF>
import datetime
from unittest import TestCase

from pytz import utc

from redash.query_runner.mongodb import parse_query_json, parse_results, _get_column_by_name
from redash.utils import json_dumps, parse_human_time


class TestParseQueryJson(TestCase):
    def test_ignores_non_isodate_fields(self):
        query = {
            'test': 1,
            'test_list': ['a', 'b', 'c'],
            'test_dict': {
                'a': 1,
                'b': 2
            }
        }

        query_data = parse_query_json(json_dumps(query))
        self.assertDictEqual(query_data, query)

    def test_parses_isodate_fields(self):
        query = {
            'test': 1,
            'test_list': ['a', 'b', 'c'],
            'test_dict': {
                'a': 1,
                'b': 2
            },
            'testIsoDate': "ISODate(\"2014-10-03T00:00\")"
        }

        query_data = parse_query_json(json_dumps(query))

        self.assertEqual(query_data['testIsoDate'], datetime.datetime(2014, 10, 3, 0, 0))

    def test_parses_isodate_in_nested_fields(self):
        query = {
            'test': 1,
            'test_list': ['a', 'b', 'c'],
            'test_dict': {
                'a': 1,
                'b': {
                    'date': "ISODate(\"2014-10-04T00:00\")"
                }
            },
            'testIsoDate': "ISODate(\"2014-10-03T00:00\")"
        }

        query_data = parse_query_json(json_dumps(query))

        self.assertEqual(query_data['testIsoDate'], datetime.datetime(2014, 10, 3, 0, 0))
        self.assertEqual(query_data['test_dict']['b']['date'], datetime.datetime(2014, 10, 4, 0, 0))

    def test_handles_nested_fields(self):
        # https://github.com/getredash/redash/issues/597
        query = {
            "collection": "bus",
            "aggregate": [
                {
                    "$geoNear": {
                        "near": {"type": "Point", "coordinates": [-22.910079, -43.205161]},
                        "maxDistance": 100000000,
                        "distanceField": "dist.calculated",
                        "includeLocs": "dist.location",
                        "spherical": True
                    }
                }
            ]
        }

        query_data = parse_query_json(json_dumps(query))

        self.assertDictEqual(query, query_data)

    def test_supports_extended_json_types(self):
        query = {
            'test': 1,
            'test_list': ['a', 'b', 'c'],
            'test_dict': {
                'a': 1,
                'b': 2
            },
            'testIsoDate': "ISODate(\"2014-10-03T00:00\")",
            'test$date': {
                '$date': '2014-10-03T00:00:00.0'
            },
            'test$undefined': {
                '$undefined': None
            }
        }
        query_data = parse_query_json(json_dumps(query))
        self.assertEqual(query_data['test$undefined'], None)
        self.assertEqual(query_data['test$date'], datetime.datetime(2014, 10, 3, 0, 0).replace(tzinfo=utc))

    def test_supports_relative_timestamps(self):
        query = {
            'ts': {'$humanTime': '1 hour ago'}
        }

        one_hour_ago = parse_human_time("1 hour ago")
        query_data = parse_query_json(json_dumps(query))
        self.assertEqual(query_data['ts'], one_hour_ago)


class TestMongoResults(TestCase):
    def test_parses_regular_results(self):
        raw_results = [
            {'column': 1, 'column2': 'test'},
            {'column': 2, 'column2': 'test', 'column3': 'hello'}
        ]
        rows, columns = parse_results(raw_results)

        for i, row in enumerate(rows):
            self.assertDictEqual(row, raw_results[i])

        self.assertIsNotNone(_get_column_by_name(columns, 'column'))
        self.assertIsNotNone(_get_column_by_name(columns, 'column2'))
        self.assertIsNotNone(_get_column_by_name(columns, 'column3'))

    def test_parses_nested_results(self):
        raw_results = [
            {'column': 1, 'column2': 'test', 'nested': {
                'a': 1,
                'b': 'str'
            }},
            {'column': 2, 'column2': 'test', 'column3': 'hello', 'nested': {
                'a': 2,
                'b': 'str2',
                'c': 'c'
            }}
        ]

        rows, columns = parse_results(raw_results)

        self.assertDictEqual(rows[0], { 'column': 1, 'column2': 'test', 'nested.a': 1, 'nested.b': 'str' })
        self.assertDictEqual(rows[1], { 'column': 2, 'column2': 'test', 'column3': 'hello', 'nested.a': 2, 'nested.b': 'str2', 'nested.c': 'c' })

        self.assertIsNotNone(_get_column_by_name(columns, 'column'))
        self.assertIsNotNone(_get_column_by_name(columns, 'column2'))
        self.assertIsNotNone(_get_column_by_name(columns, 'column3'))
        self.assertIsNotNone(_get_column_by_name(columns, 'nested.a'))
        self.assertIsNotNone(_get_column_by_name(columns, 'nested.b'))
        self.assertIsNotNone(_get_column_by_name(columns, 'nested.c'))
<EOF>
<BOF>
# -*- coding: utf-8 -*-
import datetime
from unittest import TestCase

from mock import MagicMock

from redash.query_runner import TYPE_DATETIME, TYPE_FLOAT, TYPE_INTEGER
from redash.query_runner.google_spreadsheets import TYPE_BOOLEAN, TYPE_STRING, _get_columns_and_column_names, _guess_type, _value_eval_list, parse_query
from redash.query_runner.google_spreadsheets import WorksheetNotFoundError, parse_spreadsheet, parse_worksheet


class TestGuessType(TestCase):
    def test_handles_unicode(self):
        self.assertEqual(_guess_type(u'יוניקוד'), TYPE_STRING)

    def test_detects_booleans(self):
        self.assertEqual(_guess_type('true'), TYPE_BOOLEAN)
        self.assertEqual(_guess_type('True'), TYPE_BOOLEAN)
        self.assertEqual(_guess_type('TRUE'), TYPE_BOOLEAN)
        self.assertEqual(_guess_type('false'), TYPE_BOOLEAN)
        self.assertEqual(_guess_type('False'), TYPE_BOOLEAN)
        self.assertEqual(_guess_type('FALSE'), TYPE_BOOLEAN)

    def test_detects_strings(self):
        self.assertEqual(TYPE_STRING, _guess_type(''))
        self.assertEqual(TYPE_STRING, _guess_type('redash'))

    def test_detects_integer(self):
        self.assertEqual(TYPE_INTEGER, _guess_type('42'))

    def test_detects_float(self):
        self.assertEqual(TYPE_FLOAT, _guess_type('3.14'))

    def test_detects_date(self):
        self.assertEqual(TYPE_DATETIME, _guess_type('2018-06-28'))


class TestValueEvalList(TestCase):
    def test_handles_unicode(self):
        values = [u'יוניקוד', 'test', 'value']
        self.assertEqual(values, _value_eval_list(values, [TYPE_STRING]*len(values)))

    def test_handles_boolean(self):
        values = ['true', 'false', 'True', 'False', 'TRUE', 'FALSE']
        converted_values = [True, False, True, False, True, False]
        self.assertEqual(converted_values, _value_eval_list(values, [TYPE_BOOLEAN]*len(values)))

    def test_handles_empty_values(self):
        values = ['', None]
        converted_values = [None, None]
        self.assertEqual(converted_values, _value_eval_list(values, [TYPE_STRING, TYPE_STRING]))

    def test_handles_float(self):
        values = ['3.14', '-273.15']
        converted_values = [3.14, -273.15]
        self.assertEqual(converted_values, _value_eval_list(values, [TYPE_FLOAT, TYPE_FLOAT]))

    def test_handles_datetime(self):
        values = ['2018-06-28', '2020-2-29']
        converted_values = [datetime.datetime(2018, 6, 28, 0, 0), datetime.datetime(2020, 2, 29, 0, 0)]
        self.assertEqual(converted_values, _value_eval_list(values, [TYPE_DATETIME, TYPE_DATETIME]))


class TestParseSpreadsheet(TestCase):
    def test_returns_meaningful_error_for_missing_worksheet(self):
        spreadsheet = MagicMock()

        spreadsheet.worksheets = MagicMock(return_value=[])
        self.assertRaises(WorksheetNotFoundError, parse_spreadsheet, spreadsheet, 0)

        spreadsheet.worksheets = MagicMock(return_value=[1, 2])
        self.assertRaises(WorksheetNotFoundError, parse_spreadsheet, spreadsheet, 2)


empty_worksheet = []
only_headers_worksheet = [['Column A', 'Column B']]
regular_worksheet = [['String Column', 'Boolean Column', 'Number Column'], ['A', 'TRUE', '1'], ['B', 'FALSE', '2'], ['C', 'TRUE', '3'], ['D', 'FALSE', '4']]


# The following test that the parse function doesn't crash. They don't test correct output.
class TestParseWorksheet(TestCase):
    def test_parse_empty_worksheet(self):
        parse_worksheet(empty_worksheet)

    def test_parse_only_headers_worksheet(self):
        parse_worksheet(only_headers_worksheet)

    def test_parse_regular_worksheet(self):
        parse_worksheet(regular_worksheet)

    def test_parse_worksheet_with_duplicate_column_names(self):
        worksheet = [['Column', 'Another Column', 'Column'], ['A', 'TRUE', '1'], ['B', 'FALSE', '2'], ['C', 'TRUE', '3'], ['D', 'FALSE', '4']]
        parsed = parse_worksheet(worksheet)

        columns = map(lambda c: c['name'], parsed['columns'])
        self.assertEqual('Column', columns[0])
        self.assertEqual('Another Column', columns[1])
        self.assertEqual('Column1', columns[2])

        self.assertEqual('A', parsed['rows'][0]['Column'])
        self.assertEqual(True, parsed['rows'][0]['Another Column'])
        self.assertEqual(1, parsed['rows'][0]['Column1'])


class TestParseQuery(TestCase):
    def test_parse_query(self):
        parsed = parse_query('key|0')
        self.assertEqual(('key', 0), parsed)


class TestGetColumnsAndColumnNames(TestCase):
    def test_get_columns(self):
        _columns = ['foo', 'bar', 'baz']
        columns, column_names = _get_columns_and_column_names(_columns)

        self.assertEqual(_columns, column_names)

    def test_get_columns_with_duplicated(self):
        _columns = ['foo', 'bar', 'baz', 'foo', 'baz']
        columns, column_names = _get_columns_and_column_names(_columns)

        self.assertEqual(['foo', 'bar', 'baz', 'foo1', 'baz2'], column_names)

    def test_get_columns_with_blank(self):
        _columns = ['foo', '', 'baz', '']
        columns, column_names = _get_columns_and_column_names(_columns)

        self.assertEqual(['foo', 'column_B', 'baz', 'column_D'], column_names)
<EOF>
<BOF>
import time

import mock
from tests import BaseTestCase

from redash import settings
from redash.authentication.account import invite_token
from redash.models import User


class TestInvite(BaseTestCase):
    def test_expired_invite_token(self):

        with mock.patch('time.time') as patched_time:
            patched_time.return_value = time.time() - (7 * 24 * 3600) - 10
            token = invite_token(self.factory.user)

        response = self.get_request('/invite/{}'.format(token), org=self.factory.org)
        self.assertEqual(response.status_code, 400)

    def test_invalid_invite_token(self):
        response = self.get_request('/invite/badtoken', org=self.factory.org)
        self.assertEqual(response.status_code, 400)

    def test_valid_token(self):
        token = invite_token(self.factory.user)
        response = self.get_request('/invite/{}'.format(token), org=self.factory.org)
        self.assertEqual(response.status_code, 200)

    def test_already_active_user(self):
        pass


class TestInvitePost(BaseTestCase):
    def test_empty_password(self):
        token = invite_token(self.factory.user)
        response = self.post_request('/invite/{}'.format(token), data={'password': ''}, org=self.factory.org)
        self.assertEqual(response.status_code, 400)

    def test_invalid_password(self):
        token = invite_token(self.factory.user)
        response = self.post_request('/invite/{}'.format(token), data={'password': '1234'}, org=self.factory.org)
        self.assertEqual(response.status_code, 400)

    def test_bad_token(self):
        response = self.post_request('/invite/{}'.format('jdsnfkjdsnfkj'), data={'password': '1234'}, org=self.factory.org)
        self.assertEqual(response.status_code, 400)

    def test_already_active_user(self):
        pass

    def test_valid_password(self):
        token = invite_token(self.factory.user)
        password = 'test1234'
        response = self.post_request('/invite/{}'.format(token), data={'password': password}, org=self.factory.org)
        self.assertEqual(response.status_code, 302)
        user = User.query.get(self.factory.user.id)
        self.assertTrue(user.verify_password(password))


class TestLogin(BaseTestCase):
    def test_throttle_login(self):
        # Extract the limit from settings (ex: '50/day')
        limit = settings.THROTTLE_LOGIN_PATTERN.split('/')[0]
        for _ in range(0, int(limit)):
            self.get_request('/login', org=self.factory.org)

        response = self.get_request('/login', org=self.factory.org)
        self.assertEqual(response.status_code, 429)


class TestSession(BaseTestCase):
    # really simple test just to trigger this route
    def test_get(self):
        self.make_request('get', '/default/api/session', user=self.factory.user, org=False)
<EOF>
<BOF>
from tests import BaseTestCase

from redash.models import ApiKey, Dashboard, AccessPermission, db
from redash.permissions import ACCESS_TYPE_MODIFY
from redash.serializers import serialize_dashboard
from redash.utils import json_loads


class TestDashboardListResource(BaseTestCase):
    def test_create_new_dashboard(self):
        dashboard_name = 'Test Dashboard'
        rv = self.make_request('post', '/api/dashboards', data={'name': dashboard_name})
        self.assertEquals(rv.status_code, 200)
        self.assertEquals(rv.json['name'], 'Test Dashboard')
        self.assertEquals(rv.json['user_id'], self.factory.user.id)
        self.assertEquals(rv.json['layout'], [])


class TestDashboardListGetResource(BaseTestCase):
    def test_returns_dashboards(self):
        d1 = self.factory.create_dashboard()
        d2 = self.factory.create_dashboard()
        d3 = self.factory.create_dashboard()

        rv = self.make_request('get', '/api/dashboards')

        assert len(rv.json['results']) == 3
        assert set(map(lambda d: d['id'], rv.json['results'])) == set([d1.id, d2.id, d3.id])

    def test_filters_with_tags(self):
        d1 = self.factory.create_dashboard(tags=[u'test'])
        d2 = self.factory.create_dashboard()
        d3 = self.factory.create_dashboard()

        rv = self.make_request('get', '/api/dashboards?tags=test')
        assert len(rv.json['results']) == 1
        assert set(map(lambda d: d['id'], rv.json['results'])) == set([d1.id])

    def test_search_term(self):
        d1 = self.factory.create_dashboard(name="Sales")
        d2 = self.factory.create_dashboard(name="Q1 sales")
        d3 = self.factory.create_dashboard(name="Ops")

        rv = self.make_request('get', '/api/dashboards?q=sales')
        assert len(rv.json['results']) == 2
        assert set(map(lambda d: d['id'], rv.json['results'])) == set([d1.id, d2.id])


class TestDashboardResourceGet(BaseTestCase):
    def test_get_dashboard(self):
        d1 = self.factory.create_dashboard()
        rv = self.make_request('get', '/api/dashboards/{0}'.format(d1.slug))
        self.assertEquals(rv.status_code, 200)

        expected = serialize_dashboard(d1, with_widgets=True, with_favorite_state=False)
        actual = json_loads(rv.data)

        self.assertResponseEqual(expected, actual)

    def test_get_dashboard_filters_unauthorized_widgets(self):
        dashboard = self.factory.create_dashboard()

        restricted_ds = self.factory.create_data_source(group=self.factory.create_group())
        query = self.factory.create_query(data_source=restricted_ds)
        vis = self.factory.create_visualization(query_rel=query)
        restricted_widget = self.factory.create_widget(visualization=vis, dashboard=dashboard)
        widget = self.factory.create_widget(dashboard=dashboard)
        dashboard.layout = '[[{}, {}]]'.format(widget.id, restricted_widget.id)
        db.session.commit()

        rv = self.make_request('get', '/api/dashboards/{0}'.format(dashboard.slug))
        self.assertEquals(rv.status_code, 200)
        self.assertTrue(rv.json['widgets'][0]['restricted'])
        self.assertNotIn('restricted', rv.json['widgets'][1])

    def test_get_non_existing_dashboard(self):
        rv = self.make_request('get', '/api/dashboards/not_existing')
        self.assertEquals(rv.status_code, 404)


class TestDashboardResourcePost(BaseTestCase):
    def test_update_dashboard(self):
        d = self.factory.create_dashboard()
        new_name = 'New Name'
        rv = self.make_request('post', '/api/dashboards/{0}'.format(d.id),
                               data={'name': new_name, 'layout': '[]'})
        self.assertEquals(rv.status_code, 200)
        self.assertEquals(rv.json['name'], new_name)

    def test_raises_error_in_case_of_conflict(self):
        d = self.factory.create_dashboard()
        d.name = 'Updated'
        db.session.commit()
        new_name = 'New Name'
        rv = self.make_request('post', '/api/dashboards/{0}'.format(d.id),
                               data={'name': new_name, 'layout': '[]', 'version': d.version - 1})

        self.assertEqual(rv.status_code, 409)

    def test_overrides_existing_if_no_version_specified(self):
        d = self.factory.create_dashboard()
        d.name = 'Updated'

        new_name = 'New Name'
        rv = self.make_request('post', '/api/dashboards/{0}'.format(d.id),
                               data={'name': new_name, 'layout': '[]'})

        self.assertEqual(rv.status_code, 200)

    def test_works_for_non_owner_with_permission(self):
        d = self.factory.create_dashboard()
        user = self.factory.create_user()

        new_name = 'New Name'
        rv = self.make_request('post', '/api/dashboards/{0}'.format(d.id),
                               data={'name': new_name, 'layout': '[]', 'version': d.version}, user=user)
        self.assertEqual(rv.status_code, 403)

        AccessPermission.grant(obj=d, access_type=ACCESS_TYPE_MODIFY, grantee=user, grantor=d.user)

        rv = self.make_request('post', '/api/dashboards/{0}'.format(d.id),
                               data={'name': new_name, 'layout': '[]', 'version': d.version}, user=user)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['name'], new_name)


class TestDashboardResourceDelete(BaseTestCase):
    def test_delete_dashboard(self):
        d = self.factory.create_dashboard()

        rv = self.make_request('delete', '/api/dashboards/{0}'.format(d.slug))
        self.assertEquals(rv.status_code, 200)

        d = Dashboard.get_by_slug_and_org(d.slug, d.org)
        self.assertTrue(d.is_archived)


class TestDashboardShareResourcePost(BaseTestCase):
    def test_creates_api_key(self):
        dashboard = self.factory.create_dashboard()

        res = self.make_request('post', '/api/dashboards/{}/share'.format(dashboard.id))
        self.assertEqual(res.status_code, 200)
        self.assertEqual(res.json['api_key'], ApiKey.get_by_object(dashboard).api_key)

    def test_requires_admin_or_owner(self):
        dashboard = self.factory.create_dashboard()
        user = self.factory.create_user()

        res = self.make_request('post', '/api/dashboards/{}/share'.format(dashboard.id), user=user)
        self.assertEqual(res.status_code, 403)

        user.group_ids.append(self.factory.org.admin_group.id)

        res = self.make_request('post', '/api/dashboards/{}/share'.format(dashboard.id), user=user)
        self.assertEqual(res.status_code, 200)


class TestDashboardShareResourceDelete(BaseTestCase):
    def test_disables_api_key(self):
        dashboard = self.factory.create_dashboard()
        ApiKey.create_for_object(dashboard, self.factory.user)

        res = self.make_request('delete', '/api/dashboards/{}/share'.format(dashboard.id))
        self.assertEqual(res.status_code, 200)
        self.assertIsNone(ApiKey.get_by_object(dashboard))

    def test_ignores_when_no_api_key_exists(self):
        dashboard = self.factory.create_dashboard()

        res = self.make_request('delete', '/api/dashboards/{}/share'.format(dashboard.id))
        self.assertEqual(res.status_code, 200)

    def test_requires_admin_or_owner(self):
        dashboard = self.factory.create_dashboard()
        user = self.factory.create_user()

        res = self.make_request('delete', '/api/dashboards/{}/share'.format(dashboard.id), user=user)
        self.assertEqual(res.status_code, 403)

        user.group_ids.append(self.factory.org.admin_group.id)

        res = self.make_request('delete', '/api/dashboards/{}/share'.format(dashboard.id), user=user)
        self.assertEqual(res.status_code, 200)
<EOF>
<BOF>
from tests import BaseTestCase
from redash import models
from redash.models import db

class TestQueryFavoriteResource(BaseTestCase):
    def test_favorite(self):
        query = self.factory.create_query()

        rv = self.make_request('post', '/api/queries/{}/favorite'.format(query.id))
        self.assertEqual(rv.status_code, 200)

        rv = self.make_request('get', '/api/queries/{}'.format(query.id))
        self.assertEqual(rv.json['is_favorite'], True)

    def test_duplicate_favorite(self):
        query = self.factory.create_query()

        rv = self.make_request('post', '/api/queries/{}/favorite'.format(query.id))
        self.assertEqual(rv.status_code, 200)

        rv = self.make_request('post', '/api/queries/{}/favorite'.format(query.id))
        self.assertEqual(rv.status_code, 200)

    def test_unfavorite(self):
        query = self.factory.create_query()
        rv = self.make_request('post', '/api/queries/{}/favorite'.format(query.id))
        rv = self.make_request('delete', '/api/queries/{}/favorite'.format(query.id))
        self.assertEqual(rv.status_code, 200)

        rv = self.make_request('get', '/api/queries/{}'.format(query.id))
        self.assertEqual(rv.json['is_favorite'], False)


class TestQueryFavoriteListResource(BaseTestCase):
    def test_get_favorites(self):
        rv = self.make_request('get', '/api/queries/favorites')
        self.assertEqual(rv.status_code, 200)
<EOF>
<BOF>
from funcy import pairwise
from tests import BaseTestCase

from redash.models import DataSource, Query


class TestDataSourceGetSchema(BaseTestCase):
    def test_fails_if_user_doesnt_belong_to_org(self):
        other_user = self.factory.create_user(org=self.factory.create_org())
        response = self.make_request("get", "/api/data_sources/{}/schema".format(self.factory.data_source.id), user=other_user)
        self.assertEqual(response.status_code, 404)

        other_admin = self.factory.create_admin(org=self.factory.create_org())
        response = self.make_request("get", "/api/data_sources/{}/schema".format(self.factory.data_source.id), user=other_admin)
        self.assertEqual(response.status_code, 404)


class TestDataSourceListGet(BaseTestCase):
    def test_returns_each_data_source_once(self):
        group = self.factory.create_group()
        self.factory.user.group_ids.append(group.id)
        self.factory.data_source.add_group(group)
        self.factory.data_source.add_group(self.factory.org.default_group)
        response = self.make_request("get", "/api/data_sources", user=self.factory.user)

        self.assertEqual(len(response.json), 1)

    def test_returns_data_sources_ordered_by_id(self):
        self.factory.create_data_source(group=self.factory.org.default_group)
        self.factory.create_data_source(group=self.factory.org.default_group)
        response = self.make_request("get", "/api/data_sources", user=self.factory.user)
        self.assertTrue(all(left <= right for left, right in pairwise(response.json)))


class DataSourceTypesTest(BaseTestCase):
    def test_returns_data_for_admin(self):
        admin = self.factory.create_admin()
        rv = self.make_request('get', "/api/data_sources/types", user=admin)
        self.assertEqual(rv.status_code, 200)

    def test_returns_403_for_non_admin(self):
        rv = self.make_request('get', "/api/data_sources/types")
        self.assertEqual(rv.status_code, 403)


class TestDataSourceResourcePost(BaseTestCase):
    def setUp(self):
        super(TestDataSourceResourcePost, self).setUp()
        self.path = "/api/data_sources/{}".format(self.factory.data_source.id)

    def test_returns_400_when_configuration_invalid(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', self.path,
                               data={'name': 'DS 1', 'type': 'pg', 'options': {}}, user=admin)

        self.assertEqual(rv.status_code, 400)

    def test_updates_data_source(self):
        admin = self.factory.create_admin()
        new_name = 'New Name'
        new_options = {"dbname": "newdb"}
        rv = self.make_request('post', self.path,
                               data={'name': new_name, 'type': 'pg', 'options': new_options},
                               user=admin)

        self.assertEqual(rv.status_code, 200)
        data_source = DataSource.query.get(self.factory.data_source.id)

        self.assertEqual(data_source.name, new_name)
        self.assertEqual(data_source.options.to_dict(), new_options)


class TestDataSourceResourceDelete(BaseTestCase):
    def test_deletes_the_data_source(self):
        data_source = self.factory.create_data_source()
        admin = self.factory.create_admin()

        rv = self.make_request('delete', '/api/data_sources/{}'.format(data_source.id), user=admin)

        self.assertEqual(204, rv.status_code)
        self.assertIsNone(DataSource.query.get(data_source.id))


class TestDataSourceListResourcePost(BaseTestCase):
    def test_returns_400_when_missing_fields(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', "/api/data_sources", user=admin)
        self.assertEqual(rv.status_code, 400)

        rv = self.make_request('post', "/api/data_sources", data={'name': 'DS 1'}, user=admin)

        self.assertEqual(rv.status_code, 400)

    def test_returns_400_when_configuration_invalid(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', '/api/data_sources',
                               data={'name': 'DS 1', 'type': 'pg', 'options': {}}, user=admin)

        self.assertEqual(rv.status_code, 400)

    def test_creates_data_source(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', '/api/data_sources',
                               data={'name': 'DS 1', 'type': 'pg', 'options': {"dbname": "redash"}}, user=admin)

        self.assertEqual(rv.status_code, 200)

        self.assertIsNotNone(DataSource.query.get(rv.json['id']))


class TestDataSourcePausePost(BaseTestCase):
    def test_pauses_data_source(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', '/api/data_sources/{}/pause'.format(self.factory.data_source.id), user=admin)
        self.assertEqual(rv.status_code, 200)
        self.assertEqual(DataSource.query.get(self.factory.data_source.id).paused, True)

    def test_pause_sets_reason(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', '/api/data_sources/{}/pause'.format(self.factory.data_source.id), user=admin, data={'reason': 'testing'})
        self.assertEqual(rv.status_code, 200)
        self.assertEqual(DataSource.query.get(self.factory.data_source.id).paused, True)
        self.assertEqual(DataSource.query.get(self.factory.data_source.id).pause_reason, 'testing')

        rv = self.make_request('post', '/api/data_sources/{}/pause?reason=test'.format(self.factory.data_source.id), user=admin)
        self.assertEqual(DataSource.query.get(self.factory.data_source.id).pause_reason, 'test')

    def test_requires_admin(self):
        rv = self.make_request('post', '/api/data_sources/{}/pause'.format(self.factory.data_source.id))
        self.assertEqual(rv.status_code, 403)


class TestDataSourcePauseDelete(BaseTestCase):
    def test_resumes_data_source(self):
        admin = self.factory.create_admin()
        self.factory.data_source.pause()
        rv = self.make_request('delete', '/api/data_sources/{}/pause'.format(self.factory.data_source.id), user=admin)
        self.assertEqual(rv.status_code, 200)
        self.assertEqual(DataSource.query.get(self.factory.data_source.id).paused, False)

    def test_requires_admin(self):
        rv = self.make_request('delete', '/api/data_sources/{}/pause'.format(self.factory.data_source.id))
        self.assertEqual(rv.status_code, 403)
<EOF>
<BOF>
from tests import BaseTestCase

from redash.models import NotificationDestination


class TestDestinationListResource(BaseTestCase):
    def test_get_returns_all_destinations(self):
        d1 = self.factory.create_destination()
        d2 = self.factory.create_destination()

        rv = self.make_request('get', '/api/destinations', user=self.factory.user)
        self.assertEqual(len(rv.json), 2)

    def test_get_returns_only_destinations_of_current_org(self):
        d1 = self.factory.create_destination()
        d2 = self.factory.create_destination()
        d3 = self.factory.create_destination(org=self.factory.create_org())

        rv = self.make_request('get', '/api/destinations', user=self.factory.user)
        self.assertEqual(len(rv.json), 2)

    def test_post_creates_new_destination(self):
        data = {
            'options': {'addresses': 'test@example.com'},
            'name': 'Test',
            'type': 'email'
        }
        rv = self.make_request('post', '/api/destinations', user=self.factory.create_admin(), data=data)
        self.assertEqual(rv.status_code, 200)
        pass

    def test_post_requires_admin(self):
        data = {
            'options': {'addresses': 'test@example.com'},
            'name': 'Test',
            'type': 'email'
        }
        rv = self.make_request('post', '/api/destinations', user=self.factory.user, data=data)
        self.assertEqual(rv.status_code, 403)


class TestDestinationResource(BaseTestCase):
    def test_get(self):
        d = self.factory.create_destination()
        rv = self.make_request('get', '/api/destinations/{}'.format(d.id), user=self.factory.create_admin())
        self.assertEqual(rv.status_code, 200)

    def test_delete(self):
        d = self.factory.create_destination()
        rv = self.make_request('delete', '/api/destinations/{}'.format(d.id), user=self.factory.create_admin())
        self.assertEqual(rv.status_code, 204)
        self.assertIsNone(NotificationDestination.query.get(d.id))

    def test_post(self):
        d = self.factory.create_destination()
        data = {
            'name': 'updated',
            'type': d.type,
            'options': {"url": "https://www.slack.com/updated"}
        }

        with self.app.app_context():
            rv = self.make_request('post', '/api/destinations/{}'.format(d.id), user=self.factory.create_admin(), data=data)

        self.assertEqual(rv.status_code, 200)

        d = NotificationDestination.query.get(d.id)
        self.assertEqual(d.name, data['name'])
        self.assertEqual(d.options['url'], data['options']['url'])
<EOF>
<BOF>
from werkzeug.exceptions import BadRequest

from redash.handlers.base import paginate
from unittest import TestCase
from mock import MagicMock

class DummyResults(object):
    items = [i for i in range(25)]

dummy_results = DummyResults()

class TestPaginate(TestCase):
    def setUp(self):
        self.query_set = MagicMock()
        self.query_set.count = MagicMock(return_value=102)
        self.query_set.paginate = MagicMock(return_value=dummy_results)

    def test_returns_paginated_results(self):
        page = paginate(self.query_set, 1, 25, lambda x: x)
        self.assertEqual(page['page'], 1)
        self.assertEqual(page['page_size'], 25)
        self.assertEqual(page['count'], 102)
        self.assertEqual(page['results'], dummy_results.items)

    def test_raises_error_for_bad_page(self):
        self.assertRaises(BadRequest, lambda: paginate(self.query_set, -1, 25, lambda x: x))
        self.assertRaises(BadRequest, lambda: paginate(self.query_set, 6, 25, lambda x: x))

    def test_raises_error_for_bad_page_size(self):
        self.assertRaises(BadRequest, lambda: paginate(self.query_set, 1, 251, lambda x: x))
        self.assertRaises(BadRequest, lambda: paginate(self.query_set, 1, -1, lambda x: x))

<EOF>
<BOF>
from tests import BaseTestCase

from redash.models import db
from redash.utils import json_dumps


class TestQueryResultsCacheHeaders(BaseTestCase):
    def test_uses_cache_headers_for_specific_result(self):
        query_result = self.factory.create_query_result()
        query = self.factory.create_query(latest_query_data=query_result)

        rv = self.make_request('get', '/api/queries/{}/results/{}.json'.format(query.id, query_result.id))
        self.assertIn('Cache-Control', rv.headers)

    def test_doesnt_use_cache_headers_for_non_specific_result(self):
        query_result = self.factory.create_query_result()
        query = self.factory.create_query(latest_query_data=query_result)

        rv = self.make_request('get', '/api/queries/{}/results.json'.format(query.id))
        self.assertNotIn('Cache-Control', rv.headers)

    def test_returns_404_if_no_cached_result_found(self):
        query = self.factory.create_query(latest_query_data=None)

        rv = self.make_request('get', '/api/queries/{}/results.json'.format(query.id))
        self.assertEqual(404, rv.status_code)


class TestQueryResultListAPI(BaseTestCase):
    def test_get_existing_result(self):
        query_result = self.factory.create_query_result()
        query = self.factory.create_query()

        rv = self.make_request('post', '/api/query_results',
                               data={'data_source_id': self.factory.data_source.id,
                                     'query': query.query_text})
        self.assertEquals(rv.status_code, 200)
        self.assertEquals(query_result.id, rv.json['query_result']['id'])

    def test_execute_new_query(self):
        query_result = self.factory.create_query_result()
        query = self.factory.create_query()

        rv = self.make_request('post', '/api/query_results',
                               data={'data_source_id': self.factory.data_source.id,
                                     'query': query.query_text,
                                     'max_age': 0})

        self.assertEquals(rv.status_code, 200)
        self.assertNotIn('query_result', rv.json)
        self.assertIn('job', rv.json)

    def test_execute_query_without_access(self):
        group = self.factory.create_group()
        db.session.commit()
        user = self.factory.create_user(group_ids=[group.id])
        query = self.factory.create_query()

        rv = self.make_request('post', '/api/query_results',
                               data={'data_source_id': self.factory.data_source.id,
                                     'query': query.query_text,
                                     'max_age': 0},
                               user=user)

        self.assertEquals(rv.status_code, 403)
        self.assertIn('job', rv.json)

    def test_execute_query_with_params(self):
        query = "SELECT {{param}}"

        rv = self.make_request('post', '/api/query_results',
                               data={'data_source_id': self.factory.data_source.id,
                                     'query': query,
                                     'max_age': 0})

        self.assertEquals(rv.status_code, 400)
        self.assertIn('job', rv.json)

        rv = self.make_request('post', '/api/query_results?p_param=1',
                               data={'data_source_id': self.factory.data_source.id,
                                     'query': query,
                                     'max_age': 0})

        self.assertEquals(rv.status_code, 200)
        self.assertIn('job', rv.json)

    def test_execute_on_paused_data_source(self):
        self.factory.data_source.pause()

        rv = self.make_request('post', '/api/query_results',
                               data={'data_source_id': self.factory.data_source.id,
                                     'query': 'SELECT 1',
                                     'max_age': 0})

        self.assertEquals(rv.status_code, 400)
        self.assertNotIn('query_result', rv.json)
        self.assertIn('job', rv.json)


class TestQueryResultAPI(BaseTestCase):
    def test_has_no_access_to_data_source(self):
        ds = self.factory.create_data_source(group=self.factory.create_group())
        query_result = self.factory.create_query_result(data_source=ds)

        rv = self.make_request('get', '/api/query_results/{}'.format(query_result.id))
        self.assertEquals(rv.status_code, 403)

    def test_has_view_only_access_to_data_source(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=True)
        query_result = self.factory.create_query_result(data_source=ds)

        rv = self.make_request('get', '/api/query_results/{}'.format(query_result.id))
        self.assertEquals(rv.status_code, 200)

    def test_has_full_access_to_data_source(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=False)
        query_result = self.factory.create_query_result(data_source=ds)

        rv = self.make_request('get', '/api/query_results/{}'.format(query_result.id))
        self.assertEquals(rv.status_code, 200)

    def test_access_with_query_api_key(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=False)
        query = self.factory.create_query()
        query_result = self.factory.create_query_result(data_source=ds, query_text=query.query_text)

        rv = self.make_request('get', '/api/queries/{}/results/{}.json?api_key={}'.format(query.id, query_result.id, query.api_key), user=False)
        self.assertEquals(rv.status_code, 200)

    def test_access_with_query_api_key_without_query_result_id(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=False)
        query = self.factory.create_query()
        query_result = self.factory.create_query_result(data_source=ds, query_text=query.query_text, query_hash=query.query_hash)
        query.latest_query_data = query_result

        rv = self.make_request('get', '/api/queries/{}/results.json?api_key={}'.format(query.id, query.api_key), user=False)
        self.assertEquals(rv.status_code, 200)

    def test_query_api_key_and_different_query_result(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=False)
        query = self.factory.create_query(query_text="SELECT 8")
        query_result2 = self.factory.create_query_result(data_source=ds, query_hash='something-different')

        rv = self.make_request('get', '/api/queries/{}/results/{}.json?api_key={}'.format(query.id, query_result2.id, query.api_key), user=False)
        self.assertEquals(rv.status_code, 404)

    def test_signed_in_user_and_different_query_result(self):
        ds2 = self.factory.create_data_source(group=self.factory.org.admin_group, view_only=False)
        query = self.factory.create_query(query_text="SELECT 8")
        query_result2 = self.factory.create_query_result(data_source=ds2, query_hash='something-different')

        rv = self.make_request('get', '/api/queries/{}/results/{}.json'.format(query.id, query_result2.id))
        self.assertEquals(rv.status_code, 403)


class TestQueryResultExcelResponse(BaseTestCase):
    def test_renders_excel_file(self):
        query = self.factory.create_query()
        query_result = self.factory.create_query_result()

        rv = self.make_request('get', '/api/queries/{}/results/{}.xlsx'.format(query.id, query_result.id), is_json=False)
        self.assertEquals(rv.status_code, 200)

    def test_renders_excel_file_when_rows_have_missing_columns(self):
        query = self.factory.create_query()
        data = {
            'rows': [
                {'test': 1},
                {'test': 2, 'test2': 3},
            ],
            'columns': [
                {'name': 'test'},
                {'name': 'test2'},
            ],
        }
        query_result = self.factory.create_query_result(data=json_dumps(data))

        rv = self.make_request('get', '/api/queries/{}/results/{}.xlsx'.format(query.id, query_result.id), is_json=False)
        self.assertEquals(rv.status_code, 200)

<EOF>
<BOF>
from tests import BaseTestCase

from redash.models import AccessPermission
from redash.permissions import ACCESS_TYPE_MODIFY


class TestObjectPermissionsListGet(BaseTestCase):
    def test_returns_empty_list_when_no_permissions(self):
        query = self.factory.create_query()
        user = self.factory.user
        rv = self.make_request('get', '/api/queries/{}/acl'.format(query.id), user=user)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual({}, rv.json)

    def test_returns_permissions(self):
        query = self.factory.create_query()
        user = self.factory.user

        AccessPermission.grant(obj=query, access_type=ACCESS_TYPE_MODIFY,
                               grantor=self.factory.user, grantee=self.factory.user)

        rv = self.make_request('get', '/api/queries/{}/acl'.format(query.id), user=user)

        self.assertEqual(rv.status_code, 200)
        self.assertIn('modify', rv.json)
        self.assertEqual(user.id, rv.json['modify'][0]['id'])

    def test_returns_404_for_outside_of_organization_users(self):
        query = self.factory.create_query()
        user = self.factory.create_user(org=self.factory.create_org())
        rv = self.make_request('get', '/api/queries/{}/acl'.format(query.id), user=user)

        self.assertEqual(rv.status_code, 404)


class TestObjectPermissionsListPost(BaseTestCase):
    def test_creates_permission_if_the_user_is_an_owner(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': other_user.id
        }

        rv = self.make_request('post', '/api/queries/{}/acl'.format(query.id), user=query.user, data=data)

        self.assertEqual(200, rv.status_code)
        self.assertTrue(AccessPermission.exists(query, ACCESS_TYPE_MODIFY, other_user))

    def test_returns_403_if_the_user_isnt_owner(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': other_user.id
        }

        rv = self.make_request('post', '/api/queries/{}/acl'.format(query.id), user=other_user, data=data)
        self.assertEqual(403, rv.status_code)

    def test_returns_400_if_the_grantee_isnt_from_organization(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user(org=self.factory.create_org())

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': other_user.id
        }

        rv = self.make_request('post', '/api/queries/{}/acl'.format(query.id), user=query.user, data=data)
        self.assertEqual(400, rv.status_code)

    def test_returns_404_if_the_user_from_different_org(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user(org=self.factory.create_org())

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': other_user.id
        }

        rv = self.make_request('post', '/api/queries/{}/acl'.format(query.id), user=other_user, data=data)
        self.assertEqual(404, rv.status_code)

    def test_accepts_only_correct_access_types(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()

        data = {
            'access_type': 'random string',
            'user_id': other_user.id
        }

        rv = self.make_request('post', '/api/queries/{}/acl'.format(query.id), user=query.user, data=data)

        self.assertEqual(400, rv.status_code)


class TestObjectPermissionsListDelete(BaseTestCase):
    def test_removes_permission(self):
        query = self.factory.create_query()
        user = self.factory.user
        other_user = self.factory.create_user()

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': other_user.id
        }

        AccessPermission.grant(obj=query, access_type=ACCESS_TYPE_MODIFY, grantor=self.factory.user, grantee=other_user)

        rv = self.make_request('delete', '/api/queries/{}/acl'.format(query.id), user=user, data=data)

        self.assertEqual(rv.status_code, 200)

        self.assertFalse(AccessPermission.exists(query, ACCESS_TYPE_MODIFY, other_user))

    def test_removes_permission_created_by_another_user(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': other_user.id
        }

        AccessPermission.grant(obj=query, access_type=ACCESS_TYPE_MODIFY, grantor=self.factory.user, grantee=other_user)

        rv = self.make_request('delete', '/api/queries/{}/acl'.format(query.id), user=self.factory.create_admin(),
                               data=data)

        self.assertEqual(rv.status_code, 200)

        self.assertFalse(AccessPermission.exists(query, ACCESS_TYPE_MODIFY, other_user))

    def test_returns_404_for_outside_of_organization_users(self):
        query = self.factory.create_query()
        user = self.factory.create_user(org=self.factory.create_org())
        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': user.id
        }
        rv = self.make_request('delete', '/api/queries/{}/acl'.format(query.id), user=user, data=data)

        self.assertEqual(rv.status_code, 404)

    def test_returns_403_for_non_owner(self):
        query = self.factory.create_query()
        user = self.factory.create_user()

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': user.id
        }
        rv = self.make_request('delete', '/api/queries/{}/acl'.format(query.id), user=user, data=data)

        self.assertEqual(rv.status_code, 403)

    def test_returns_200_even_if_there_is_no_permission(self):
        query = self.factory.create_query()
        user = self.factory.create_user()

        data = {
            'access_type': ACCESS_TYPE_MODIFY,
            'user_id': user.id
        }

        rv = self.make_request('delete', '/api/queries/{}/acl'.format(query.id), user=query.user, data=data)

        self.assertEqual(rv.status_code, 200)


class TestCheckPermissionsGet(BaseTestCase):
    def test_returns_true_for_existing_permission(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()

        AccessPermission.grant(obj=query, access_type=ACCESS_TYPE_MODIFY, grantor=self.factory.user, grantee=other_user)

        rv = self.make_request('get', '/api/queries/{}/acl/{}'.format(query.id, ACCESS_TYPE_MODIFY), user=other_user)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual(True, rv.json['response'])

    def test_returns_false_for_existing_permission(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()

        rv = self.make_request('get', '/api/queries/{}/acl/{}'.format(query.id, ACCESS_TYPE_MODIFY), user=other_user)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual(False, rv.json['response'])

    def test_returns_404_for_outside_of_org_users(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user(org=self.factory.create_org())

        rv = self.make_request('get', '/api/queries/{}/acl/{}'.format(query.id, ACCESS_TYPE_MODIFY), user=other_user)

        self.assertEqual(rv.status_code, 404)
<EOF>
<BOF>
from tests import BaseTestCase
from redash import models


class WidgetAPITest(BaseTestCase):
    def create_widget(self, dashboard, visualization, width=1):
        data = {
            'visualization_id': visualization.id,
            'dashboard_id': dashboard.id,
            'options': {},
            'width': width
        }

        rv = self.make_request('post', '/api/widgets', data=data)

        return rv

    def test_create_widget(self):
        dashboard = self.factory.create_dashboard()
        vis = self.factory.create_visualization()

        rv = self.create_widget(dashboard, vis)
        self.assertEquals(rv.status_code, 200)

    def test_wont_create_widget_for_visualization_you_dont_have_access_to(self):
        dashboard = self.factory.create_dashboard()
        vis = self.factory.create_visualization()
        ds = self.factory.create_data_source(group=self.factory.create_group())
        vis.query_rel.data_source = ds

        models.db.session.add(vis.query_rel)

        data = {
            'visualization_id': vis.id,
            'dashboard_id': dashboard.id,
            'options': {},
            'width': 1
        }

        rv = self.make_request('post', '/api/widgets', data=data)
        self.assertEqual(rv.status_code, 403)

    def test_create_text_widget(self):
        dashboard = self.factory.create_dashboard()

        data = {
            'visualization_id': None,
            'text': 'Sample text.',
            'dashboard_id': dashboard.id,
            'options': {},
            'width': 2
        }

        rv = self.make_request('post', '/api/widgets', data=data)

        self.assertEquals(rv.status_code, 200)
        self.assertEquals(rv.json['text'], 'Sample text.')

    def test_delete_widget(self):
        widget = self.factory.create_widget()

        rv = self.make_request('delete', '/api/widgets/{0}'.format(widget.id))

        self.assertEquals(rv.status_code, 200)
        dashboard = models.Dashboard.get_by_slug_and_org(widget.dashboard.slug, widget.dashboard.org)
        self.assertEquals(dashboard.widgets.count(), 0)
<EOF>
<BOF>
from tests import BaseTestCase

from redash import models


class VisualizationResourceTest(BaseTestCase):
    def test_create_visualization(self):
        query = self.factory.create_query()
        models.db.session.commit()
        data = {
            'query_id': query.id,
            'name': 'Chart',
            'description': '',
            'options': {},
            'type': 'CHART'
        }

        rv = self.make_request('post', '/api/visualizations', data=data)

        self.assertEquals(rv.status_code, 200)
        data.pop('query_id')
        self.assertDictContainsSubset(data, rv.json)

    def test_delete_visualization(self):
        visualization = self.factory.create_visualization()
        models.db.session.commit()
        rv = self.make_request('delete', '/api/visualizations/{}'.format(visualization.id))

        self.assertEquals(rv.status_code, 200)
        self.assertEquals(models.db.session.query(models.Visualization).count(), 0)

    def test_update_visualization(self):
        visualization = self.factory.create_visualization()
        models.db.session.commit()
        rv = self.make_request('post', '/api/visualizations/{0}'.format(visualization.id), data={'name': 'After Update'})

        self.assertEquals(rv.status_code, 200)
        self.assertEquals(rv.json['name'], 'After Update')

    def test_only_owner_collaborator_or_admin_can_create_visualization(self):
        query = self.factory.create_query()
        other_user = self.factory.create_user()
        admin = self.factory.create_admin()
        admin_from_diff_org = self.factory.create_admin(org=self.factory.create_org())
        models.db.session.commit()
        models.db.session.refresh(admin)
        models.db.session.refresh(other_user)
        models.db.session.refresh(admin_from_diff_org)
        data = {
            'query_id': query.id,
            'name': 'Chart',
            'description': '',
            'options': {},
            'type': 'CHART'
        }

        rv = self.make_request('post', '/api/visualizations', data=data, user=admin)
        self.assertEquals(rv.status_code, 200)

        rv = self.make_request('post', '/api/visualizations', data=data, user=other_user)
        self.assertEquals(rv.status_code, 403)

        self.make_request('post', '/api/queries/{}/acl'.format(query.id), data={'access_type': 'modify', 'user_id': other_user.id})
        rv = self.make_request('post', '/api/visualizations', data=data, user=other_user)
        self.assertEquals(rv.status_code, 200)

        rv = self.make_request('post', '/api/visualizations', data=data, user=admin_from_diff_org)
        self.assertEquals(rv.status_code, 404)

    def test_only_owner_collaborator_or_admin_can_edit_visualization(self):
        vis = self.factory.create_visualization()
        models.db.session.flush()
        path = '/api/visualizations/{}'.format(vis.id)
        data = {'name': 'After Update'}

        other_user = self.factory.create_user()
        admin = self.factory.create_admin()
        admin_from_diff_org = self.factory.create_admin(org=self.factory.create_org())
        models.db.session.commit()
        models.db.session.refresh(admin)
        models.db.session.refresh(other_user)
        models.db.session.refresh(admin_from_diff_org)

        rv = self.make_request('post', path, user=admin, data=data)
        self.assertEquals(rv.status_code, 200)

        rv = self.make_request('post', path, user=other_user, data=data)
        self.assertEquals(rv.status_code, 403)

        self.make_request('post', '/api/queries/{}/acl'.format(vis.query_id), data={'access_type': 'modify', 'user_id': other_user.id})
        rv = self.make_request('post', path, user=other_user, data=data)
        self.assertEquals(rv.status_code, 200)

        rv = self.make_request('post', path, user=admin_from_diff_org, data=data)
        self.assertEquals(rv.status_code, 404)

    def test_only_owner_collaborator_or_admin_can_delete_visualization(self):
        vis = self.factory.create_visualization()
        models.db.session.flush()
        path = '/api/visualizations/{}'.format(vis.id)

        other_user = self.factory.create_user()
        admin = self.factory.create_admin()
        admin_from_diff_org = self.factory.create_admin(org=self.factory.create_org())

        models.db.session.commit()
        models.db.session.refresh(admin)
        models.db.session.refresh(other_user)
        models.db.session.refresh(admin_from_diff_org)
        rv = self.make_request('delete', path, user=admin)
        self.assertEquals(rv.status_code, 200)

        vis = self.factory.create_visualization()
        models.db.session.commit()
        path = '/api/visualizations/{}'.format(vis.id)

        rv = self.make_request('delete', path, user=other_user)
        self.assertEquals(rv.status_code, 403)

        self.make_request('post', '/api/queries/{}/acl'.format(vis.query_id), data={'access_type': 'modify', 'user_id': other_user.id})

        rv = self.make_request('delete', path, user=other_user)
        self.assertEquals(rv.status_code, 200)

        vis = self.factory.create_visualization()
        models.db.session.commit()
        path = '/api/visualizations/{}'.format(vis.id)

        rv = self.make_request('delete', path, user=admin_from_diff_org)
        self.assertEquals(rv.status_code, 404)
<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import db


class TestEmbedVisualization(BaseTestCase):
    def test_sucesss(self):
        vis = self.factory.create_visualization()
        vis.query_rel.latest_query_data = self.factory.create_query_result()
        db.session.add(vis.query_rel)

        res = self.make_request("get", "/embed/query/{}/visualization/{}".format(vis.query_rel.id, vis.id), is_json=False)
        self.assertEqual(res.status_code, 200)

    # TODO: bring back?
    # def test_parameters_on_embeds(self):
    #     previous = settings.ALLOW_PARAMETERS_IN_EMBEDS
    #     # set configuration
    #     settings.ALLOW_PARAMETERS_IN_EMBEDS = True
    #
    #     try:
    #         vis = self.factory.create_visualization_with_params()
    #         param1_name = "param1"
    #         param1_value = "12345"
    #
    #         res = self.make_request("get", "/embed/query/{}/visualization/{}?p_{}={}".format(vis.query.id, vis.id, param1_name, param1_value), is_json=False)
    #
    #         # Currently we are expecting a 503 error which indicates that
    #         # the database is unavailable. This ensures that the code in embed.py
    #         # reaches the point where a DB query is made, where we then fail
    #         # intentionally (because DB connection is not available in the tests).
    #         self.assertEqual(res.status_code, 503)
    #
    #         # run embed query with maxAge to test caching
    #         res = self.make_request("get", "/embed/query/{}/visualization/{}?p_{}={}&maxAge=60".format(vis.query.id, vis.id, param1_name, param1_value), is_json=False)
    #         # If the 'maxAge' parameter is set and the query fails (because DB connection
    #         # is not available in the tests), we're expecting a 404 error here.
    #         self.assertEqual(res.status_code, 404)
    #
    #     finally:
    #         # reset configuration
    #         settings.ALLOW_PARAMETERS_IN_EMBEDS = previous


# TODO: this should be applied to the new API endpoint
class TestPublicDashboard(BaseTestCase):
    def test_success(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard)

        res = self.make_request('get', '/public/dashboards/{}'.format(api_key.api_key), user=False, is_json=False)
        self.assertEqual(res.status_code, 200)

    def test_works_for_logged_in_user(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard)

        res = self.make_request('get', '/public/dashboards/{}'.format(api_key.api_key), is_json=False)
        self.assertEqual(res.status_code, 200)

    def test_bad_token(self):
        res = self.make_request('get', '/public/dashboards/bad-token', user=False, is_json=False)
        self.assertEqual(res.status_code, 302)

    def test_inactive_token(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard, active=False)
        res = self.make_request('get', '/public/dashboards/{}'.format(api_key.api_key), user=False, is_json=False)
        self.assertEqual(res.status_code, 302)

    # Not relevant for now, as tokens in api_keys table are only created for dashboards. Once this changes, we should
    # add this test.
    # def test_token_doesnt_belong_to_dashboard(self):
    #     pass

class TestAPIPublicDashboard(BaseTestCase):
    def test_success(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard)

        res = self.make_request('get', '/api/dashboards/public/{}'.format(api_key.api_key), user=False, is_json=False)
        self.assertEqual(res.status_code, 200)

    def test_works_for_logged_in_user(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard)

        res = self.make_request('get', '/api/dashboards/public/{}'.format(api_key.api_key), is_json=False)
        self.assertEqual(res.status_code, 200)

    def test_bad_token(self):
        res = self.make_request('get', '/api/dashboards/public/bad-token', user=False, is_json=False)
        self.assertEqual(res.status_code, 404)

    def test_inactive_token(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard, active=False)
        res = self.make_request('get', '/api/dashboards/public/{}'.format(api_key.api_key), user=False, is_json=False)
        self.assertEqual(res.status_code, 404)

    # Not relevant for now, as tokens in api_keys table are only created for dashboards. Once this changes, we should
    # add this test.
    # def test_token_doesnt_belong_to_dashboard(self):
    #     pass
<EOF>
<BOF>
from tests import BaseTestCase

from redash.models import Alert, AlertSubscription, db


class TestAlertResourceGet(BaseTestCase):
    def test_returns_200_if_allowed(self):
        alert = self.factory.create_alert()

        rv = self.make_request('get', "/api/alerts/{}".format(alert.id))
        self.assertEqual(rv.status_code, 200)

    def test_returns_403_if_not_allowed(self):
        data_source = self.factory.create_data_source(group=self.factory.create_group())
        query = self.factory.create_query(data_source=data_source)
        alert = self.factory.create_alert(query_rel=query)
        db.session.commit()
        rv = self.make_request('get', "/api/alerts/{}".format(alert.id))
        self.assertEqual(rv.status_code, 403)

    def test_returns_404_if_admin_from_another_org(self):
        second_org = self.factory.create_org()
        second_org_admin = self.factory.create_admin(org=second_org)

        alert = self.factory.create_alert()

        rv = self.make_request('get', "/api/alerts/{}".format(alert.id), org=second_org, user=second_org_admin)
        self.assertEqual(rv.status_code, 404)


class TestAlertResourcePost(BaseTestCase):
    def test_updates_alert(self):
        alert = self.factory.create_alert()
        rv = self.make_request('post', '/api/alerts/{}'.format(alert.id), data={"name": "Testing"})


class TestAlertResourceDelete(BaseTestCase):
    def test_removes_alert_and_subscriptions(self):
        subscription = self.factory.create_alert_subscription()
        alert = subscription.alert
        db.session.commit()
        rv = self.make_request('delete', "/api/alerts/{}".format(alert.id))
        self.assertEqual(rv.status_code, 200)

        self.assertEqual(Alert.query.get(subscription.alert.id), None)
        self.assertEqual(AlertSubscription.query.get(subscription.id), None)

    def test_returns_403_if_not_allowed(self):
        alert = self.factory.create_alert()

        user = self.factory.create_user()
        rv = self.make_request('delete', "/api/alerts/{}".format(alert.id), user=user)
        self.assertEqual(rv.status_code, 403)

        rv = self.make_request('delete', "/api/alerts/{}".format(alert.id), user=self.factory.create_admin())
        self.assertEqual(rv.status_code, 200)

    def test_returns_404_for_unauthorized_users(self):
        alert = self.factory.create_alert()

        second_org = self.factory.create_org()
        second_org_admin = self.factory.create_admin(org=second_org)
        rv = self.make_request('delete', "/api/alerts/{}".format(alert.id), user=second_org_admin)
        self.assertEqual(rv.status_code, 404)


class TestAlertListGet(BaseTestCase):
    def test_returns_all_alerts(self):
        alert = self.factory.create_alert()
        rv = self.make_request('get', "/api/alerts")

        self.assertEqual(rv.status_code, 200)

        alert_ids = [a['id'] for a in rv.json]
        self.assertIn(alert.id, alert_ids)

    def test_returns_alerts_only_from_users_groups(self):
        alert = self.factory.create_alert()
        query = self.factory.create_query(data_source=self.factory.create_data_source(group=self.factory.create_group()))
        alert2 = self.factory.create_alert(query_rel=query)
        rv = self.make_request('get', "/api/alerts")

        self.assertEqual(rv.status_code, 200)

        alert_ids = [a['id'] for a in rv.json]
        self.assertIn(alert.id, alert_ids)
        self.assertNotIn(alert2.id, alert_ids)


class TestAlertListPost(BaseTestCase):
    def test_returns_200_if_has_access_to_query(self):
        query = self.factory.create_query()
        destination = self.factory.create_destination()
        db.session.commit()
        rv = self.make_request('post', "/api/alerts", data=dict(name='Alert', query_id=query.id,
                                                                destination_id=destination.id, options={},
                                                                rearm=100))
        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['rearm'], 100)

    def test_fails_if_doesnt_have_access_to_query(self):
        data_source = self.factory.create_data_source(group=self.factory.create_group())
        query = self.factory.create_query(data_source=data_source)
        destination = self.factory.create_destination()
        db.session.commit()
        rv = self.make_request('post', "/api/alerts", data=dict(name='Alert', query_id=query.id,
                                                                destination_id=destination.id, options={}))
        self.assertEqual(rv.status_code, 403)


class TestAlertSubscriptionListResourcePost(BaseTestCase):
    def test_subscribers_user_to_alert(self):
        alert = self.factory.create_alert()
        destination = self.factory.create_destination()

        rv = self.make_request('post', "/api/alerts/{}/subscriptions".format(alert.id), data=dict(destination_id=destination.id))
        self.assertEqual(rv.status_code, 200)
        self.assertIn(self.factory.user, alert.subscribers())

    def test_doesnt_subscribers_user_to_alert_without_access(self):
        data_source = self.factory.create_data_source(group=self.factory.create_group())
        query = self.factory.create_query(data_source=data_source)
        alert = self.factory.create_alert(query_rel=query)
        destination = self.factory.create_destination()

        rv = self.make_request('post', "/api/alerts/{}/subscriptions".format(alert.id), data=dict(destination_id=destination.id))
        self.assertEqual(rv.status_code, 403)
        self.assertNotIn(self.factory.user, alert.subscribers())


class TestAlertSubscriptionListResourceGet(BaseTestCase):
    def test_returns_subscribers(self):
        alert = self.factory.create_alert()

        rv = self.make_request('get', "/api/alerts/{}/subscriptions".format(alert.id))
        self.assertEqual(rv.status_code, 200)

    def test_doesnt_return_subscribers_when_not_allowed(self):
        data_source = self.factory.create_data_source(group=self.factory.create_group())
        query = self.factory.create_query(data_source=data_source)
        alert = self.factory.create_alert(query_rel=query)

        rv = self.make_request('get', "/api/alerts/{}/subscriptions".format(alert.id))
        self.assertEqual(rv.status_code, 403)


class TestAlertSubscriptionresourceDelete(BaseTestCase):
    def test_only_subscriber_or_admin_can_unsubscribe(self):
        subscription = self.factory.create_alert_subscription()
        alert = subscription.alert
        user = subscription.user
        path = '/api/alerts/{}/subscriptions/{}'.format(alert.id,
                                                        subscription.id)

        other_user = self.factory.create_user()

        response = self.make_request('delete', path, user=other_user)
        self.assertEqual(response.status_code, 403)

        response = self.make_request('delete', path, user=user)
        self.assertEqual(response.status_code, 200)

        subscription_two = AlertSubscription(alert=alert, user=other_user)
        admin_user = self.factory.create_admin()
        db.session.add_all([subscription_two, admin_user])
        db.session.commit()
        path = '/api/alerts/{}/subscriptions/{}'.format(alert.id,
                                                        subscription_two.id)
        response = self.make_request('delete', path, user=admin_user)
        self.assertEqual(response.status_code, 200)
<EOF>
<BOF>
from funcy import project

from tests import BaseTestCase
from redash.models import Group, DataSource, NoResultFound, db


class TestGroupDataSourceListResource(BaseTestCase):
    def test_returns_only_groups_for_current_org(self):
        group = self.factory.create_group(org=self.factory.create_org())
        data_source = self.factory.create_data_source(group=group)
        db.session.flush()
        response = self.make_request('get', '/api/groups/{}/data_sources'.format(group.id), user=self.factory.create_admin())
        self.assertEqual(response.status_code, 404)

    def test_list(self):
        group = self.factory.create_group()
        ds = self.factory.create_data_source(group=group)
        db.session.flush()
        response = self.make_request(
            'get', '/api/groups/{}/data_sources'.format(group.id),
            user=self.factory.create_admin())
        self.assertEqual(response.status_code, 200)
        self.assertEqual(len(response.json), 1)
        self.assertEqual(response.json[0]['id'], ds.id)


class TestGroupResourceList(BaseTestCase):

    def test_list_admin(self):
        self.factory.create_group(org=self.factory.create_org())
        response = self.make_request('get', '/api/groups',
                                     user=self.factory.create_admin())
        g_keys = ['type', 'id', 'name', 'permissions']

        def filtergroups(gs):
            return [project(g, g_keys) for g in gs]
        self.assertEqual(filtergroups(response.json),
                         filtergroups(g.to_dict() for g in [
                             self.factory.admin_group,
                             self.factory.default_group]))

    def test_list(self):
        group1 = self.factory.create_group(org=self.factory.create_org(),
                                           permissions=['view_dashboard'])
        db.session.flush()
        u = self.factory.create_user(group_ids=[self.factory.default_group.id,
                                                group1.id])
        db.session.flush()
        response = self.make_request('get', '/api/groups',
                                     user=u)
        g_keys = ['type', 'id', 'name', 'permissions']

        def filtergroups(gs):
            return [project(g, g_keys) for g in gs]
        self.assertEqual(filtergroups(response.json),
                         filtergroups(g.to_dict() for g in [
                             self.factory.default_group,
                             group1]))


class TestGroupResourcePost(BaseTestCase):
    def test_doesnt_change_builtin_groups(self):
        current_name = self.factory.default_group.name

        response = self.make_request('post', '/api/groups/{}'.format(self.factory.default_group.id),
                                     user=self.factory.create_admin(),
                                     data={'name': 'Another Name'})

        self.assertEqual(response.status_code, 400)
        self.assertEqual(current_name, Group.query.get(self.factory.default_group.id).name)


class TestGroupResourceDelete(BaseTestCase):
    def test_allowed_only_to_admin(self):
        group = self.factory.create_group()

        response = self.make_request('delete', '/api/groups/{}'.format(group.id))
        self.assertEqual(response.status_code, 403)

        response = self.make_request('delete', '/api/groups/{}'.format(group.id), user=self.factory.create_admin())
        self.assertEqual(response.status_code, 200)
        self.assertIsNone(Group.query.get(group.id))

    def test_cant_delete_builtin_group(self):
        for group in [self.factory.default_group, self.factory.admin_group]:
            response = self.make_request('delete', '/api/groups/{}'.format(group.id), user=self.factory.create_admin())
            self.assertEqual(response.status_code, 400)

    def test_can_delete_group_with_data_sources(self):
        group = self.factory.create_group()
        data_source = self.factory.create_data_source(group=group)

        response = self.make_request('delete', '/api/groups/{}'.format(group.id), user=self.factory.create_admin())

        self.assertEqual(response.status_code, 200)

        self.assertEqual(data_source, DataSource.query.get(data_source.id))


class TestGroupResourceGet(BaseTestCase):
    def test_returns_group(self):
        rv = self.make_request('get', '/api/groups/{}'.format(self.factory.default_group.id))
        self.assertEqual(rv.status_code, 200)

    def test_doesnt_return_if_user_not_member_or_admin(self):
        rv = self.make_request('get', '/api/groups/{}'.format(self.factory.admin_group.id))
        self.assertEqual(rv.status_code, 403)


<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import Organization


class TestOrganizationSettings(BaseTestCase):
    def test_post(self):
        admin = self.factory.create_admin()
        rv = self.make_request('post', '/api/settings/organization', data={'auth_password_login_enabled': False}, user=admin)
        self.assertEqual(rv.json['settings']['auth_password_login_enabled'], False)
        self.assertEqual(self.factory.org.settings['settings']['auth_password_login_enabled'], False)

        rv = self.make_request('post', '/api/settings/organization', data={'auth_password_login_enabled': True}, user=admin)
        updated_org = Organization.get_by_slug(self.factory.org.slug)
        self.assertEqual(rv.json['settings']['auth_password_login_enabled'], True)
        self.assertEqual(updated_org.settings['settings']['auth_password_login_enabled'], True)
    
    def test_updates_google_apps_domains(self):
        admin = self.factory.create_admin()
        domains = ['example.com']
        rv = self.make_request('post', '/api/settings/organization', data={'auth_google_apps_domains': domains}, user=admin)
        updated_org = Organization.get_by_slug(self.factory.org.slug)
        self.assertEqual(updated_org.google_apps_domains, domains)
    
    def test_get_returns_google_appas_domains(self):
        admin = self.factory.create_admin()
        domains = ['example.com']
        admin.org.settings[Organization.SETTING_GOOGLE_APPS_DOMAINS] = domains

        rv = self.make_request('get', '/api/settings/organization', user=admin)
        self.assertEqual(rv.json['settings']['auth_google_apps_domains'], domains)

<EOF>
<BOF>
from tests import BaseTestCase
from redash import models
from redash.models import db

from redash.serializers import serialize_query
from redash.permissions import ACCESS_TYPE_MODIFY


class TestQueryResourceGet(BaseTestCase):
    def test_get_query(self):
        query = self.factory.create_query()

        rv = self.make_request('get', '/api/queries/{0}'.format(query.id))

        self.assertEquals(rv.status_code, 200)
        expected = serialize_query(query, with_visualizations=True)
        expected['can_edit'] = True
        expected['is_favorite'] = False
        self.assertResponseEqual(expected, rv.json)

    def test_get_all_queries(self):
        queries = [self.factory.create_query() for _ in range(10)]

        rv = self.make_request('get', '/api/queries')

        self.assertEquals(rv.status_code, 200)
        self.assertEquals(len(rv.json['results']), 10)

    def test_query_without_data_source_should_be_available_only_by_admin(self):
        query = self.factory.create_query()
        query.data_source = None
        db.session.add(query)

        rv = self.make_request('get', '/api/queries/{}'.format(query.id))
        self.assertEquals(rv.status_code, 403)

        rv = self.make_request('get', '/api/queries/{}'.format(query.id), user=self.factory.create_admin())
        self.assertEquals(rv.status_code, 200)

    def test_query_only_accessible_to_users_from_its_organization(self):
        second_org = self.factory.create_org()
        second_org_admin = self.factory.create_admin(org=second_org)

        query = self.factory.create_query()
        query.data_source = None
        db.session.add(query)

        rv = self.make_request('get', '/api/queries/{}'.format(query.id), user=second_org_admin)
        self.assertEquals(rv.status_code, 404)

        rv = self.make_request('get', '/api/queries/{}'.format(query.id), user=self.factory.create_admin())
        self.assertEquals(rv.status_code, 200)

    def test_query_search(self):
        names = [
            'Harder',
            'Better',
            'Faster',
            'Stronger',
        ]
        for name in names:
            self.factory.create_query(name=name)

        rv = self.make_request('get', '/api/queries?q=better')

        self.assertEquals(rv.status_code, 200)
        self.assertEquals(len(rv.json['results']), 1)

        rv = self.make_request('get', '/api/queries?q=better OR faster')

        self.assertEquals(rv.status_code, 200)
        self.assertEquals(len(rv.json['results']), 2)

        # test the old search API and that it redirects to the new one
        rv = self.make_request('get', '/api/queries/search?q=stronger')
        self.assertEquals(rv.status_code, 301)
        self.assertIn('/api/queries?q=stronger', rv.headers['Location'])

        rv = self.make_request('get', '/api/queries/search?q=stronger', follow_redirects=True)
        self.assertEquals(rv.status_code, 200)
        self.assertEquals(len(rv.json['results']), 1)


class TestQueryResourcePost(BaseTestCase):
    def test_update_query(self):
        admin = self.factory.create_admin()
        query = self.factory.create_query()

        new_ds = self.factory.create_data_source()
        new_qr = self.factory.create_query_result()

        data = {
            'name': 'Testing',
            'query': 'select 2',
            'latest_query_data_id': new_qr.id,
            'data_source_id': new_ds.id
        }

        rv = self.make_request('post', '/api/queries/{0}'.format(query.id), data=data, user=admin)
        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['name'], data['name'])
        self.assertEqual(rv.json['last_modified_by']['id'], admin.id)
        self.assertEqual(rv.json['query'], data['query'])
        self.assertEqual(rv.json['data_source_id'], data['data_source_id'])
        self.assertEqual(rv.json['latest_query_data_id'], data['latest_query_data_id'])

    def test_raises_error_in_case_of_conflict(self):
        q = self.factory.create_query()
        q.name = "Another Name"
        db.session.add(q)

        rv = self.make_request('post', '/api/queries/{0}'.format(q.id), data={'name': 'Testing', 'version': q.version - 1}, user=self.factory.user)
        self.assertEqual(rv.status_code, 409)

    def test_overrides_existing_if_no_version_specified(self):
        q = self.factory.create_query()
        q.name = "Another Name"
        db.session.add(q)

        rv = self.make_request('post', '/api/queries/{0}'.format(q.id), data={'name': 'Testing'}, user=self.factory.user)
        self.assertEqual(rv.status_code, 200)

    def test_works_for_non_owner_with_permission(self):
        query = self.factory.create_query()
        user = self.factory.create_user()

        rv = self.make_request('post', '/api/queries/{0}'.format(query.id), data={'name': 'Testing'}, user=user)
        self.assertEqual(rv.status_code, 403)

        models.AccessPermission.grant(obj=query, access_type=ACCESS_TYPE_MODIFY, grantee=user, grantor=query.user)

        rv = self.make_request('post', '/api/queries/{0}'.format(query.id), data={'name': 'Testing'}, user=user)
        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['name'], 'Testing')
        self.assertEqual(rv.json['last_modified_by']['id'], user.id)

class TestQueryListResourceGet(BaseTestCase):
    def test_returns_queries(self):
        q1 = self.factory.create_query()
        q2 = self.factory.create_query()
        q3 = self.factory.create_query()

        rv = self.make_request('get', '/api/queries')

        assert len(rv.json['results']) == 3
        assert set(map(lambda d: d['id'], rv.json['results'])) == set([q1.id, q2.id, q3.id])

    def test_filters_with_tags(self):
        q1 = self.factory.create_query(tags=[u'test'])
        q2 = self.factory.create_query()
        q3 = self.factory.create_query()

        rv = self.make_request('get', '/api/queries?tags=test')
        assert len(rv.json['results']) == 1
        assert set(map(lambda d: d['id'], rv.json['results'])) == set([q1.id])

    def test_search_term(self):
        q1 = self.factory.create_query(name="Sales")
        q2 = self.factory.create_query(name="Q1 sales")
        q3 = self.factory.create_query(name="Ops")

        rv = self.make_request('get', '/api/queries?q=sales')
        assert len(rv.json['results']) == 2
        assert set(map(lambda d: d['id'], rv.json['results'])) == set([q1.id, q2.id])

class TestQueryListResourcePost(BaseTestCase):
    def test_create_query(self):
        query_data = {
            'name': 'Testing',
            'query': 'SELECT 1',
            'schedule': "3600",
            'data_source_id': self.factory.data_source.id
        }

        rv = self.make_request('post', '/api/queries', data=query_data)

        self.assertEquals(rv.status_code, 200)
        self.assertDictContainsSubset(query_data, rv.json)
        self.assertEquals(rv.json['user']['id'], self.factory.user.id)
        self.assertIsNotNone(rv.json['api_key'])
        self.assertIsNotNone(rv.json['query_hash'])

        query = models.Query.query.get(rv.json['id'])
        self.assertEquals(len(list(query.visualizations)), 1)
        self.assertTrue(query.is_draft)


class QueryRefreshTest(BaseTestCase):
    def setUp(self):
        super(QueryRefreshTest, self).setUp()

        self.query = self.factory.create_query()
        self.path = '/api/queries/{}/refresh'.format(self.query.id)

    def test_refresh_regular_query(self):
        response = self.make_request('post', self.path)
        self.assertEqual(200, response.status_code)

    def test_refresh_of_query_with_parameters(self):
        self.query.query_text = u"SELECT {{param}}"
        db.session.add(self.query)

        response = self.make_request('post', "{}?p_param=1".format(self.path))
        self.assertEqual(200, response.status_code)

    def test_refresh_of_query_with_parameters_without_parameters(self):
        self.query.query_text = u"SELECT {{param}}"
        db.session.add(self.query)

        response = self.make_request('post', "{}".format(self.path))
        self.assertEqual(400, response.status_code)

    def test_refresh_query_you_dont_have_access_to(self):
        group = self.factory.create_group()
        db.session.add(group)
        db.session.commit()
        user = self.factory.create_user(group_ids=[group.id])
        response = self.make_request('post', self.path, user=user)
        self.assertEqual(403, response.status_code)

    def test_refresh_forbiden_with_query_api_key(self):
        response = self.make_request('post', '{}?api_key={}'.format(self.path, self.query.api_key), user=False)
        self.assertEqual(403, response.status_code)

        response = self.make_request('post', '{}?api_key={}'.format(self.path, self.factory.user.api_key), user=False)
        self.assertEqual(200, response.status_code)


class TestQueryForkResourcePost(BaseTestCase):
    def test_forks_a_query(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=False)
        query = self.factory.create_query(data_source=ds)

        rv = self.make_request('post', '/api/queries/{}/fork'.format(query.id))

        self.assertEqual(rv.status_code, 200)

    def test_must_have_full_access_to_data_source(self):
        ds = self.factory.create_data_source(group=self.factory.org.default_group, view_only=True)
        query = self.factory.create_query(data_source=ds)

        rv = self.make_request('post', '/api/queries/{}/fork'.format(query.id))

        self.assertEqual(rv.status_code, 403)


class TestFormatSQLQueryAPI(BaseTestCase):
    def test_format_sql_query(self):
        admin = self.factory.create_admin()
        query = 'select a,b,c FROM foobar Where x=1 and y=2;'
        expected = """SELECT a,
       b,
       c
FROM foobar
WHERE x=1
  AND y=2;"""

        rv = self.make_request('post', '/api/queries/format', user=admin, data={'query': query})

        self.assertEqual(rv.json['query'], expected)

<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import QuerySnippet


class TestQuerySnippetResource(BaseTestCase):
    def test_get_snippet(self):
        snippet = self.factory.create_query_snippet()

        rv = self.make_request('get', '/api/query_snippets/{}'.format(snippet.id))

        for field in ('snippet', 'description', 'trigger'):
            self.assertEqual(rv.json[field], getattr(snippet, field))

    def test_update_snippet(self):
        snippet = self.factory.create_query_snippet()

        data = {
            'snippet': 'updated',
            'trigger': 'updated trigger',
            'description': 'updated description'
        }

        rv = self.make_request('post', '/api/query_snippets/{}'.format(snippet.id), data=data)

        for field in ('snippet', 'description', 'trigger'):
            self.assertEqual(rv.json[field], data[field])

    def test_delete_snippet(self):
        snippet = self.factory.create_query_snippet()
        rv = self.make_request('delete', '/api/query_snippets/{}'.format(snippet.id))

        self.assertIsNone(QuerySnippet.query.get(snippet.id))


class TestQuerySnippetListResource(BaseTestCase):
    def test_create_snippet(self):
        data = {
            'snippet': 'updated',
            'trigger': 'updated trigger',
            'description': 'updated description'
        }

        rv = self.make_request('post', '/api/query_snippets', data=data)
        self.assertEqual(rv.status_code, 200)

    def test_list_all_snippets(self):
        snippet1 = self.factory.create_query_snippet()
        snippet2 = self.factory.create_query_snippet()
        snippet_diff_org = self.factory.create_query_snippet(org=self.factory.create_org())

        rv = self.make_request('get', '/api/query_snippets')
        ids = [s['id'] for s in rv.json]

        self.assertIn(snippet1.id, ids)
        self.assertIn(snippet2.id, ids)
        self.assertNotIn(snippet_diff_org.id, ids)

<EOF>
<BOF>
from redash import models
from tests import BaseTestCase
from mock import patch


class TestUserListResourcePost(BaseTestCase):
    def test_returns_403_for_non_admin(self):
        rv = self.make_request('post', "/api/users")
        self.assertEqual(rv.status_code, 403)

    def test_returns_400_when_missing_fields(self):
        admin = self.factory.create_admin()

        rv = self.make_request('post', "/api/users", user=admin)
        self.assertEqual(rv.status_code, 400)

        rv = self.make_request('post', '/api/users', data={'name': 'User'}, user=admin)
        self.assertEqual(rv.status_code, 400)
    
    def test_returns_400_when_using_temporary_email(self):
        admin = self.factory.create_admin()

        test_user = {'name': 'User', 'email': 'user@mailinator.com', 'password': 'test'}
        rv = self.make_request('post', '/api/users', data=test_user, user=admin)
        self.assertEqual(rv.status_code, 400)

        test_user['email'] = 'arik@qq.com'
        rv = self.make_request('post', '/api/users', data=test_user, user=admin)
        self.assertEqual(rv.status_code, 400)


    def test_creates_user(self):
        admin = self.factory.create_admin()

        test_user = {'name': 'User', 'email': 'user@example.com', 'password': 'test'}
        rv = self.make_request('post', '/api/users', data=test_user, user=admin)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['name'], test_user['name'])
        self.assertEqual(rv.json['email'], test_user['email'])

    def test_creates_user_case_insensitive_email(self):
        admin = self.factory.create_admin()

        test_user = {'name': 'User', 'email': 'User@Example.com', 'password': 'test'}
        rv = self.make_request('post', '/api/users', data=test_user, user=admin)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['name'], test_user['name'])
        self.assertEqual(rv.json['email'], 'user@example.com')

    def test_returns_400_when_email_taken(self):
        admin = self.factory.create_admin()

        test_user = {'name': 'User', 'email': admin.email, 'password': 'test'}
        rv = self.make_request('post', '/api/users', data=test_user, user=admin)

        self.assertEqual(rv.status_code, 400)

    def test_returns_400_when_email_taken_case_insensitive(self):
        admin = self.factory.create_admin()

        test_user1 = {'name': 'User', 'email': 'user@example.com', 'password': 'test'}
        rv = self.make_request('post', '/api/users', data=test_user1, user=admin)

        self.assertEqual(rv.status_code, 200)
        self.assertEqual(rv.json['email'], 'user@example.com')

        test_user2 = {'name': 'User', 'email': 'user@Example.com', 'password': 'test'}
        rv = self.make_request('post', '/api/users', data=test_user2, user=admin)

        self.assertEqual(rv.status_code, 400)


class TestUserListGet(BaseTestCase):
    def test_returns_users_for_given_org_only(self):
        user1 = self.factory.user
        user2 = self.factory.create_user()
        org = self.factory.create_org()
        user3 = self.factory.create_user(org=org)

        rv = self.make_request('get', "/api/users")
        user_ids = map(lambda u: u['id'], rv.json['results'])
        self.assertIn(user1.id, user_ids)
        self.assertIn(user2.id, user_ids)
        self.assertNotIn(user3.id, user_ids)


class TestUserResourceGet(BaseTestCase):
    def test_returns_api_key_for_your_own_user(self):
        rv = self.make_request('get', "/api/users/{}".format(self.factory.user.id))
        self.assertIn('api_key', rv.json)

    def test_returns_api_key_for_other_user_when_admin(self):
        other_user = self.factory.user
        admin = self.factory.create_admin()

        rv = self.make_request('get', "/api/users/{}".format(other_user.id), user=admin)
        self.assertIn('api_key', rv.json)

    def test_doesnt_return_api_key_for_other_user(self):
        other_user = self.factory.create_user()

        rv = self.make_request('get', "/api/users/{}".format(other_user.id))
        self.assertNotIn('api_key', rv.json)

    def test_doesnt_return_user_from_different_org(self):
        org = self.factory.create_org()
        other_user = self.factory.create_user(org=org)

        rv = self.make_request('get', "/api/users/{}".format(other_user.id))
        self.assertEqual(rv.status_code, 404)


class TestUserResourcePost(BaseTestCase):
    def test_returns_403_for_non_admin_changing_not_his_own(self):
        other_user = self.factory.create_user()

        rv = self.make_request('post', "/api/users/{}".format(other_user.id), data={"name": "New Name"})
        self.assertEqual(rv.status_code, 403)

    def test_returns_200_for_non_admin_changing_his_own(self):
        rv = self.make_request('post', "/api/users/{}".format(self.factory.user.id), data={"name": "New Name"})
        self.assertEqual(rv.status_code, 200)

    def test_returns_200_for_admin_changing_other_user(self):
        admin = self.factory.create_admin()

        rv = self.make_request('post', "/api/users/{}".format(self.factory.user.id), data={"name": "New Name"}, user=admin)
        self.assertEqual(rv.status_code, 200)

    def test_fails_password_change_without_old_password(self):
        rv = self.make_request('post', "/api/users/{}".format(self.factory.user.id), data={"password": "new password"})
        self.assertEqual(rv.status_code, 403)

    def test_fails_password_change_with_incorrect_old_password(self):
        rv = self.make_request('post', "/api/users/{}".format(self.factory.user.id), data={"password": "new password", "old_password": "wrong"})
        self.assertEqual(rv.status_code, 403)

    def test_changes_password(self):
        new_password = "new password"
        old_password = "old password"

        self.factory.user.hash_password(old_password)
        models.db.session.add(self.factory.user)

        rv = self.make_request('post', "/api/users/{}".format(self.factory.user.id), data={"password": new_password, "old_password": old_password})
        self.assertEqual(rv.status_code, 200)

        user = models.User.query.get(self.factory.user.id)
        self.assertTrue(user.verify_password(new_password))
    
    def test_returns_400_when_using_temporary_email(self):
        admin = self.factory.create_admin()

        test_user = {'email': 'user@mailinator.com'}
        rv = self.make_request('post', '/api/users/{}'.format(self.factory.user.id), data=test_user, user=admin)
        self.assertEqual(rv.status_code, 400)

        test_user['email'] = 'arik@qq.com'
        rv = self.make_request('post', '/api/users', data=test_user, user=admin)
        self.assertEqual(rv.status_code, 400)


class TestUserDisable(BaseTestCase):
    def test_non_admin_cannot_disable_user(self):
        other_user = self.factory.create_user()
        self.assertFalse(other_user.is_disabled)

        rv = self.make_request('post', "/api/users/{}/disable".format(other_user.id), user=other_user)
        self.assertEqual(rv.status_code, 403)

        # user should stay enabled
        other_user = models.User.query.get(other_user.id)
        self.assertFalse(other_user.is_disabled)

    def test_admin_can_disable_user(self):
        admin_user = self.factory.create_admin()
        other_user = self.factory.create_user()
        self.assertFalse(other_user.is_disabled)

        rv = self.make_request('post', "/api/users/{}/disable".format(other_user.id), user=admin_user)
        self.assertEqual(rv.status_code, 200)

        # user should become disabled
        other_user = models.User.query.get(other_user.id)
        self.assertTrue(other_user.is_disabled)

    def test_admin_can_disable_another_admin(self):
        admin_user1 = self.factory.create_admin()
        admin_user2 = self.factory.create_admin()
        self.assertFalse(admin_user2.is_disabled)

        rv = self.make_request('post', "/api/users/{}/disable".format(admin_user2.id), user=admin_user1)
        self.assertEqual(rv.status_code, 200)

        # user should become disabled
        admin_user2 = models.User.query.get(admin_user2.id)
        self.assertTrue(admin_user2.is_disabled)

    def test_admin_cannot_disable_self(self):
        admin_user = self.factory.create_admin()
        self.assertFalse(admin_user.is_disabled)

        rv = self.make_request('post', "/api/users/{}/disable".format(admin_user.id), user=admin_user)
        self.assertEqual(rv.status_code, 400)

        # user should stay enabled
        admin_user = models.User.query.get(admin_user.id)
        self.assertFalse(admin_user.is_disabled)

    def test_admin_can_enable_user(self):
        admin_user = self.factory.create_admin()
        other_user = self.factory.create_user(disabled_at='2018-03-08 00:00')
        self.assertTrue(other_user.is_disabled)

        rv = self.make_request('delete', "/api/users/{}/disable".format(other_user.id), user=admin_user)
        self.assertEqual(rv.status_code, 200)

        # user should become enabled
        other_user = models.User.query.get(other_user.id)
        self.assertFalse(other_user.is_disabled)

    def test_admin_can_enable_another_admin(self):
        admin_user1 = self.factory.create_admin()
        admin_user2 = self.factory.create_admin(disabled_at='2018-03-08 00:00')
        self.assertTrue(admin_user2.is_disabled)

        rv = self.make_request('delete', "/api/users/{}/disable".format(admin_user2.id), user=admin_user1)
        self.assertEqual(rv.status_code, 200)

        # user should become enabled
        admin_user2 = models.User.query.get(admin_user2.id)
        self.assertFalse(admin_user2.is_disabled)

    def test_disabled_user_cannot_login(self):
        user = self.factory.create_user(disabled_at='2018-03-08 00:00')
        user.hash_password('password')

        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.authentication.login_user') as login_user_mock:
            rv = self.client.post('/login', data={'email': user.email, 'password': 'password'})
            # login handler should not be called
            login_user_mock.assert_not_called()
            # check for redirect back to login page
            self.assertEquals(rv.status_code, 301)
            self.assertIn('/login', rv.headers.get('Location', None))

    def test_disabled_user_should_not_access_api(self):
        # Note: some API does not require user, so check the one which requires

        # 1. create user; the user should have access to API
        user = self.factory.create_user()
        rv = self.make_request('get', '/api/dashboards', user=user)
        self.assertEquals(rv.status_code, 200)

        # 2. disable user; now API access should be forbidden
        user.disable()
        self.db.session.add(user)
        self.db.session.commit()

        rv = self.make_request('get', '/api/dashboards', user=user)
        self.assertNotEquals(rv.status_code, 200)

    def test_disabled_user_should_not_receive_restore_password_email(self):
        admin_user = self.factory.create_admin()

        # user should receive email
        user = self.factory.create_user()
        with patch('redash.handlers.users.send_password_reset_email') as send_password_reset_email_mock:
            send_password_reset_email_mock.return_value = 'reset_token'
            rv = self.make_request('post', '/api/users/{}/reset_password'.format(user.id), user=admin_user)
            self.assertEqual(rv.status_code, 200)
            send_password_reset_email_mock.assert_called_with(user)

        # disable user; now should not receive email
        user.disable()
        self.db.session.add(user)
        self.db.session.commit()

        with patch('redash.handlers.users.send_password_reset_email') as send_password_reset_email_mock:
            send_password_reset_email_mock.return_value = 'reset_token'
            rv = self.make_request('post', '/api/users/{}/reset_password'.format(user.id), user=admin_user)
            self.assertEqual(rv.status_code, 404)
            send_password_reset_email_mock.assert_not_called()
<EOF>
<BOF>
import datetime

from mock import ANY, call, patch
from tests import BaseTestCase

from redash.tasks import refresh_schemas


class TestRefreshSchemas(BaseTestCase):
    def test_calls_refresh_of_all_data_sources(self):
        self.factory.data_source # trigger creation
        with patch('redash.tasks.queries.refresh_schema.apply_async') as refresh_job:
            refresh_schemas()
            refresh_job.assert_called()

    def test_skips_paused_data_sources(self):
        self.factory.data_source.pause()

        with patch('redash.tasks.queries.refresh_schema.apply_async') as refresh_job:
            refresh_schemas()
            refresh_job.assert_not_called()

        self.factory.data_source.resume()

        with patch('redash.tasks.queries.refresh_schema.apply_async') as refresh_job:
            refresh_schemas()
            refresh_job.assert_called()
<EOF>
<BOF>
from mock import patch, call, ANY
from tests import BaseTestCase
from redash.tasks import refresh_queries
from redash.models import Query


class TestRefreshQuery(BaseTestCase):
    def test_enqueues_outdated_queries(self):
        """
        refresh_queries() launches an execution task for each query returned
        from Query.outdated_queries().
        """
        query1 = self.factory.create_query()
        query2 = self.factory.create_query(
            query_text="select 42;",
            data_source=self.factory.create_data_source())
        oq = staticmethod(lambda: [query1, query2])
        with patch('redash.tasks.queries.enqueue_query') as add_job_mock, \
                patch.object(Query, 'outdated_queries', oq):
            refresh_queries()
            self.assertEqual(add_job_mock.call_count, 2)
            add_job_mock.assert_has_calls([
                call(query1.query_text, query1.data_source, query1.user_id,
                     scheduled_query=query1, metadata=ANY),
                call(query2.query_text, query2.data_source, query2.user_id,
                     scheduled_query=query2, metadata=ANY)], any_order=True)

    def test_doesnt_enqueue_outdated_queries_for_paused_data_source(self):
        """
        refresh_queries() does not launch execution tasks for queries whose
        data source is paused.
        """
        query = self.factory.create_query()
        oq = staticmethod(lambda: [query])
        query.data_source.pause()
        with patch.object(Query, 'outdated_queries', oq):
            with patch('redash.tasks.queries.enqueue_query') as add_job_mock:
                refresh_queries()
                add_job_mock.assert_not_called()

            query.data_source.resume()

            with patch('redash.tasks.queries.enqueue_query') as add_job_mock:
                refresh_queries()
                add_job_mock.assert_called_with(
                    query.query_text, query.data_source, query.user_id,
                    scheduled_query=query, metadata=ANY)

    def test_enqueues_parameterized_queries(self):
        """
        Scheduled queries with parameters use saved values.
        """
        query = self.factory.create_query(
            query_text="select {{n}}",
            options={"parameters": [{
                "global": False,
                "type": "text",
                "name": "n",
                "value": "42",
                "title": "n"}]})
        oq = staticmethod(lambda: [query])
        with patch('redash.tasks.queries.enqueue_query') as add_job_mock, \
                patch.object(Query, 'outdated_queries', oq):
            refresh_queries()
            add_job_mock.assert_called_with(
                "select 42", query.data_source, query.user_id,
                scheduled_query=query, metadata=ANY)
<EOF>
<BOF>

<EOF>
<BOF>
from tests import BaseTestCase
from mock import MagicMock, ANY

import redash.tasks.alerts
from redash.tasks.alerts import check_alerts_for_query, notify_subscriptions, should_notify
from redash.models import Alert


class TestCheckAlertsForQuery(BaseTestCase):
    def test_notifies_subscribers_when_should(self):
        redash.tasks.alerts.notify_subscriptions = MagicMock()
        Alert.evaluate = MagicMock(return_value=Alert.TRIGGERED_STATE)

        alert = self.factory.create_alert()
        check_alerts_for_query(alert.query_id)

        self.assertTrue(redash.tasks.alerts.notify_subscriptions.called)

    def test_doesnt_notify_when_nothing_changed(self):
        redash.tasks.alerts.notify_subscriptions = MagicMock()
        Alert.evaluate = MagicMock(return_value=Alert.OK_STATE)

        alert = self.factory.create_alert()
        check_alerts_for_query(alert.query_id)

        self.assertFalse(redash.tasks.alerts.notify_subscriptions.called)


class TestNotifySubscriptions(BaseTestCase):
    def test_calls_notify_for_subscribers(self):
        subscription = self.factory.create_alert_subscription()
        subscription.notify = MagicMock()
        notify_subscriptions(subscription.alert, Alert.OK_STATE)
        subscription.notify.assert_called_with(subscription.alert, subscription.alert.query_rel, subscription.user, Alert.OK_STATE, ANY, ANY)
<EOF>
<BOF>
from unittest import TestCase
from collections import namedtuple
import uuid

import mock

from tests import BaseTestCase
from redash import redis_connection, models
from redash.query_runner.pg import PostgreSQL
from redash.tasks.queries import (QueryExecutionError, QueryTaskTracker,
                                    enqueue_query, execute_query)


class TestPrune(TestCase):
    def setUp(self):
        self.list = "test_list"
        redis_connection.delete(self.list)
        self.keys = []
        for score in range(0, 100):
            key = 'k:{}'.format(score)
            self.keys.append(key)
            redis_connection.zadd(self.list, score, key)
            redis_connection.set(key, 1)

    def test_does_nothing_when_below_threshold(self):
        remove_count = QueryTaskTracker.prune(self.list, 100)
        self.assertEqual(remove_count, 0)
        self.assertEqual(redis_connection.zcard(self.list), 100)

    def test_removes_oldest_items_first(self):
        remove_count = QueryTaskTracker.prune(self.list, 50)
        self.assertEqual(remove_count, 50)
        self.assertEqual(redis_connection.zcard(self.list), 50)

        self.assertEqual(redis_connection.zscore(self.list, 'k:99'), 99.0)
        self.assertIsNone(redis_connection.zscore(self.list, 'k:1'))

        for k in self.keys[0:50]:
            self.assertFalse(redis_connection.exists(k))


FakeResult = namedtuple('FakeResult', 'id')


def gen_hash(*args, **kwargs):
    return FakeResult(uuid.uuid4().hex)


class TestEnqueueTask(BaseTestCase):
    def test_multiple_enqueue_of_same_query(self):
        query = self.factory.create_query()
        execute_query.apply_async = mock.MagicMock(side_effect=gen_hash)

        enqueue_query(query.query_text, query.data_source, query.user_id, query, {'Username': 'Arik', 'Query ID': query.id})
        enqueue_query(query.query_text, query.data_source, query.user_id, query, {'Username': 'Arik', 'Query ID': query.id})
        enqueue_query(query.query_text, query.data_source, query.user_id, query, {'Username': 'Arik', 'Query ID': query.id})

        self.assertEqual(1, execute_query.apply_async.call_count)
        self.assertEqual(1, redis_connection.zcard(QueryTaskTracker.WAITING_LIST))
        self.assertEqual(0, redis_connection.zcard(QueryTaskTracker.IN_PROGRESS_LIST))
        self.assertEqual(0, redis_connection.zcard(QueryTaskTracker.DONE_LIST))

    def test_multiple_enqueue_of_different_query(self):
        query = self.factory.create_query()
        execute_query.apply_async = mock.MagicMock(side_effect=gen_hash)

        enqueue_query(query.query_text, query.data_source, query.user_id, None, {'Username': 'Arik', 'Query ID': query.id})
        enqueue_query(query.query_text + '2', query.data_source, query.user_id, None, {'Username': 'Arik', 'Query ID': query.id})
        enqueue_query(query.query_text + '3', query.data_source, query.user_id, None, {'Username': 'Arik', 'Query ID': query.id})

        self.assertEqual(3, execute_query.apply_async.call_count)
        self.assertEqual(3, redis_connection.zcard(QueryTaskTracker.WAITING_LIST))
        self.assertEqual(0, redis_connection.zcard(QueryTaskTracker.IN_PROGRESS_LIST))
        self.assertEqual(0, redis_connection.zcard(QueryTaskTracker.DONE_LIST))


class QueryExecutorTests(BaseTestCase):

    def test_success(self):
        """
        ``execute_query`` invokes the query runner and stores a query result.
        """
        cm = mock.patch("celery.app.task.Context.delivery_info", {'routing_key': 'test'})
        with cm, mock.patch.object(PostgreSQL, "run_query") as qr:
            qr.return_value = ([1, 2], None)
            result_id = execute_query("SELECT 1, 2", self.factory.data_source.id, {})
            self.assertEqual(1, qr.call_count)
            result = models.QueryResult.query.get(result_id)
            self.assertEqual(result.data, '{1,2}')

    def test_success_scheduled(self):
        """
        Scheduled queries remember their latest results.
        """
        cm = mock.patch("celery.app.task.Context.delivery_info",
                        {'routing_key': 'test'})
        q = self.factory.create_query(query_text="SELECT 1, 2", schedule=300)
        with cm, mock.patch.object(PostgreSQL, "run_query") as qr:
            qr.return_value = ([1, 2], None)
            result_id = execute_query(
                "SELECT 1, 2",
                self.factory.data_source.id, {},
                scheduled_query_id=q.id)
            q = models.Query.get_by_id(q.id)
            self.assertEqual(q.schedule_failures, 0)
            result = models.QueryResult.query.get(result_id)
            self.assertEqual(q.latest_query_data, result)

    def test_failure_scheduled(self):
        """
        Scheduled queries that fail have their failure recorded.
        """
        cm = mock.patch("celery.app.task.Context.delivery_info",
                        {'routing_key': 'test'})
        q = self.factory.create_query(query_text="SELECT 1, 2", schedule=300)
        with cm, mock.patch.object(PostgreSQL, "run_query") as qr:
            qr.side_effect = ValueError("broken")
            with self.assertRaises(QueryExecutionError):
                execute_query("SELECT 1, 2", self.factory.data_source.id, {},
                              scheduled_query_id=q.id)
            q = models.Query.get_by_id(q.id)
            self.assertEqual(q.schedule_failures, 1)
            with self.assertRaises(QueryExecutionError):
                execute_query("SELECT 1, 2", self.factory.data_source.id, {},
                              scheduled_query_id=q.id)
            q = models.Query.get_by_id(q.id)
            self.assertEqual(q.schedule_failures, 2)

    def test_success_after_failure(self):
        """
        Query execution success resets the failure counter.
        """
        cm = mock.patch("celery.app.task.Context.delivery_info",
                        {'routing_key': 'test'})
        q = self.factory.create_query(query_text="SELECT 1, 2", schedule=300)
        with cm, mock.patch.object(PostgreSQL, "run_query") as qr:
            qr.side_effect = ValueError("broken")
            with self.assertRaises(QueryExecutionError):
                execute_query("SELECT 1, 2",
                              self.factory.data_source.id, {},
                              scheduled_query_id=q.id)
            q = models.Query.get_by_id(q.id)
            self.assertEqual(q.schedule_failures, 1)

        with cm, mock.patch.object(PostgreSQL, "run_query") as qr:
            qr.return_value = ([1, 2], None)
            execute_query("SELECT 1, 2",
                          self.factory.data_source.id, {},
                          scheduled_query_id=q.id)
            q = models.Query.get_by_id(q.id)
            self.assertEqual(q.schedule_failures, 0)
<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import db, Dashboard


class DashboardTest(BaseTestCase):
    def create_tagged_dashboard(self, tags):
        dashboard = self.factory.create_dashboard(tags=tags)
        ds = self.factory.create_data_source(group=self.factory.default_group)
        query = self.factory.create_query(data_source=ds)
        # We need a bunch of visualizations and widgets configured
        # to trigger wrong counts via the left outer joins.
        vis1 = self.factory.create_visualization(query_rel=query)
        vis2 = self.factory.create_visualization(query_rel=query)
        vis3 = self.factory.create_visualization(query_rel=query)
        widget1 = self.factory.create_widget(visualization=vis1, dashboard=dashboard)
        widget2 = self.factory.create_widget(visualization=vis2, dashboard=dashboard)
        widget3 = self.factory.create_widget(visualization=vis3, dashboard=dashboard)
        dashboard.layout = '[[{}, {}, {}]]'.format(widget1.id, widget2.id, widget3.id)
        db.session.commit()
        return dashboard

    def test_all_tags(self):
        self.create_tagged_dashboard(tags=['tag1'])
        self.create_tagged_dashboard(tags=['tag1', 'tag2'])
        self.create_tagged_dashboard(tags=['tag1', 'tag2', 'tag3'])

        self.assertEqual(
            list(Dashboard.all_tags(self.factory.org, self.factory.user)),
            [('tag1', 3), ('tag2', 2), ('tag3', 1)]
        )
<EOF>
<BOF>
import mock
from tests import BaseTestCase

from redash.models import DataSource, Query, QueryResult
from redash.utils.configuration import ConfigurationContainer


class DataSourceTest(BaseTestCase):
    def test_get_schema(self):
        return_value = [{'name': 'table', 'columns': []}]

        with mock.patch('redash.query_runner.pg.PostgreSQL.get_schema') as patched_get_schema:
            patched_get_schema.return_value = return_value

            schema = self.factory.data_source.get_schema()

            self.assertEqual(return_value, schema)

    def test_get_schema_uses_cache(self):
        return_value = [{'name': 'table', 'columns': []}]
        with mock.patch('redash.query_runner.pg.PostgreSQL.get_schema') as patched_get_schema:
            patched_get_schema.return_value = return_value

            self.factory.data_source.get_schema()
            schema = self.factory.data_source.get_schema()

            self.assertEqual(return_value, schema)
            self.assertEqual(patched_get_schema.call_count, 1)

    def test_get_schema_skips_cache_with_refresh_true(self):
        return_value = [{'name': 'table', 'columns': []}]
        with mock.patch('redash.query_runner.pg.PostgreSQL.get_schema') as patched_get_schema:
            patched_get_schema.return_value = return_value

            self.factory.data_source.get_schema()
            new_return_value = [{'name': 'new_table', 'columns': []}]
            patched_get_schema.return_value = new_return_value
            schema = self.factory.data_source.get_schema(refresh=True)

            self.assertEqual(new_return_value, schema)
            self.assertEqual(patched_get_schema.call_count, 2)


class TestDataSourceCreate(BaseTestCase):
    def test_adds_data_source_to_default_group(self):
        data_source = DataSource.create_with_group(org=self.factory.org, name='test', options=ConfigurationContainer.from_json('{"dbname": "test"}'), type='pg')
        self.assertIn(self.factory.org.default_group.id, data_source.groups)


class TestDataSourceIsPaused(BaseTestCase):
    def test_returns_false_by_default(self):
        self.assertFalse(self.factory.data_source.paused)

    def test_persists_selection(self):
        self.factory.data_source.pause()
        self.assertTrue(self.factory.data_source.paused)

        self.factory.data_source.resume()
        self.assertFalse(self.factory.data_source.paused)

    def test_allows_setting_reason(self):
        reason = "Some good reason."
        self.factory.data_source.pause(reason)
        self.assertTrue(self.factory.data_source.paused)
        self.assertEqual(self.factory.data_source.pause_reason, reason)

    def test_resume_clears_reason(self):
        self.factory.data_source.pause("Reason")
        self.factory.data_source.resume()
        self.assertEqual(self.factory.data_source.pause_reason, None)

    def test_reason_is_none_by_default(self):
        self.assertEqual(self.factory.data_source.pause_reason, None)


class TestDataSourceDelete(BaseTestCase):
    def test_deletes_the_data_source(self):
        data_source = self.factory.create_data_source()
        data_source.delete()

        self.assertIsNone(DataSource.query.get(data_source.id))

    def test_sets_queries_data_source_to_null(self):
        data_source = self.factory.create_data_source()
        query = self.factory.create_query(data_source=data_source)

        data_source.delete()
        self.assertIsNone(DataSource.query.get(data_source.id))
        self.assertIsNone(Query.query.get(query.id).data_source_id)

    def test_deletes_child_models(self):
        data_source = self.factory.create_data_source()
        self.factory.create_query_result(data_source=data_source)
        self.factory.create_query(data_source=data_source, latest_query_data=self.factory.create_query_result(data_source=data_source))

        data_source.delete()
        self.assertIsNone(DataSource.query.get(data_source.id))
        self.assertEqual(0, QueryResult.query.filter(QueryResult.data_source == data_source).count())
<EOF>
<BOF>
from tests import BaseTestCase

from redash.models import db, Query, Change, ChangeTrackingMixin


def create_object(factory):
    obj = Query(name='Query',
                description='',
                query_text='SELECT 1',
                user=factory.user,
                data_source=factory.data_source,
                org=factory.org)

    return obj


class TestChangesProperty(BaseTestCase):
    def test_returns_initial_state(self):
        obj = create_object(self.factory)

        for change in Change.query.filter(Change.object == obj):
            self.assertIsNone(change.change['previous'])


class TestLogChange(BaseTestCase):
    def obj(self):
        obj = Query(name='Query',
                    description='',
                    query_text='SELECT 1',
                    user=self.factory.user,
                    data_source=self.factory.data_source,
                    org=self.factory.org)

        return obj

    def test_properly_logs_first_creation(self):
        obj = create_object(self.factory)
        obj.record_changes(changed_by=self.factory.user)
        change = Change.last_change(obj)

        self.assertIsNotNone(change)
        self.assertEqual(change.object_version, 1)

    def test_skips_unnecessary_fields(self):
        obj = create_object(self.factory)
        obj.record_changes(changed_by=self.factory.user)
        change = Change.last_change(obj)

        self.assertIsNotNone(change)
        self.assertEqual(change.object_version, 1)
        for field in ChangeTrackingMixin.skipped_fields:
            self.assertNotIn(field, change.change)

    def test_properly_log_modification(self):
        obj = create_object(self.factory)
        obj.record_changes(changed_by=self.factory.user)
        obj.name = 'Query 2'
        obj.description = 'description'
        db.session.flush()
        obj.record_changes(changed_by=self.factory.user)

        change = Change.last_change(obj)

        self.assertIsNotNone(change)
        # TODO: https://github.com/getredash/redash/issues/1550
        # self.assertEqual(change.object_version, 2)
        self.assertEqual(change.object_version, obj.version)
        self.assertIn('name', change.change)
        self.assertIn('description', change.change)

    def test_logs_create_method(self):
        q = Query(name='Query', description='', query_text='',
                  user=self.factory.user, data_source=self.factory.data_source,
                  org=self.factory.org)
        change = Change.last_change(q)

        self.assertIsNotNone(change)
        self.assertEqual(q.user, change.user)
<EOF>
<BOF>
#encoding: utf8
import datetime

from tests import BaseTestCase

from redash import models
from redash.utils import utcnow


class QueryResultTest(BaseTestCase):
    def test_get_latest_returns_none_if_not_found(self):
        found_query_result = models.QueryResult.get_latest(self.factory.data_source, "SELECT 1", 60)
        self.assertIsNone(found_query_result)

    def test_get_latest_returns_when_found(self):
        qr = self.factory.create_query_result()
        found_query_result = models.QueryResult.get_latest(qr.data_source, qr.query_text, 60)

        self.assertEqual(qr, found_query_result)

    def test_get_latest_doesnt_return_query_from_different_data_source(self):
        qr = self.factory.create_query_result()
        data_source = self.factory.create_data_source()
        found_query_result = models.QueryResult.get_latest(data_source, qr.query_text, 60)

        self.assertIsNone(found_query_result)

    def test_get_latest_doesnt_return_if_ttl_expired(self):
        yesterday = utcnow() - datetime.timedelta(days=1)
        qr = self.factory.create_query_result(retrieved_at=yesterday)

        found_query_result = models.QueryResult.get_latest(qr.data_source, qr.query_text, max_age=60)

        self.assertIsNone(found_query_result)

    def test_get_latest_returns_if_ttl_not_expired(self):
        yesterday = utcnow() - datetime.timedelta(seconds=30)
        qr = self.factory.create_query_result(retrieved_at=yesterday)

        found_query_result = models.QueryResult.get_latest(qr.data_source, qr.query_text, max_age=120)

        self.assertEqual(found_query_result, qr)

    def test_get_latest_returns_the_most_recent_result(self):
        yesterday = utcnow() - datetime.timedelta(seconds=30)
        old_qr = self.factory.create_query_result(retrieved_at=yesterday)
        qr = self.factory.create_query_result()

        found_query_result = models.QueryResult.get_latest(qr.data_source, qr.query_text, 60)

        self.assertEqual(found_query_result.id, qr.id)

    def test_get_latest_returns_the_last_cached_result_for_negative_ttl(self):
        yesterday = utcnow() + datetime.timedelta(days=-100)
        very_old = self.factory.create_query_result(retrieved_at=yesterday)

        yesterday = utcnow() + datetime.timedelta(days=-1)
        qr = self.factory.create_query_result(retrieved_at=yesterday)
        found_query_result = models.QueryResult.get_latest(qr.data_source, qr.query_text, -1)

        self.assertEqual(found_query_result.id, qr.id)

    def test_store_result_does_not_modify_query_update_at(self):
        original_updated_at = utcnow() - datetime.timedelta(hours=1)
        query = self.factory.create_query(updated_at=original_updated_at)

        models.QueryResult.store_result(query.org_id, query.data_source, query.query_hash, query.query_text, "", 0, utcnow())

        self.assertEqual(original_updated_at, query.updated_at)
<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import AccessPermission
from redash.permissions import ACCESS_TYPE_MODIFY, ACCESS_TYPE_VIEW


class TestAccessPermissionGrant(BaseTestCase):
    def test_creates_correct_object(self):
        q = self.factory.create_query()
        permission = AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                                            grantor=self.factory.user,
                                            grantee=self.factory.user)

        self.assertEqual(permission.object, q)
        self.assertEqual(permission.grantor, self.factory.user)
        self.assertEqual(permission.grantee, self.factory.user)
        self.assertEqual(permission.access_type, ACCESS_TYPE_MODIFY)

    def test_returns_existing_object_if_exists(self):
        q = self.factory.create_query()
        permission1 = AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                                            grantor=self.factory.user,
                                            grantee=self.factory.user)

        permission2 = AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                                            grantor=self.factory.user,
                                            grantee=self.factory.user)

        self.assertEqual(permission1.id, permission2.id)


class TestAccessPermissionRevoke(BaseTestCase):
    def test_deletes_nothing_when_no_permission_exists(self):
        q = self.factory.create_query()
        self.assertEqual(0, AccessPermission.revoke(q, self.factory.user, ACCESS_TYPE_MODIFY))

    def test_deletes_permission(self):
        q = self.factory.create_query()
        permission = AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                                            grantor=self.factory.user,
                                            grantee=self.factory.user)
        self.assertEqual(1, AccessPermission.revoke(q, self.factory.user, ACCESS_TYPE_MODIFY))

    def test_deletes_permission_for_only_given_grantee_on_given_grant_type(self):
        q = self.factory.create_query()
        first_user  = self.factory.create_user()
        second_user = self.factory.create_user()

        AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                               grantor=self.factory.user,
                               grantee=first_user)

        AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                               grantor=self.factory.user,
                               grantee=second_user)

        AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_VIEW,
                               grantor=self.factory.user,
                               grantee=second_user)

        self.assertEqual(1, AccessPermission.revoke(q, second_user, ACCESS_TYPE_VIEW))

    def test_deletes_all_permissions_if_no_type_given(self):
        q = self.factory.create_query()

        permission = AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_MODIFY,
                                            grantor=self.factory.user,
                                            grantee=self.factory.user)

        permission = AccessPermission.grant(obj=q, access_type=ACCESS_TYPE_VIEW,
                                            grantor=self.factory.user,
                                            grantee=self.factory.user)

        self.assertEqual(2, AccessPermission.revoke(q, self.factory.user))


class TestAccessPermissionFind(BaseTestCase):
    pass


class TestAccessPermissionExists(BaseTestCase):
    pass
<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import Alert, db


class TestAlertAll(BaseTestCase):
    def test_returns_all_alerts_for_given_groups(self):
        ds1 = self.factory.data_source
        group = self.factory.create_group()
        ds2 = self.factory.create_data_source(group=group)

        query1 = self.factory.create_query(data_source=ds1)
        query2 = self.factory.create_query(data_source=ds2)

        alert1 = self.factory.create_alert(query_rel=query1)
        alert2 = self.factory.create_alert(query_rel=query2)
        db.session.flush()

        alerts = Alert.all(group_ids=[group.id, self.factory.default_group.id])
        self.assertIn(alert1, alerts)
        self.assertIn(alert2, alerts)

        alerts = Alert.all(group_ids=[self.factory.default_group.id])
        self.assertIn(alert1, alerts)
        self.assertNotIn(alert2, alerts)

        alerts = Alert.all(group_ids=[group.id])
        self.assertNotIn(alert1, alerts)
        self.assertIn(alert2, alerts)

    def test_return_each_alert_only_once(self):
        group = self.factory.create_group()
        self.factory.data_source.add_group(group)

        alert = self.factory.create_alert()

        alerts = Alert.all(group_ids=[self.factory.default_group.id, group.id])
        self.assertEqual(1, len(list(alerts)))
        self.assertIn(alert, alerts)
<EOF>
<BOF>
from tests import BaseTestCase
from redash.models import ApiKey


class TestApiKeyGetByObject(BaseTestCase):
    def test_returns_none_if_not_exists(self):
        dashboard = self.factory.create_dashboard()
        self.assertIsNone(ApiKey.get_by_object(dashboard))

    def test_returns_only_active_key(self):
        dashboard = self.factory.create_dashboard()
        api_key = self.factory.create_api_key(object=dashboard, active=False)
        self.assertIsNone(ApiKey.get_by_object(dashboard))

        api_key = self.factory.create_api_key(object=dashboard)
        self.assertEqual(api_key, ApiKey.get_by_object(dashboard))

<EOF>
<BOF>
# encoding: utf8

from tests import BaseTestCase
import datetime
from redash.models import Query, Group, Event, db
from redash.utils import utcnow


class QueryTest(BaseTestCase):
    def test_changing_query_text_changes_hash(self):
        q = self.factory.create_query()
        old_hash = q.query_hash

        q.query_text = "SELECT 2;"
        db.session.flush()
        self.assertNotEquals(old_hash, q.query_hash)

    def create_tagged_query(self, tags):
        ds = self.factory.create_data_source(group=self.factory.default_group)
        query = self.factory.create_query(data_source=ds, tags=tags)
        return query

    def test_all_tags(self):
        self.create_tagged_query(tags=['tag1'])
        self.create_tagged_query(tags=['tag1', 'tag2'])
        self.create_tagged_query(tags=['tag1', 'tag2', 'tag3'])

        self.assertEqual(
            list(Query.all_tags(self.factory.user)),
            [('tag1', 3), ('tag2', 2), ('tag3', 1)]
        )

    def test_search_finds_in_name(self):
        q1 = self.factory.create_query(name=u"Testing seåřċħ")
        q2 = self.factory.create_query(name=u"Testing seåřċħing")
        q3 = self.factory.create_query(name=u"Testing seå řċħ")
        queries = list(Query.search(u"seåřċħ", [self.factory.default_group.id]))

        self.assertIn(q1, queries)
        self.assertIn(q2, queries)
        self.assertNotIn(q3, queries)

    def test_search_finds_in_description(self):
        q1 = self.factory.create_query(description=u"Testing seåřċħ")
        q2 = self.factory.create_query(description=u"Testing seåřċħing")
        q3 = self.factory.create_query(description=u"Testing seå řċħ")

        queries = Query.search(u"seåřċħ", [self.factory.default_group.id])

        self.assertIn(q1, queries)
        self.assertIn(q2, queries)
        self.assertNotIn(q3, queries)

    def test_search_by_id_returns_query(self):
        q1 = self.factory.create_query(description="Testing search")
        q2 = self.factory.create_query(description="Testing searching")
        q3 = self.factory.create_query(description="Testing sea rch")
        db.session.flush()
        queries = Query.search(str(q3.id), [self.factory.default_group.id])

        self.assertIn(q3, queries)
        self.assertNotIn(q1, queries)
        self.assertNotIn(q2, queries)

    def test_search_by_number(self):
        q = self.factory.create_query(description="Testing search 12345")
        db.session.flush()
        queries = Query.search('12345', [self.factory.default_group.id])

        self.assertIn(q, queries)

    def test_search_respects_groups(self):
        other_group = Group(org=self.factory.org, name="Other Group")
        db.session.add(other_group)
        ds = self.factory.create_data_source(group=other_group)

        q1 = self.factory.create_query(description="Testing search", data_source=ds)
        q2 = self.factory.create_query(description="Testing searching")
        q3 = self.factory.create_query(description="Testing sea rch")

        queries = list(Query.search("Testing", [self.factory.default_group.id]))

        self.assertNotIn(q1, queries)
        self.assertIn(q2, queries)
        self.assertIn(q3, queries)

        queries = list(Query.search("Testing", [other_group.id, self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertIn(q2, queries)
        self.assertIn(q3, queries)

        queries = list(Query.search("Testing", [other_group.id]))
        self.assertIn(q1, queries)
        self.assertNotIn(q2, queries)
        self.assertNotIn(q3, queries)

    def test_returns_each_query_only_once(self):
        other_group = self.factory.create_group()
        second_group = self.factory.create_group()
        ds = self.factory.create_data_source(group=other_group)
        ds.add_group(second_group, False)

        q1 = self.factory.create_query(description="Testing search", data_source=ds)
        db.session.flush()
        queries = list(Query.search("Testing", [self.factory.default_group.id, other_group.id, second_group.id]))

        self.assertEqual(1, len(queries))

    def test_save_updates_updated_at_field(self):
        # This should be a test of ModelTimestampsMixin, but it's easier to test in context of existing model... :-\
        one_day_ago = utcnow().date() - datetime.timedelta(days=1)
        q = self.factory.create_query(created_at=one_day_ago, updated_at=one_day_ago)
        db.session.flush()
        q.name = 'x'
        db.session.flush()
        self.assertNotEqual(q.updated_at, one_day_ago)

    def test_search_is_case_insensitive(self):
        q = self.factory.create_query(name="Testing search")

        self.assertIn(q, Query.search('testing', [self.factory.default_group.id]))

    def test_search_query_parser_or(self):
        q1 = self.factory.create_query(name="Testing")
        q2 = self.factory.create_query(name="search")

        queries = list(Query.search('testing or search', [self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertIn(q2, queries)

    def test_search_query_parser_negation(self):
        q1 = self.factory.create_query(name="Testing")
        q2 = self.factory.create_query(name="search")

        queries = list(Query.search('testing -search', [self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertNotIn(q2, queries)

    def test_search_query_parser_parenthesis(self):
        q1 = self.factory.create_query(name="Testing search")
        q2 = self.factory.create_query(name="Testing searching")
        q3 = self.factory.create_query(name="Testing finding")

        queries = list(Query.search('(testing search) or finding', [self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertIn(q2, queries)
        self.assertIn(q3, queries)

    def test_search_query_parser_hyphen(self):
        q1 = self.factory.create_query(name="Testing search")
        q2 = self.factory.create_query(name="Testing-search")

        queries = list(Query.search('testing search', [self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertIn(q2, queries)

    def test_search_query_parser_emails(self):
        q1 = self.factory.create_query(name="janedoe@example.com")
        q2 = self.factory.create_query(name="johndoe@example.com")

        queries = list(Query.search('example', [self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertIn(q2, queries)

        queries = list(Query.search('com', [self.factory.default_group.id]))
        self.assertIn(q1, queries)
        self.assertIn(q2, queries)

        queries = list(Query.search('johndoe', [self.factory.default_group.id]))
        self.assertNotIn(q1, queries)
        self.assertIn(q2, queries)


class QueryRecentTest(BaseTestCase):
    def test_global_recent(self):
        q1 = self.factory.create_query()
        q2 = self.factory.create_query()
        db.session.flush()
        e = Event(org=self.factory.org, user=self.factory.user, action="edit",
                  object_type="query", object_id=q1.id)
        db.session.add(e)
        recent = Query.recent([self.factory.default_group.id])
        self.assertIn(q1, recent)
        self.assertNotIn(q2, recent)

    def test_recent_excludes_drafts(self):
        q1 = self.factory.create_query()
        q2 = self.factory.create_query(is_draft=True)

        db.session.add_all([
            Event(org=self.factory.org, user=self.factory.user,
                  action="edit", object_type="query",
                  object_id=q1.id),
            Event(org=self.factory.org, user=self.factory.user,
                  action="edit", object_type="query",
                  object_id=q2.id)
        ])
        recent = Query.recent([self.factory.default_group.id])

        self.assertIn(q1, recent)
        self.assertNotIn(q2, recent)

    def test_recent_for_user(self):
        q1 = self.factory.create_query()
        q2 = self.factory.create_query()
        db.session.flush()
        e = Event(org=self.factory.org, user=self.factory.user, action="edit",
                  object_type="query", object_id=q1.id)
        db.session.add(e)
        recent = Query.recent([self.factory.default_group.id], user_id=self.factory.user.id)

        self.assertIn(q1, recent)
        self.assertNotIn(q2, recent)

        recent = Query.recent([self.factory.default_group.id], user_id=self.factory.user.id + 1)
        self.assertNotIn(q1, recent)
        self.assertNotIn(q2, recent)

    def test_respects_groups(self):
        q1 = self.factory.create_query()
        ds = self.factory.create_data_source(group=self.factory.create_group())
        q2 = self.factory.create_query(data_source=ds)
        db.session.flush()
        Event(org=self.factory.org, user=self.factory.user, action="edit",
              object_type="query", object_id=q1.id)
        Event(org=self.factory.org, user=self.factory.user, action="edit",
              object_type="query", object_id=q2.id)

        recent = Query.recent([self.factory.default_group.id])

        self.assertIn(q1, recent)
        self.assertNotIn(q2, recent)


class TestQueryByUser(BaseTestCase):
    def test_returns_only_users_queries(self):
        q = self.factory.create_query(user=self.factory.user)
        q2 = self.factory.create_query(user=self.factory.create_user())

        queries = Query.by_user(self.factory.user)

        # not using self.assertIn/NotIn because otherwise this fails :O
        self.assertTrue(q in list(queries))
        self.assertFalse(q2 in list(queries))

    def test_returns_drafts_by_the_user(self):
        q = self.factory.create_query(is_draft=True)
        q2 = self.factory.create_query(is_draft=True, user=self.factory.create_user())

        queries = Query.by_user(self.factory.user)

        # not using self.assertIn/NotIn because otherwise this fails :O
        self.assertTrue(q in queries)
        self.assertFalse(q2 in queries)

    def test_returns_only_queries_from_groups_the_user_is_member_in(self):
        q = self.factory.create_query()
        q2 = self.factory.create_query(data_source=self.factory.create_data_source(group=self.factory.create_group()))

        queries = Query.by_user(self.factory.user)

        # not using self.assertIn/NotIn because otherwise this fails :O
        self.assertTrue(q in queries)
        self.assertFalse(q2 in queries)


class TestQueryFork(BaseTestCase):
    def assert_visualizations(self, origin_q, origin_v, forked_q, forked_v):
        self.assertEqual(origin_v.options, forked_v.options)
        self.assertEqual(origin_v.type, forked_v.type)
        self.assertNotEqual(origin_v.id, forked_v.id)
        self.assertNotEqual(origin_v.query_rel, forked_v.query_rel)
        self.assertEqual(forked_q.id, forked_v.query_rel.id)

    def test_fork_with_visualizations(self):
        # prepare original query and visualizations
        data_source = self.factory.create_data_source(
            group=self.factory.create_group())
        query = self.factory.create_query(data_source=data_source,
                                          description="this is description")
        visualization_chart = self.factory.create_visualization(
            query_rel=query, description="chart vis", type="CHART",
            options="""{"yAxis": [{"type": "linear"}, {"type": "linear", "opposite": true}], "series": {"stacking": null}, "globalSeriesType": "line", "sortX": true, "seriesOptions": {"count": {"zIndex": 0, "index": 0, "type": "line", "yAxis": 0}}, "xAxis": {"labels": {"enabled": true}, "type": "datetime"}, "columnMapping": {"count": "y", "created_at": "x"}, "bottomMargin": 50, "legend": {"enabled": true}}""")
        visualization_box = self.factory.create_visualization(
            query_rel=query, description="box vis", type="BOXPLOT",
            options="{}")
        fork_user = self.factory.create_user()
        forked_query = query.fork(fork_user)
        db.session.flush()

        forked_visualization_chart = None
        forked_visualization_box = None
        forked_table = None
        count_table = 0
        for v in forked_query.visualizations:
            if v.description == "chart vis":
                forked_visualization_chart = v
            if v.description == "box vis":
                forked_visualization_box = v
            if v.type == "TABLE":
                count_table += 1
                forked_table = v
        self.assert_visualizations(query, visualization_chart, forked_query,
                                   forked_visualization_chart)
        self.assert_visualizations(query, visualization_box, forked_query,
                                   forked_visualization_box)

        self.assertEqual(forked_query.org, query.org)
        self.assertEqual(forked_query.data_source, query.data_source)
        self.assertEqual(forked_query.latest_query_data,
                         query.latest_query_data)
        self.assertEqual(forked_query.description, query.description)
        self.assertEqual(forked_query.query_text, query.query_text)
        self.assertEqual(forked_query.query_hash, query.query_hash)
        self.assertEqual(forked_query.user, fork_user)
        self.assertEqual(forked_query.description, query.description)
        self.assertTrue(forked_query.name.startswith('Copy'))
        # num of TABLE must be 1. default table only
        self.assertEqual(count_table, 1)
        self.assertEqual(forked_table.name, "Table")
        self.assertEqual(forked_table.description, "")
        self.assertEqual(forked_table.options, "{}")

    def test_fork_from_query_that_has_no_visualization(self):
        # prepare original query and visualizations
        data_source = self.factory.create_data_source(
            group=self.factory.create_group())
        query = self.factory.create_query(data_source=data_source,
                                          description="this is description")
        fork_user = self.factory.create_user()

        forked_query = query.fork(fork_user)

        count_table = 0
        count_vis = 0
        for v in forked_query.visualizations:
            count_vis += 1
            if v.type == "TABLE":
                count_table += 1

        self.assertEqual(count_table, 1)
        self.assertEqual(count_vis, 1)
<EOF>
<BOF>
# -*- coding: utf-8 -*-
from tests import BaseTestCase

from redash.models import User, db

class TestUserUpdateGroupAssignments(BaseTestCase):
    def test_default_group_always_added(self):
        user = self.factory.create_user()

        user.update_group_assignments(["g_unknown"])
        db.session.refresh(user)

        self.assertItemsEqual([user.org.default_group.id], user.group_ids)

    def test_update_group_assignments(self):
        user = self.factory.user
        new_group = self.factory.create_group(name="g1")

        user.update_group_assignments(["g1"])
        db.session.refresh(user)

        self.assertItemsEqual([user.org.default_group.id, new_group.id], user.group_ids)


class TestUserFindByEmail(BaseTestCase):
    def test_finds_users(self):
        user = self.factory.create_user(email=u'test@example.com')
        user2 = self.factory.create_user(email=u'test@example.com', org=self.factory.create_org())

        users = User.find_by_email(user.email)
        self.assertIn(user, users)
        self.assertIn(user2, users)

    def test_finds_users_case_insensitive(self):
        user = self.factory.create_user(email=u'test@example.com')

        users = User.find_by_email(u'test@EXAMPLE.com')
        self.assertIn(user, users)


class TestUserGetByEmailAndOrg(BaseTestCase):
    def test_get_user_by_email_and_org(self):
        user = self.factory.create_user(email=u'test@example.com')

        found_user = User.get_by_email_and_org(user.email, user.org)
        self.assertEqual(user, found_user)

    def test_get_user_by_email_and_org_case_insensitive(self):
        user = self.factory.create_user(email=u'test@example.com')

        found_user = User.get_by_email_and_org("TEST@example.com", user.org)
        self.assertEqual(user, found_user)


class TestUserSearch(BaseTestCase):
    def test_non_unicode_search_string(self):
        user = self.factory.create_user(name=u'אריק')

        assert user in User.search(User.all(user.org), term=u'א')
<EOF>
<BOF>
from redash import create_app

app = create_app()
<EOF>
<BOF>
from flask_admin import Admin
from flask_admin.base import MenuLink
from flask_admin.contrib.sqla import ModelView
from flask_admin.contrib.sqla.form import AdminModelConverter
from wtforms import fields
from wtforms.widgets import TextInput

from redash import models
from redash.permissions import require_super_admin
from redash.utils import json_loads


class ArrayListField(fields.Field):
    widget = TextInput()

    def _value(self):
        if self.data:
            return u', '.join(self.data)
        else:
            return u''

    def process_formdata(self, valuelist):
        if valuelist:
            self.data = [x.strip() for x in valuelist[0].split(',')]
        else:
            self.data = []


class JSONTextAreaField(fields.TextAreaField):
    def process_formdata(self, valuelist):
        if valuelist:
            try:
                json_loads(valuelist[0])
            except ValueError:
                raise ValueError(self.gettext(u'Invalid JSON'))
            self.data = valuelist[0]
        else:
            self.data = ''


class BaseModelView(ModelView):
    column_display_pk = True
    model_form_converter = AdminModelConverter
    form_excluded_columns = ('created_at', 'updated_at')

    @require_super_admin
    def is_accessible(self):
        return True


class QueryResultModelView(BaseModelView):
    column_exclude_list = ('data',)


class QueryModelView(BaseModelView):
    column_exclude_list = ('latest_query_data',)
    form_excluded_columns = ('version', 'visualizations', 'alerts', 'org', 'created_at',
                             'updated_at', 'latest_query_data', 'search_vector')


class DashboardModelView(BaseModelView):
    column_searchable_list = ('name', 'slug')
    column_exclude_list = ('version', )
    form_excluded_columns = ('version', 'widgets', 'org', 'created_at', 'updated_at')


def init_admin(app):
    admin = Admin(app, name='Redash Admin', template_mode='bootstrap3')

    admin.add_view(QueryModelView(models.Query, models.db.session))
    admin.add_view(QueryResultModelView(models.QueryResult, models.db.session))
    admin.add_view(DashboardModelView(models.Dashboard, models.db.session))
    logout_link = MenuLink('Logout', '/logout', 'logout')

    for m in (models.Visualization, models.Widget, models.Event, models.Organization):
        admin.add_view(BaseModelView(m, models.db.session))

    admin.add_link(logout_link)
<EOF>
<BOF>
"""
This will eventually replace all the `to_dict` methods of the different model
classes we have. This will ensure cleaner code and better
separation of concerns.
"""
from funcy import project

from flask_login import current_user

from redash import models
from redash.permissions import has_access, view_only
from redash.utils import json_loads


def public_widget(widget):
    res = {
        'id': widget.id,
        'width': widget.width,
        'options': json_loads(widget.options),
        'text': widget.text,
        'updated_at': widget.updated_at,
        'created_at': widget.created_at
    }

    if widget.visualization and widget.visualization.id:
        query_data = models.QueryResult.query.get(widget.visualization.query_rel.latest_query_data_id).to_dict()
        res['visualization'] = {
            'type': widget.visualization.type,
            'name': widget.visualization.name,
            'description': widget.visualization.description,
            'options': json_loads(widget.visualization.options),
            'updated_at': widget.visualization.updated_at,
            'created_at': widget.visualization.created_at,
            'query': {
                'query': ' ',  # workaround, as otherwise the query data won't be loaded.
                'name': widget.visualization.query_rel.name,
                'description': widget.visualization.query_rel.description,
                'options': {},
                'latest_query_data': query_data
            }
        }

    return res


def public_dashboard(dashboard):
    dashboard_dict = project(serialize_dashboard(dashboard, with_favorite_state=False), (
        'name', 'layout', 'dashboard_filters_enabled', 'updated_at',
        'created_at'
    ))

    widget_list = (models.Widget.query
                   .filter(models.Widget.dashboard_id == dashboard.id)
                   .outerjoin(models.Visualization)
                   .outerjoin(models.Query))

    dashboard_dict['widgets'] = [public_widget(w) for w in widget_list]
    return dashboard_dict


class Serializer(object):
    pass


class QuerySerializer(Serializer):
    def __init__(self, object_or_list, **kwargs):
        self.object_or_list = object_or_list
        self.options = kwargs

    def serialize(self):
        if isinstance(self.object_or_list, models.Query):
            result = serialize_query(self.object_or_list, **self.options)
            if self.options.get('with_favorite_state', True) and not current_user.is_api_user():
                result['is_favorite'] = models.Favorite.is_favorite(current_user.id, self.object_or_list)
        else:
            result = [serialize_query(query, **self.options) for query in self.object_or_list]
            if self.options.get('with_favorite_state', True):
                favorite_ids = models.Favorite.are_favorites(current_user.id, self.object_or_list)
                for query in result:
                    query['is_favorite'] = query['id'] in favorite_ids

        return result


def serialize_query(query, with_stats=False, with_visualizations=False, with_user=True, with_last_modified_by=True):
    d = {
        'id': query.id,
        'latest_query_data_id': query.latest_query_data_id,
        'name': query.name,
        'description': query.description,
        'query': query.query_text,
        'query_hash': query.query_hash,
        'schedule': query.schedule,
        'api_key': query.api_key,
        'is_archived': query.is_archived,
        'is_draft': query.is_draft,
        'updated_at': query.updated_at,
        'created_at': query.created_at,
        'data_source_id': query.data_source_id,
        'options': query.options,
        'version': query.version,
        'tags': query.tags or [],
    }

    if with_user:
        d['user'] = query.user.to_dict()
    else:
        d['user_id'] = query.user_id

    if with_last_modified_by:
        d['last_modified_by'] = query.last_modified_by.to_dict() if query.last_modified_by is not None else None
    else:
        d['last_modified_by_id'] = query.last_modified_by_id

    if with_stats:
        if query.latest_query_data is not None:
            d['retrieved_at'] = query.retrieved_at
            d['runtime'] = query.runtime
        else:
            d['retrieved_at'] = None
            d['runtime'] = None

    if with_visualizations:
        d['visualizations'] = [serialize_visualization(vis, with_query=False)
                               for vis in query.visualizations]

    return d


def serialize_visualization(object, with_query=True):
    d = {
        'id': object.id,
        'type': object.type,
        'name': object.name,
        'description': object.description,
        'options': json_loads(object.options),
        'updated_at': object.updated_at,
        'created_at': object.created_at
    }

    if with_query:
        d['query'] = serialize_query(object.query_rel)

    return d


def serialize_widget(object):
    d = {
        'id': object.id,
        'width': object.width,
        'options': json_loads(object.options),
        'dashboard_id': object.dashboard_id,
        'text': object.text,
        'updated_at': object.updated_at,
        'created_at': object.created_at
    }

    if object.visualization and object.visualization.id:
        d['visualization'] = serialize_visualization(object.visualization)

    return d


def serialize_alert(alert, full=True):
    d = {
        'id': alert.id,
        'name': alert.name,
        'options': alert.options,
        'state': alert.state,
        'last_triggered_at': alert.last_triggered_at,
        'updated_at': alert.updated_at,
        'created_at': alert.created_at,
        'rearm': alert.rearm
    }

    if full:
        d['query'] = serialize_query(alert.query_rel)
        d['user'] = alert.user.to_dict()
    else:
        d['query_id'] = alert.query_id
        d['user_id'] = alert.user_id

    return d


def serialize_dashboard(obj, with_widgets=False, user=None, with_favorite_state=True):
    layout = json_loads(obj.layout)

    widgets = []

    if with_widgets:
        for w in obj.widgets:
            if w.visualization_id is None:
                widgets.append(serialize_widget(w))
            elif user and has_access(w.visualization.query_rel.groups, user, view_only):
                widgets.append(serialize_widget(w))
            else:
                widget = project(serialize_widget(w),
                                ('id', 'width', 'dashboard_id', 'options', 'created_at', 'updated_at'))
                widget['restricted'] = True
                widgets.append(widget)
    else:
        widgets = None

    d = {
        'id': obj.id,
        'slug': obj.slug,
        'name': obj.name,
        'user_id': obj.user_id,
        # TODO: we should properly load the users
        'user': obj.user.to_dict(),
        'layout': layout,
        'dashboard_filters_enabled': obj.dashboard_filters_enabled,
        'widgets': widgets,
        'is_archived': obj.is_archived,
        'is_draft': obj.is_draft,
        'tags': obj.tags or [],
        # TODO: bulk load favorites
        'updated_at': obj.updated_at,
        'created_at': obj.created_at,
        'version': obj.version
    }

    if with_favorite_state:
        d['is_favorite'] = models.Favorite.is_favorite(current_user.id, obj)

    return d
<EOF>
<BOF>
import cStringIO
import csv
import datetime
import functools
import hashlib
import itertools
import logging
import time
from functools import reduce

from six import python_2_unicode_compatible, string_types, text_type
import xlsxwriter
from flask import current_app as app, url_for
from flask_login import AnonymousUserMixin, UserMixin
from flask_sqlalchemy import SQLAlchemy, BaseQuery
from passlib.apps import custom_app_context as pwd_context

from redash import settings, redis_connection, utils
from redash.destinations import (get_configuration_schema_for_destination_type,
                                 get_destination)
from redash.metrics import database  # noqa: F401
from redash.query_runner import (get_configuration_schema_for_query_runner_type,
                                 get_query_runner)
from redash.utils import generate_token, json_dumps, json_loads
from redash.utils.configuration import ConfigurationContainer
from redash.settings.organization import settings as org_settings

from sqlalchemy import distinct, or_, and_, UniqueConstraint
from sqlalchemy.dialects import postgresql
from sqlalchemy.event import listens_for
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy.ext.mutable import Mutable
from sqlalchemy.inspection import inspect
from sqlalchemy.orm import backref, contains_eager, joinedload, object_session, load_only
from sqlalchemy.orm.exc import NoResultFound  # noqa: F401
from sqlalchemy.types import TypeDecorator
from sqlalchemy.orm.attributes import flag_modified
from sqlalchemy import func
from sqlalchemy_searchable import SearchQueryMixin, make_searchable, vectorizer
from sqlalchemy_utils import generic_relationship, EmailType
from sqlalchemy_utils.types import TSVectorType


class SQLAlchemyExt(SQLAlchemy):
    def apply_pool_defaults(self, app, options):
        if settings.SQLALCHEMY_DISABLE_POOL:
            from sqlalchemy.pool import NullPool
            options['poolclass'] = NullPool
        else:
            return super(SQLAlchemyExt, self).apply_pool_defaults(app, options)


db = SQLAlchemyExt(session_options={
    'expire_on_commit': False
})
# Make sure the SQLAlchemy mappers are all properly configured first.
# This is required by SQLAlchemy-Searchable as it adds DDL listeners
# on the configuration phase of models.
db.configure_mappers()

# listen to a few database events to set up functions, trigger updates
# and indexes for the full text search
make_searchable(options={'regconfig': 'pg_catalog.simple'})


class SearchBaseQuery(BaseQuery, SearchQueryMixin):
    """
    The SQA query class to use when full text search is wanted.
    """


Column = functools.partial(db.Column, nullable=False)


class ScheduledQueriesExecutions(object):
    KEY_NAME = 'sq:executed_at'

    def __init__(self):
        self.executions = {}

    def refresh(self):
        self.executions = redis_connection.hgetall(self.KEY_NAME)

    def update(self, query_id):
        redis_connection.hmset(self.KEY_NAME, {
            query_id: time.time()
        })

    def get(self, query_id):
        timestamp = self.executions.get(str(query_id))
        if timestamp:
            timestamp = utils.dt_from_timestamp(timestamp)

        return timestamp


scheduled_queries_executions = ScheduledQueriesExecutions()

# AccessPermission and Change use a 'generic foreign key' approach to refer to
# either queries or dashboards.
# TODO replace this with association tables.
_gfk_types = {}


class GFKBase(object):
    """
    Compatibility with 'generic foreign key' approach Peewee used.
    """
    # XXX Replace this with table-per-association.
    object_type = Column(db.String(255))
    object_id = Column(db.Integer)

    _object = None

    @property
    def object(self):
        session = object_session(self)
        if self._object or not session:
            return self._object
        else:
            object_class = _gfk_types[self.object_type]
            self._object = session.query(object_class).filter(
                object_class.id == self.object_id).first()
            return self._object

    @object.setter
    def object(self, value):
        self._object = value
        self.object_type = value.__class__.__tablename__
        self.object_id = value.id


# XXX replace PseudoJSON and MutableDict with real JSON field
class PseudoJSON(TypeDecorator):
    impl = db.Text

    def process_bind_param(self, value, dialect):
        return json_dumps(value)

    def process_result_value(self, value, dialect):
        if not value:
            return value
        return json_loads(value)


class MutableDict(Mutable, dict):
    @classmethod
    def coerce(cls, key, value):
        "Convert plain dictionaries to MutableDict."

        if not isinstance(value, MutableDict):
            if isinstance(value, dict):
                return MutableDict(value)

            # this call will raise ValueError
            return Mutable.coerce(key, value)
        else:
            return value

    def __setitem__(self, key, value):
        "Detect dictionary set events and emit change events."

        dict.__setitem__(self, key, value)
        self.changed()

    def __delitem__(self, key):
        "Detect dictionary del events and emit change events."

        dict.__delitem__(self, key)
        self.changed()


class MutableList(Mutable, list):
    def append(self, value):
        list.append(self, value)
        self.changed()

    def remove(self, value):
        list.remove(self, value)
        self.changed()

    @classmethod
    def coerce(cls, key, value):
        if not isinstance(value, MutableList):
            if isinstance(value, list):
                return MutableList(value)
            return Mutable.coerce(key, value)
        else:
            return value


class TimestampMixin(object):
    updated_at = Column(db.DateTime(True), default=db.func.now(), nullable=False)
    created_at = Column(db.DateTime(True), default=db.func.now(), nullable=False)


@listens_for(TimestampMixin, 'before_update', propagate=True)
def timestamp_before_update(mapper, connection, target):
    # Check if we really want to update the updated_at value
    if hasattr(target, 'skip_updated_at'):
        return

    target.updated_at = db.func.now()


class ChangeTrackingMixin(object):
    skipped_fields = ('id', 'created_at', 'updated_at', 'version')
    _clean_values = None

    def __init__(self, *a, **kw):
        super(ChangeTrackingMixin, self).__init__(*a, **kw)
        self.record_changes(self.user)

    def prep_cleanvalues(self):
        self.__dict__['_clean_values'] = {}
        for attr in inspect(self.__class__).column_attrs:
            col, = attr.columns
            # 'query' is col name but not attr name
            self._clean_values[col.name] = None

    def __setattr__(self, key, value):
        if self._clean_values is None:
            self.prep_cleanvalues()
        for attr in inspect(self.__class__).column_attrs:
            col, = attr.columns
            previous = getattr(self, attr.key, None)
            self._clean_values[col.name] = previous

        super(ChangeTrackingMixin, self).__setattr__(key, value)

    def record_changes(self, changed_by):
        db.session.add(self)
        db.session.flush()
        changes = {}
        for attr in inspect(self.__class__).column_attrs:
            col, = attr.columns
            if attr.key not in self.skipped_fields:
                changes[col.name] = {'previous': self._clean_values[col.name],
                                     'current': getattr(self, attr.key)}

        db.session.add(Change(object=self,
                              object_version=self.version,
                              user=changed_by,
                              change=changes))


class BelongsToOrgMixin(object):
    @classmethod
    def get_by_id_and_org(cls, object_id, org):
        return db.session.query(cls).filter(cls.id == object_id, cls.org == org).one()


class PermissionsCheckMixin(object):
    def has_permission(self, permission):
        return self.has_permissions((permission,))

    def has_permissions(self, permissions):
        has_permissions = reduce(lambda a, b: a and b,
                                 map(lambda permission: permission in self.permissions,
                                     permissions),
                                 True)

        return has_permissions


class AnonymousUser(AnonymousUserMixin, PermissionsCheckMixin):
    @property
    def permissions(self):
        return []

    def is_api_user(self):
        return False


class ApiUser(UserMixin, PermissionsCheckMixin):
    def __init__(self, api_key, org, groups, name=None):
        self.object = None
        if isinstance(api_key, string_types):
            self.id = api_key
            self.name = name
        else:
            self.id = api_key.api_key
            self.name = "ApiKey: {}".format(api_key.id)
            self.object = api_key.object
        self.group_ids = groups
        self.org = org

    def __repr__(self):
        return u"<{}>".format(self.name)

    def is_api_user(self):
        return True

    @property
    def permissions(self):
        return ['view_query']

    def has_access(self, obj, access_type):
        return False


@python_2_unicode_compatible
class Organization(TimestampMixin, db.Model):
    SETTING_GOOGLE_APPS_DOMAINS = 'google_apps_domains'
    SETTING_IS_PUBLIC = "is_public"

    id = Column(db.Integer, primary_key=True)
    name = Column(db.String(255))
    slug = Column(db.String(255), unique=True)
    settings = Column(MutableDict.as_mutable(PseudoJSON))
    groups = db.relationship("Group", lazy="dynamic")
    events = db.relationship("Event", lazy="dynamic", order_by="desc(Event.created_at)",)


    __tablename__ = 'organizations'

    def __repr__(self):
        return u"<Organization: {}, {}>".format(self.id, self.name)

    def __str__(self):
        return u'%s (%s)' % (self.name, self.id)

    @classmethod
    def get_by_slug(cls, slug):
        return cls.query.filter(cls.slug == slug).first()

    @property
    def default_group(self):
        return self.groups.filter(Group.name == 'default', Group.type == Group.BUILTIN_GROUP).first()

    @property
    def google_apps_domains(self):
        return self.settings.get(self.SETTING_GOOGLE_APPS_DOMAINS, [])

    @property
    def is_public(self):
        return self.settings.get(self.SETTING_IS_PUBLIC, False)

    @property
    def is_disabled(self):
        return self.settings.get('is_disabled', False)

    def disable(self):
        self.settings['is_disabled'] = True

    def enable(self):
        self.settings['is_disabled'] = False

    def set_setting(self, key, value):
        if key not in org_settings:
            raise KeyError(key)

        self.settings.setdefault('settings', {})
        self.settings['settings'][key] = value
        flag_modified(self, 'settings')

    def get_setting(self, key, raise_on_missing=True):
        if key in self.settings.get('settings', {}):
            return self.settings['settings'][key]

        if key in org_settings:
            return org_settings[key]

        if raise_on_missing:
            raise KeyError(key)

        return None

    @property
    def admin_group(self):
        return self.groups.filter(Group.name == 'admin', Group.type == Group.BUILTIN_GROUP).first()

    def has_user(self, email):
        return self.users.filter(User.email == email).count() == 1


@python_2_unicode_compatible
class Group(db.Model, BelongsToOrgMixin):
    DEFAULT_PERMISSIONS = ['create_dashboard', 'create_query', 'edit_dashboard', 'edit_query',
                           'view_query', 'view_source', 'execute_query', 'list_users', 'schedule_query',
                           'list_dashboards', 'list_alerts', 'list_data_sources']

    BUILTIN_GROUP = 'builtin'
    REGULAR_GROUP = 'regular'

    id = Column(db.Integer, primary_key=True)
    data_sources = db.relationship("DataSourceGroup", back_populates="group",
                                         cascade="all")
    org_id = Column(db.Integer, db.ForeignKey('organizations.id'))
    org = db.relationship(Organization, back_populates="groups")
    type = Column(db.String(255), default=REGULAR_GROUP)
    name = Column(db.String(100))
    permissions = Column(postgresql.ARRAY(db.String(255)),
                         default=DEFAULT_PERMISSIONS)
    created_at = Column(db.DateTime(True), default=db.func.now())

    __tablename__ = 'groups'

    def to_dict(self):
        return {
            'id': self.id,
            'name': self.name,
            'permissions': self.permissions,
            'type': self.type,
            'created_at': self.created_at
        }

    @classmethod
    def all(cls, org):
        return cls.query.filter(cls.org == org)

    @classmethod
    def members(cls, group_id):
        return User.query.filter(User.group_ids.any(group_id))

    @classmethod
    def find_by_name(cls, org, group_names):
        result = cls.query.filter(cls.org == org, cls.name.in_(group_names))
        return list(result)

    def __str__(self):
        return text_type(self.id)


@python_2_unicode_compatible
class User(TimestampMixin, db.Model, BelongsToOrgMixin, UserMixin, PermissionsCheckMixin):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey('organizations.id'))
    org = db.relationship(Organization, backref=db.backref("users", lazy="dynamic"))
    name = Column(db.String(320))
    email = Column(EmailType)
    _profile_image_url = Column('profile_image_url', db.String(320), nullable=True)
    password_hash = Column(db.String(128), nullable=True)
    # XXX replace with association table
    group_ids = Column('groups', MutableList.as_mutable(postgresql.ARRAY(db.Integer)), nullable=True)
    api_key = Column(db.String(40),
                     default=lambda: generate_token(40),
                     unique=True)

    disabled_at = Column(db.DateTime(True), default=None, nullable=True)

    __tablename__ = 'users'
    __table_args__ = (db.Index('users_org_id_email', 'org_id', 'email', unique=True),)

    @property
    def is_disabled(self):
        return self.disabled_at is not None

    def disable(self):
        self.disabled_at = db.func.now()

    def enable(self):
        self.disabled_at = None

    def __init__(self, *args, **kwargs):
        if kwargs.get('email') is not None:
            kwargs['email'] = kwargs['email'].lower()
        super(User, self).__init__(*args, **kwargs)

    def to_dict(self, with_api_key=False):
        profile_image_url = self.profile_image_url
        if self.is_disabled:
            assets = app.extensions['webpack']['assets'] or {}
            path = 'images/avatar.svg'
            profile_image_url = url_for('static', filename=assets.get(path, path))

        d = {
            'id': self.id,
            'name': self.name,
            'email': self.email,
            'profile_image_url': profile_image_url,
            'groups': self.group_ids,
            'updated_at': self.updated_at,
            'created_at': self.created_at,
            'disabled_at': self.disabled_at,
            'is_disabled': self.is_disabled,
        }

        if self.password_hash is None:
            d['auth_type'] = 'external'
        else:
            d['auth_type'] = 'password'

        if with_api_key:
            d['api_key'] = self.api_key

        return d

    def is_api_user(self):
        return False

    @property
    def profile_image_url(self):
        if self._profile_image_url is not None:
            return self._profile_image_url

        email_md5 = hashlib.md5(self.email.lower()).hexdigest()
        return "https://www.gravatar.com/avatar/{}?s=40&d=identicon".format(email_md5)

    @property
    def permissions(self):
        # TODO: this should be cached.
        return list(itertools.chain(*[g.permissions for g in
                                      Group.query.filter(Group.id.in_(self.group_ids))]))

    @classmethod
    def get_by_email_and_org(cls, email, org):
        return cls.query.filter(cls.email == email, cls.org == org).one()

    @classmethod
    def get_by_api_key_and_org(cls, api_key, org):
        return cls.query.filter(cls.api_key == api_key, cls.org == org).one()

    @classmethod
    def all(cls, org):
        return cls.query.filter(cls.org == org).filter(cls.disabled_at == None)

    @classmethod
    def search(cls, base_query, term):
        term = u'%{}%'.format(term)
        search_filter = or_(cls.name.ilike(term), cls.email.like(term))

        return base_query.filter(search_filter)

    @classmethod
    def all_disabled(cls, org):
        return cls.query.filter(cls.org == org).filter(cls.disabled_at != None)

    @classmethod
    def all_not_disabled(cls, org):
        return cls.all(org).filter(cls.disabled_at == None)

    @classmethod
    def find_by_email(cls, email):
        return cls.query.filter(cls.email == email)

    def __str__(self):
        return u'%s (%s)' % (self.name, self.email)

    def hash_password(self, password):
        self.password_hash = pwd_context.encrypt(password)

    def verify_password(self, password):
        return self.password_hash and pwd_context.verify(password, self.password_hash)

    def update_group_assignments(self, group_names):
        groups = Group.find_by_name(self.org, group_names)
        groups.append(self.org.default_group)
        self.group_ids = [g.id for g in groups]
        db.session.add(self)
        db.session.commit()

    def has_access(self, obj, access_type):
        return AccessPermission.exists(obj, access_type, grantee=self)


class Configuration(TypeDecorator):
    impl = db.Text

    def process_bind_param(self, value, dialect):
        return value.to_json()

    def process_result_value(self, value, dialect):
        return ConfigurationContainer.from_json(value)


@python_2_unicode_compatible
class DataSource(BelongsToOrgMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey('organizations.id'))
    org = db.relationship(Organization, backref="data_sources")

    name = Column(db.String(255))
    type = Column(db.String(255))
    options = Column(ConfigurationContainer.as_mutable(Configuration))
    queue_name = Column(db.String(255), default="queries")
    scheduled_queue_name = Column(db.String(255), default="scheduled_queries")
    created_at = Column(db.DateTime(True), default=db.func.now())

    data_source_groups = db.relationship("DataSourceGroup", back_populates="data_source",
                                         cascade="all")
    __tablename__ = 'data_sources'
    __table_args__ = (db.Index('data_sources_org_id_name', 'org_id', 'name'),)

    def __eq__(self, other):
        return self.id == other.id

    def to_dict(self, all=False, with_permissions_for=None):
        d = {
            'id': self.id,
            'name': self.name,
            'type': self.type,
            'syntax': self.query_runner.syntax,
            'paused': self.paused,
            'pause_reason': self.pause_reason
        }

        if all:
            schema = get_configuration_schema_for_query_runner_type(self.type)
            self.options.set_schema(schema)
            d['options'] = self.options.to_dict(mask_secrets=True)
            d['queue_name'] = self.queue_name
            d['scheduled_queue_name'] = self.scheduled_queue_name
            d['groups'] = self.groups

        if with_permissions_for is not None:
            d['view_only'] = db.session.query(DataSourceGroup.view_only).filter(
                DataSourceGroup.group == with_permissions_for,
                DataSourceGroup.data_source == self).one()[0]

        return d

    def __str__(self):
        return text_type(self.name)

    @classmethod
    def create_with_group(cls, *args, **kwargs):
        data_source = cls(*args, **kwargs)
        data_source_group = DataSourceGroup(
            data_source=data_source,
            group=data_source.org.default_group)
        db.session.add_all([data_source, data_source_group])
        return data_source

    @classmethod
    def all(cls, org, group_ids=None):
        data_sources = cls.query.filter(cls.org == org).order_by(cls.id.asc())

        if group_ids:
            data_sources = data_sources.join(DataSourceGroup).filter(
                DataSourceGroup.group_id.in_(group_ids))

        return data_sources.distinct()

    @classmethod
    def get_by_id(cls, _id):
        return cls.query.filter(cls.id == _id).one()

    def delete(self):
        Query.query.filter(Query.data_source == self).update(dict(data_source_id=None, latest_query_data_id=None))
        QueryResult.query.filter(QueryResult.data_source == self).delete()
        res = db.session.delete(self)
        db.session.commit()
        return res

    def get_schema(self, refresh=False):
        key = "data_source:schema:{}".format(self.id)

        cache = None
        if not refresh:
            cache = redis_connection.get(key)

        if cache is None:
            query_runner = self.query_runner
            schema = sorted(query_runner.get_schema(get_stats=refresh), key=lambda t: t['name'])

            redis_connection.set(key, json_dumps(schema))
        else:
            schema = json_loads(cache)

        return schema

    def _pause_key(self):
        return 'ds:{}:pause'.format(self.id)

    @property
    def paused(self):
        return redis_connection.exists(self._pause_key())

    @property
    def pause_reason(self):
        return redis_connection.get(self._pause_key())

    def pause(self, reason=None):
        redis_connection.set(self._pause_key(), reason)

    def resume(self):
        redis_connection.delete(self._pause_key())

    def add_group(self, group, view_only=False):
        dsg = DataSourceGroup(group=group, data_source=self, view_only=view_only)
        db.session.add(dsg)
        return dsg

    def remove_group(self, group):
        db.session.query(DataSourceGroup).filter(
            DataSourceGroup.group == group,
            DataSourceGroup.data_source == self).delete()
        db.session.commit()

    def update_group_permission(self, group, view_only):
        dsg = DataSourceGroup.query.filter(
            DataSourceGroup.group == group,
            DataSourceGroup.data_source == self).one()
        dsg.view_only = view_only
        db.session.add(dsg)
        return dsg

    @property
    def query_runner(self):
        return get_query_runner(self.type, self.options)

    @classmethod
    def get_by_name(cls, name):
        return cls.query.filter(cls.name == name).one()

    # XXX examine call sites to see if a regular SQLA collection would work better
    @property
    def groups(self):
        groups = db.session.query(DataSourceGroup).filter(
            DataSourceGroup.data_source == self)
        return dict(map(lambda g: (g.group_id, g.view_only), groups))


class DataSourceGroup(db.Model):
    # XXX drop id, use datasource/group as PK
    id = Column(db.Integer, primary_key=True)
    data_source_id = Column(db.Integer, db.ForeignKey("data_sources.id"))
    data_source = db.relationship(DataSource, back_populates="data_source_groups")
    group_id = Column(db.Integer, db.ForeignKey("groups.id"))
    group = db.relationship(Group, back_populates="data_sources")
    view_only = Column(db.Boolean, default=False)

    __tablename__ = "data_source_groups"


@python_2_unicode_compatible
class QueryResult(db.Model, BelongsToOrgMixin):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey('organizations.id'))
    org = db.relationship(Organization)
    data_source_id = Column(db.Integer, db.ForeignKey("data_sources.id"))
    data_source = db.relationship(DataSource, backref=backref('query_results'))
    query_hash = Column(db.String(32), index=True)
    query_text = Column('query', db.Text)
    data = Column(db.Text)
    runtime = Column(postgresql.DOUBLE_PRECISION)
    retrieved_at = Column(db.DateTime(True))

    __tablename__ = 'query_results'

    def to_dict(self):
        return {
            'id': self.id,
            'query_hash': self.query_hash,
            'query': self.query_text,
            'data': json_loads(self.data),
            'data_source_id': self.data_source_id,
            'runtime': self.runtime,
            'retrieved_at': self.retrieved_at
        }

    @classmethod
    def unused(cls, days=7):
        age_threshold = datetime.datetime.now() - datetime.timedelta(days=days)

        unused_results = (db.session.query(QueryResult.id).filter(
            Query.id == None, QueryResult.retrieved_at < age_threshold)
            .outerjoin(Query))

        return unused_results

    @classmethod
    def get_latest(cls, data_source, query, max_age=0):
        query_hash = utils.gen_query_hash(query)

        if max_age == -1:
            q = db.session.query(QueryResult).filter(
                cls.query_hash == query_hash,
                cls.data_source == data_source).order_by(
                    QueryResult.retrieved_at.desc())
        else:
            q = db.session.query(QueryResult).filter(
                QueryResult.query_hash == query_hash,
                QueryResult.data_source == data_source,
                db.func.timezone('utc', QueryResult.retrieved_at) +
                datetime.timedelta(seconds=max_age) >=
                db.func.timezone('utc', db.func.now())
            ).order_by(QueryResult.retrieved_at.desc())

        return q.first()

    @classmethod
    def store_result(cls, org, data_source, query_hash, query, data, run_time, retrieved_at):
        query_result = cls(org_id=org,
                           query_hash=query_hash,
                           query_text=query,
                           runtime=run_time,
                           data_source=data_source,
                           retrieved_at=retrieved_at,
                           data=data)
        db.session.add(query_result)
        logging.info("Inserted query (%s) data; id=%s", query_hash, query_result.id)
        # TODO: Investigate how big an impact this select-before-update makes.
        queries = db.session.query(Query).filter(
            Query.query_hash == query_hash,
            Query.data_source == data_source)
        for q in queries:
            q.latest_query_data = query_result
            q.skip_updated_at = True
            db.session.add(q)
        query_ids = [q.id for q in queries]
        logging.info("Updated %s queries with result (%s).", len(query_ids), query_hash)

        return query_result, query_ids

    def __str__(self):
        return u"%d | %s | %s" % (self.id, self.query_hash, self.retrieved_at)

    @property
    def groups(self):
        return self.data_source.groups

    def make_csv_content(self):
        s = cStringIO.StringIO()

        query_data = json_loads(self.data)
        writer = csv.DictWriter(s, extrasaction="ignore", fieldnames=[col['name'] for col in query_data['columns']])
        writer.writer = utils.UnicodeWriter(s)
        writer.writeheader()
        for row in query_data['rows']:
            writer.writerow(row)

        return s.getvalue()

    def make_excel_content(self):
        s = cStringIO.StringIO()

        query_data = json_loads(self.data)
        book = xlsxwriter.Workbook(s, {'constant_memory': True})
        sheet = book.add_worksheet("result")

        column_names = []
        for (c, col) in enumerate(query_data['columns']):
            sheet.write(0, c, col['name'])
            column_names.append(col['name'])

        for (r, row) in enumerate(query_data['rows']):
            for (c, name) in enumerate(column_names):
                v = row.get(name)
                if isinstance(v, list):
                    v = str(v).encode('utf-8')
                sheet.write(r + 1, c, v)

        book.close()

        return s.getvalue()


def should_schedule_next(previous_iteration, now, schedule, failures):
    if schedule.isdigit():
        ttl = int(schedule)
        next_iteration = previous_iteration + datetime.timedelta(seconds=ttl)
    else:
        hour, minute = schedule.split(':')
        hour, minute = int(hour), int(minute)

        # The following logic is needed for cases like the following:
        # - The query scheduled to run at 23:59.
        # - The scheduler wakes up at 00:01.
        # - Using naive implementation of comparing timestamps, it will skip the execution.
        normalized_previous_iteration = previous_iteration.replace(hour=hour, minute=minute)
        if normalized_previous_iteration > previous_iteration:
            previous_iteration = normalized_previous_iteration - datetime.timedelta(days=1)

        next_iteration = (previous_iteration + datetime.timedelta(days=1)).replace(hour=hour, minute=minute)
    if failures:
        next_iteration += datetime.timedelta(minutes=2**failures)
    return now > next_iteration


@python_2_unicode_compatible
class Query(ChangeTrackingMixin, TimestampMixin, BelongsToOrgMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    version = Column(db.Integer, default=1)
    org_id = Column(db.Integer, db.ForeignKey('organizations.id'))
    org = db.relationship(Organization, backref="queries")
    data_source_id = Column(db.Integer, db.ForeignKey("data_sources.id"), nullable=True)
    data_source = db.relationship(DataSource, backref='queries')
    latest_query_data_id = Column(db.Integer, db.ForeignKey("query_results.id"), nullable=True)
    latest_query_data = db.relationship(QueryResult)
    name = Column(db.String(255))
    description = Column(db.String(4096), nullable=True)
    query_text = Column("query", db.Text)
    query_hash = Column(db.String(32))
    api_key = Column(db.String(40), default=lambda: generate_token(40))
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User, foreign_keys=[user_id])
    last_modified_by_id = Column(db.Integer, db.ForeignKey('users.id'), nullable=True)
    last_modified_by = db.relationship(User, backref="modified_queries",
                                       foreign_keys=[last_modified_by_id])
    is_archived = Column(db.Boolean, default=False, index=True)
    is_draft = Column(db.Boolean, default=True, index=True)
    schedule = Column(db.String(10), nullable=True)
    schedule_failures = Column(db.Integer, default=0)
    visualizations = db.relationship("Visualization", cascade="all, delete-orphan")
    options = Column(MutableDict.as_mutable(PseudoJSON), default={})
    search_vector = Column(TSVectorType('id', 'name', 'description', 'query',
                                        weights={'name': 'A',
                                                 'id': 'B',
                                                 'description': 'C',
                                                 'query': 'D'}),
                           nullable=True)
    tags = Column('tags', MutableList.as_mutable(postgresql.ARRAY(db.Unicode)), nullable=True)

    query_class = SearchBaseQuery
    __tablename__ = 'queries'
    __mapper_args__ = {
        "version_id_col": version,
        'version_id_generator': False
    }

    def archive(self, user=None):
        db.session.add(self)
        self.is_archived = True
        self.schedule = None

        for vis in self.visualizations:
            for w in vis.widgets:
                db.session.delete(w)

        for a in self.alerts:
            db.session.delete(a)

        if user:
            self.record_changes(user)

    @classmethod
    def create(cls, **kwargs):
        query = cls(**kwargs)
        db.session.add(Visualization(query_rel=query,
                                     name="Table",
                                     description='',
                                     type="TABLE",
                                     options="{}"))
        return query

    @classmethod
    def all_queries(cls, group_ids, user_id=None, drafts=False):
        query_ids = (
            db.session
            .query(distinct(cls.id))
            .join(
                DataSourceGroup,
                Query.data_source_id == DataSourceGroup.data_source_id
            )
            .filter(Query.is_archived == False)
            .filter(DataSourceGroup.group_id.in_(group_ids))
        )
        q = (
            cls
            .query
            .options(
                joinedload(Query.user),
                joinedload(
                    Query.latest_query_data
                ).load_only(
                    'runtime',
                    'retrieved_at',
                )
            )
            .filter(cls.id.in_(query_ids))
            # Adding outer joins to be able to order by relationship
            .outerjoin(User, User.id == Query.user_id)
            .outerjoin(
                QueryResult,
                QueryResult.id == Query.latest_query_data_id
            )
            .options(
                contains_eager(Query.user),
                contains_eager(Query.latest_query_data),
            )
            .order_by(Query.created_at.desc())
        )

        if not drafts:
            q = q.filter(
                or_(
                    Query.is_draft == False,
                    Query.user_id == user_id
                )
            )
        return q

    @classmethod
    def favorites(cls, user, base_query=None):
        if base_query == None:
            base_query = cls.all_queries(user.group_ids, user.id, drafts=True)
        return base_query.join((Favorite, and_(Favorite.object_type==u'Query', Favorite.object_id==Query.id))).filter(Favorite.user_id==user.id)

    @classmethod
    def all_tags(cls, user, include_drafts=False):
        queries = cls.all_queries(
            group_ids=user.group_ids,
            user_id=user.id,
            drafts=include_drafts,
        )

        tag_column = func.unnest(cls.tags).label('tag')
        usage_count = func.count(1).label('usage_count')

        query = (
            db.session
            .query(tag_column, usage_count)
            .group_by(tag_column)
            .filter(Query.id.in_(queries.options(load_only('id'))))
            .order_by(usage_count.desc())
        )
        return query

    @classmethod
    def by_user(cls, user):
        return cls.all_queries(user.group_ids, user.id).filter(Query.user == user)

    @classmethod
    def outdated_queries(cls):
        queries = (db.session.query(Query)
                   .options(joinedload(Query.latest_query_data).load_only('retrieved_at'))
                   .filter(Query.schedule != None)
                   .order_by(Query.id))

        now = utils.utcnow()
        outdated_queries = {}
        scheduled_queries_executions.refresh()

        for query in queries:
            if query.latest_query_data:
                retrieved_at = query.latest_query_data.retrieved_at
            else:
                retrieved_at = now

            retrieved_at = scheduled_queries_executions.get(query.id) or retrieved_at

            if should_schedule_next(retrieved_at, now, query.schedule, query.schedule_failures):
                key = "{}:{}".format(query.query_hash, query.data_source_id)
                outdated_queries[key] = query

        return outdated_queries.values()

    @classmethod
    def search(cls, term, group_ids, user_id=None, include_drafts=False, limit=None):
        all_queries = cls.all_queries(group_ids, user_id=user_id, drafts=include_drafts)
        # sort the result using the weight as defined in the search vector column
        return all_queries.search(term, sort=True).limit(limit)

    @classmethod
    def search_by_user(cls, term, user, limit=None):
        return cls.by_user(user).search(term, sort=True).limit(limit)

    @classmethod
    def recent(cls, group_ids, user_id=None, limit=20):
        query = (cls.query
                 .filter(Event.created_at > (db.func.current_date() - 7))
                 .join(Event, Query.id == Event.object_id.cast(db.Integer))
                 .join(DataSourceGroup, Query.data_source_id == DataSourceGroup.data_source_id)
                 .filter(
                     Event.action.in_(['edit', 'execute', 'edit_name',
                                       'edit_description', 'view_source']),
                     Event.object_id != None,
                     Event.object_type == 'query',
                     DataSourceGroup.group_id.in_(group_ids),
                     or_(Query.is_draft == False, Query.user_id == user_id),
                     Query.is_archived == False)
                 .group_by(Event.object_id, Query.id)
                 .order_by(db.desc(db.func.count(0))))

        if user_id:
            query = query.filter(Event.user_id == user_id)

        query = query.limit(limit)

        return query

    @classmethod
    def get_by_id(cls, _id):
        return cls.query.filter(cls.id == _id).one()

    def fork(self, user):
        forked_list = ['org', 'data_source', 'latest_query_data', 'description',
                       'query_text', 'query_hash', 'options']
        kwargs = {a: getattr(self, a) for a in forked_list}
        forked_query = Query.create(name=u'Copy of (#{}) {}'.format(self.id, self.name),
                                    user=user, **kwargs)

        for v in self.visualizations:
            if v.type == 'TABLE':
                continue
            forked_v = v.copy()
            forked_v['query_rel'] = forked_query
            forked_query.visualizations.append(Visualization(**forked_v))
        db.session.add(forked_query)
        return forked_query

    @property
    def runtime(self):
        return self.latest_query_data.runtime

    @property
    def retrieved_at(self):
        return self.latest_query_data.retrieved_at

    @property
    def groups(self):
        if self.data_source is None:
            return {}

        return self.data_source.groups

    @hybrid_property
    def lowercase_name(self):
        "Optional property useful for sorting purposes."
        return self.name.lower()

    @lowercase_name.expression
    def lowercase_name(cls):
        "The SQLAlchemy expression for the property above."
        return func.lower(cls.name)

    def __str__(self):
        return text_type(self.id)

    def __repr__(self):
        return '<Query %s: "%s">' % (self.id, self.name or 'untitled')


@vectorizer(db.Integer)
def integer_vectorizer(column):
    return db.func.cast(column, db.Text)


@listens_for(Query.query_text, 'set')
def gen_query_hash(target, val, oldval, initiator):
    target.query_hash = utils.gen_query_hash(val)
    target.schedule_failures = 0


@listens_for(Query.user_id, 'set')
def query_last_modified_by(target, val, oldval, initiator):
    target.last_modified_by_id = val


class Favorite(TimestampMixin, db.Model):
    __tablename__ = "favorites"

    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey("organizations.id"))

    object_type = Column(db.Unicode(255))
    object_id = Column(db.Integer)
    object = generic_relationship(object_type, object_id)

    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User, backref='favorites')

    __table_args__ = (UniqueConstraint("object_type", "object_id", "user_id", name="unique_favorite"),)

    @classmethod
    def is_favorite(cls, user, object):
        return cls.query.filter(cls.object == object, cls.user_id == user).count() > 0

    @classmethod
    def are_favorites(cls, user, objects):
        objects = list(objects)
        if not objects:
            return []

        object_type = text_type(objects[0].__class__.__name__)
        return map(lambda fav: fav.object_id, cls.query.filter(cls.object_id.in_(map(lambda o: o.id, objects)), cls.object_type == object_type, cls.user_id == user))


class AccessPermission(GFKBase, db.Model):
    id = Column(db.Integer, primary_key=True)
    # 'object' defined in GFKBase
    access_type = Column(db.String(255))
    grantor_id = Column(db.Integer, db.ForeignKey("users.id"))
    grantor = db.relationship(User, backref='grantor', foreign_keys=[grantor_id])
    grantee_id = Column(db.Integer, db.ForeignKey("users.id"))
    grantee = db.relationship(User, backref='grantee', foreign_keys=[grantee_id])

    __tablename__ = 'access_permissions'

    @classmethod
    def grant(cls, obj, access_type, grantee, grantor):
        grant = cls.query.filter(cls.object_type == obj.__tablename__,
                                 cls.object_id == obj.id,
                                 cls.access_type == access_type,
                                 cls.grantee == grantee,
                                 cls.grantor == grantor).one_or_none()

        if not grant:
            grant = cls(object_type=obj.__tablename__,
                        object_id=obj.id,
                        access_type=access_type,
                        grantee=grantee,
                        grantor=grantor)
            db.session.add(grant)

        return grant

    @classmethod
    def revoke(cls, obj, grantee, access_type=None):
        permissions = cls._query(obj, access_type, grantee)
        return permissions.delete()

    @classmethod
    def find(cls, obj, access_type=None, grantee=None, grantor=None):
        return cls._query(obj, access_type, grantee, grantor)

    @classmethod
    def exists(cls, obj, access_type, grantee):
        return cls.find(obj, access_type, grantee).count() > 0

    @classmethod
    def _query(cls, obj, access_type=None, grantee=None, grantor=None):
        q = cls.query.filter(cls.object_id == obj.id,
                             cls.object_type == obj.__tablename__)

        if access_type:
            q = q.filter(AccessPermission.access_type == access_type)

        if grantee:
            q = q.filter(AccessPermission.grantee == grantee)

        if grantor:
            q = q.filter(AccessPermission.grantor == grantor)

        return q

    def to_dict(self):
        d = {
            'id': self.id,
            'object_id': self.object_id,
            'object_type': self.object_type,
            'access_type': self.access_type,
            'grantor': self.grantor_id,
            'grantee': self.grantee_id
        }
        return d


class Change(GFKBase, db.Model):
    id = Column(db.Integer, primary_key=True)
    # 'object' defined in GFKBase
    object_version = Column(db.Integer, default=0)
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User, backref='changes')
    change = Column(PseudoJSON)
    created_at = Column(db.DateTime(True), default=db.func.now())

    __tablename__ = 'changes'

    def to_dict(self, full=True):
        d = {
            'id': self.id,
            'object_id': self.object_id,
            'object_type': self.object_type,
            'change_type': self.change_type,
            'object_version': self.object_version,
            'change': self.change,
            'created_at': self.created_at
        }

        if full:
            d['user'] = self.user.to_dict()
        else:
            d['user_id'] = self.user_id

        return d

    @classmethod
    def last_change(cls, obj):
        return db.session.query(cls).filter(
            cls.object_id == obj.id,
            cls.object_type == obj.__class__.__tablename__).order_by(
                cls.object_version.desc()).first()


class Alert(TimestampMixin, db.Model):
    UNKNOWN_STATE = 'unknown'
    OK_STATE = 'ok'
    TRIGGERED_STATE = 'triggered'

    id = Column(db.Integer, primary_key=True)
    name = Column(db.String(255))
    query_id = Column(db.Integer, db.ForeignKey("queries.id"))
    query_rel = db.relationship(Query, backref=backref('alerts', cascade="all"))
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User, backref='alerts')
    options = Column(MutableDict.as_mutable(PseudoJSON))
    state = Column(db.String(255), default=UNKNOWN_STATE)
    subscriptions = db.relationship("AlertSubscription", cascade="all, delete-orphan")
    last_triggered_at = Column(db.DateTime(True), nullable=True)
    rearm = Column(db.Integer, nullable=True)

    __tablename__ = 'alerts'

    @classmethod
    def all(cls, group_ids):
        return db.session.query(Alert)\
            .options(joinedload(Alert.user), joinedload(Alert.query_rel))\
            .join(Query)\
            .join(DataSourceGroup, DataSourceGroup.data_source_id == Query.data_source_id)\
            .filter(DataSourceGroup.group_id.in_(group_ids))

    @classmethod
    def get_by_id_and_org(cls, id, org):
        return db.session.query(Alert).join(Query).filter(Alert.id == id, Query.org == org).one()

    def evaluate(self):
        data = json_loads(self.query_rel.latest_query_data.data)
        if data['rows']:
            value = data['rows'][0][self.options['column']]
            op = self.options['op']

            if op == 'greater than' and value > self.options['value']:
                new_state = self.TRIGGERED_STATE
            elif op == 'less than' and value < self.options['value']:
                new_state = self.TRIGGERED_STATE
            elif op == 'equals' and value == self.options['value']:
                new_state = self.TRIGGERED_STATE
            else:
                new_state = self.OK_STATE
        else:
            new_state = self.UNKNOWN_STATE

        return new_state

    def subscribers(self):
        return User.query.join(AlertSubscription).filter(AlertSubscription.alert == self)

    @property
    def groups(self):
        return self.query_rel.groups


def generate_slug(ctx):
    slug = utils.slugify(ctx.current_parameters['name'])
    tries = 1
    while Dashboard.query.filter(Dashboard.slug == slug).first() is not None:
        slug = utils.slugify(ctx.current_parameters['name']) + "_" + str(tries)
        tries += 1
    return slug


@python_2_unicode_compatible
class Dashboard(ChangeTrackingMixin, TimestampMixin, BelongsToOrgMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    version = Column(db.Integer)
    org_id = Column(db.Integer, db.ForeignKey("organizations.id"))
    org = db.relationship(Organization, backref="dashboards")
    slug = Column(db.String(140), index=True, default=generate_slug)
    name = Column(db.String(100))
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User)
    # layout is no longer used, but kept so we know how to render old dashboards.
    layout = Column(db.Text)
    dashboard_filters_enabled = Column(db.Boolean, default=False)
    is_archived = Column(db.Boolean, default=False, index=True)
    is_draft = Column(db.Boolean, default=True, index=True)
    widgets = db.relationship('Widget', backref='dashboard', lazy='dynamic')
    tags = Column('tags', MutableList.as_mutable(postgresql.ARRAY(db.Unicode)), nullable=True)

    __tablename__ = 'dashboards'
    __mapper_args__ = {
        "version_id_col": version
    }

    @classmethod
    def all(cls, org, group_ids, user_id):
        query = (
            Dashboard.query
            .options(joinedload(Dashboard.user))
            .outerjoin(Widget)
            .outerjoin(Visualization)
            .outerjoin(Query)
            .outerjoin(DataSourceGroup, Query.data_source_id == DataSourceGroup.data_source_id)
            .filter(
                Dashboard.is_archived == False,
                (DataSourceGroup.group_id.in_(group_ids) |
                 (Dashboard.user_id == user_id) |
                 ((Widget.dashboard != None) & (Widget.visualization == None))),
                Dashboard.org == org)
            .distinct())

        query = query.filter(or_(Dashboard.user_id == user_id, Dashboard.is_draft == False))

        return query

    @classmethod
    def search(cls, org, groups_ids, user_id, search_term):
        # TODO: switch to FTS
        return cls.all(org, groups_ids, user_id).filter(cls.name.ilike(u'%{}%'.format(search_term)))

    @classmethod
    def all_tags(cls, org, user):
        dashboards = cls.all(org, user.group_ids, user.id)

        tag_column = func.unnest(cls.tags).label('tag')
        usage_count = func.count(1).label('usage_count')

        query = (
            db.session
            .query(tag_column, usage_count)
            .group_by(tag_column)
            .filter(Dashboard.id.in_(dashboards.options(load_only('id'))))
            .order_by(usage_count.desc())
        )
        return query

    @classmethod
    def favorites(cls, user, base_query=None):
        if base_query is None:
            base_query = cls.all(user.org, user.group_ids, user.id)
        return base_query.join((Favorite, and_(Favorite.object_type==u'Dashboard', Favorite.object_id==Dashboard.id))).filter(Favorite.user_id==user.id)

    @classmethod
    def get_by_slug_and_org(cls, slug, org):
        return cls.query.filter(cls.slug == slug, cls.org == org).one()

    @hybrid_property
    def lowercase_name(self):
        "Optional property useful for sorting purposes."
        return self.name.lower()

    @lowercase_name.expression
    def lowercase_name(cls):
        "The SQLAlchemy expression for the property above."
        return func.lower(cls.name)

    def __str__(self):
        return u"%s=%s" % (self.id, self.name)


@python_2_unicode_compatible
class Visualization(TimestampMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    type = Column(db.String(100))
    query_id = Column(db.Integer, db.ForeignKey("queries.id"))
    # query_rel and not query, because db.Model already has query defined.
    query_rel = db.relationship(Query, back_populates='visualizations')
    name = Column(db.String(255))
    description = Column(db.String(4096), nullable=True)
    options = Column(db.Text)

    __tablename__ = 'visualizations'

    @classmethod
    def get_by_id_and_org(cls, visualization_id, org):
        return db.session.query(Visualization).join(Query).filter(
            cls.id == visualization_id,
            Query.org == org).one()

    def __str__(self):
        return u"%s %s" % (self.id, self.type)

    def copy(self):
        return {
            'type': self.type,
            'name': self.name,
            'description': self.description,
            'options': self.options
        }


@python_2_unicode_compatible
class Widget(TimestampMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    visualization_id = Column(db.Integer, db.ForeignKey('visualizations.id'), nullable=True)
    visualization = db.relationship(Visualization, backref='widgets')
    text = Column(db.Text, nullable=True)
    width = Column(db.Integer)
    options = Column(db.Text)
    dashboard_id = Column(db.Integer, db.ForeignKey("dashboards.id"), index=True)

    __tablename__ = 'widgets'

    def __str__(self):
        return u"%s" % self.id

    @classmethod
    def get_by_id_and_org(cls, widget_id, org):
        return db.session.query(cls).join(Dashboard).filter(cls.id == widget_id, Dashboard.org == org).one()


@python_2_unicode_compatible
class Event(db.Model):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey("organizations.id"))
    org = db.relationship(Organization, back_populates="events")
    user_id = Column(db.Integer, db.ForeignKey("users.id"), nullable=True)
    user = db.relationship(User, backref="events")
    action = Column(db.String(255))
    object_type = Column(db.String(255))
    object_id = Column(db.String(255), nullable=True)
    additional_properties = Column(MutableDict.as_mutable(PseudoJSON), nullable=True, default={})
    created_at = Column(db.DateTime(True), default=db.func.now())

    __tablename__ = 'events'

    def __str__(self):
        return u"%s,%s,%s,%s" % (self.user_id, self.action, self.object_type, self.object_id)

    def to_dict(self):
        return {
            'org_id': self.org_id,
            'user_id': self.user_id,
            'action': self.action,
            'object_type': self.object_type,
            'object_id': self.object_id,
            'additional_properties': self.additional_properties,
            'created_at': self.created_at.isoformat()
        }

    @classmethod
    def record(cls, event):
        org_id = event.pop('org_id')
        user_id = event.pop('user_id', None)
        action = event.pop('action')
        object_type = event.pop('object_type')
        object_id = event.pop('object_id', None)

        created_at = datetime.datetime.utcfromtimestamp(event.pop('timestamp'))

        event = cls(org_id=org_id, user_id=user_id, action=action,
                    object_type=object_type, object_id=object_id,
                    additional_properties=event,
                    created_at=created_at)
        db.session.add(event)
        return event


class ApiKey(TimestampMixin, GFKBase, db.Model):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey("organizations.id"))
    org = db.relationship(Organization)
    api_key = Column(db.String(255), index=True, default=lambda: generate_token(40))
    active = Column(db.Boolean, default=True)
    # 'object' provided by GFKBase
    created_by_id = Column(db.Integer, db.ForeignKey("users.id"), nullable=True)
    created_by = db.relationship(User)

    __tablename__ = 'api_keys'
    __table_args__ = (db.Index('api_keys_object_type_object_id', 'object_type', 'object_id'),)

    @classmethod
    def get_by_api_key(cls, api_key):
        return cls.query.filter(cls.api_key == api_key, cls.active == True).one()

    @classmethod
    def get_by_object(cls, object):
        return cls.query.filter(cls.object_type == object.__class__.__tablename__, cls.object_id == object.id, cls.active == True).first()

    @classmethod
    def create_for_object(cls, object, user):
        k = cls(org=user.org, object=object, created_by=user)
        db.session.add(k)
        return k


@python_2_unicode_compatible
class NotificationDestination(BelongsToOrgMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey("organizations.id"))
    org = db.relationship(Organization, backref="notification_destinations")
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User, backref="notification_destinations")
    name = Column(db.String(255))
    type = Column(db.String(255))
    options = Column(ConfigurationContainer.as_mutable(Configuration))
    created_at = Column(db.DateTime(True), default=db.func.now())

    __tablename__ = 'notification_destinations'
    __table_args__ = (db.Index('notification_destinations_org_id_name', 'org_id',
                               'name', unique=True),)

    def to_dict(self, all=False):
        d = {
            'id': self.id,
            'name': self.name,
            'type': self.type,
            'icon': self.destination.icon()
        }

        if all:
            schema = get_configuration_schema_for_destination_type(self.type)
            self.options.set_schema(schema)
            d['options'] = self.options.to_dict(mask_secrets=True)

        return d

    def __str__(self):
        return text_type(self.name)

    @property
    def destination(self):
        return get_destination(self.type, self.options)

    @classmethod
    def all(cls, org):
        notification_destinations = cls.query.filter(cls.org == org).order_by(cls.id.asc())

        return notification_destinations

    def notify(self, alert, query, user, new_state, app, host):
        schema = get_configuration_schema_for_destination_type(self.type)
        self.options.set_schema(schema)
        return self.destination.notify(alert, query, user, new_state,
                                       app, host, self.options)


class AlertSubscription(TimestampMixin, db.Model):
    id = Column(db.Integer, primary_key=True)
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User)
    destination_id = Column(db.Integer,
                               db.ForeignKey("notification_destinations.id"),
                               nullable=True)
    destination = db.relationship(NotificationDestination)
    alert_id = Column(db.Integer, db.ForeignKey("alerts.id"))
    alert = db.relationship(Alert, back_populates="subscriptions")

    __tablename__ = 'alert_subscriptions'
    __table_args__ = (db.Index('alert_subscriptions_destination_id_alert_id',
                               'destination_id', 'alert_id', unique=True),)

    def to_dict(self):
        d = {
            'id': self.id,
            'user': self.user.to_dict(),
            'alert_id': self.alert_id
        }

        if self.destination:
            d['destination'] = self.destination.to_dict()

        return d

    @classmethod
    def all(cls, alert_id):
        return AlertSubscription.query.join(User).filter(AlertSubscription.alert_id == alert_id)

    def notify(self, alert, query, user, new_state, app, host):
        if self.destination:
            return self.destination.notify(alert, query, user, new_state,
                                           app, host)
        else:
            # User email subscription, so create an email destination object
            config = {'addresses': self.user.email}
            schema = get_configuration_schema_for_destination_type('email')
            options = ConfigurationContainer(config, schema)
            destination = get_destination('email', options)
            return destination.notify(alert, query, user, new_state, app, host, options)


class QuerySnippet(TimestampMixin, db.Model, BelongsToOrgMixin):
    id = Column(db.Integer, primary_key=True)
    org_id = Column(db.Integer, db.ForeignKey("organizations.id"))
    org = db.relationship(Organization, backref="query_snippets")
    trigger = Column(db.String(255), unique=True)
    description = Column(db.Text)
    user_id = Column(db.Integer, db.ForeignKey("users.id"))
    user = db.relationship(User, backref="query_snippets")
    snippet = Column(db.Text)
    __tablename__ = 'query_snippets'

    @classmethod
    def all(cls, org):
        return cls.query.filter(cls.org == org)

    def to_dict(self):
        d = {
            'id': self.id,
            'trigger': self.trigger,
            'description': self.description,
            'snippet': self.snippet,
            'user': self.user.to_dict(),
            'updated_at': self.updated_at,
            'created_at': self.created_at
        }

        return d


_gfk_types = {'queries': Query, 'dashboards': Dashboard}


def init_db():
    default_org = Organization(name="Default", slug='default', settings={})
    admin_group = Group(name='admin', permissions=['admin', 'super_admin'], org=default_org, type=Group.BUILTIN_GROUP)
    default_group = Group(name='default', permissions=Group.DEFAULT_PERMISSIONS, org=default_org, type=Group.BUILTIN_GROUP)

    db.session.add_all([default_org, admin_group, default_group])
    # XXX remove after fixing User.group_ids
    db.session.commit()
    return default_org, admin_group, default_group
<EOF>
<BOF>
import sys
import logging
import urlparse
import urllib
import redis
from flask import Flask
from flask_sslify import SSLify
from werkzeug.contrib.fixers import ProxyFix
from werkzeug.routing import BaseConverter
from statsd import StatsClient
from flask_mail import Mail
from flask_limiter import Limiter
from flask_limiter.util import get_ipaddr
from flask_migrate import Migrate

from redash import settings
from redash.query_runner import import_query_runners
from redash.destinations import import_destinations


__version__ = '6.0.0-beta'


def setup_logging():
    handler = logging.StreamHandler(sys.stdout if settings.LOG_STDOUT else sys.stderr)
    formatter = logging.Formatter(settings.LOG_FORMAT)
    handler.setFormatter(formatter)
    logging.getLogger().addHandler(handler)
    logging.getLogger().setLevel(settings.LOG_LEVEL)

    # Make noisy libraries less noisy
    if settings.LOG_LEVEL != "DEBUG":
        logging.getLogger("passlib").setLevel("ERROR")
        logging.getLogger("requests.packages.urllib3").setLevel("ERROR")
        logging.getLogger("snowflake.connector").setLevel("ERROR")
        logging.getLogger('apiclient').setLevel("ERROR")


def create_redis_connection():
    logging.debug("Creating Redis connection (%s)", settings.REDIS_URL)
    redis_url = urlparse.urlparse(settings.REDIS_URL)

    if redis_url.scheme == 'redis+socket':
        qs = urlparse.parse_qs(redis_url.query)
        if 'virtual_host' in qs:
            db = qs['virtual_host'][0]
        else:
            db = 0

        r = redis.StrictRedis(unix_socket_path=redis_url.path, db=db)
    else:
        if redis_url.path:
            redis_db = redis_url.path[1]
        else:
            redis_db = 0
        # Redis passwords might be quoted with special characters
        redis_password = redis_url.password and urllib.unquote(redis_url.password)
        r = redis.StrictRedis(host=redis_url.hostname, port=redis_url.port, db=redis_db, password=redis_password)

    return r


setup_logging()
redis_connection = create_redis_connection()
mail = Mail()
migrate = Migrate()
mail.init_mail(settings.all_settings())
statsd_client = StatsClient(host=settings.STATSD_HOST, port=settings.STATSD_PORT, prefix=settings.STATSD_PREFIX)
limiter = Limiter(key_func=get_ipaddr, storage_uri=settings.LIMITER_STORAGE)

import_query_runners(settings.QUERY_RUNNERS)
import_destinations(settings.DESTINATIONS)

from redash.version_check import reset_new_version_status
reset_new_version_status()


class SlugConverter(BaseConverter):
    def to_python(self, value):
        # This is ay workaround for when we enable multi-org and some files are being called by the index rule:
        # for path in settings.STATIC_ASSETS_PATHS:
        #     full_path = safe_join(path, value)
        #     if os.path.isfile(full_path):
        #         raise ValidationError()

        return value

    def to_url(self, value):
        return value


def create_app(load_admin=True):
    from redash import extensions, handlers
    from redash.handlers.webpack import configure_webpack
    from redash.handlers import chrome_logger
    from redash.admin import init_admin
    from redash.models import db
    from redash.authentication import setup_authentication
    from redash.metrics.request import provision_app

    app = Flask(__name__,
                template_folder=settings.STATIC_ASSETS_PATH,
                static_folder=settings.STATIC_ASSETS_PATH,
                static_path='/static')

    # Make sure we get the right referral address even behind proxies like nginx.
    app.wsgi_app = ProxyFix(app.wsgi_app, settings.PROXIES_COUNT)
    app.url_map.converters['org_slug'] = SlugConverter

    if settings.ENFORCE_HTTPS:
        SSLify(app, skips=['ping'])

    if settings.SENTRY_DSN:
        from raven import Client
        from raven.contrib.flask import Sentry
        from raven.handlers.logging import SentryHandler

        client = Client(settings.SENTRY_DSN, release=__version__, install_logging_hook=False)
        sentry = Sentry(app, client=client)
        sentry.client.release = __version__

        sentry_handler = SentryHandler(client=client)
        sentry_handler.setLevel(logging.ERROR)
        logging.getLogger().addHandler(sentry_handler)

    # configure our database
    app.config['SQLALCHEMY_DATABASE_URI'] = settings.SQLALCHEMY_DATABASE_URI
    app.config.update(settings.all_settings())

    provision_app(app)
    db.init_app(app)
    migrate.init_app(app, db)
    if load_admin:
        init_admin(app)
    mail.init_app(app)
    setup_authentication(app)
    limiter.init_app(app)
    handlers.init_app(app)
    configure_webpack(app)
    extensions.init_extensions(app)
    chrome_logger.init_app(app)

    return app
<EOF>
<BOF>
import os
from pkg_resources import iter_entry_points, resource_isdir, resource_listdir


def init_extensions(app):
    """
    Load the Redash extensions for the given Redash Flask app.
    """
    if not hasattr(app, 'redash_extensions'):
        app.redash_extensions = {}

    for entry_point in iter_entry_points('redash.extensions'):
        app.logger.info('Loading Redash extension %s.', entry_point.name)
        try:
            extension = entry_point.load()
            app.redash_extensions[entry_point.name] = {
                "entry_function": extension(app),
                "resources_list": []
            }
        except ImportError:
            app.logger.info('%s does not have a callable and will not be loaded.', entry_point.name)
            (root_module, _) = os.path.splitext(entry_point.module_name)
            content_folder_relative = os.path.join(entry_point.name, 'bundle')

            # If it's a frontend extension only, store a list of files in the bundle directory.
            if resource_isdir(root_module, content_folder_relative):
                app.redash_extensions[entry_point.name] = {
                    "entry_function": None,
                    "resources_list": resource_listdir(root_module, content_folder_relative)
                }
<EOF>
<BOF>
from __future__ import absolute_import

from datetime import timedelta
from random import randint

from flask import current_app

from celery import Celery
from celery.schedules import crontab
from celery.signals import worker_process_init
from redash import __version__, create_app, settings
from redash.metrics import celery as celery_metrics

celery = Celery('redash',
                broker=settings.CELERY_BROKER,
                include='redash.tasks')

celery_schedule = {
    'refresh_queries': {
        'task': 'redash.tasks.refresh_queries',
        'schedule': timedelta(seconds=30)
    },
    'cleanup_tasks': {
        'task': 'redash.tasks.cleanup_tasks',
        'schedule': timedelta(minutes=5)
    },
    'refresh_schemas': {
        'task': 'redash.tasks.refresh_schemas',
        'schedule': timedelta(minutes=settings.SCHEMAS_REFRESH_SCHEDULE)
    }
}

if settings.VERSION_CHECK:
    celery_schedule['version_check'] = {
        'task': 'redash.tasks.version_check',
        # We need to schedule the version check to run at a random hour/minute, to spread the requests from all users
        # evenly.
        'schedule': crontab(minute=randint(0, 59), hour=randint(0, 23))
    }

if settings.QUERY_RESULTS_CLEANUP_ENABLED:
    celery_schedule['cleanup_query_results'] = {
        'task': 'redash.tasks.cleanup_query_results',
        'schedule': timedelta(minutes=5)
    }

celery.conf.update(result_backend=settings.CELERY_RESULT_BACKEND,
                   beat_schedule=celery_schedule,
                   timezone='UTC',
                   result_expires=settings.CELERY_RESULT_EXPIRES,
                   worker_log_format=settings.CELERYD_WORKER_LOG_FORMAT,
                   worker_task_log_format=settings.CELERYD_WORKER_TASK_LOG_FORMAT)

if settings.SENTRY_DSN:
    from raven import Client
    from raven.contrib.celery import register_signal

    client = Client(settings.SENTRY_DSN, release=__version__, install_logging_hook=False)
    register_signal(client)


# Create a new Task base class, that pushes a new Flask app context to allow DB connections if needed.
TaskBase = celery.Task


class ContextTask(TaskBase):
    abstract = True

    def __call__(self, *args, **kwargs):
        with current_app.app_context():
            return TaskBase.__call__(self, *args, **kwargs)


celery.Task = ContextTask


# Create Flask app after forking a new worker, to make sure no resources are shared between processes.
@worker_process_init.connect
def init_celery_flask_app(**kwargs):
    app = create_app()
    app.app_context().push()


# Hook for extensions to add periodic tasks.
@celery.on_after_configure.connect
def add_periodic_tasks(sender, **kwargs):
    app = create_app()
    periodic_tasks = getattr(app, 'periodic_tasks', {})
    for params in periodic_tasks.values():
        sender.add_periodic_task(**params)
<EOF>
<BOF>
from redash import redis_connection, models, __version__, settings


def get_redis_status():
    info = redis_connection.info()
    return {'redis_used_memory': info['used_memory'], 'redis_used_memory_human': info['used_memory_human']}


def get_object_counts():
    status = {}
    status['queries_count'] = models.db.session.query(models.Query).count()
    if settings.FEATURE_SHOW_QUERY_RESULTS_COUNT:
        status['query_results_count'] = models.db.session.query(models.QueryResult).count()
        status['unused_query_results_count'] = models.QueryResult.unused().count()
    status['dashboards_count'] = models.Dashboard.query.count()
    status['widgets_count'] = models.Widget.query.count()
    return status


def get_queues():
    queues = {}
    for ds in models.DataSource.query:
        for queue in (ds.queue_name, ds.scheduled_queue_name):
            queues.setdefault(queue, set())
            queues[queue].add(ds.name)

    return queues


def get_queues_status():
    queues = get_queues()

    for queue, sources in queues.iteritems():
        queues[queue] = {
            'data_sources': ', '.join(sources),
            'size': redis_connection.llen(queue)
        }
    
    queues['celery'] = {
        'size': redis_connection.llen('celery'),
        'data_sources': ''
    }

    return queues


def get_db_sizes():
    database_metrics = []
    queries = [
        ['Query Results Size', "select pg_total_relation_size('query_results') as size from (select 1) as a"],
        ['Redash DB Size', "select pg_database_size('postgres') as size"]
    ]
    for query_name, query in queries:
        result = models.db.session.execute(query).first()
        database_metrics.append([query_name, result[0]])

    return database_metrics


def get_status():
    status = {
        'version': __version__,
        'workers': []
    }
    status.update(get_redis_status())
    status.update(get_object_counts())
    status['manager'] = redis_connection.hgetall('redash:status')
    status['manager']['queues'] = get_queues_status()
    status['database_metrics'] = {}
    status['database_metrics']['metrics'] = get_db_sizes()

    return status
<EOF>
<BOF>
import functools

from flask_login import current_user
from flask_restful import abort
from funcy import flatten

view_only = True
not_view_only = False

ACCESS_TYPE_VIEW = 'view'
ACCESS_TYPE_MODIFY = 'modify'
ACCESS_TYPE_DELETE = 'delete'

ACCESS_TYPES = (ACCESS_TYPE_VIEW, ACCESS_TYPE_MODIFY, ACCESS_TYPE_DELETE)


def has_access(object_groups, user, need_view_only):
    if 'admin' in user.permissions:
        return True

    matching_groups = set(object_groups.keys()).intersection(user.group_ids)

    if not matching_groups:
        return False

    required_level = 1 if need_view_only else 2

    group_level = 1 if all(flatten([object_groups[group] for group in matching_groups])) else 2

    return required_level <= group_level


def require_access(object_groups, user, need_view_only):
    if not has_access(object_groups, user, need_view_only):
        abort(403)


class require_permissions(object):
    def __init__(self, permissions):
        self.permissions = permissions

    def __call__(self, fn):
        @functools.wraps(fn)
        def decorated(*args, **kwargs):
            has_permissions = current_user.has_permissions(self.permissions)

            if has_permissions:
                return fn(*args, **kwargs)
            else:
                abort(403)

        return decorated


def require_permission(permission):
    return require_permissions((permission,))


def require_admin(fn):
    return require_permission('admin')(fn)


def require_super_admin(fn):
    return require_permission('super_admin')(fn)


def has_permission_or_owner(permission, object_owner_id):
    return int(object_owner_id) == current_user.id or current_user.has_permission(permission)


def is_admin_or_owner(object_owner_id):
    return has_permission_or_owner('admin', object_owner_id)


def require_permission_or_owner(permission, object_owner_id):
    if not has_permission_or_owner(permission, object_owner_id):
        abort(403)


def require_admin_or_owner(object_owner_id):
    if not is_admin_or_owner(object_owner_id):
        abort(403, message="You don't have permission to edit this resource.")


def can_modify(obj, user):
    return is_admin_or_owner(obj.user_id) or user.has_access(obj, ACCESS_TYPE_MODIFY)


def require_object_modify_permission(obj, user):
    if not can_modify(obj, user):
        abort(403)
<EOF>
<BOF>
import logging
import requests
import semver

from redash import __version__ as current_version
from redash import redis_connection
from redash.utils import json_dumps

REDIS_KEY = "new_version_available"


def run_version_check():
    logging.info("Performing version check.")
    logging.info("Current version: %s", current_version)

    data = json_dumps({
        'current_version': current_version
    })
    headers = {'content-type': 'application/json'}

    try:
        response = requests.post('https://version.redash.io/api/report?channel=stable',
                                 data=data, headers=headers, timeout=3.0)
        latest_version = response.json()['release']['version']

        _compare_and_update(latest_version)
    except requests.RequestException:
        logging.exception("Failed checking for new version.")
    except (ValueError, KeyError):
        logging.exception("Failed checking for new version (probably bad/non-JSON response).")


def reset_new_version_status():
    latest_version = get_latest_version()
    if latest_version:
        _compare_and_update(latest_version)


def get_latest_version():
    return redis_connection.get(REDIS_KEY)


def _compare_and_update(latest_version):
    # TODO: support alpha channel (allow setting which channel to check & parse build number)
    is_newer = semver.compare(current_version, latest_version) == -1
    logging.info("Latest version: %s (newer: %s)", latest_version, is_newer)

    if is_newer:
        redis_connection.set(REDIS_KEY, latest_version)
    else:
        redis_connection.delete(REDIS_KEY)
<EOF>
<BOF>
import datetime
import logging
import sys
import time
from base64 import b64decode

import httplib2
import requests

from redash import settings
from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    import apiclient.errors
    from apiclient.discovery import build
    from apiclient.errors import HttpError
    from oauth2client.service_account import ServiceAccountCredentials
    from oauth2client.contrib import gce

    enabled = True
except ImportError:
    enabled = False

types_map = {
    'INTEGER': TYPE_INTEGER,
    'FLOAT': TYPE_FLOAT,
    'BOOLEAN': TYPE_BOOLEAN,
    'STRING': TYPE_STRING,
    'TIMESTAMP': TYPE_DATETIME,
}


def transform_row(row, fields):
    column_index = 0
    row_data = {}

    for cell in row["f"]:
        field = fields[column_index]
        cell_value = cell['v']

        if cell_value is None:
            pass
        # Otherwise just cast the value
        elif field['type'] == 'INTEGER':
            cell_value = int(cell_value)
        elif field['type'] == 'FLOAT':
            cell_value = float(cell_value)
        elif field['type'] == 'BOOLEAN':
            cell_value = cell_value.lower() == "true"
        elif field['type'] == 'TIMESTAMP':
            cell_value = datetime.datetime.fromtimestamp(float(cell_value))

        row_data[field["name"]] = cell_value
        column_index += 1

    return row_data


def _load_key(filename):
    f = file(filename, "rb")
    try:
        return f.read()
    finally:
        f.close()


def _get_query_results(jobs, project_id, location, job_id, start_index):
    query_reply = jobs.getQueryResults(projectId=project_id,
                                       location=location,
                                       jobId=job_id,
                                       startIndex=start_index).execute()
    logging.debug('query_reply %s', query_reply)
    if not query_reply['jobComplete']:
        time.sleep(10)
        return _get_query_results(jobs, project_id, location, job_id, start_index)

    return query_reply


class BigQuery(BaseQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'projectId': {
                    'type': 'string',
                    'title': 'Project ID'
                },
                'jsonKeyFile': {
                    "type": "string",
                    'title': 'JSON Key File'
                },
                'totalMBytesProcessedLimit': {
                    "type": "number",
                    'title': 'Scanned Data Limit (MB)'
                },
                'userDefinedFunctionResourceUri': {
                    "type": "string",
                    'title': 'UDF Source URIs (i.e. gs://bucket/date_utils.js, gs://bucket/string_utils.js )'
                },
                'useStandardSql': {
                    "type": "boolean",
                    'title': "Use Standard SQL",
                    "default": True,
                },
                'location': {
                    "type": "string",
                    "title": "Processing Location",
                },
                'loadSchema': {
                    "type": "boolean",
                    "title": "Load Schema"
                },
                'maximumBillingTier': {
                    "type": "number",
                    "title": "Maximum Billing Tier"
                }
            },
            'required': ['jsonKeyFile', 'projectId'],
            "order": ['projectId', 'jsonKeyFile', 'loadSchema', 'useStandardSql', 'location', 'totalMBytesProcessedLimit', 'maximumBillingTier', 'userDefinedFunctionResourceUri'],
            'secret': ['jsonKeyFile']
        }

    @classmethod
    def annotate_query(cls):
        return False

    def _get_bigquery_service(self):
        scope = [
            "https://www.googleapis.com/auth/bigquery",
            "https://www.googleapis.com/auth/drive"
        ]

        key = json_loads(b64decode(self.configuration['jsonKeyFile']))

        creds = ServiceAccountCredentials.from_json_keyfile_dict(key, scope)
        http = httplib2.Http(timeout=settings.BIGQUERY_HTTP_TIMEOUT)
        http = creds.authorize(http)

        return build("bigquery", "v2", http=http)

    def _get_project_id(self):
        return self.configuration["projectId"]

    def _get_location(self):
        return self.configuration.get("location")

    def _get_total_bytes_processed(self, jobs, query):
        job_data = {
            "query": query,
            "dryRun": True,
        }

        if self._get_location():
            job_data['location'] = self._get_location()

        if self.configuration.get('useStandardSql', False):
            job_data['useLegacySql'] = False

        response = jobs.query(projectId=self._get_project_id(), body=job_data).execute()
        return int(response["totalBytesProcessed"])

    def _get_query_result(self, jobs, query):
        project_id = self._get_project_id()
        job_data = {
            "configuration": {
                "query": {
                    "query": query,
                }
            }
        }

        if self._get_location():
            job_data['jobReference'] = {
                'location': self._get_location()
            }

        if self.configuration.get('useStandardSql', False):
            job_data['configuration']['query']['useLegacySql'] = False

        if self.configuration.get('userDefinedFunctionResourceUri'):
            resource_uris = self.configuration["userDefinedFunctionResourceUri"].split(',')
            job_data["configuration"]["query"]["userDefinedFunctionResources"] = map(
                lambda resource_uri: {"resourceUri": resource_uri}, resource_uris)

        if "maximumBillingTier" in self.configuration:
            job_data["configuration"]["query"]["maximumBillingTier"] = self.configuration["maximumBillingTier"]

        insert_response = jobs.insert(projectId=project_id, body=job_data).execute()
        current_row = 0
        query_reply = _get_query_results(jobs, project_id=project_id, location=self._get_location(),
                                         job_id=insert_response['jobReference']['jobId'], start_index=current_row)

        logger.debug("bigquery replied: %s", query_reply)

        rows = []

        while ("rows" in query_reply) and current_row < query_reply['totalRows']:
            for row in query_reply["rows"]:
                rows.append(transform_row(row, query_reply["schema"]["fields"]))

            current_row += len(query_reply['rows'])

            query_result_request = {
                'projectId': project_id,
                'jobId': query_reply['jobReference']['jobId'],
                'startIndex': current_row
            }

            if self._get_location():
                query_result_request['location'] = self._get_location()

            query_reply = jobs.getQueryResults(**query_result_request).execute()

        columns = [{'name': f["name"],
                    'friendly_name': f["name"],
                    'type': types_map.get(f['type'], "string")} for f in query_reply["schema"]["fields"]]

        data = {
            "columns": columns,
            "rows": rows,
            'metadata': {'data_scanned': int(query_reply['totalBytesProcessed'])}
        }

        return data

    def _get_columns_schema(self, table_data):
        columns = []
        for column in table_data['schema']['fields']:
            columns.extend(self._get_columns_schema_column(column))

        project_id = self._get_project_id()
        table_name = table_data['id'].replace("%s:" % project_id, "")
        return {'name': table_name, 'columns': columns}

    def _get_columns_schema_column(self, column):
        columns = []
        if column['type'] == 'RECORD':
            for field in column['fields']:
                columns.append(u"{}.{}".format(column['name'], field['name']))
        else:
            columns.append(column['name'])

        return columns

    def get_schema(self, get_stats=False):
        if not self.configuration.get('loadSchema', False):
            return []

        service = self._get_bigquery_service()
        project_id = self._get_project_id()
        datasets = service.datasets().list(projectId=project_id).execute()
        schema = []
        for dataset in datasets.get('datasets', []):
            dataset_id = dataset['datasetReference']['datasetId']
            tables = service.tables().list(projectId=project_id, datasetId=dataset_id).execute()
            while True:
                for table in tables.get('tables', []):
                    table_data = service.tables().get(projectId=project_id,
                                                      datasetId=dataset_id,
                                                      tableId=table['tableReference']['tableId']).execute()
                    table_schema = self._get_columns_schema(table_data)
                    schema.append(table_schema)

                next_token = tables.get('nextPageToken', None)
                if next_token is None:
                    break

                tables = service.tables().list(projectId=project_id,
                                               datasetId=dataset_id,
                                               pageToken=next_token).execute()

        return schema

    def run_query(self, query, user):
        logger.debug("BigQuery got query: %s", query)

        bigquery_service = self._get_bigquery_service()
        jobs = bigquery_service.jobs()

        try:
            if "totalMBytesProcessedLimit" in self.configuration:
                limitMB = self.configuration["totalMBytesProcessedLimit"]
                processedMB = self._get_total_bytes_processed(jobs, query) / 1000.0 / 1000.0
                if limitMB < processedMB:
                    return None, "Larger than %d MBytes will be processed (%f MBytes)" % (limitMB, processedMB)

            data = self._get_query_result(jobs, query)
            error = None

            json_data = json_dumps(data)
        except apiclient.errors.HttpError as e:
            json_data = None
            if e.resp.status == 400:
                error = json_loads(e.content)['error']['message']
            else:
                error = e.content
        except KeyboardInterrupt:
            error = "Query cancelled by user."
            json_data = None

        return json_data, error


class BigQueryGCE(BigQuery):
    @classmethod
    def type(cls):
        return "bigquery_gce"

    @classmethod
    def enabled(cls):
        try:
            # check if we're on a GCE instance
            requests.get('http://metadata.google.internal')
        except requests.exceptions.ConnectionError:
            return False

        return True

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'totalMBytesProcessedLimit': {
                    "type": "number",
                    'title': 'Total MByte Processed Limit'
                },
                'userDefinedFunctionResourceUri': {
                    "type": "string",
                    'title': 'UDF Source URIs (i.e. gs://bucket/date_utils.js, gs://bucket/string_utils.js )'
                },
                'useStandardSql': {
                    "type": "boolean",
                    'title': "Use Standard SQL",
                    "default": True,
                },
                'location': {
                    "type": "string",
                    "title": "Processing Location",
                    "default": "US",
                },
                'loadSchema': {
                    "type": "boolean",
                    "title": "Load Schema"
                }
            }
        }

    def _get_project_id(self):
        return requests.get('http://metadata/computeMetadata/v1/project/project-id', headers={'Metadata-Flavor': 'Google'}).content

    def _get_bigquery_service(self):
        credentials = gce.AppAssertionCredentials(scope='https://www.googleapis.com/auth/bigquery')
        http = httplib2.Http()
        http = credentials.authorize(http)

        return build("bigquery", "v2", http=http)


register(BigQuery)
register(BigQueryGCE)
<EOF>
<BOF>
import os
import logging
import select

import psycopg2

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

types_map = {
    20: TYPE_INTEGER,
    21: TYPE_INTEGER,
    23: TYPE_INTEGER,
    700: TYPE_FLOAT,
    1700: TYPE_FLOAT,
    701: TYPE_FLOAT,
    16: TYPE_BOOLEAN,
    1082: TYPE_DATE,
    1114: TYPE_DATETIME,
    1184: TYPE_DATETIME,
    1014: TYPE_STRING,
    1015: TYPE_STRING,
    1008: TYPE_STRING,
    1009: TYPE_STRING,
    2951: TYPE_STRING
}


def _wait(conn, timeout=None):
    while 1:
        try:
            state = conn.poll()
            if state == psycopg2.extensions.POLL_OK:
                break
            elif state == psycopg2.extensions.POLL_WRITE:
                select.select([], [conn.fileno()], [], timeout)
            elif state == psycopg2.extensions.POLL_READ:
                select.select([conn.fileno()], [], [], timeout)
            else:
                raise psycopg2.OperationalError("poll() returned %s" % state)
        except select.error:
            raise psycopg2.OperationalError("select.error received")


class PostgreSQL(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "host": {
                    "type": "string",
                    "default": "127.0.0.1"
                },
                "port": {
                    "type": "number",
                    "default": 5432
                },
                "dbname": {
                    "type": "string",
                    "title": "Database Name"
                },
                "sslmode": {
                   "type": "string",
                   "title": "SSL Mode",
                   "default": "prefer"
                }
            },
            "order": ['host', 'port', 'user', 'password'],
            "required": ["dbname"],
            "secret": ["password"]
        }

    @classmethod
    def type(cls):
        return "pg"

    def _get_definitions(self, schema, query):
        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            if row['table_schema'] != 'public':
                table_name = u'{}.{}'.format(row['table_schema'], row['table_name'])
            else:
                table_name = row['table_name']

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['column_name'])

    def _get_tables(self, schema):
        '''
        relkind constants per https://www.postgresql.org/docs/10/static/catalog-pg-class.html
        r = regular table
        v = view
        m = materialized view
        f = foreign table
        p = partitioned table (new in 10)
        ---
        i = index
        S = sequence
        t = TOAST table
        c = composite type
        '''

        query = """
        SELECT s.nspname as table_schema,
               c.relname as table_name,
               a.attname as column_name
        FROM pg_class c
        JOIN pg_namespace s
        ON c.relnamespace = s.oid
        AND s.nspname NOT IN ('pg_catalog', 'information_schema')
        JOIN pg_attribute a
        ON a.attrelid = c.oid
        AND a.attnum > 0
        AND NOT a.attisdropped
        WHERE c.relkind IN ('r', 'v', 'm', 'f', 'p')
        """

        self._get_definitions(schema, query)

        return schema.values()

    def _get_connection(self):
        connection = psycopg2.connect(user=self.configuration.get('user'),
                                      password=self.configuration.get('password'),
                                      host=self.configuration.get('host'),
                                      port=self.configuration.get('port'),
                                      dbname=self.configuration.get('dbname'),
                                      sslmode=self.configuration.get('sslmode'),
                                      async_=True)

        return connection

    def run_query(self, query, user):
        connection = self._get_connection()
        _wait(connection, timeout=10)

        cursor = connection.cursor()

        try:
            cursor.execute(query)
            _wait(connection)

            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], types_map.get(i[1], None)) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]

                data = {'columns': columns, 'rows': rows}
                error = None
                json_data = json_dumps(data, ignore_nan=True)
            else:
                error = 'Query completed but it returned no data.'
                json_data = None
        except (select.error, OSError) as e:
            error = "Query interrupted. Please retry."
            json_data = None
        except psycopg2.DatabaseError as e:
            error = e.message
            json_data = None
        except (KeyboardInterrupt, InterruptException):
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            connection.close()

        return json_data, error


class Redshift(PostgreSQL):
    @classmethod
    def type(cls):
        return "redshift"

    def _get_connection(self):
        sslrootcert_path = os.path.join(os.path.dirname(__file__), './files/redshift-ca-bundle.crt')

        connection = psycopg2.connect(user=self.configuration.get('user'),
                                      password=self.configuration.get('password'),
                                      host=self.configuration.get('host'),
                                      port=self.configuration.get('port'),
                                      dbname=self.configuration.get('dbname'),
                                      sslmode=self.configuration.get('sslmode', 'prefer'),
                                      sslrootcert=sslrootcert_path,
                                      async_=True)

        return connection

    @classmethod
    def configuration_schema(cls):

        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "host": {
                    "type": "string"
                },
                "port": {
                    "type": "number"
                },
                "dbname": {
                    "type": "string",
                    "title": "Database Name"
                },
                "sslmode": {
                   "type": "string",
                   "title": "SSL Mode",
                   "default": "prefer"
                }
            },
            "order": ['host', 'port', 'user', 'password'],
            "required": ["dbname", "user", "password", "host", "port"],
            "secret": ["password"]
        }

    def _get_tables(self, schema):
        # Use svv_columns to include internal & external (Spectrum) tables and views data for Redshift
        # https://docs.aws.amazon.com/redshift/latest/dg/r_SVV_COLUMNS.html
        # Use PG_GET_LATE_BINDING_VIEW_COLS to include schema for late binding views data for Redshift
        # https://docs.aws.amazon.com/redshift/latest/dg/PG_GET_LATE_BINDING_VIEW_COLS.html
        # Use HAS_SCHEMA_PRIVILEGE(), SVV_EXTERNAL_SCHEMAS and HAS_TABLE_PRIVILEGE() to filter
        # out tables the current user cannot access.
        # https://docs.aws.amazon.com/redshift/latest/dg/r_HAS_SCHEMA_PRIVILEGE.html
        # https://docs.aws.amazon.com/redshift/latest/dg/r_SVV_EXTERNAL_SCHEMAS.html
        # https://docs.aws.amazon.com/redshift/latest/dg/r_HAS_TABLE_PRIVILEGE.html
        query = """
        WITH tables AS (
            SELECT DISTINCT table_name,
                            table_schema,
                            column_name,
                            ordinal_position AS pos
            FROM svv_columns
            WHERE table_schema NOT IN ('pg_internal','pg_catalog','information_schema')
            UNION ALL
            SELECT DISTINCT view_name::varchar AS table_name,
                            view_schema::varchar AS table_schema,
                            col_name::varchar AS column_name,
                            col_num AS pos
            FROM pg_get_late_binding_view_cols()
                 cols(view_schema name, view_name name, col_name name, col_type varchar, col_num int)
        )
        SELECT table_name, table_schema, column_name
        FROM tables
        WHERE
            HAS_SCHEMA_PRIVILEGE(table_schema, 'USAGE') AND
            (
                table_schema IN (SELECT schemaname FROM SVV_EXTERNAL_SCHEMAS) OR
                HAS_TABLE_PRIVILEGE(table_schema || '.' || table_name, 'SELECT')
            )
        ORDER BY table_name, pos
        """

        self._get_definitions(schema, query)

        return schema.values()


class CockroachDB(PostgreSQL):

    @classmethod
    def type(cls):
        return "cockroach"

register(PostgreSQL)
register(Redshift)
register(CockroachDB)
<EOF>
<BOF>
import logging
import re

import requests

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)


class ClickHouse(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "default": "http://127.0.0.1:8123"
                },
                "user": {
                    "type": "string",
                    "default": "default"
                },
                "password": {
                    "type": "string"
                },
                "dbname": {
                    "type": "string",
                    "title": "Database Name"
                },
                "timeout": {
                    "type": "number",
                    "title": "Request Timeout",
                    "default": 30
                }
            },
            "required": ["dbname"],
            "secret": ["password"]
        }

    @classmethod
    def type(cls):
        return "clickhouse"

    def _get_tables(self, schema):
        query = "SELECT database, table, name FROM system.columns WHERE database NOT IN ('system')"

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            table_name = '{}.{}'.format(row['database'], row['table'])

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['name'])

        return schema.values()

    def _send_query(self, data, stream=False):
        r = requests.post(
            self.configuration['url'],
            data=data.encode("utf-8"),
            stream=stream,
            timeout=self.configuration.get('timeout', 30),
            params={
                'user': self.configuration['user'],
                'password':  self.configuration['password'],
                'database': self.configuration['dbname']
            }
        )
        if r.status_code != 200:
            raise Exception(r.text)
        # logging.warning(r.json())
        return r.json()

    @staticmethod
    def _define_column_type(column):
        c = column.lower()
        f = re.search(r'^nullable\((.*)\)$', c)
        if f is not None:
            c = f.group(1)
        if c.startswith('int') or c.startswith('uint'):
            return TYPE_INTEGER
        elif c.startswith('float'):
            return TYPE_FLOAT
        elif c == 'datetime':
            return TYPE_DATETIME
        elif c == 'date':
            return TYPE_DATE
        else:
            return TYPE_STRING

    def _clickhouse_query(self, query):
        query += '\nFORMAT JSON'
        result = self._send_query(query)
        columns = []
        columns_int64 = []  # db converts value to string if its type equals UInt64
        columns_totals = {}

        for r in result['meta']:
            column_name = r['name']
            column_type = self._define_column_type(r['type'])

            if 'Int64' in r['type']:
                columns_int64.append(column_name)
            else:
                columns_totals[column_name] = 'Total' if column_type == TYPE_STRING else None

            columns.append({'name': column_name, 'friendly_name': column_name, 'type': column_type})

        rows = result['data']
        for row in rows:
            for column in columns_int64:
                row[column] = int(row[column])

        if 'totals' in result:
            totals = result['totals']
            for column, value in columns_totals.iteritems():
                totals[column] = value
            rows.append(totals)

        return {'columns': columns, 'rows': rows}

    def run_query(self, query, user):
        logger.debug("Clickhouse is about to execute query: %s", query)
        if query == "":
            json_data = None
            error = "Query is empty"
            return json_data, error
        try:
            q = self._clickhouse_query(query)
            data = json_dumps(q)
            error = None
        except Exception as e:
            data = None
            logging.exception(e)
            error = unicode(e)
        return data, error

register(ClickHouse)
<EOF>
<BOF>
import logging

from redash.utils import json_dumps, json_loads
from redash.query_runner import *

try:
    import cx_Oracle

    TYPES_MAP = {
        cx_Oracle.DATETIME: TYPE_DATETIME,
        cx_Oracle.CLOB: TYPE_STRING,
        cx_Oracle.LOB: TYPE_STRING,
        cx_Oracle.FIXED_CHAR: TYPE_STRING,
        cx_Oracle.FIXED_NCHAR: TYPE_STRING,
        cx_Oracle.INTERVAL: TYPE_DATETIME,
        cx_Oracle.LONG_STRING: TYPE_STRING,
        cx_Oracle.NATIVE_FLOAT: TYPE_FLOAT,
        cx_Oracle.NCHAR: TYPE_STRING,
        cx_Oracle.NUMBER: TYPE_FLOAT,
        cx_Oracle.ROWID: TYPE_INTEGER,
        cx_Oracle.STRING: TYPE_STRING,
        cx_Oracle.TIMESTAMP: TYPE_DATETIME,
    }


    ENABLED = True
except ImportError:
    ENABLED = False

logger = logging.getLogger(__name__)

class Oracle(BaseSQLQueryRunner):
    noop_query = "SELECT 1 FROM dual"

    @classmethod
    def get_col_type(cls, col_type, scale):
        if col_type == cx_Oracle.NUMBER:
            return TYPE_FLOAT if scale > 0 else TYPE_INTEGER
        else:
            return TYPES_MAP.get(col_type, None)

    @classmethod
    def enabled(cls):
        return ENABLED

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "host": {
                    "type": "string"
                },
                "port": {
                    "type": "number"
                },
                "servicename": {
                    "type": "string",
                    "title": "DSN Service Name"
                }
            },
            "required": ["servicename", "user", "password", "host", "port"],
            "secret": ["password"]
        }

    @classmethod
    def type(cls):
        return "oracle"

    def __init__(self, configuration):
        super(Oracle, self).__init__(configuration)

        dsn = cx_Oracle.makedsn(
            self.configuration["host"],
            self.configuration["port"],
            service_name=self.configuration["servicename"])

        self.connection_string = "{}/{}@{}".format(self.configuration["user"], self.configuration["password"], dsn)

    def _get_tables(self, schema):
        query = """
        SELECT
            all_tab_cols.OWNER,
            all_tab_cols.TABLE_NAME,
            all_tab_cols.COLUMN_NAME
        FROM all_tab_cols
        WHERE all_tab_cols.OWNER NOT IN('SYS','SYSTEM','ORDSYS','CTXSYS','WMSYS','MDSYS','ORDDATA','XDB','OUTLN','DMSYS','DSSYS','EXFSYS','LBACSYS','TSMSYS')
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            if row['OWNER'] != None:
                table_name = '{}.{}'.format(row['OWNER'], row['TABLE_NAME'])
            else:
                table_name = row['TABLE_NAME']

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['COLUMN_NAME'])

        return schema.values()

    @classmethod
    def _convert_number(cls, value):
        try:
            return int(value)
        except:
            return value

    @classmethod
    def output_handler(cls, cursor, name, default_type, length, precision, scale):
        if default_type in (cx_Oracle.CLOB, cx_Oracle.LOB):
            return cursor.var(cx_Oracle.LONG_STRING, 80000, cursor.arraysize)

        if default_type in (cx_Oracle.STRING, cx_Oracle.FIXED_CHAR):
            return cursor.var(unicode, length, cursor.arraysize)

        if default_type == cx_Oracle.NUMBER:
            if scale <= 0:
                return cursor.var(cx_Oracle.STRING, 255, outconverter=Oracle._convert_number, arraysize=cursor.arraysize)

    def run_query(self, query, user):
        connection = cx_Oracle.connect(self.connection_string)
        connection.outputtypehandler = Oracle.output_handler

        cursor = connection.cursor()

        try:
            cursor.execute(query)
            rows_count = cursor.rowcount
            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], Oracle.get_col_type(i[1], i[5])) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]
                data = {'columns': columns, 'rows': rows}
                error = None
                json_data = json_dumps(data)
            else:
                columns = [{'name': 'Row(s) Affected', 'type': 'TYPE_INTEGER'}]
                rows = [{'Row(s) Affected': rows_count}]
                data = {'columns': columns, 'rows': rows}
                json_data = json_dumps(data)
                connection.commit()
        except cx_Oracle.DatabaseError as err:
            error = u"Query failed. {}.".format(err.message)
            json_data = None
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            connection.close()

        return json_data, error

register(Oracle)
<EOF>
<BOF>
import logging
import sqlite3
import sys

from six import reraise

from redash.query_runner import BaseSQLQueryRunner, register
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)


class Sqlite(BaseSQLQueryRunner):
    noop_query = "pragma quick_check"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "dbpath": {
                    "type": "string",
                    "title": "Database Path"
                }
            },
            "required": ["dbpath"],
        }

    @classmethod
    def type(cls):
        return "sqlite"

    def __init__(self, configuration):
        super(Sqlite, self).__init__(configuration)

        self._dbpath = self.configuration['dbpath']

    def _get_tables(self, schema):
        query_table = "select tbl_name from sqlite_master where type='table'"
        query_columns = "PRAGMA table_info(%s)"

        results, error = self.run_query(query_table, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            table_name = row['tbl_name']
            schema[table_name] = {'name': table_name, 'columns': []}
            results_table, error = self.run_query(query_columns % (table_name,), None)
            if error is not None:
                raise Exception("Failed getting schema.")

            results_table = json_loads(results_table)
            for row_column in results_table['rows']:
                schema[table_name]['columns'].append(row_column['name'])

        return schema.values()

    def run_query(self, query, user):
        connection = sqlite3.connect(self._dbpath)

        cursor = connection.cursor()

        try:
            cursor.execute(query)

            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], None) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]

                data = {'columns': columns, 'rows': rows}
                error = None
                json_data = json_dumps(data)
            else:
                error = 'Query completed but it returned no data.'
                json_data = None
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        except Exception as e:
            # handle unicode error message
            err_class = sys.exc_info()[1].__class__
            err_args = [arg.decode('utf-8') for arg in sys.exc_info()[1].args]
            unicode_err = err_class(*err_args)
            reraise(unicode_err, None, sys.exc_info()[2])
        finally:
            connection.close()
        return json_data, error

register(Sqlite)
<EOF>
<BOF>
import re
from collections import OrderedDict

from redash.query_runner import *
from redash.utils import json_dumps, json_loads


# TODO: make this more general and move into __init__.py
class ResultSet(object):
    def __init__(self):
        self.columns = OrderedDict()
        self.rows = []

    def add_row(self, row):
        for key in row.keys():
            self.add_column(key)

        self.rows.append(row)

    def add_column(self, column, column_type=TYPE_STRING):
        if column not in self.columns:
            self.columns[column] = {'name': column, 'type': column_type, 'friendly_name': column}

    def to_json(self):
        return json_dumps({'rows': self.rows, 'columns': self.columns.values()})


def parse_issue(issue, field_mapping):
    result = OrderedDict()
    result['key'] = issue['key']

    for k, v in issue['fields'].iteritems():#
        output_name = field_mapping.get_output_field_name(k)
        member_names = field_mapping.get_dict_members(k)

        if isinstance(v, dict):
            if len(member_names) > 0:
                # if field mapping with dict member mappings defined get value of each member
                for member_name in member_names:
                    if member_name in v:
                        result[field_mapping.get_dict_output_field_name(k, member_name)] = v[member_name]

            else:
                # these special mapping rules are kept for backwards compatibility
                if 'key' in v:
                    result['{}_key'.format(output_name)] = v['key']
                if 'name' in v:
                    result['{}_name'.format(output_name)] = v['name']

                if k in v:
                    result[output_name] = v[k]

                if 'watchCount' in v:
                    result[output_name] = v['watchCount']

        elif isinstance(v, list):
            if len(member_names) > 0:
                # if field mapping with dict member mappings defined get value of each member
                for member_name in member_names:
                    listValues = []
                    for listItem in v:
                        if isinstance(listItem, dict):
                            if member_name in listItem:
                                listValues.append(listItem[member_name])
                    if len(listValues) > 0:
                        result[field_mapping.get_dict_output_field_name(k, member_name)] = ','.join(listValues)

            else:
                # otherwise support list values only for non-dict items
                listValues = []
                for listItem in v:
                    if not isinstance(listItem, dict):
                        listValues.append(listItem)
                if len(listValues) > 0:
                    result[output_name] = ','.join(listValues)

        else:
            result[output_name] = v

    return result


def parse_issues(data, field_mapping):
    results = ResultSet()

    for issue in data['issues']:
        results.add_row(parse_issue(issue, field_mapping))

    return results


def parse_count(data):
    results = ResultSet()
    results.add_row({'count': data['total']})
    return results


class FieldMapping:

    def __init__(cls, query_field_mapping):
        cls.mapping = []
        for k, v in query_field_mapping.iteritems():
            field_name = k
            member_name = None

            # check for member name contained in field name
            member_parser = re.search('(\w+)\.(\w+)', k)
            if (member_parser):
                field_name = member_parser.group(1)
                member_name = member_parser.group(2)

            cls.mapping.append({
                'field_name': field_name,
                'member_name': member_name,
                'output_field_name': v
                })

    def get_output_field_name(cls,field_name):
        for item in cls.mapping:
            if item['field_name'] == field_name and not item['member_name']:
                return item['output_field_name']
        return field_name

    def get_dict_members(cls,field_name):
        member_names = []
        for item in cls.mapping:
            if item['field_name'] == field_name and item['member_name']:
                member_names.append(item['member_name'])
        return member_names

    def get_dict_output_field_name(cls,field_name, member_name):
        for item in cls.mapping:
            if item['field_name'] == field_name and item['member_name'] == member_name:
                return item['output_field_name']
        return None


class JiraJQL(BaseHTTPQueryRunner):
    noop_query = '{"queryType": "count"}'
    response_error = "JIRA returned unexpected status code"
    requires_authentication = True
    url_title = 'JIRA URL'
    username_title = 'Username'
    password_title = 'Password'

    @classmethod
    def name(cls):
        return "JIRA (JQL)"

    @classmethod
    def annotate_query(cls):
        return False

    def __init__(self, configuration):
        super(JiraJQL, self).__init__(configuration)
        self.syntax = 'json'

    def run_query(self, query, user):
        jql_url = '{}/rest/api/2/search'.format(self.configuration["url"])

        try:
            query = json_loads(query)
            query_type = query.pop('queryType', 'select')
            field_mapping = FieldMapping(query.pop('fieldMapping', {}))

            if query_type == 'count':
                query['maxResults'] = 1
                query['fields'] = ''
            else:
                query['maxResults'] = query.get('maxResults', 1000)

            response, error = self.get_response(jql_url, params=query)
            if error is not None:
                return None, error

            data = response.json()

            if query_type == 'count':
                results = parse_count(data)
            else:
                results = parse_issues(data, field_mapping)

            return results.to_json(), None
        except KeyboardInterrupt:
            return None, "Query cancelled by user."

register(JiraJQL)
<EOF>
<BOF>
import logging
import requests

from redash import settings
from redash.utils import json_loads

logger = logging.getLogger(__name__)

__all__ = [
    'BaseQueryRunner',
    'BaseHTTPQueryRunner',
    'InterruptException',
    'BaseSQLQueryRunner',
    'TYPE_DATETIME',
    'TYPE_BOOLEAN',
    'TYPE_INTEGER',
    'TYPE_STRING',
    'TYPE_DATE',
    'TYPE_FLOAT',
    'SUPPORTED_COLUMN_TYPES',
    'register',
    'get_query_runner',
    'import_query_runners'
]

# Valid types of columns returned in results:
TYPE_INTEGER = 'integer'
TYPE_FLOAT = 'float'
TYPE_BOOLEAN = 'boolean'
TYPE_STRING = 'string'
TYPE_DATETIME = 'datetime'
TYPE_DATE = 'date'

SUPPORTED_COLUMN_TYPES = set([
    TYPE_INTEGER,
    TYPE_FLOAT,
    TYPE_BOOLEAN,
    TYPE_STRING,
    TYPE_DATETIME,
    TYPE_DATE
])


class InterruptException(Exception):
    pass


class NotSupported(Exception):
    pass


class BaseQueryRunner(object):
    noop_query = None

    def __init__(self, configuration):
        self.syntax = 'sql'
        self.configuration = configuration

    @classmethod
    def name(cls):
        return cls.__name__

    @classmethod
    def type(cls):
        return cls.__name__.lower()

    @classmethod
    def enabled(cls):
        return True

    @classmethod
    def annotate_query(cls):
        return True

    @classmethod
    def configuration_schema(cls):
        return {}

    def test_connection(self):
        if self.noop_query is None:
            raise NotImplementedError()
        data, error = self.run_query(self.noop_query, None)

        if error is not None:
            raise Exception(error)

    def run_query(self, query, user):
        raise NotImplementedError()

    def fetch_columns(self, columns):
        column_names = []
        duplicates_counter = 1
        new_columns = []

        for col in columns:
            column_name = col[0]
            if column_name in column_names:
                column_name = "{}{}".format(column_name, duplicates_counter)
                duplicates_counter += 1

            column_names.append(column_name)
            new_columns.append({'name': column_name,
                                'friendly_name': column_name,
                                'type': col[1]})

        return new_columns

    def get_schema(self, get_stats=False):
        raise NotSupported()

    def _run_query_internal(self, query):
        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed running query [%s]." % query)
        return json_loads(results)['rows']

    @classmethod
    def to_dict(cls):
        return {
            'name': cls.name(),
            'type': cls.type(),
            'configuration_schema': cls.configuration_schema()
        }


class BaseSQLQueryRunner(BaseQueryRunner):

    def get_schema(self, get_stats=False):
        schema_dict = {}
        self._get_tables(schema_dict)
        if settings.SCHEMA_RUN_TABLE_SIZE_CALCULATIONS and get_stats:
            self._get_tables_stats(schema_dict)
        return schema_dict.values()

    def _get_tables(self, schema_dict):
        return []

    def _get_tables_stats(self, tables_dict):
        for t in tables_dict.keys():
            if type(tables_dict[t]) == dict:
                res = self._run_query_internal('select count(*) as cnt from %s' % t)
                tables_dict[t]['size'] = res[0]['cnt']


class BaseHTTPQueryRunner(BaseQueryRunner):
    response_error = "Endpoint returned unexpected status code"
    requires_authentication = False
    requires_url = True
    url_title = 'URL base path'
    username_title = 'HTTP Basic Auth Username'
    password_title = 'HTTP Basic Auth Password'

    @classmethod
    def configuration_schema(cls):
        schema = {
            'type': 'object',
            'properties': {
                'url': {
                    'type': 'string',
                    'title': cls.url_title,
                },
                'username': {
                    'type': 'string',
                    'title': cls.username_title,
                },
                'password': {
                    'type': 'string',
                    'title': cls.password_title,
                },
            },
            'secret': ['password']
        }

        if cls.requires_url or cls.requires_authentication:
            schema['required'] = []

        if cls.requires_url:
            schema['required'] += ['url']

        if cls.requires_authentication:
            schema['required'] += ['username', 'password']
        return schema

    def get_auth(self):
        username = self.configuration.get('username')
        password = self.configuration.get('password')
        if username and password:
            return (username, password)
        if self.requires_authentication:
            raise ValueError("Username and Password required")
        else:
            return None

    def get_response(self, url, auth=None, **kwargs):
        # Get authentication values if not given
        if auth is None:
            auth = self.get_auth()

        # Then call requests to get the response from the given endpoint
        # URL optionally, with the additional requests parameters.
        error = None
        response = None
        try:
            response = requests.get(url, auth=auth, **kwargs)
            # Raise a requests HTTP exception with the appropriate reason
            # for 4xx and 5xx response status codes which is later caught
            # and passed back.
            response.raise_for_status()

            # Any other responses (e.g. 2xx and 3xx):
            if response.status_code != 200:
                error = '{} ({}).'.format(
                    self.response_error,
                    response.status_code,
                )

        except requests.HTTPError as exc:
            logger.exception(exc)
            error = (
                "Failed to execute query. "
                "Return Code: {} Reason: {}".format(
                    response.status_code,
                    response.text
                )
            )
        except requests.RequestException as exc:
            # Catch all other requests exceptions and return the error.
            logger.exception(exc)
            error = str(exc)

        # Return response and error.
        return response, error


query_runners = {}


def register(query_runner_class):
    global query_runners
    if query_runner_class.enabled():
        logger.debug("Registering %s (%s) query runner.", query_runner_class.name(), query_runner_class.type())
        query_runners[query_runner_class.type()] = query_runner_class
    else:
        logger.debug("%s query runner enabled but not supported, not registering. Either disable or install missing "
                     "dependencies.", query_runner_class.name())


def get_query_runner(query_runner_type, configuration):
    query_runner_class = query_runners.get(query_runner_type, None)
    if query_runner_class is None:
        return None

    return query_runner_class(configuration)


def get_configuration_schema_for_query_runner_type(query_runner_type):
    query_runner_class = query_runners.get(query_runner_type, None)
    if query_runner_class is None:
        return None

    return query_runner_class.configuration_schema()


def import_query_runners(query_runner_imports):
    for runner_import in query_runner_imports:
        __import__(runner_import)
<EOF>
<BOF>
import logging

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

try:
    from impala.dbapi import connect
    from impala.error import DatabaseError, RPCError
    enabled = True
except ImportError as e:
    enabled = False

COLUMN_NAME = 0
COLUMN_TYPE = 1

types_map = {
    'BIGINT': TYPE_INTEGER,
    'TINYINT': TYPE_INTEGER,
    'SMALLINT': TYPE_INTEGER,
    'INT': TYPE_INTEGER,
    'DOUBLE': TYPE_FLOAT,
    'DECIMAL': TYPE_FLOAT,
    'FLOAT': TYPE_FLOAT,
    'REAL': TYPE_FLOAT,
    'BOOLEAN': TYPE_BOOLEAN,
    'TIMESTAMP': TYPE_DATETIME,
    'CHAR': TYPE_STRING,
    'STRING': TYPE_STRING,
    'VARCHAR': TYPE_STRING
}


class Impala(BaseSQLQueryRunner):
    noop_query = "show schemas"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "host": {
                    "type": "string"
                },
                "port": {
                    "type": "number"
                },
                "protocol": {
                    "type": "string",
                    "title": "Please specify beeswax or hiveserver2"
                },
                "database": {
                    "type": "string"
                },
                "use_ldap": {
                    "type": "boolean"
                },
                "ldap_user": {
                    "type": "string"
                },
                "ldap_password": {
                    "type": "string"
                },
                "timeout": {
                    "type": "number"
                }
            },
            "required": ["host"],
            "secret": ["ldap_password"]
        }

    @classmethod
    def type(cls):
        return "impala"

    def _get_tables(self, schema_dict):
        schemas_query = "show schemas;"
        tables_query = "show tables in %s;"
        columns_query = "show column stats %s.%s;"

        for schema_name in map(lambda a: unicode(a['name']), self._run_query_internal(schemas_query)):
            for table_name in map(lambda a: unicode(a['name']), self._run_query_internal(tables_query % schema_name)):
                columns = map(lambda a: unicode(a['Column']), self._run_query_internal(columns_query % (schema_name, table_name)))

                if schema_name != 'default':
                    table_name = '{}.{}'.format(schema_name, table_name)

                schema_dict[table_name] = {'name': table_name, 'columns': columns}

        return schema_dict.values()

    def run_query(self, query, user):

        connection = None
        try:
            connection = connect(**self.configuration.to_dict())

            cursor = connection.cursor()

            cursor.execute(query)

            column_names = []
            columns = []

            for column in cursor.description:
                column_name = column[COLUMN_NAME]
                column_names.append(column_name)

                columns.append({
                    'name': column_name,
                    'friendly_name': column_name,
                    'type': types_map.get(column[COLUMN_TYPE], None)
                })

            rows = [dict(zip(column_names, row)) for row in cursor]

            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None
            cursor.close()
        except DatabaseError as e:
            json_data = None
            error = e.message
        except RPCError as e:
            json_data = None
            error = "Metastore Error [%s]" % e.message
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            if connection:
                connection.close()

        return json_data, error

register(Impala)
<EOF>
<BOF>
from __future__ import absolute_import
import time
import requests
import logging
from cStringIO import StringIO

from redash.query_runner import BaseQueryRunner, register
from redash.query_runner import TYPE_STRING
from redash.utils import json_dumps

try:
    import qds_sdk
    from qds_sdk.qubole import Qubole as qbol
    from qds_sdk.commands import Command, HiveCommand, PrestoCommand
    enabled = True
except ImportError:
    enabled = False


class Qubole(BaseQueryRunner):

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "endpoint": {
                    "type": "string",
                    "title": "API Endpoint",
                    "default": "https://api.qubole.com"
                },
                "token": {
                    "type": "string",
                    "title": "Auth Token"
                },
                "cluster": {
                    "type": "string",
                    "title": "Cluster Label",
                    "default": "default"
                },
                "query_type": {
                    "type": "string",
                    "title": "Query Type (hive or presto)",
                    "default": "hive"
                }
            },
            "order": ["endpoint", "token", "cluster"],
            "required": ["endpoint", "token", "cluster"],
            "secret": ["token"]
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def annotate_query(cls):
        return False

    def test_connection(self):
        headers = self._get_header()
        r = requests.head("%s/api/latest/users" % self.configuration['endpoint'], headers=headers)
        r.status_code == 200

    def run_query(self, query, user):
        qbol.configure(api_token=self.configuration['token'],
                       api_url='%s/api' % self.configuration['endpoint'])

        try:
            cls = PrestoCommand if(self.configuration['query_type'] == 'presto') else HiveCommand
            cmd = cls.create(query=query, label=self.configuration['cluster'])
            logging.info("Qubole command created with Id: %s and Status: %s", cmd.id, cmd.status)

            while not Command.is_done(cmd.status):
                time.sleep(qbol.poll_interval)
                cmd = Command.find(cmd.id)
                logging.info("Qubole command Id: %s and Status: %s", cmd.id, cmd.status)

            rows = []
            columns = []
            error = None

            if cmd.status == 'done':
                fp = StringIO()
                cmd.get_results(fp=fp, inline=True, delim='\t', fetch=False,
                                qlog=None, arguments=['true'])

                results = fp.getvalue()
                fp.close()

                data = results.split('\r\n')
                columns = self.fetch_columns([(i, TYPE_STRING) for i in data.pop(0).split('\t')])
                rows = [dict(zip((c['name'] for c in columns), row.split('\t'))) for row in data]

            json_data = json_dumps({'columns': columns, 'rows': rows})
        except KeyboardInterrupt:
            logging.info('Sending KILL signal to Qubole Command Id: %s', cmd.id)
            cmd.cancel()
            error = "Query cancelled by user."
            json_data = None

        return json_data, error

    def get_schema(self, get_stats=False):
        schemas = {}
        try:
            headers = self._get_header()
            content = requests.get("%s/api/latest/hive?describe=true&per_page=10000" %
                                   self.configuration['endpoint'], headers=headers)
            data = content.json()

            for schema in data['schemas']:
                tables = data['schemas'][schema]
                for table in tables:
                    table_name = table.keys()[0]
                    columns = [f['name'] for f in table[table_name]['columns']]

                    if schema != 'default':
                        table_name = '{}.{}'.format(schema, table_name)

                    schemas[table_name] = {'name': table_name, 'columns': columns}

        except Exception as e:
            logging.error("Failed to get schema information from Qubole. Error {}".format(str(e)))

        return schemas.values()

    def _get_header(self):
        return {"Content-type": "application/json", "Accept": "application/json",
                "X-AUTH-TOKEN": self.configuration['token']}

register(Qubole)
<EOF>
<BOF>
from io import StringIO
import logging
import sys
import uuid
import csv

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    import atsd_client
    from atsd_client.exceptions import SQLException
    from atsd_client.services import SQLService, MetricsService
    enabled = True
except ImportError:
    enabled = False

types_map = {
    'long': TYPE_INTEGER,

    'bigint': TYPE_INTEGER,
    'integer': TYPE_INTEGER,
    'smallint': TYPE_INTEGER,

    'float': TYPE_FLOAT,
    'double': TYPE_FLOAT,
    'decimal': TYPE_FLOAT,

    'string': TYPE_STRING,
    'date': TYPE_DATE,
    'xsd:dateTimeStamp': TYPE_DATETIME
}


def resolve_redash_type(type_in_atsd):
    """
    Retrieve corresponding redash type
    :param type_in_atsd: `str`
    :return: redash type constant
    """
    if isinstance(type_in_atsd, dict):
        type_in_redash = types_map.get(type_in_atsd['base'])
    else:
        type_in_redash = types_map.get(type_in_atsd)
    return type_in_redash


def generate_rows_and_columns(csv_response):
    """
    Prepare rows and columns in redash format from ATSD csv response
    :param csv_response: `str`
    :return: prepared rows and columns
    """
    meta, data = csv_response.split('\n', 1)
    meta = meta[1:]

    meta_with_padding = meta + '=' * (4 - len(meta) % 4)
    meta_decoded = meta_with_padding.decode('base64')
    meta_json = json_loads(meta_decoded)
    meta_columns = meta_json['tableSchema']['columns']

    reader = csv.reader(data.splitlines())
    next(reader)

    columns = [{'friendly_name': i['titles'],
                'type': resolve_redash_type(i['datatype']),
                'name': i['name']}
               for i in meta_columns]
    column_names = [c['name'] for c in columns]
    rows = [dict(zip(column_names, row)) for row in reader]
    return columns, rows


class AxibaseTSD(BaseQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def name(cls):
        return "Axibase Time Series Database"

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'protocol': {
                    'type': 'string',
                    'title': 'Protocol',
                    'default': 'http'
                },
                'hostname': {
                    'type': 'string',
                    'title': 'Host',
                    'default': 'axibase_tsd_hostname'
                },
                'port': {
                    'type': 'number',
                    'title': 'Port',
                    'default': 8088
                },
                'username': {
                    'type': 'string'
                },
                'password': {
                    'type': 'string',
                    'title': 'Password'
                },
                'timeout': {
                    'type': 'number',
                    'default': 600,
                    'title': 'Connection Timeout'
                },
                'min_insert_date': {
                    'type': 'string',
                    'title': 'Metric Minimum Insert Date'
                },
                'expression': {
                    'type': 'string',
                    'title': 'Metric Filter'
                },
                'limit': {
                    'type': 'number',
                    'default': 5000,
                    'title': 'Metric Limit'
                },
                'trust_certificate': {
                    'type': 'boolean',
                    'title': 'Trust SSL Certificate'
                }
            },
            'required': ['username', 'password', 'hostname', 'protocol', 'port'],
            'secret': ['password']
        }

    def __init__(self, configuration):
        super(AxibaseTSD, self).__init__(configuration)
        self.url = '{0}://{1}:{2}'.format(self.configuration.get('protocol', 'http'),
                                          self.configuration.get('hostname', 'localhost'),
                                          self.configuration.get('port', 8088))

    def run_query(self, query, user):
        connection = atsd_client.connect_url(self.url,
                                             self.configuration.get('username'),
                                             self.configuration.get('password'),
                                             verify=self.configuration.get('trust_certificate', False),
                                             timeout=self.configuration.get('timeout', 600))
        sql = SQLService(connection)
        query_id = str(uuid.uuid4())

        try:
            logger.debug("SQL running query: %s", query)
            data = sql.query_with_params(query, {'outputFormat': 'csv', 'metadataFormat': 'EMBED',
                                                 'queryId': query_id})

            columns, rows = generate_rows_and_columns(data)

            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None

        except SQLException as e:
            json_data = None
            error = e.content
        except (KeyboardInterrupt, InterruptException):
            sql.cancel_query(query_id)
            error = "Query cancelled by user."
            json_data = None

        return json_data, error

    def get_schema(self, get_stats=False):
        connection = atsd_client.connect_url(self.url,
                                             self.configuration.get('username'),
                                             self.configuration.get('password'),
                                             verify=self.configuration.get('trust_certificate', False),
                                             timeout=self.configuration.get('timeout', 600))
        metrics = MetricsService(connection)
        ml = metrics.list(expression=self.configuration.get('expression', None),
                          minInsertDate=self.configuration.get('min_insert_date', None),
                          limit=self.configuration.get('limit', 5000))
        metrics_list = [i.name.encode('utf-8') for i in ml]
        metrics_list.append('atsd_series')
        schema = {}
        default_columns = ['entity', 'datetime', 'time', 'metric', 'value', 'text',
                           'tags', 'entity.tags', 'metric.tags']
        for table_name in metrics_list:
            schema[table_name] = {'name': "'{}'".format(table_name),
                                  'columns': default_columns}
        values = schema.values()
        return values

register(AxibaseTSD)
<EOF>
<BOF>
import os
import json
import logging

from redash.query_runner import *
from redash.utils import JSONEncoder

logger = logging.getLogger(__name__)

try:
    import select
    import ibm_db_dbi

    types_map = {
        ibm_db_dbi.NUMBER: TYPE_INTEGER,
        ibm_db_dbi.BIGINT: TYPE_INTEGER,
        ibm_db_dbi.ROWID: TYPE_INTEGER,
        ibm_db_dbi.FLOAT: TYPE_FLOAT,
        ibm_db_dbi.DECIMAL: TYPE_FLOAT,
        ibm_db_dbi.DATE: TYPE_DATE,
        ibm_db_dbi.TIME: TYPE_DATETIME,
        ibm_db_dbi.DATETIME: TYPE_DATETIME,
        ibm_db_dbi.BINARY: TYPE_STRING,
        ibm_db_dbi.XML: TYPE_STRING,
        ibm_db_dbi.TEXT: TYPE_STRING,
        ibm_db_dbi.STRING: TYPE_STRING
    }

    enabled = True
except ImportError:
    enabled = False


class DB2(BaseSQLQueryRunner):
    noop_query = "SELECT 1 FROM SYSIBM.SYSDUMMY1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "host": {
                    "type": "string",
                    "default": "127.0.0.1"
                },
                "port": {
                    "type": "number",
                    "default": 50000
                },
                "dbname": {
                    "type": "string",
                    "title": "Database Name"
                }
            },
            "order": ['host', 'port', 'user', 'password', 'dbname'],
            "required": ["dbname"],
            "secret": ["password"]
        }

    @classmethod
    def type(cls):
        return "db2"

    @classmethod
    def enabled(cls):
        try:
            import ibm_db
        except ImportError:
            return False

        return True

    def _get_definitions(self, schema, query):
        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json.loads(results)

        for row in results['rows']:
            if row['TABLE_SCHEMA'] != u'public':
                table_name = '{}.{}'.format(row['TABLE_SCHEMA'], row['TABLE_NAME'])
            else:
                table_name = row['TABLE_NAME']

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['COLUMN_NAME'])

    def _get_tables(self, schema):
        query = """
        SELECT rtrim(t.TABSCHEMA) as table_schema,
               t.TABNAME as table_name,
               c.COLNAME as column_name
        from syscat.tables t
        join syscat.columns c
        on t.TABSCHEMA = c.TABSCHEMA AND t.TABNAME = c.TABNAME
        WHERE t.type IN ('T') and t.TABSCHEMA not in ('SYSIBM')
        """
        self._get_definitions(schema, query)

        return schema.values()

    def _get_connection(self):
        self.connection_string = "DATABASE={};HOSTNAME={};PORT={};PROTOCOL=TCPIP;UID={};PWD={};".format(
            self.configuration["dbname"], self.configuration["host"], self.configuration["port"], self.configuration["user"], self.configuration["password"])
        connection = ibm_db_dbi.connect(self.connection_string, "", "")

        return connection

    def run_query(self, query, user):
        connection = self._get_connection()
        cursor = connection.cursor()

        try:
            cursor.execute(query)

            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], types_map.get(i[1], None)) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]

                data = {'columns': columns, 'rows': rows}
                error = None
                json_data = json.dumps(data, cls=JSONEncoder)
            else:
                error = 'Query completed but it returned no data.'
                json_data = None
        except (select.error, OSError) as e:
            error = "Query interrupted. Please retry."
            json_data = None
        except ibm_db_dbi.DatabaseError as e:
            error = e.message
            json_data = None
        except (KeyboardInterrupt, InterruptException):
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            connection.close()

        return json_data, error


register(DB2)
<EOF>
<BOF>
from __future__ import absolute_import

try:
    import snowflake.connector
    enabled = True
except ImportError:
    enabled = False


from redash.query_runner import BaseQueryRunner, register
from redash.query_runner import TYPE_STRING, TYPE_DATE, TYPE_DATETIME, TYPE_INTEGER, TYPE_FLOAT, TYPE_BOOLEAN
from redash.utils import json_dumps, json_loads

TYPES_MAP = {
    0: TYPE_INTEGER,
    1: TYPE_FLOAT,
    2: TYPE_STRING,
    3: TYPE_DATE,
    4: TYPE_DATETIME,
    5: TYPE_STRING,
    6: TYPE_DATETIME,
    13: TYPE_BOOLEAN
}


class Snowflake(BaseQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "account": {
                    "type": "string"
                },
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "warehouse": {
                    "type": "string"
                },
                "database": {
                    "type": "string"
                }
            },
            "required": ["user", "password", "account", "database", "warehouse"],
            "secret": ["password"]
        }

    @classmethod
    def enabled(cls):
        return enabled

    def run_query(self, query, user):
        connection = snowflake.connector.connect(
            user=self.configuration['user'],
            password=self.configuration['password'],
            account=self.configuration['account'],
        )

        cursor = connection.cursor()

        try:
            cursor.execute("USE WAREHOUSE {}".format(self.configuration['warehouse']))
            cursor.execute("USE {}".format(self.configuration['database']))

            cursor.execute(query)

            columns = self.fetch_columns([(i[0], TYPES_MAP.get(i[1], None)) for i in cursor.description])
            rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]

            data = {'columns': columns, 'rows': rows}
            error = None
            json_data = json_dumps(data)
        finally:
            cursor.close()
            connection.close()

        return json_data, error

    def get_schema(self, get_stats=False):
        query = """
        SELECT col.table_schema,
               col.table_name,
               col.column_name
        FROM {database}.information_schema.columns col
        WHERE col.table_schema <> 'INFORMATION_SCHEMA'
        """.format(database=self.configuration['database'])

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        schema = {}
        results = json_loads(results)

        for row in results['rows']:
            table_name = '{}.{}'.format(row['TABLE_SCHEMA'], row['TABLE_NAME'])

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['COLUMN_NAME'])

        return schema.values()

register(Snowflake)
<EOF>
<BOF>
import logging

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

try:
    from influxdb import InfluxDBClusterClient
    enabled = True

except ImportError:
    enabled = False


def _transform_result(results):
    result_columns = []
    result_rows = []

    for result in results:
        for series in result.raw.get('series', []):
            for column in series['columns']:
                if column not in result_columns:
                    result_columns.append(column)
            tags = series.get('tags', {})
            for key in tags.keys():
                if key not in result_columns:
                    result_columns.append(key)

    for result in results:
        for series in result.raw.get('series', []):
            for point in series['values']:
                result_row = {}
                for column in result_columns:
                    tags = series.get('tags', {})
                    if column in tags:
                        result_row[column] = tags[column]
                    elif column in series['columns']:
                        index = series['columns'].index(column)
                        value = point[index]
                        result_row[column] = value
                result_rows.append(result_row)

    return json_dumps({
        "columns": [{'name': c} for c in result_columns],
        "rows": result_rows
    })


class InfluxDB(BaseQueryRunner):
    noop_query = "show measurements limit 1"

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'url': {
                    'type': 'string'
                }
            },
            'required': ['url']
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "influxdb"

    def run_query(self, query, user):
        client = InfluxDBClusterClient.from_DSN(self.configuration['url'])

        logger.debug("influxdb url: %s", self.configuration['url'])
        logger.debug("influxdb got query: %s", query)

        try:
            results = client.query(query)
            if not isinstance(results, list):
                results = [results]

            json_data = _transform_result(results)
            error = None
        except Exception as ex:
            json_data = None
            error = ex.message

        return json_data, error


register(InfluxDB)
<EOF>
<BOF>
import requests
from datetime import datetime
from urlparse import parse_qs
from redash.query_runner import BaseQueryRunner, register, TYPE_DATETIME, TYPE_STRING
from redash.utils import json_dumps


def get_instant_rows(metrics_data):
    rows = []

    for metric in metrics_data:
        row_data = metric['metric']

        timestamp, value = metric['value']
        date_time = datetime.fromtimestamp(timestamp)

        row_data.update({"timestamp": date_time, "value": value})
        rows.append(row_data)
    return rows


def get_range_rows(metrics_data):
    rows = []

    for metric in metrics_data:
        ts_values = metric['values']
        metric_labels = metric['metric']

        for values in ts_values:
            row_data = metric_labels.copy()

            timestamp, value = values
            date_time = datetime.fromtimestamp(timestamp)

            row_data.update({'timestamp': date_time, 'value': value})
            rows.append(row_data)
    return rows


class Prometheus(BaseQueryRunner):

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'url': {
                    'type': 'string',
                    'title': 'Prometheus API URL'
                }
            },
            "required": ["url"]
        }

    @classmethod
    def annotate_query(cls):
        return False

    def test_connection(self):
        resp = requests.get(self.configuration.get("url", None))
        return resp.ok

    def get_schema(self, get_stats=False):
        base_url = self.configuration["url"]
        metrics_path = '/api/v1/label/__name__/values'
        response = requests.get(base_url + metrics_path)
        response.raise_for_status()
        data = response.json()['data']

        schema = {}
        for name in data:
            schema[name] = {'name': name}

        return schema.values()

    def run_query(self, query, user):
        """
        Query Syntax, actually it is the URL query string.
        Check the Prometheus HTTP API for the details of the supported query string.

        https://prometheus.io/docs/prometheus/latest/querying/api/

        example: instant query
            query=http_requests_total

        example: range query
            query=http_requests_total&start=2018-01-20T00:00:00.000Z&end=2018-01-25T00:00:00.000Z&step=60s

        example: until now range query
            query=http_requests_total&start=2018-01-20T00:00:00.000Z&step=60s
            query=http_requests_total&start=2018-01-20T00:00:00.000Z&end=now&step=60s
        """

        base_url = self.configuration["url"]
        columns = [
            {
                'friendly_name': 'timestamp',
                'type': TYPE_DATETIME,
                'name': 'timestamp'
            },
            {
                'friendly_name': 'value',
                'type': TYPE_STRING,
                'name': 'value'
            },
        ]

        try:
            error = None
            query = query.strip()
            # for backward compatibility
            query = 'query={}'.format(query) if not query.startswith('query=') else query

            payload = parse_qs(query)
            query_type = 'query_range' if 'step' in payload.keys() else 'query'

            # for the range of until now
            if query_type == 'query_range' and ('end' not in payload.keys() or 'now' in payload['end']):
                date_now = datetime.now()
                payload.update({"end": [date_now.isoformat("T") + "Z"]})

            api_endpoint = base_url + '/api/v1/{}'.format(query_type)

            response = requests.get(api_endpoint, params=payload)
            response.raise_for_status()

            metrics = response.json()['data']['result']

            if len(metrics) == 0:
                return None, 'query result is empty.'

            metric_labels = metrics[0]['metric'].keys()

            for label_name in metric_labels:
                columns.append({
                    'friendly_name': label_name,
                    'type': TYPE_STRING,
                    'name': label_name
                })

            if query_type == 'query_range':
                rows = get_range_rows(metrics)
            else:
                rows = get_instant_rows(metrics)

            json_data = json_dumps(
                {
                    'rows': rows,
                    'columns': columns
                }
            )

        except requests.RequestException as e:
            return None, str(e)
        except KeyboardInterrupt:
            error = "Query cancelled by user."
            json_data = None

        return json_data, error


register(Prometheus)
<EOF>
<BOF>
import logging

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

try:
    import tdclient
    from tdclient import errors
    enabled = True

except ImportError:
    enabled = False

TD_TYPES_MAPPING = {
    'bigint': TYPE_INTEGER,
    'tinyint': TYPE_INTEGER,
    'smallint': TYPE_INTEGER,
    'int': TYPE_INTEGER,
    'integer': TYPE_INTEGER,
    'long': TYPE_INTEGER,
    'double': TYPE_FLOAT,
    'decimal': TYPE_FLOAT,
    'float': TYPE_FLOAT,
    'real': TYPE_FLOAT,
    'boolean': TYPE_BOOLEAN,
    'timestamp': TYPE_DATETIME,
    'date': TYPE_DATETIME,
    'char': TYPE_STRING,
    'string': TYPE_STRING,
    'varchar': TYPE_STRING,
}


class TreasureData(BaseQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'endpoint': {
                    'type': 'string'
                },
                'apikey': {
                    'type': 'string'
                },
                'type': {
                    'type': 'string'
                },
                'db': {
                    'type': 'string',
                    'title': 'Database Name'
                },
                'get_schema': {
                    'type': 'boolean',
                    'title': 'Auto Schema Retrieval',
                    'default': False
                }
            },
            'required': ['apikey','db']
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "treasuredata"

    def get_schema(self, get_stats=False):
        schema = {}
        if self.configuration.get('get_schema', False):
            try:
                with tdclient.Client(self.configuration.get('apikey')) as client:
                    for table in client.tables(self.configuration.get('db')):
                        table_name = '{}.{}'.format(self.configuration.get('db'), table.name)
                        for table_schema in table.schema:
                            schema[table_name] = {
                                'name': table_name,
                                'columns': [column[0] for column in table.schema],
                            }
            except Exception as ex:
                raise Exception("Failed getting schema")
        return schema.values()

    def run_query(self, query, user):
        connection = tdclient.connect(
                endpoint=self.configuration.get('endpoint', 'https://api.treasuredata.com'),
                apikey=self.configuration.get('apikey'),
                type=self.configuration.get('type', 'hive').lower(),
                db=self.configuration.get('db'))

        cursor = connection.cursor()
        try:
            cursor.execute(query)
            columns_tuples = [(i[0], TD_TYPES_MAPPING.get(i[1], None)) for i in cursor.show_job()['hive_result_schema']]
            columns = self.fetch_columns(columns_tuples)

            if cursor.rowcount == 0:
                rows = []
            else:
                rows = [dict(zip(([c['name'] for c in columns]), r)) for i, r in enumerate(cursor.fetchall())]
            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None
        except errors.InternalError as e:
            json_data = None
            error = "%s: %s" % (e.message, cursor.show_job().get('debug', {}).get('stderr', 'No stderr message in the response'))
        return json_data, error

register(TreasureData)
<EOF>
<BOF>
import base64
from .hive_ds import Hive
from redash.query_runner import register

try:
    from pyhive import hive
    from thrift.transport import THttpClient
    enabled = True
except ImportError:
    enabled = False


class Databricks(Hive):

    @classmethod
    def type(cls):
        return "databricks"

    @classmethod
    def enabled(cls):
        return enabled


register(Databricks)
<EOF>
<BOF>
import logging
import sys

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

try:
    from dql import Engine, FragmentEngine
    from pyparsing import ParseException
    enabled = True
except ImportError as e:
    enabled = False

types_map = {
    'UNICODE': TYPE_INTEGER,
    'TINYINT': TYPE_INTEGER,
    'SMALLINT': TYPE_INTEGER,
    'INT': TYPE_INTEGER,
    'DOUBLE': TYPE_FLOAT,
    'DECIMAL': TYPE_FLOAT,
    'FLOAT': TYPE_FLOAT,
    'REAL': TYPE_FLOAT,
    'BOOLEAN': TYPE_BOOLEAN,
    'TIMESTAMP': TYPE_DATETIME,
    'DATE': TYPE_DATETIME,
    'CHAR': TYPE_STRING,
    'STRING': TYPE_STRING,
    'VARCHAR': TYPE_STRING
}


class DynamoDBSQL(BaseSQLQueryRunner):
    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "region": {
                    "type": "string",
                    "default": "us-east-1"
                },
                "access_key": {
                    "type": "string",
                },
                "secret_key": {
                    "type": "string",
                }
            },
            "required": ["access_key", "secret_key"],
            "secret": ["secret_key"]
        }

    def test_connection(self):
        engine = self._connect()
        list(engine.connection.list_tables())

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "dynamodb_sql"

    @classmethod
    def name(cls):
        return "DynamoDB (with DQL)"

    def _connect(self):
        engine = FragmentEngine()
        config = self.configuration.to_dict()

        if not config.get('region'):
            config['region'] = 'us-east-1'

        if config.get('host') == '':
            config['host'] = None

        engine.connect(**config)

        return engine

    def _get_tables(self, schema):
        engine = self._connect()

        for table in engine.describe_all():
            schema[table.name] = {'name': table.name, 'columns': table.attrs.keys()}

    def run_query(self, query, user):
        engine = None
        try:
            engine = self._connect()

            result = engine.execute(query if str(query).endswith(';') else str(query)+';')

            columns = []
            rows = []

            # When running a count query it returns the value as a string, in which case
            # we transform it into a dictionary to be the same as regular queries.
            if isinstance(result, basestring):
                # when count < scanned_count, dql returns a string with number of rows scanned
                value = result.split(" (")[0]
                if value:
                    value = int(value)
                result = [{"value": value}]

            for item in result:
                if not columns:
                    for k, v in item.iteritems():
                        columns.append({
                            'name': k,
                            'friendly_name': k,
                            'type': types_map.get(str(type(v)).upper(), None)
                        })
                rows.append(item)

            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None
        except ParseException as e:
            error = u"Error parsing query at line {} (column {}):\n{}".format(e.lineno, e.column, e.line)
            json_data = None
        except (SyntaxError, RuntimeError) as e:
            error = e.message
            json_data = None
        except KeyboardInterrupt:
            if engine and engine.connection:
                engine.connection.cancel()
            error = "Query cancelled by user."
            json_data = None

        return json_data, error

register(DynamoDBSQL)
<EOF>
<BOF>
import logging

from redash.query_runner import BaseQueryRunner, register
from redash.utils import JSONEncoder, json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    from cassandra.cluster import Cluster
    from cassandra.auth import PlainTextAuthProvider
    from cassandra.util import sortedset
    enabled = True
except ImportError:
    enabled = False


class CassandraJSONEncoder(JSONEncoder):
    def default(self, o):
        if isinstance(o, sortedset):
            return list(o)
        return super(CassandraJSONEncoder, self).default(o)


class Cassandra(BaseQueryRunner):
    noop_query = "SELECT dateof(now()) FROM system.local"

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'host': {
                    'type': 'string',
                },
                'port': {
                    'type': 'number',
                    'default': 9042,
                },
                'keyspace': {
                    'type': 'string',
                    'title': 'Keyspace name'
                },
                'username': {
                    'type': 'string',
                    'title': 'Username'
                },
                'password': {
                    'type': 'string',
                    'title': 'Password'
                },
                'protocol': {
                    'type': 'number',
                    'title': 'Protocol Version',
                    'default': 3
                },
                'timeout': {
                    'type': 'number',
                    'title': 'Timeout',
                    'default': 10
                }
            },
            'required': ['keyspace', 'host']
        }

    @classmethod
    def type(cls):
        return "Cassandra"

    def get_schema(self, get_stats=False):
        query = """
        select release_version from system.local;
        """
        results, error = self.run_query(query, None)
        results = json_loads(results)
        release_version = results['rows'][0]['release_version']

        query = """
        SELECT table_name, column_name
        FROM system_schema.columns
        WHERE keyspace_name ='{}';
        """.format(self.configuration['keyspace'])

        if release_version.startswith('2'):
                query = """
                SELECT columnfamily_name AS table_name, column_name
                FROM system.schema_columns
                WHERE keyspace_name ='{}';
                """.format(self.configuration['keyspace'])

        results, error = self.run_query(query, None)
        results = json_loads(results)

        schema = {}
        for row in results['rows']:
            table_name = row['table_name']
            column_name = row['column_name']
            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}
            schema[table_name]['columns'].append(column_name)

        return schema.values()

    def run_query(self, query, user):
        connection = None
        try:
            if self.configuration.get('username', '') and self.configuration.get('password', ''):
                auth_provider = PlainTextAuthProvider(username='{}'.format(self.configuration.get('username', '')),
                                                      password='{}'.format(self.configuration.get('password', '')))
                connection = Cluster([self.configuration.get('host', '')],
                                     auth_provider=auth_provider,
                                     port=self.configuration.get('port', ''),
                                     protocol_version=self.configuration.get('protocol', 3))
            else:
                connection = Cluster([self.configuration.get('host', '')],
                                     port=self.configuration.get('port', ''),
                                     protocol_version=self.configuration.get('protocol', 3))
            session = connection.connect()
            session.set_keyspace(self.configuration['keyspace'])
            session.default_timeout = self.configuration.get('timeout', 10)
            logger.debug("Cassandra running query: %s", query)
            result = session.execute(query)

            column_names = result.column_names

            columns = self.fetch_columns(map(lambda c: (c, 'string'), column_names))

            rows = [dict(zip(column_names, row)) for row in result]

            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data, cls=CassandraJSONEncoder)

            error = None
        except KeyboardInterrupt:
            error = "Query cancelled by user."
            json_data = None

        return json_data, error


class ScyllaDB(Cassandra):

    @classmethod
    def type(cls):
        return "scylla"


register(Cassandra)
register(ScyllaDB)
<EOF>
<BOF>
import requests
import os
from redash.query_runner import *
from redash.utils import JSONEncoder
import json


def _get_type(value):
    if isinstance(value, int):
        return TYPE_INTEGER
    elif isinstance(value, float):
        return TYPE_FLOAT
    elif isinstance(value, bool):
        return TYPE_BOOLEAN
    elif isinstance(value, str):
        return TYPE_STRING
    return TYPE_STRING


# The following is here, because Rockset's PyPi package is Python 3 only.
# Should be removed once we move to Python 3.
class RocksetAPI(object):
    def __init__(self, api_key, api_server):
        self.api_key = api_key
        self.api_server = api_server

    def _request(self, endpoint, method='GET', body=None):
        headers = {'Authorization': 'ApiKey {}'.format(self.api_key)}
        url = '{}/v1/orgs/self/{}'.format(self.api_server, endpoint)

        if method == 'GET':
            r = requests.get(url, headers=headers)
            return r.json()
        elif method == 'POST':
            r = requests.post(url, headers=headers, json=body)
            return r.json()
        else:
            raise 'Unknown method: {}'.format(method)

    def list(self):
        response = self._request('ws/commons/collections')
        return response['data']

    def query(self, sql):
        return self._request('queries', 'POST', {'sql': {'query': sql}})


class Rockset(BaseSQLQueryRunner):
    noop_query = 'SELECT 1'

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "api_server": {
                    "type": "string",
                    "title": "API Server",
                    "default": "https://api.rs2.usw2.rockset.com"
                },
                "api_key": {
                    "title": "API Key",
                    "type": "string",
                },
            },
            "order": ["api_key", "api_server"],
            "required": ["api_server", "api_key"],
            "secret": ["api_key"]
        }

    @classmethod
    def type(cls):
        return "rockset"

    def __init__(self, configuration):
        super(Rockset, self).__init__(configuration)
        self.api = RocksetAPI(self.configuration.get('api_key'), self.configuration.get(
            'api_server', "https://api.rs2.usw2.rockset.com"))

    def _get_tables(self, schema):
        for col in self.api.list():
            table_name = col['name']
            describe = self.api.query('DESCRIBE "{}"'.format(table_name))
            columns = list(set(map(lambda x: x['field'][0], describe['results'])))
            schema[table_name] = {'name': table_name, 'columns': columns}
        return schema.values()

    def run_query(self, query, user):
        results = self.api.query(query)
        if 'code' in results and results['code'] != 200:
            return None, '{}: {}'.format(results['type'], results['message'])

        rows = results['results']
        columns = []
        if len(rows) > 0:
            columns = []
            for k in rows[0]:
                columns.append({'name': k, 'friendly_name': k, 'type': _get_type(rows[0][k])})
        data = json.dumps({'columns': columns, 'rows': rows}, cls=JSONEncoder)
        return data, None


register(Rockset)
<EOF>
<BOF>
import os
import subprocess
import sys

from redash.query_runner import *


def query_to_script_path(path, query):
    if path != "*":
        script = os.path.join(path, query.split(" ")[0])
        if not os.path.exists(script):
            raise IOError("Script '{}' not found in script directory".format(query))

        return os.path.join(path, query).split(" ")

    return query


def run_script(script, shell):
    output = subprocess.check_output(script, shell=shell)
    if output is None:
        return None, "Error reading output"

    output = output.strip()
    if not output:
        return None, "Empty output from script"

    return output, None


class Script(BaseQueryRunner):
    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def enabled(cls):
        return "check_output" in subprocess.__dict__

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'path': {
                    'type': 'string',
                    'title': 'Scripts path'
                },
                'shell': {
                    'type': 'boolean',
                    'title': 'Execute command through the shell'
                }
            },
            'required': ['path']
        }

    @classmethod
    def type(cls):
        return "insecure_script"

    def __init__(self, configuration):
        super(Script, self).__init__(configuration)

        # If path is * allow any execution path
        if self.configuration["path"] == "*":
            return

        # Poor man's protection against running scripts from outside the scripts directory
        if self.configuration["path"].find("../") > -1:
            raise ValueError("Scripts can only be run from the configured scripts directory")

    def test_connection(self):
        pass

    def run_query(self, query, user):
        try:
            script = query_to_script_path(self.configuration["path"], query)
            return run_script(script, self.configuration['shell'])
        except IOError as e:
            return None, e.message
        except subprocess.CalledProcessError as e:
            return None, str(e)
        except KeyboardInterrupt:
            return None, "Query cancelled by user."


register(Script)
<EOF>
<BOF>
import logging
import sys

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

try:
    from memsql.common import database
    enabled = True
except ImportError:
    enabled = False

COLUMN_NAME = 0
COLUMN_TYPE = 1

types_map = {
    'BIGINT': TYPE_INTEGER,
    'TINYINT': TYPE_INTEGER,
    'SMALLINT': TYPE_INTEGER,
    'MEDIUMINT': TYPE_INTEGER,
    'INT': TYPE_INTEGER,
    'DOUBLE': TYPE_FLOAT,
    'DECIMAL': TYPE_FLOAT,
    'FLOAT': TYPE_FLOAT,
    'REAL': TYPE_FLOAT,
    'BOOL': TYPE_BOOLEAN,
    'BOOLEAN': TYPE_BOOLEAN,
    'TIMESTAMP': TYPE_DATETIME,
    'DATETIME': TYPE_DATETIME,
    'DATE': TYPE_DATETIME,
    'JSON': TYPE_STRING,
    'CHAR': TYPE_STRING,
    'VARCHAR': TYPE_STRING
}


class MemSQL(BaseSQLQueryRunner):
    noop_query = 'SELECT 1'

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "host": {
                    "type": "string"
                },
                "port": {
                    "type": "number"
                },
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                }

            },
            "required": ["host", "port"],
            "secret": ["password"]
        }

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "memsql"

    @classmethod
    def enabled(cls):
        return enabled

    def _get_tables(self, schema):
        schemas_query = "show schemas"

        tables_query = "show tables in %s"

        columns_query = "show columns in %s"

        for schema_name in filter(lambda a: len(a) > 0,
                                  map(lambda a: str(a['Database']), self._run_query_internal(schemas_query))):
            for table_name in filter(lambda a: len(a) > 0, map(lambda a: str(a['Tables_in_%s' % schema_name]),
                                                               self._run_query_internal(
                                                                       tables_query % schema_name))):
                table_name = '.'.join((schema_name, table_name))
                columns = filter(lambda a: len(a) > 0, map(lambda a: str(a['Field']),
                                                           self._run_query_internal(columns_query % table_name)))

                schema[table_name] = {'name': table_name, 'columns': columns}
        return schema.values()

    def run_query(self, query, user):

        cursor = None
        try:
            cursor = database.connect(**self.configuration.to_dict())

            res = cursor.query(query)
            # column_names = []
            # columns = []
            #
            # for column in cursor.description:
            #     column_name = column[COLUMN_NAME]
            #     column_names.append(column_name)
            #
            #     columns.append({
            #         'name': column_name,
            #         'friendly_name': column_name,
            #         'type': types_map.get(column[COLUMN_TYPE], None)
            #     })

            rows = [dict(zip(list(row.keys()), list(row.values()))) for row in res]

            # ====================================================================================================
            # temporary - until https://github.com/memsql/memsql-python/pull/8 gets merged
            # ====================================================================================================
            columns = []
            column_names = rows[0].keys() if rows else None

            if column_names:
                for column in column_names:
                    columns.append({
                        'name': column,
                        'friendly_name': column,
                        'type': TYPE_STRING
                    })

            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None
        except KeyboardInterrupt:
            cursor.close()
            error = "Query cancelled by user."
            json_data = None
        finally:
            if cursor:
                cursor.close()

        return json_data, error


register(MemSQL)
<EOF>
<BOF>
# -*- coding: utf-8 -*-

import logging
from base64 import b64decode
from datetime import datetime
from urlparse import parse_qs, urlparse

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    from oauth2client.service_account import ServiceAccountCredentials
    from apiclient.discovery import build
    from apiclient.errors import HttpError
    import httplib2
    enabled = True
except ImportError as e:
    enabled = False


types_conv = dict(
    STRING=TYPE_STRING,
    INTEGER=TYPE_INTEGER,
    FLOAT=TYPE_FLOAT,
    DATE=TYPE_DATE,
    DATETIME=TYPE_DATETIME
)


def parse_ga_response(response):
    columns = []
    for h in response['columnHeaders']:
        if h['name'] in ('ga:date', 'mcf:conversionDate'):
            h['dataType'] = 'DATE'
        elif h['name'] == 'ga:dateHour':
            h['dataType'] = 'DATETIME'
        columns.append({
            'name': h['name'],
            'friendly_name': h['name'].split(':', 1)[1],
            'type': types_conv.get(h['dataType'], 'string')
        })

    rows = []
    for r in response.get('rows', []):
        d = {}
        for c, value in enumerate(r):
            column_name = response['columnHeaders'][c]['name']
            column_type = filter(lambda col: col['name'] == column_name, columns)[0]['type']

            # mcf results come a bit different than ga results:
            if isinstance(value, dict):
                if 'primitiveValue' in value:
                    value = value['primitiveValue']
                elif 'conversionPathValue' in value:
                    steps = []
                    for step in value['conversionPathValue']:
                        steps.append('{}:{}'.format(step['interactionType'], step['nodeValue']))
                    value = ', '.join(steps)
                else:
                    raise Exception("Results format not supported")

            if column_type == TYPE_DATE:
                value = datetime.strptime(value, '%Y%m%d')
            elif column_type == TYPE_DATETIME:
                if len(value) == 10:
                    value = datetime.strptime(value, '%Y%m%d%H')
                elif len(value) == 12:
                    value = datetime.strptime(value, '%Y%m%d%H%M')
                else:
                    raise Exception("Unknown date/time format in results: '{}'".format(value))

            d[column_name] = value
        rows.append(d)

    return {'columns': columns, 'rows': rows}


class GoogleAnalytics(BaseSQLQueryRunner):
    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "google_analytics"

    @classmethod
    def name(cls):
        return "Google Analytics"

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'jsonKeyFile': {
                    "type": "string",
                    'title': 'JSON Key File'
                }
            },
            'required': ['jsonKeyFile'],
            'secret': ['jsonKeyFile']
        }

    def __init__(self, configuration):
        super(GoogleAnalytics, self).__init__(configuration)
        self.syntax = 'json'

    def _get_analytics_service(self):
        scope = ['https://www.googleapis.com/auth/analytics.readonly']
        key = json_loads(b64decode(self.configuration['jsonKeyFile']))
        creds = ServiceAccountCredentials.from_json_keyfile_dict(key, scope)
        return build('analytics', 'v3', http=creds.authorize(httplib2.Http()))

    def _get_tables(self, schema):
        accounts = self._get_analytics_service().management().accounts().list().execute().get('items')
        if accounts is None:
            raise Exception("Failed getting accounts.")
        else:
            for account in accounts:
                schema[account['name']] = {'name': account['name'], 'columns': []}
                properties = self._get_analytics_service().management().webproperties().list(
                    accountId=account['id']).execute().get('items', [])
                for property_ in properties:
                    if 'defaultProfileId' in property_ and 'name' in property_:
                        schema[account['name']]['columns'].append(
                            u'{0} (ga:{1})'.format(property_['name'], property_['defaultProfileId'])
                        )

        return schema.values()

    def test_connection(self):
        try:
            service = self._get_analytics_service()
            service.management().accounts().list().execute()
        except HttpError as e:
            # Make sure we return a more readable error to the end user
            raise Exception(e._get_reason())

    def run_query(self, query, user):
        logger.debug("Analytics is about to execute query: %s", query)
        try:
            params = json_loads(query)
        except:
            params = parse_qs(urlparse(query).query, keep_blank_values=True)
            for key in params.keys():
                params[key] = ','.join(params[key])
                if '-' in key:
                    params[key.replace('-', '_')] = params.pop(key)

        if 'mcf:' in params['metrics'] and 'ga:' in params['metrics']:
            raise Exception("Can't mix mcf: and ga: metrics.")

        if 'mcf:' in params.get('dimensions', '') and 'ga:' in params.get('dimensions', ''):
            raise Exception("Can't mix mcf: and ga: dimensions.")

        if 'mcf:' in params['metrics']:
            api = self._get_analytics_service().data().mcf()
        else:
            api = self._get_analytics_service().data().ga()

        if len(params) > 0:
            try:
                response = api.get(**params).execute()
                data = parse_ga_response(response)
                error = None
                json_data = json_dumps(data)
            except HttpError as e:
                # Make sure we return a more readable error to the end user
                error = e._get_reason()
                json_data = None
        else:
            error = 'Wrong query format.'
            json_data = None
        return json_data, error


register(GoogleAnalytics)
<EOF>
<BOF>
import sys
import logging

from redash.utils import json_loads, json_dumps
from redash.query_runner import *

logger = logging.getLogger(__name__)

types_map = {
    5: TYPE_BOOLEAN,
    6: TYPE_INTEGER,
    7: TYPE_FLOAT,
    8: TYPE_STRING,
    9: TYPE_STRING,
    10: TYPE_DATE,
    11: TYPE_DATETIME,
    12: TYPE_DATETIME,
    13: TYPE_DATETIME,
    14: TYPE_DATETIME,
    15: TYPE_DATETIME,
    16: TYPE_FLOAT,
    17: TYPE_STRING,
    114: TYPE_DATETIME,
    115: TYPE_STRING,
    116: TYPE_STRING,
    117: TYPE_STRING
}


class Vertica(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'host': {
                    'type': 'string'
                },
                'user': {
                    'type': 'string'
                },
                'password': {
                    'type': 'string',
                    'title': 'Password'
                },
                'database': {
                    'type': 'string',
                    'title': 'Database name'
                },
                "port": {
                    "type": "number"
                },
                "read_timeout": {
                    "type": "number",
                    "title": "Read Timeout"
                },
                "connection_timeout": {
                    "type": "number",
                    "title": "Connection Timeout"
                },
            },
            'required': ['database'],
            'order': ['host', 'port', 'user', 'password', 'database', 'read_timeout', 'connection_timeout'],
            'secret': ['password']
        }

    @classmethod
    def enabled(cls):
        try:
            import vertica_python
        except ImportError:
            return False

        return True

    def _get_tables(self, schema):
        query = """
        Select table_schema, table_name, column_name from columns where is_system_table=false
        union all
        select table_schema, table_name, column_name from view_columns;
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            table_name = '{}.{}'.format(row['table_schema'], row['table_name'])

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['column_name'])

        return schema.values()

    def run_query(self, query, user):
        import vertica_python

        if query == "":
            json_data = None
            error = "Query is empty"
            return json_data, error

        connection = None
        try:
            conn_info = {
                'host': self.configuration.get('host', ''),
                'port': self.configuration.get('port', 5433),
                'user': self.configuration.get('user', ''),
                'password': self.configuration.get('password', ''),
                'database': self.configuration.get('database', ''),
                'read_timeout': self.configuration.get('read_timeout', 600)
            }
            
            if self.configuration.get('connection_timeout'):
                conn_info['connection_timeout'] = self.configuration.get('connection_timeout')

            connection = vertica_python.connect(**conn_info)
            cursor = connection.cursor()
            logger.debug("Vetica running query: %s", query)
            cursor.execute(query)

            # TODO - very similar to pg.py
            if cursor.description is not None:
                columns_data = [(i[0], i[1]) for i in cursor.description]

                rows = [dict(zip((c[0] for c in columns_data), row)) for row in cursor.fetchall()]
                columns = [{'name': col[0],
                            'friendly_name': col[0],
                            'type': types_map.get(col[1], None)} for col in columns_data]

                data = {'columns': columns, 'rows': rows}
                json_data = json_dumps(data)
                error = None
            else:
                json_data = None
                error = "No data was returned."

            cursor.close()
        except KeyboardInterrupt:
            error = "Query cancelled by user."
            json_data = None
        finally:
            if connection:
                connection.close()

        return json_data, error

register(Vertica)
<EOF>
<BOF>
from redash.query_runner import BaseHTTPQueryRunner, register


class Url(BaseHTTPQueryRunner):
    requires_url = False

    @classmethod
    def annotate_query(cls):
        return False

    def test_connection(self):
        pass

    def run_query(self, query, user):
        base_url = self.configuration.get("url", None)

        try:
            query = query.strip()

            if base_url is not None and base_url != "":
                if query.find("://") > -1:
                    return None, "Accepting only relative URLs to '%s'" % base_url

            if base_url is None:
                base_url = ""

            url = base_url + query

            response, error = self.get_response(url)
            if error is not None:
                return None, error

            json_data = response.content.strip()

            if json_data:
                return json_data, None
            else:
                return None, "Got empty response from '{}'.".format(url)
        except KeyboardInterrupt:
            return None, "Query cancelled by user."


register(Url)
<EOF>
<BOF>
# -*- coding: utf-8 -*-

import re
import logging
from collections import OrderedDict
from redash.query_runner import BaseQueryRunner, register
from redash.query_runner import TYPE_STRING, TYPE_DATE, TYPE_DATETIME, TYPE_INTEGER, TYPE_FLOAT, TYPE_BOOLEAN
from redash.utils import json_dumps
logger = logging.getLogger(__name__)

try:
    from simple_salesforce import Salesforce as SimpleSalesforce
    from simple_salesforce.api import SalesforceError, DEFAULT_API_VERSION
    enabled = True
except ImportError as e:
    enabled = False

# See https://developer.salesforce.com/docs/atlas.en-us.api.meta/api/field_types.htm
TYPES_MAP = dict(
    id=TYPE_STRING,
    string=TYPE_STRING,
    currency=TYPE_FLOAT,
    reference=TYPE_STRING,
    double=TYPE_FLOAT,
    picklist=TYPE_STRING,
    date=TYPE_DATE,
    url=TYPE_STRING,
    phone=TYPE_STRING,
    textarea=TYPE_STRING,
    int=TYPE_INTEGER,
    datetime=TYPE_DATETIME,
    boolean=TYPE_BOOLEAN,
    percent=TYPE_FLOAT,
    multipicklist=TYPE_STRING,
    masterrecord=TYPE_STRING,
    location=TYPE_STRING,
    JunctionIdList=TYPE_STRING,
    encryptedstring=TYPE_STRING,
    email=TYPE_STRING,
    DataCategoryGroupReference=TYPE_STRING,
    combobox=TYPE_STRING,
    calculated=TYPE_STRING,
    anyType=TYPE_STRING,
    address=TYPE_STRING
)

# Query Runner for Salesforce SOQL Queries
# For example queries, see:
# https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql_select_examples.htm


class Salesforce(BaseQueryRunner):

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "username": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "token": {
                    "type": "string",
                    "title": "Security Token"
                },
                "sandbox": {
                    "type": "boolean"
                },
                "api_version": {
                    "type": "string",
                    "title": "Salesforce API Version",
                    "default": DEFAULT_API_VERSION
                }
            },
            "required": ["username", "password", "token"],
            "secret": ["password", "token"]
        }

    def test_connection(self):
        response = self._get_sf().describe()
        if response is None:
            raise Exception("Failed describing objects.")
        pass

    def _get_sf(self):
        sf = SimpleSalesforce(username=self.configuration['username'],
                              password=self.configuration['password'],
                              security_token=self.configuration['token'],
                              sandbox=self.configuration.get('sandbox', False),
                              version=self.configuration.get('api_version', DEFAULT_API_VERSION),
                              client_id='Redash')
        return sf

    def _clean_value(self, value):
        if isinstance(value, OrderedDict) and 'records' in value:
            value = value['records']
            for row in value:
                row.pop('attributes', None)
        return value

    def _get_value(self, dct, dots):
        for key in dots.split('.'):
            if dct is not None and key in dct:
                dct = dct.get(key)
            else:
                dct = None
        return dct

    def _get_column_name(self, key, parents=[]):
        return '.'.join(parents + [key])

    def _build_columns(self, sf, child, parents=[]):
        child_type = child['attributes']['type']
        child_desc = sf.__getattr__(child_type).describe()
        child_type_map = dict((f['name'], f['type'])for f in child_desc['fields'])
        columns = []
        for key in child.keys():
            if key != 'attributes':
                if isinstance(child[key], OrderedDict) and 'attributes' in child[key]:
                    columns.extend(self._build_columns(sf, child[key], parents + [key]))
                else:
                    column_name = self._get_column_name(key, parents)
                    key_type = child_type_map.get(key, 'string')
                    column_type = TYPES_MAP.get(key_type, TYPE_STRING)
                    columns.append((column_name, column_type))
        return columns

    def _build_rows(self, columns, records):
        rows = []
        for record in records:
            record.pop('attributes', None)
            row = dict()
            for column in columns:
                key = column[0]
                value = self._get_value(record, key)
                row[key] = self._clean_value(value)
            rows.append(row)
        return rows

    def run_query(self, query, user):
        logger.debug("Salesforce is about to execute query: %s", query)
        query = re.sub(r"/\*(.|\n)*?\*/", "", query).strip()
        try:
            columns = []
            rows = []
            sf = self._get_sf()
            response = sf.query_all(query)
            records = response['records']
            if response['totalSize'] > 0 and len(records) == 0:
                columns = self.fetch_columns([('Count', TYPE_INTEGER)])
                rows = [{'Count': response['totalSize']}]
            elif len(records) > 0:
                cols = self._build_columns(sf, records[0])
                rows = self._build_rows(cols, records)
                columns = self.fetch_columns(cols)
            error = None
            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
        except SalesforceError as err:
            error = err.content
            json_data = None
        return json_data, error

    def get_schema(self, get_stats=False):
        sf = self._get_sf()
        response = sf.describe()
        if response is None:
            raise Exception("Failed describing objects.")

        schema = {}
        for sobject in response['sobjects']:
            table_name = sobject['name']
            if sobject['queryable'] is True and table_name not in schema:
                desc = sf.__getattr__(sobject['name']).describe()
                fields = desc['fields']
                schema[table_name] = {'name': table_name, 'columns': [f['name'] for f in fields]}
        return schema.values()

register(Salesforce)
<EOF>
<BOF>
from redash.query_runner import *
from redash.utils import json_dumps, json_loads

import logging
logger = logging.getLogger(__name__)

from collections import defaultdict

try:
    from pyhive import presto
    from pyhive.exc import DatabaseError
    enabled = True

except ImportError:
    enabled = False

PRESTO_TYPES_MAPPING = {
    "integer": TYPE_INTEGER,
    "tinyint": TYPE_INTEGER,
    "smallint": TYPE_INTEGER,
    "long": TYPE_INTEGER,
    "bigint": TYPE_INTEGER,
    "float": TYPE_FLOAT,
    "double": TYPE_FLOAT,
    "boolean": TYPE_BOOLEAN,
    "string": TYPE_STRING,
    "varchar": TYPE_STRING,
    "date": TYPE_DATE,
}


class Presto(BaseQueryRunner):
    noop_query = 'SHOW TABLES'

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'host': {
                    'type': 'string'
                },
                'protocol': {
                    'type': 'string',
                    'default': 'http'
                },
                'port': {
                    'type': 'number'
                },
                'schema': {
                    'type': 'string'
                },
                'catalog': {
                    'type': 'string'
                },
                'username': {
                    'type': 'string'
                },
            },
            'order': ['host', 'protocol', 'port', 'username', 'schema', 'catalog'],
            'required': ['host']
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def type(cls):
        return "presto"

    def get_schema(self, get_stats=False):
        schema = {}
        query = """
        SELECT table_schema, table_name, column_name
        FROM information_schema.columns
        WHERE table_schema NOT IN ('pg_catalog', 'information_schema')
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            table_name = '{}.{}'.format(row['table_schema'], row['table_name'])

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['column_name'])

        return schema.values()

    def run_query(self, query, user):
        connection = presto.connect(
                host=self.configuration.get('host', ''),
                port=self.configuration.get('port', 8080),
                protocol=self.configuration.get('protocol', 'http'),
                username=self.configuration.get('username', 'redash'),
                catalog=self.configuration.get('catalog', 'hive'),
                schema=self.configuration.get('schema', 'default'))

        cursor = connection.cursor()


        try:
            cursor.execute(query)
            column_tuples = [(i[0], PRESTO_TYPES_MAPPING.get(i[1], None)) for i in cursor.description]
            columns = self.fetch_columns(column_tuples)
            rows = [dict(zip(([c['name'] for c in columns]), r)) for i, r in enumerate(cursor.fetchall())]
            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None
        except DatabaseError as db:
            json_data = None
            default_message = 'Unspecified DatabaseError: {0}'.format(db.message)
            if isinstance(db.message, dict):
                message = db.message.get('failureInfo', {'message', None}).get('message')
            else:
                message = None
            error = default_message if message is None else message
        except (KeyboardInterrupt, InterruptException) as e:
            cursor.cancel()
            error = "Query cancelled by user."
            json_data = None
        except Exception as ex:
            json_data = None
            error = ex.message
            if not isinstance(error, basestring):
                error = unicode(error)

        return json_data, error

register(Presto)
<EOF>
<BOF>
import logging
import sys
import uuid

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    import pymssql
    enabled = True
except ImportError:
    enabled = False

# from _mssql.pyx ## DB-API type definitions & http://www.freetds.org/tds.html#types ##
types_map = {
    1: TYPE_STRING,
    2: TYPE_STRING,
    # Type #3 supposed to be an integer, but in some cases decimals are returned
    # with this type. To be on safe side, marking it as float.
    3: TYPE_FLOAT,
    4: TYPE_DATETIME,
    5: TYPE_FLOAT,
}


class SqlServer(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "server": {
                    "type": "string",
                    "default": "127.0.0.1"
                },
                "port": {
                    "type": "number",
                    "default": 1433
                },
                "tds_version": {
                    "type": "string",
                    "default": "7.0",
                    "title": "TDS Version"
                },
                "charset": {
                    "type": "string",
                    "default": "UTF-8",
                    "title": "Character Set"
                },
                "db": {
                    "type": "string",
                    "title": "Database Name"
                }
            },
            "required": ["db"],
            "secret": ["password"]
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def name(cls):
        return "Microsoft SQL Server"

    @classmethod
    def type(cls):
        return "mssql"

    @classmethod
    def annotate_query(cls):
        return False

    def _get_tables(self, schema):
        query = """
        SELECT table_schema, table_name, column_name
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE table_schema NOT IN ('guest','INFORMATION_SCHEMA','sys','db_owner','db_accessadmin'
                                  ,'db_securityadmin','db_ddladmin','db_backupoperator','db_datareader'
                                  ,'db_datawriter','db_denydatareader','db_denydatawriter'
                                  );
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            if row['table_schema'] != self.configuration['db']:
                table_name = u'{}.{}'.format(row['table_schema'], row['table_name'])
            else:
                table_name = row['table_name']

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['column_name'])

        return schema.values()

    def run_query(self, query, user):
        connection = None

        try:
            server = self.configuration.get('server', '')
            user = self.configuration.get('user', '')
            password = self.configuration.get('password', '')
            db = self.configuration['db']
            port = self.configuration.get('port', 1433)
            tds_version = self.configuration.get('tds_version', '7.0')
            charset = self.configuration.get('charset', 'UTF-8')

            if port != 1433:
                server = server + ':' + str(port)

            connection = pymssql.connect(server=server, user=user, password=password, database=db, tds_version=tds_version, charset=charset)

            if isinstance(query, unicode):
                query = query.encode(charset)

            cursor = connection.cursor()
            logger.debug("SqlServer running query: %s", query)

            cursor.execute(query)
            data = cursor.fetchall()

            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], types_map.get(i[1], None)) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in data]

                data = {'columns': columns, 'rows': rows}
                json_data = json_dumps(data)
                error = None
            else:
                error = "No data was returned."
                json_data = None

            cursor.close()
        except pymssql.Error as e:
            try:
                # Query errors are at `args[1]`
                error = e.args[1]
            except IndexError:
                # Connection errors are `args[0][1]`
                error = e.args[0][1]
            json_data = None
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            if connection:
                connection.close()

        return json_data, error

register(SqlServer)
<EOF>
<BOF>
import logging
from base64 import b64decode

from dateutil import parser
from requests import Session
from xlsxwriter.utility import xl_col_to_name

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    import gspread
    from gspread.httpsession import HTTPSession
    from oauth2client.service_account import ServiceAccountCredentials

    enabled = True
except ImportError:
    enabled = False


def _load_key(filename):
    with open(filename, "rb") as f:
        return json_loads(f.read())


def _get_columns_and_column_names(row):
    column_names = []
    columns = []
    duplicate_counter = 1

    for i, column_name in enumerate(row):
        if not column_name:
            column_name = 'column_{}'.format(xl_col_to_name(i))

        if column_name in column_names:
            column_name = u"{}{}".format(column_name, duplicate_counter)
            duplicate_counter += 1

        column_names.append(column_name)
        columns.append({
            'name': column_name,
            'friendly_name': column_name,
            'type': TYPE_STRING
        })

    return columns, column_names


def _guess_type(value):
    if value == '':
        return TYPE_STRING
    try:
        val = int(value)
        return TYPE_INTEGER
    except ValueError:
        pass
    try:
        val = float(value)
        return TYPE_FLOAT
    except ValueError:
        pass
    if unicode(value).lower() in ('true', 'false'):
        return TYPE_BOOLEAN
    try:
        val = parser.parse(value)
        return TYPE_DATETIME
    except (ValueError, OverflowError):
        pass
    return TYPE_STRING


def _value_eval_list(row_values, col_types):
    value_list = []
    raw_values = zip(col_types, row_values)
    for typ, rval in raw_values:
        try:
            if rval is None or rval == '':
                val = None
            elif typ == TYPE_BOOLEAN:
                val = True if unicode(rval).lower() == 'true' else False
            elif typ == TYPE_DATETIME:
                val = parser.parse(rval)
            elif typ == TYPE_FLOAT:
                val = float(rval)
            elif typ == TYPE_INTEGER:
                val = int(rval)
            else:
                # for TYPE_STRING and default
                val = unicode(rval)
            value_list.append(val)
        except (ValueError, OverflowError):
            value_list.append(rval)
    return value_list


HEADER_INDEX = 0


class WorksheetNotFoundError(Exception):
    def __init__(self, worksheet_num, worksheet_count):
        message = "Worksheet number {} not found. Spreadsheet has {} worksheets. Note that the worksheet count is zero based.".format(worksheet_num, worksheet_count)
        super(WorksheetNotFoundError, self).__init__(message)


def parse_query(query):
    values = query.split("|")
    key = values[0]  # key of the spreadsheet
    worksheet_num = 0 if len(values) != 2 else int(values[1])  # if spreadsheet contains more than one worksheet - this is the number of it

    return key, worksheet_num


def parse_worksheet(worksheet):
    if not worksheet:
        return {'columns': [], 'rows': []}

    columns, column_names = _get_columns_and_column_names(worksheet[HEADER_INDEX])

    if len(worksheet) > 1:
        for j, value in enumerate(worksheet[HEADER_INDEX + 1]):
            columns[j]['type'] = _guess_type(value)

    column_types = [c['type'] for c in columns]
    rows = [dict(zip(column_names, _value_eval_list(row, column_types))) for row in worksheet[HEADER_INDEX + 1:]]
    data = {'columns': columns, 'rows': rows}

    return data


def parse_spreadsheet(spreadsheet, worksheet_num):
    worksheets = spreadsheet.worksheets()
    worksheet_count = len(worksheets)
    if worksheet_num >= worksheet_count:
        raise WorksheetNotFoundError(worksheet_num, worksheet_count)

    worksheet = worksheets[worksheet_num].get_all_values()

    return parse_worksheet(worksheet)


class TimeoutSession(Session):
    def request(self, *args, **kwargs):
        kwargs.setdefault('timeout', 300)
        return super(TimeoutSession, self).request(*args, **kwargs)


class GoogleSpreadsheet(BaseQueryRunner):

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "google_spreadsheets"

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'jsonKeyFile': {
                    "type": "string",
                    'title': 'JSON Key File'
                }
            },
            'required': ['jsonKeyFile'],
            'secret': ['jsonKeyFile']
        }

    def _get_spreadsheet_service(self):
        scope = [
            'https://spreadsheets.google.com/feeds',
        ]

        key = json_loads(b64decode(self.configuration['jsonKeyFile']))
        creds = ServiceAccountCredentials.from_json_keyfile_dict(key, scope)

        timeout_session = HTTPSession()
        timeout_session.requests_session = TimeoutSession()
        spreadsheetservice = gspread.Client(auth=creds, http_session=timeout_session)
        spreadsheetservice.login()
        return spreadsheetservice

    def test_connection(self):
        self._get_spreadsheet_service()

    def is_url_key(self, key):
        if key.startswith('https://'):
            return True
        return False

    def run_query(self, query, user):
        logger.debug("Spreadsheet is about to execute query: %s", query)
        key, worksheet_num = parse_query(query)

        try:
            spreadsheet_service = self._get_spreadsheet_service()

            if self.is_url_key(key):
                spreadsheet = spreadsheet_service.open_by_url(key)
            else:
                spreadsheet = spreadsheet_service.open_by_key(key)

            data = parse_spreadsheet(spreadsheet, worksheet_num)

            return json_dumps(data), None
        except gspread.SpreadsheetNotFound:
            return None, "Spreadsheet ({}) not found. Make sure you used correct id.".format(key)


register(GoogleSpreadsheet)
<EOF>
<BOF>
try:
    from pydruid.db import connect
    enabled = True
except ImportError:
    enabled = False

from redash.query_runner import BaseQueryRunner, register
from redash.query_runner import TYPE_STRING, TYPE_INTEGER, TYPE_BOOLEAN
from redash.utils import json_dumps, json_loads

TYPES_MAP = {
    1: TYPE_STRING,
    2: TYPE_INTEGER,
    3: TYPE_BOOLEAN,
}


class Druid(BaseQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "host": {
                    "type": "string",
                    "default": "localhost"
                },
                "port": {
                    "type": "number",
                    "default": 8082
                },
                "scheme": {
                    "type": "string",
                    "default": "http"
                }
            },
            "order": ['scheme', 'host', 'port'],
            "required": ['host']
        }

    @classmethod
    def enabled(cls):
        return enabled

    def run_query(self, query, user):
        connection = connect(host=self.configuration['host'],
                             port=self.configuration['port'],
                             path='/druid/v2/sql/',
                             scheme=self.configuration['scheme'])

        cursor = connection.cursor()

        try:
            cursor.execute(query)
            columns = self.fetch_columns([(i[0], TYPES_MAP.get(i[1], None)) for i in cursor.description])
            rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]

            data = {'columns': columns, 'rows': rows}
            error = None
            json_data = json_dumps(data)
            print(json_data)
        finally:
            connection.close()

        return json_data, error

    def get_schema(self, get_stats=False):
        query = """
        SELECT TABLE_SCHEMA,
               TABLE_NAME,
               COLUMN_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA <> 'INFORMATION_SCHEMA'
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        schema = {}
        results = json_loads(results)

        for row in results['rows']:
            table_name = '{}.{}'.format(row['TABLE_SCHEMA'], row['TABLE_NAME'])

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['COLUMN_NAME'])

        return schema.values()


register(Druid)
<EOF>
<BOF>
import os
import json
import logging
import requests
from requests.auth import HTTPBasicAuth

from redash import settings
from redash.query_runner import *
from redash.utils import JSONEncoder

logger = logging.getLogger(__name__)

types_map = {
    'tinyint': TYPE_INTEGER,
    'smallint': TYPE_INTEGER,
    'integer': TYPE_INTEGER,
    'bigint': TYPE_INTEGER,
    'int4': TYPE_INTEGER,
    'long8': TYPE_INTEGER,
    'int': TYPE_INTEGER,
    'short': TYPE_INTEGER,
    'long': TYPE_INTEGER,
    'byte': TYPE_INTEGER,
    'hllc10': TYPE_INTEGER,
    'hllc12': TYPE_INTEGER,
    'hllc14': TYPE_INTEGER,
    'hllc15': TYPE_INTEGER,
    'hllc16': TYPE_INTEGER,
    'hllc(10)': TYPE_INTEGER,
    'hllc(12)': TYPE_INTEGER,
    'hllc(14)': TYPE_INTEGER,
    'hllc(15)': TYPE_INTEGER,
    'hllc(16)': TYPE_INTEGER,
    'float': TYPE_FLOAT,
    'double': TYPE_FLOAT,
    'decimal': TYPE_FLOAT,
    'real': TYPE_FLOAT,
    'numeric': TYPE_FLOAT,
    'boolean': TYPE_BOOLEAN,
    'bool': TYPE_BOOLEAN,
    'date': TYPE_DATE,
    'datetime': TYPE_DATETIME,
    'timestamp': TYPE_DATETIME,
    'time': TYPE_DATETIME,
    'varchar': TYPE_STRING,
    'char': TYPE_STRING,
    'string': TYPE_STRING,
}


class Kylin(BaseQueryRunner):
    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string",
                    "title": "Kylin Username",
                },
                "password": {
                    "type": "string",
                    "title": "Kylin Password",
                },
                "url": {
                    "type": "string",
                    "title": "Kylin API URL",
                    "default": "http://kylin.example.com/kylin/",
                },
                "project": {
                    "type": "string",
                    "title": "Kylin Project",
                },
            },
            "order": ['url', 'project', 'user', 'password'],
            "required": ["url", "project", "user", "password"],
            "secret": ["password"]
        }

    def run_query(self, query, user):
        url = self.configuration['url']
        kylinuser = self.configuration['user']
        kylinpass = self.configuration['password']
        kylinproject = self.configuration['project']

        resp = requests.post(
            os.path.join(url, "api/query"),
            auth=HTTPBasicAuth(kylinuser, kylinpass),
            json={
                "sql": query,
                "offset": settings.KYLIN_OFFSET,
                "limit": settings.KYLIN_LIMIT,
                "acceptPartial": settings.KYLIN_ACCEPT_PARTIAL,
                "project": kylinproject
            }
        )

        if not resp.ok:
            return {}, resp.text or str(resp.reason)

        data = resp.json()
        columns = self.get_columns(data['columnMetas'])
        rows = self.get_rows(columns, data['results'])

        return json.dumps({'columns': columns, 'rows': rows}), None

    def get_schema(self, get_stats=False):
        url = self.configuration['url']
        kylinuser = self.configuration['user']
        kylinpass = self.configuration['password']
        kylinproject = self.configuration['project']

        resp = requests.get(
            os.path.join(url, "api/tables_and_columns"),
            params={"project": kylinproject},
            auth=HTTPBasicAuth(kylinuser, kylinpass),
        )

        resp.raise_for_status()

        data = resp.json()
        return [self.get_table_schema(table) for table in data]

    def test_connection(self):
        url = self.configuration['url']
        requests.get(url).raise_for_status()

    def get_columns(self, colmetas):
        return self.fetch_columns([
            (meta['name'], types_map.get(meta['columnTypeName'].lower(), TYPE_STRING))
            for meta in colmetas
        ])

    def get_rows(self, columns, results):
        return [
            dict(zip((c['name'] for c in columns), row))
            for row in results
        ]

    def get_table_schema(self, table):
        name = table['table_NAME']
        columns = [col['column_NAME'].lower() for col in table['columns']]
        return {"name": name, "columns": columns}


register(Kylin)
<EOF>
<BOF>
import logging
import sys
import urllib

import requests
from requests.auth import HTTPBasicAuth

from redash.query_runner import *
from redash.utils import json_dumps, json_loads

try:
    import http.client as http_client
except ImportError:
    # Python 2
    import httplib as http_client

logger = logging.getLogger(__name__)

ELASTICSEARCH_TYPES_MAPPING = {
    "integer": TYPE_INTEGER,
    "long": TYPE_INTEGER,
    "float": TYPE_FLOAT,
    "double": TYPE_FLOAT,
    "boolean": TYPE_BOOLEAN,
    "string": TYPE_STRING,
    "date": TYPE_DATE,
    "object": TYPE_STRING,
    # "geo_point" TODO: Need to split to 2 fields somehow
}

ELASTICSEARCH_BUILTIN_FIELDS_MAPPING = {
    "_id": "Id",
    "_score": "Score"
}

PYTHON_TYPES_MAPPING = {
    str: TYPE_STRING,
    unicode: TYPE_STRING,
    bool: TYPE_BOOLEAN,
    int: TYPE_INTEGER,
    long: TYPE_INTEGER,
    float: TYPE_FLOAT
}


class BaseElasticSearch(BaseQueryRunner):
    DEBUG_ENABLED = False

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'server': {
                    'type': 'string',
                    'title': 'Base URL'
                },
                'basic_auth_user': {
                    'type': 'string',
                    'title': 'Basic Auth User'
                },
                'basic_auth_password': {
                    'type': 'string',
                    'title': 'Basic Auth Password'
                }
            },
            "secret": ["basic_auth_password"],
            "required": ["server"]
        }

    @classmethod
    def enabled(cls):
        return False

    def __init__(self, configuration):
        super(BaseElasticSearch, self).__init__(configuration)

        self.syntax = "json"

        if self.DEBUG_ENABLED:
            http_client.HTTPConnection.debuglevel = 1

            # you need to initialize logging, otherwise you will not see anything from requests
            logging.basicConfig()
            logging.getLogger().setLevel(logging.DEBUG)
            requests_log = logging.getLogger("requests.packages.urllib3")
            requests_log.setLevel(logging.DEBUG)
            requests_log.propagate = True

            logger.setLevel(logging.DEBUG)

        self.server_url = self.configuration["server"]
        if self.server_url[-1] == "/":
            self.server_url = self.server_url[:-1]

        basic_auth_user = self.configuration.get("basic_auth_user", None)
        basic_auth_password = self.configuration.get("basic_auth_password", None)
        self.auth = None
        if basic_auth_user and basic_auth_password:
            self.auth = HTTPBasicAuth(basic_auth_user, basic_auth_password)

    def _get_mappings(self, url):
        mappings = {}
        error = None
        try:
            r = requests.get(url, auth=self.auth)
            r.raise_for_status()

            mappings = r.json()
        except requests.HTTPError as e:
            logger.exception(e)
            error = "Failed to execute query. Return Code: {0}   Reason: {1}".format(r.status_code, r.text)
            mappings = None
        except requests.exceptions.RequestException as e:
            logger.exception(e)
            error = "Connection refused"
            mappings = None

        return mappings, error

    def _get_query_mappings(self, url):
        mappings_data, error = self._get_mappings(url)
        if error:
            return mappings_data, error

        mappings = {}
        for index_name in mappings_data:
            index_mappings = mappings_data[index_name]
            for m in index_mappings.get("mappings", {}):
                if "properties" not in index_mappings["mappings"][m]:
                    continue
                for property_name in index_mappings["mappings"][m]["properties"]:
                    property_data = index_mappings["mappings"][m]["properties"][property_name]
                    if property_name not in mappings:
                        property_type = property_data.get("type", None)
                        if property_type:
                            if property_type in ELASTICSEARCH_TYPES_MAPPING:
                                mappings[property_name] = ELASTICSEARCH_TYPES_MAPPING[property_type]
                            else:
                                mappings[property_name] = TYPE_STRING
                                #raise Exception("Unknown property type: {0}".format(property_type))

        return mappings, error

    def get_schema(self, *args, **kwargs):
        def parse_doc(doc, path=None):
            '''Recursively parse a doc type dictionary
            '''
            path = path or []
            result = []
            for field, description in doc['properties'].items():
                if 'properties' in description:
                    result.extend(parse_doc(description, path + [field]))
                else:
                    result.append('.'.join(path + [field]))
            return result

        schema = {}
        url = "{0}/_mappings".format(self.server_url)
        mappings, error = self._get_mappings(url)

        if mappings:
            # make a schema for each index
            # the index contains a mappings dict with documents
            # in a hierarchical format
            for name, index in mappings.items():
                columns = []
                schema[name] = {'name': name}
                for doc, items in index['mappings'].items():
                    columns.extend(parse_doc(items))

                # remove duplicates
                # sort alphabetically
                schema[name]['columns'] = sorted(set(columns))
        return schema.values()

    def _parse_results(self, mappings, result_fields, raw_result, result_columns, result_rows):
        def add_column_if_needed(mappings, column_name, friendly_name, result_columns, result_columns_index):
            if friendly_name not in result_columns_index:
                result_columns.append({
                    "name": friendly_name,
                    "friendly_name": friendly_name,
                    "type": mappings.get(column_name, "string")})
                result_columns_index[friendly_name] = result_columns[-1]

        def get_row(rows, row):
            if row is None:
                row = {}
                rows.append(row)
            return row

        def collect_value(mappings, row, key, value, type):
            if result_fields and key not in result_fields_index:
                return

            mappings[key] = type
            add_column_if_needed(mappings, key, key, result_columns, result_columns_index)
            row[key] = value

        def collect_aggregations(mappings, rows, parent_key, data, row, result_columns, result_columns_index):
            if isinstance(data, dict):
                for key, value in data.iteritems():
                    val = collect_aggregations(mappings, rows, parent_key if key == 'buckets' else key, value, row, result_columns, result_columns_index)
                    if val:
                        row = get_row(rows, row)
                        collect_value(mappings, row, key, val, 'long')

                for data_key in ['value', 'doc_count']:
                    if data_key not in data:
                        continue
                    if 'key' in data and len(data.keys()) == 2:
                        key_is_string = 'key_as_string' in data
                        collect_value(mappings, row, data['key'] if not key_is_string else data['key_as_string'], data[data_key], 'long' if not key_is_string else 'string')
                    else:
                        return data[data_key]

            elif isinstance(data, list):
                for value in data:
                    result_row = get_row(rows, row)
                    collect_aggregations(mappings, rows, parent_key, value, result_row, result_columns, result_columns_index)
                    if 'doc_count' in value:
                        collect_value(mappings, result_row, 'doc_count', value['doc_count'], 'integer')
                    if 'key' in value:
                        if 'key_as_string' in value:
                            collect_value(mappings, result_row, parent_key, value['key_as_string'], 'string')
                        else:
                            collect_value(mappings, result_row, parent_key, value['key'], 'string')

            return None

        result_columns_index = {c["name"]: c for c in result_columns}

        result_fields_index = {}
        if result_fields:
            for r in result_fields:
                result_fields_index[r] = None

        if 'error' in raw_result:
            error = raw_result['error']
            if len(error) > 10240:
                error = error[:10240] + '... continues'

            raise Exception(error)
        elif 'aggregations' in raw_result:
            if result_fields:
                for field in result_fields:
                    add_column_if_needed(mappings, field, field, result_columns, result_columns_index)

            for key, data in raw_result["aggregations"].iteritems():
                collect_aggregations(mappings, result_rows, key, data, None, result_columns, result_columns_index)

            logger.debug("result_rows %s", str(result_rows))
            logger.debug("result_columns %s", str(result_columns))
        elif 'hits' in raw_result and 'hits' in raw_result['hits']:
            if result_fields:
                for field in result_fields:
                    add_column_if_needed(mappings, field, field, result_columns, result_columns_index)

            for h in raw_result["hits"]["hits"]:
                row = {}

                column_name = "_source" if "_source" in h else "fields"
                for column in h[column_name]:
                    if result_fields and column not in result_fields_index:
                        continue

                    add_column_if_needed(mappings, column, column, result_columns, result_columns_index)

                    value = h[column_name][column]
                    row[column] = value[0] if isinstance(value, list) and len(value) == 1 else value

                result_rows.append(row)
        else:
            raise Exception("Redash failed to parse the results it got from Elasticsearch.")

    def test_connection(self):
        try:
            r = requests.get("{0}/_cluster/health".format(self.server_url), auth=self.auth)
            r.raise_for_status()
        except requests.HTTPError as e:
            logger.exception(e)
            raise Exception("Failed to execute query. Return Code: {0}   Reason: {1}".format(r.status_code, r.text))
        except requests.exceptions.RequestException as e:
            logger.exception(e)
            raise Exception("Connection refused")


class Kibana(BaseElasticSearch):

    @classmethod
    def enabled(cls):
        return True

    @classmethod
    def annotate_query(cls):
        return False

    def _execute_simple_query(self, url, auth, _from, mappings, result_fields, result_columns, result_rows):
        url += "&from={0}".format(_from)
        r = requests.get(url, auth=self.auth)
        r.raise_for_status()

        raw_result = r.json()

        self._parse_results(mappings, result_fields, raw_result, result_columns, result_rows)

        total = raw_result["hits"]["total"]
        result_size = len(raw_result["hits"]["hits"])
        logger.debug("Result Size: {0}  Total: {1}".format(result_size, total))

        return raw_result["hits"]["total"]

    def run_query(self, query, user):
        try:
            error = None

            logger.debug(query)
            query_params = json_loads(query)

            index_name = query_params["index"]
            query_data = query_params["query"]
            size = int(query_params.get("size", 500))
            limit = int(query_params.get("limit", 500))
            result_fields = query_params.get("fields", None)
            sort = query_params.get("sort", None)

            if not self.server_url:
                error = "Missing configuration key 'server'"
                return None, error

            url = "{0}/{1}/_search?".format(self.server_url, index_name)
            mapping_url = "{0}/{1}/_mapping".format(self.server_url, index_name)

            mappings, error = self._get_query_mappings(mapping_url)
            if error:
                return None, error

            if sort:
                url += "&sort={0}".format(urllib.quote_plus(sort))

            url += "&q={0}".format(urllib.quote_plus(query_data))

            logger.debug("Using URL: {0}".format(url))
            logger.debug("Using Query: {0}".format(query_data))

            result_columns = []
            result_rows = []
            if isinstance(query_data, str) or isinstance(query_data, unicode):
                _from = 0
                while True:
                    query_size = size if limit >= (_from + size) else (limit - _from)
                    total = self._execute_simple_query(url + "&size={0}".format(query_size), self.auth, _from, mappings, result_fields, result_columns, result_rows)
                    _from += size
                    if _from >= limit:
                        break
            else:
                # TODO: Handle complete ElasticSearch queries (JSON based sent over HTTP POST)
                raise Exception("Advanced queries are not supported")

            json_data = json_dumps({
                "columns": result_columns,
                "rows": result_rows
            })
        except KeyboardInterrupt:
            error = "Query cancelled by user."
            json_data = None
        except requests.HTTPError as e:
            logger.exception(e)
            error = "Failed to execute query. Return Code: {0}   Reason: {1}".format(r.status_code, r.text)
            json_data = None
        except requests.exceptions.RequestException as e:
            logger.exception(e)
            error = "Connection refused"
            json_data = None

        return json_data, error


class ElasticSearch(BaseElasticSearch):

    @classmethod
    def enabled(cls):
        return True

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def name(cls):
        return 'Elasticsearch'

    def run_query(self, query, user):
        try:
            error = None

            logger.debug(query)
            query_dict = json_loads(query)

            index_name = query_dict.pop("index", "")
            result_fields = query_dict.pop("result_fields", None)

            if not self.server_url:
                error = "Missing configuration key 'server'"
                return None, error

            url = "{0}/{1}/_search".format(self.server_url, index_name)
            mapping_url = "{0}/{1}/_mapping".format(self.server_url, index_name)

            mappings, error = self._get_query_mappings(mapping_url)
            if error:
                return None, error

            logger.debug("Using URL: %s", url)
            logger.debug("Using query: %s", query_dict)
            r = requests.get(url, json=query_dict, auth=self.auth)
            r.raise_for_status()
            logger.debug("Result: %s", r.json())

            result_columns = []
            result_rows = []
            self._parse_results(mappings, result_fields, r.json(), result_columns, result_rows)

            json_data = json_dumps({
                "columns": result_columns,
                "rows": result_rows
            })
        except KeyboardInterrupt:
            logger.exception(e)
            error = "Query cancelled by user."
            json_data = None
        except requests.HTTPError as e:
            logger.exception(e)
            error = "Failed to execute query. Return Code: {0}   Reason: {1}".format(r.status_code, r.text)
            json_data = None
        except requests.exceptions.RequestException as e:
            logger.exception(e)
            error = "Connection refused"
            json_data = None

        return json_data, error


register(Kibana)
register(ElasticSearch)
<EOF>
<BOF>
from __future__ import absolute_import

try:
    import pymapd
    enabled = True
except ImportError:
    enabled = False

from redash.query_runner import BaseSQLQueryRunner, register
from redash.query_runner import TYPE_STRING, TYPE_DATE, TYPE_DATETIME, TYPE_INTEGER, TYPE_FLOAT, TYPE_BOOLEAN
from redash.utils import json_dumps

TYPES_MAP = {
    0: TYPE_INTEGER,
    1: TYPE_INTEGER,
    2: TYPE_INTEGER,
    3: TYPE_FLOAT,
    4: TYPE_FLOAT,
    5: TYPE_FLOAT,
    6: TYPE_STRING,
    7: TYPE_DATE,
    8: TYPE_DATETIME,
    9: TYPE_DATE,
    10: TYPE_BOOLEAN,
    11: TYPE_DATE,
    12: TYPE_DATE
}


class Mapd(BaseSQLQueryRunner):

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "host": {
                    "type": "string",
                    "default": "localhost"
                },
                "port": {
                    "type": "number",
                    "default": 9091
                },
                "user": {
                    "type": "string",
                    "default": "mapd",
                    "title": "username"
                },
                "password": {
                    "type": "string",
                    "default": "HyperInteractive"
                },
                "database": {
                    "type": "string",
                    "default": "mapd"
                }
            },
            "order": ["user", "password", "host", "port", "database"],
            "required": ["host", "port", "user", "password", "database"],
            "secret": ["password"]
        }

    @classmethod
    def enabled(cls):
        return enabled

    def connect_database(self):
        connection = pymapd.connect(
                        user=self.configuration['user'],
                        password=self.configuration['password'],
                        host=self.configuration['host'],
                        port=self.configuration['port'],
                        dbname=self.configuration['database']
                )
        return connection

    def run_query(self, query, user):
        connection = self.connect_database()
        cursor = connection.cursor()

        try:
            cursor.execute(query)
            columns = self.fetch_columns([(i[0], TYPES_MAP.get(i[1], None)) for i in cursor.description])
            rows = [dict(zip((c['name'] for c in columns), row)) for row in cursor]
            data = {'columns': columns, 'rows': rows}
            error = None
            json_data = json_dumps(data)
        finally:
            cursor.close()
            connection.close()

        return json_data, error

    def _get_tables(self, schema):
        connection = self.connect_database()
        try:
            for table_name in connection.get_tables():
                schema[table_name] = {'name': table_name, 'columns': []}
                for row_column in connection.get_table_details(table_name):
                    schema[table_name]['columns'].append(row_column[0])
        finally:
            connection.close

        return schema.values()

    def test_connection(self):
        connection = self.connect_database()
        try:
            tables = connection.get_tables()
            num_tables = tables.count(tables)
        finally:
            connection.close

register(Mapd)
<EOF>
<BOF>
import logging
import numbers
import re
import sqlite3

from dateutil import parser
from six import text_type

from redash import models
from redash.permissions import has_access, not_view_only
from redash.query_runner import (TYPE_BOOLEAN, TYPE_DATETIME, TYPE_FLOAT,
                                 TYPE_INTEGER, TYPE_STRING, BaseQueryRunner,
                                 register)
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)


class PermissionError(Exception):
    pass


class CreateTableError(Exception):
    pass


def _guess_type(value):
    if value == '' or value is None:
        return TYPE_STRING

    if isinstance(value, numbers.Integral):
        return TYPE_INTEGER

    if isinstance(value, float):
        return TYPE_FLOAT

    if text_type(value).lower() in ('true', 'false'):
        return TYPE_BOOLEAN

    try:
        parser.parse(value)
        return TYPE_DATETIME
    except (ValueError, OverflowError):
        pass

    return TYPE_STRING


def extract_query_ids(query):
    queries = re.findall(r'(?:join|from)\s+query_(\d+)', query, re.IGNORECASE)
    return [int(q) for q in queries]


def extract_cached_query_ids(query):
    queries = re.findall(r'(?:join|from)\s+cached_query_(\d+)', query, re.IGNORECASE)
    return [int(q) for q in queries]


def _load_query(user, query_id):
    query = models.Query.get_by_id(query_id)

    if user.org_id != query.org_id:
        raise PermissionError("Query id {} not found.".format(query.id))

    if not has_access(query.data_source.groups, user, not_view_only):
        raise PermissionError(u"You are not allowed to execute queries on {} data source (used for query id {}).".format(
            query.data_source.name, query.id))

    return query


def get_query_results(user, query_id, bring_from_cache):
    query = _load_query(user, query_id)
    if bring_from_cache:
        if query.latest_query_data_id is not None:
            results = query.latest_query_data.data
        else:
            raise Exception("No cached result available for query {}.".format(query.id))
    else:
        results, error = query.data_source.query_runner.run_query(query.query_text, user)
        if error:
            raise Exception("Failed loading results for query id {}.".format(query.id))

    return json_loads(results)


def create_tables_from_query_ids(user, connection, query_ids, cached_query_ids=[]):
    for query_id in set(cached_query_ids):
        results = get_query_results(user, query_id, True)
        table_name = 'cached_query_{query_id}'.format(query_id=query_id)
        create_table(connection, table_name, results)

    for query_id in set(query_ids):
        results = get_query_results(user, query_id, False)
        table_name = 'query_{query_id}'.format(query_id=query_id)
        create_table(connection, table_name, results)


def fix_column_name(name):
    return u'"{}"'.format(name.replace(':', '_').replace('.', '_').replace(' ', '_'))


def create_table(connection, table_name, query_results):
    try:
        columns = [column['name']
                   for column in query_results['columns']]
        safe_columns = [fix_column_name(column) for column in columns]

        column_list = ", ".join(safe_columns)
        create_table = u"CREATE TABLE {table_name} ({column_list})".format(
            table_name=table_name, column_list=column_list)
        logger.debug("CREATE TABLE query: %s", create_table)
        connection.execute(create_table)
    except sqlite3.OperationalError as exc:
        raise CreateTableError(u"Error creating table {}: {}".format(table_name, exc.message))

    insert_template = u"insert into {table_name} ({column_list}) values ({place_holders})".format(
        table_name=table_name,
        column_list=column_list,
        place_holders=','.join(['?'] * len(columns)))

    for row in query_results['rows']:
        values = [row.get(column) for column in columns]
        connection.execute(insert_template, values)


class Results(BaseQueryRunner):
    noop_query = 'SELECT 1'

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
            }
        }

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def name(cls):
        return "Query Results"

    def run_query(self, query, user):
        connection = sqlite3.connect(':memory:')

        query_ids = extract_query_ids(query)
        cached_query_ids = extract_cached_query_ids(query)
        create_tables_from_query_ids(user, connection, query_ids, cached_query_ids)

        cursor = connection.cursor()

        try:
            cursor.execute(query)

            if cursor.description is not None:
                columns = self.fetch_columns(
                    [(i[0], None) for i in cursor.description])

                rows = []
                column_names = [c['name'] for c in columns]

                for i, row in enumerate(cursor):
                    for j, col in enumerate(row):
                        guess = _guess_type(col)

                        if columns[j]['type'] is None:
                            columns[j]['type'] = guess
                        elif columns[j]['type'] != guess:
                            columns[j]['type'] = TYPE_STRING

                    rows.append(dict(zip(column_names, row)))

                data = {'columns': columns, 'rows': rows}
                error = None
                json_data = json_dumps(data)
            else:
                error = 'Query completed but it returned no data.'
                json_data = None
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            connection.close()
        return json_data, error


register(Results)
<EOF>
<BOF>
import datetime
import logging
import re

from dateutil.parser import parse

from redash.query_runner import *
from redash.utils import JSONEncoder, json_dumps, json_loads, parse_human_time

logger = logging.getLogger(__name__)

try:
    import pymongo
    from bson.objectid import ObjectId
    from bson.timestamp import Timestamp
    from bson.son import SON
    from bson.json_util import object_hook as bson_object_hook
    enabled = True

except ImportError:
    enabled = False


TYPES_MAP = {
    str: TYPE_STRING,
    unicode: TYPE_STRING,
    int: TYPE_INTEGER,
    long: TYPE_INTEGER,
    float: TYPE_FLOAT,
    bool: TYPE_BOOLEAN,
    datetime.datetime: TYPE_DATETIME,
}


class MongoDBJSONEncoder(JSONEncoder):
    def default(self, o):
        if isinstance(o, ObjectId):
            return str(o)
        elif isinstance(o, Timestamp):
            return super(MongoDBJSONEncoder, self).default(o.as_datetime())

        return super(MongoDBJSONEncoder, self).default(o)


date_regex = re.compile("ISODate\(\"(.*)\"\)", re.IGNORECASE)


def parse_oids(oids):
    if not isinstance(oids, list):
        raise Exception("$oids takes an array as input.")

    return [bson_object_hook({'$oid': oid}) for oid in oids]


def datetime_parser(dct):
    for k, v in dct.iteritems():
        if isinstance(v, basestring):
            m = date_regex.findall(v)
            if len(m) > 0:
                dct[k] = parse(m[0], yearfirst=True)

    if '$humanTime' in dct:
        return parse_human_time(dct['$humanTime'])

    if '$oids' in dct:
        return parse_oids(dct['$oids'])

    return bson_object_hook(dct)


def parse_query_json(query):
    query_data = json_loads(query, object_hook=datetime_parser)
    return query_data


def _get_column_by_name(columns, column_name):
    for c in columns:
        if "name" in c and c["name"] == column_name:
            return c

    return None


def parse_results(results):
    rows = []
    columns = []

    for row in results:
        parsed_row = {}

        for key in row:
            if isinstance(row[key], dict):
                for inner_key in row[key]:
                    column_name = u'{}.{}'.format(key, inner_key)
                    if _get_column_by_name(columns, column_name) is None:
                        columns.append({
                            "name": column_name,
                            "friendly_name": column_name,
                            "type": TYPES_MAP.get(type(row[key][inner_key]), TYPE_STRING)
                        })

                    parsed_row[column_name] = row[key][inner_key]

            else:
                if _get_column_by_name(columns, key) is None:
                    columns.append({
                        "name": key,
                        "friendly_name": key,
                        "type": TYPES_MAP.get(type(row[key]), TYPE_STRING)
                    })

                parsed_row[key] = row[key]

        rows.append(parsed_row)

    return rows, columns


class MongoDB(BaseQueryRunner):
    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'connectionString': {
                    'type': 'string',
                    'title': 'Connection String'
                },
                'dbName': {
                    'type': 'string',
                    'title': "Database Name"
                },
                'replicaSetName': {
                    'type': 'string',
                    'title': 'Replica Set Name'
                },
            },
            'required': ['connectionString', 'dbName']
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def annotate_query(cls):
        return False

    def __init__(self, configuration):
        super(MongoDB, self).__init__(configuration)

        self.syntax = 'json'

        self.db_name = self.configuration["dbName"]

        self.is_replica_set = True if "replicaSetName" in self.configuration and self.configuration["replicaSetName"] else False

    def _get_db(self):
        if self.is_replica_set:
            db_connection = pymongo.MongoClient(self.configuration["connectionString"],
                                                replicaSet=self.configuration["replicaSetName"])
        else:
            db_connection = pymongo.MongoClient(self.configuration["connectionString"])

        return db_connection[self.db_name]

    def test_connection(self):
        db = self._get_db()
        if not db.command("connectionStatus")["ok"]:
            raise Exception("MongoDB connection error")

    def _merge_property_names(self, columns, document):
        for property in document:
              if property not in columns:
                  columns.append(property)

    def _is_collection_a_view(self, db, collection_name):
        try:
            db.command('collstats', collection_name)
            return False
        except Exception:
            return True

    def _get_collection_fields(self, db, collection_name):
        # Since MongoDB is a document based database and each document doesn't have
        # to have the same fields as another documet in the collection its a bit hard to
        # show these attributes as fields in the schema.
        #
        # For now, the logic is to take the first and last documents (last is determined
        # by the Natural Order (http://www.mongodb.org/display/DOCS/Sorting+and+Natural+Order)
        # as we don't know the correct order. In most single server installations it would be
        # find. In replicaset when reading from non master it might not return the really last
        # document written.
        collection_is_a_view = self._is_collection_a_view(db, collection_name)
        documents_sample = []
        if collection_is_a_view:
            for d in db[collection_name].find().limit(2):
                documents_sample.append(d)
        else:
            for d in db[collection_name].find().sort([("$natural", 1)]).limit(1):
                documents_sample.append(d)

            for d in db[collection_name].find().sort([("$natural", -1)]).limit(1):
                documents_sample.append(d)
        columns = []
        for d in documents_sample:
            self._merge_property_names(columns, d)
        return columns

    def get_schema(self, get_stats=False):
        schema = {}
        db = self._get_db()
        for collection_name in db.collection_names():
            if collection_name.startswith('system.'):
                continue
            columns = self._get_collection_fields(db, collection_name)
            schema[collection_name] = {
                "name": collection_name, "columns": sorted(columns)}

        return schema.values()


    def run_query(self, query, user):
        db = self._get_db()

        logger.debug("mongodb connection string: %s", self.configuration['connectionString'])
        logger.debug("mongodb got query: %s", query)

        try:
            query_data = parse_query_json(query)
        except ValueError:
            return None, "Invalid query format. The query is not a valid JSON."

        if "collection" not in query_data:
            return None, "'collection' must have a value to run a query"
        else:
            collection = query_data["collection"]

        q = query_data.get("query", None)
        f = None

        aggregate = query_data.get("aggregate", None)
        if aggregate:
            for step in aggregate:
                if "$sort" in step:
                    sort_list = []
                    for sort_item in step["$sort"]:
                        sort_list.append((sort_item["name"], sort_item["direction"]))

                    step["$sort"] = SON(sort_list)

        if not aggregate:
            s = None
            if "sort" in query_data and query_data["sort"]:
                s = []
                for field in query_data["sort"]:
                    s.append((field["name"], field["direction"]))

        if "fields" in query_data:
            f = query_data["fields"]

        s = None
        if "sort" in query_data and query_data["sort"]:
            s = []
            for field_data in query_data["sort"]:
                s.append((field_data["name"], field_data["direction"]))

        columns = []
        rows = []

        cursor = None
        if q or (not q and not aggregate):
            if s:
                cursor = db[collection].find(q, f).sort(s)
            else:
                cursor = db[collection].find(q, f)

            if "skip" in query_data:
                cursor = cursor.skip(query_data["skip"])

            if "limit" in query_data:
                cursor = cursor.limit(query_data["limit"])

            if "count" in query_data:
                cursor = cursor.count()

        elif aggregate:
            allow_disk_use = query_data.get('allowDiskUse', False)
            r = db[collection].aggregate(aggregate, allowDiskUse=allow_disk_use)

            # Backwards compatibility with older pymongo versions.
            #
            # Older pymongo version would return a dictionary from an aggregate command.
            # The dict would contain a "result" key which would hold the cursor.
            # Newer ones return pymongo.command_cursor.CommandCursor.
            if isinstance(r, dict):
                cursor = r["result"]
            else:
                cursor = r

        if "count" in query_data:
            columns.append({
                "name" : "count",
                "friendly_name" : "count",
                "type" : TYPE_INTEGER
            })

            rows.append({ "count" : cursor })
        else:
            rows, columns = parse_results(cursor)

        if f:
            ordered_columns = []
            for k in sorted(f, key=f.get):
                column = _get_column_by_name(columns, k)
                if column:
                    ordered_columns.append(column)

            columns = ordered_columns

        if query_data.get('sortColumns'):
            reverse = query_data['sortColumns'] == 'desc'
            columns = sorted(columns, key=lambda col: col['name'], reverse=reverse)

        data = {
            "columns": columns,
            "rows": rows
        }
        error = None
        json_data = json_dumps(data, cls=MongoDBJSONEncoder)

        return json_data, error

register(MongoDB)
<EOF>
<BOF>
import logging
import sys
import uuid

from redash.query_runner import *
from redash.query_runner.mssql import types_map
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)

try:
    import pyodbc
    enabled = True
except ImportError:
    enabled = False


class SQLServerODBC(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "user": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                },
                "server": {
                    "type": "string",
                    "default": "127.0.0.1"
                },
                "port": {
                    "type": "number",
                    "default": 1433
                },
                "charset": {
                    "type": "string",
                    "default": "UTF-8",
                    "title": "Character Set"
                },
                "db": {
                    "type": "string",
                    "title": "Database Name"
                },
                "driver": {
                    "type": "string",
                    "title": "Driver Identifier",
                    "default": "{ODBC Driver 13 for SQL Server}"
                }
            },
            "required": ["db"],
            "secret": ["password"]
        }

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def name(cls):
        return "Microsoft SQL Server (ODBC)"

    @classmethod
    def type(cls):
        return "mssql_odbc"

    @classmethod
    def annotate_query(cls):
        return False

    def _get_tables(self, schema):
        query = """
        SELECT table_schema, table_name, column_name
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE table_schema NOT IN ('guest','INFORMATION_SCHEMA','sys','db_owner','db_accessadmin'
                                  ,'db_securityadmin','db_ddladmin','db_backupoperator','db_datareader'
                                  ,'db_datawriter','db_denydatareader','db_denydatawriter'
                                  );
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            if row['table_schema'] != self.configuration['db']:
                table_name = u'{}.{}'.format(row['table_schema'], row['table_name'])
            else:
                table_name = row['table_name']

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['column_name'])

        return schema.values()

    def run_query(self, query, user):
        connection = None

        try:
            server = self.configuration.get('server', '')
            user = self.configuration.get('user', '')
            password = self.configuration.get('password', '')
            db = self.configuration['db']
            port = self.configuration.get('port', 1433)
            charset = self.configuration.get('charset', 'UTF-8')
            driver = self.configuration.get('driver', '{ODBC Driver 13 for SQL Server}')

            connection_string_fmt = 'DRIVER={};PORT={};SERVER={};DATABASE={};UID={};PWD={}'
            connection_string = connection_string_fmt.format(driver,
                                                             port,
                                                             server,
                                                             db,
                                                             user,
                                                             password)
            connection = pyodbc.connect(connection_string)
            cursor = connection.cursor()
            logger.debug("SQLServerODBC running query: %s", query)
            cursor.execute(query)
            data = cursor.fetchall()

            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], types_map.get(i[1], None)) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in data]

                data = {'columns': columns, 'rows': rows}
                json_data = json_dumps(data)
                error = None
            else:
                error = "No data was returned."
                json_data = None

            cursor.close()
        except pyodbc.Error as e:
            try:
                # Query errors are at `args[1]`
                error = e.args[1]
            except IndexError:
                # Connection errors are `args[0][1]`
                error = e.args[0][1]
            json_data = None
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            if connection:
                connection.close()

        return json_data, error

register(SQLServerODBC)
<EOF>
<BOF>
import logging
import os

from redash.query_runner import *
from redash.settings import parse_boolean
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)
ANNOTATE_QUERY = parse_boolean(os.environ.get('ATHENA_ANNOTATE_QUERY', 'true'))
SHOW_EXTRA_SETTINGS = parse_boolean(os.environ.get('ATHENA_SHOW_EXTRA_SETTINGS', 'true'))
OPTIONAL_CREDENTIALS = parse_boolean(os.environ.get('ATHENA_OPTIONAL_CREDENTIALS', 'true'))

try:
    import pyathena
    import boto3
    enabled = True
except ImportError:
    enabled = False


_TYPE_MAPPINGS = {
    'boolean': TYPE_BOOLEAN,
    'tinyint': TYPE_INTEGER,
    'smallint': TYPE_INTEGER,
    'integer': TYPE_INTEGER,
    'bigint': TYPE_INTEGER,
    'double': TYPE_FLOAT,
    'varchar': TYPE_STRING,
    'timestamp': TYPE_DATETIME,
    'date': TYPE_DATE,
    'varbinary': TYPE_STRING,
    'array': TYPE_STRING,
    'map': TYPE_STRING,
    'row': TYPE_STRING,
    'decimal': TYPE_FLOAT,
}


class SimpleFormatter(object):
    def format(self, operation, parameters=None):
        return operation


class Athena(BaseQueryRunner):
    noop_query = 'SELECT 1'

    @classmethod
    def name(cls):
        return "Amazon Athena"

    @classmethod
    def configuration_schema(cls):
        schema = {
            'type': 'object',
            'properties': {
                'region': {
                    'type': 'string',
                    'title': 'AWS Region'
                },
                'aws_access_key': {
                    'type': 'string',
                    'title': 'AWS Access Key'
                },
                'aws_secret_key': {
                    'type': 'string',
                    'title': 'AWS Secret Key'
                },
                's3_staging_dir': {
                    'type': 'string',
                    'title': 'S3 Staging (Query Results) Bucket Path'
                },
                'schema': {
                    'type': 'string',
                    'title': 'Schema Name',
                    'default': 'default'
                },
                'glue': {
                    'type': 'boolean',
                    'title': 'Use Glue Data Catalog',
                },
            },
            'required': ['region', 's3_staging_dir'],
            'order': ['region', 'aws_access_key', 'aws_secret_key', 's3_staging_dir', 'schema'],
            'secret': ['aws_secret_key']
        }

        if SHOW_EXTRA_SETTINGS:
            schema['properties'].update({
                'encryption_option': {
                    'type': 'string',
                    'title': 'Encryption Option',
                },
                'kms_key': {
                    'type': 'string',
                    'title': 'KMS Key',
                },
            })

        if not OPTIONAL_CREDENTIALS:
            schema['required'] += ['aws_access_key', 'aws_secret_key']

        return schema

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def annotate_query(cls):
        return ANNOTATE_QUERY

    @classmethod
    def type(cls):
        return "athena"

    def __get_schema_from_glue(self):
        client = boto3.client(
                'glue',
                aws_access_key_id=self.configuration.get('aws_access_key', None),
                aws_secret_access_key=self.configuration.get('aws_secret_key', None),
                region_name=self.configuration['region']
                )
        schema = {}
        paginator = client.get_paginator('get_tables')

        for database in client.get_databases()['DatabaseList']:
            iterator = paginator.paginate(DatabaseName=database['Name'])
            for table in iterator.search('TableList[]'):
                table_name = '%s.%s' % (database['Name'], table['Name'])
                if table_name not in schema:
                    column = [columns['Name'] for columns in table['StorageDescriptor']['Columns']]
                    schema[table_name] = {'name': table_name, 'columns': column}
                    for partition in table.get('PartitionKeys', []):
                        schema[table_name]['columns'].append(partition['Name'])

        return schema.values()

    def get_schema(self, get_stats=False):
        if self.configuration.get('glue', False):
            return self.__get_schema_from_glue()

        schema = {}
        query = """
        SELECT table_schema, table_name, column_name
        FROM information_schema.columns
        WHERE table_schema NOT IN ('information_schema')
        """

        results, error = self.run_query(query, None)
        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)
        for row in results['rows']:
            table_name = '{0}.{1}'.format(row['table_schema'], row['table_name'])
            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}
            schema[table_name]['columns'].append(row['column_name'])

        return schema.values()

    def run_query(self, query, user):
        cursor = pyathena.connect(
            s3_staging_dir=self.configuration['s3_staging_dir'],
            region_name=self.configuration['region'],
            aws_access_key_id=self.configuration.get('aws_access_key', None),
            aws_secret_access_key=self.configuration.get('aws_secret_key', None),
            schema_name=self.configuration.get('schema', 'default'),
            encryption_option=self.configuration.get('encryption_option', None),
            kms_key=self.configuration.get('kms_key', None),
            formatter=SimpleFormatter()).cursor()

        try:
            cursor.execute(query)
            column_tuples = [(i[0], _TYPE_MAPPINGS.get(i[1], None)) for i in cursor.description]
            columns = self.fetch_columns(column_tuples)
            rows = [dict(zip(([c['name'] for c in columns]), r)) for i, r in enumerate(cursor.fetchall())]
            qbytes = None
            athena_query_id = None
            try:
                qbytes = cursor.data_scanned_in_bytes
            except AttributeError as e:
                logger.debug("Athena Upstream can't get data_scanned_in_bytes: %s", e)
            try:
                athena_query_id = cursor.query_id
            except AttributeError as e:
                logger.debug("Athena Upstream can't get query_id: %s", e)
            data = {
                'columns': columns,
                'rows': rows,
                'metadata': {
                    'data_scanned': qbytes,
                    'athena_query_id': athena_query_id
                }
            }
            json_data = json_dumps(data, ignore_nan=True)
            error = None
        except KeyboardInterrupt:
            if cursor.query_id:
                cursor.cancel()
            error = "Query cancelled by user."
            json_data = None
        except Exception as ex:
            if cursor.query_id:
                cursor.cancel()
            error = ex.message
            json_data = None

        return json_data, error


register(Athena)
<EOF>
<BOF>
import logging
import yaml
from urlparse import parse_qs, urlparse

import requests

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

COLUMN_TYPES = {
    'date': (
        'firstVisitDate', 'firstVisitStartOfYear', 'firstVisitStartOfQuarter',
        'firstVisitStartOfMonth', 'firstVisitStartOfWeek',
    ),
    'datetime': (
        'firstVisitStartOfHour', 'firstVisitStartOfDekaminute', 'firstVisitStartOfMinute',
        'firstVisitDateTime', 'firstVisitHour', 'firstVisitHourMinute'

    ),
    'int': (
        'pageViewsInterval', 'pageViews', 'firstVisitYear', 'firstVisitMonth',
        'firstVisitDayOfMonth', 'firstVisitDayOfWeek', 'firstVisitMinute',
        'firstVisitDekaminute',
    )
}

for type_, elements in COLUMN_TYPES.items():
    for el in elements:
        if 'first' in el:
            el = el.replace('first', 'last')
            COLUMN_TYPES[type_] += (el, )


def parse_ym_response(response):
    columns = []
    dimensions_len = len(response['query']['dimensions'])

    for h in response['query']['dimensions'] + response['query']['metrics']:
        friendly_name = h.split(':')[-1]
        if friendly_name in COLUMN_TYPES['date']:
            data_type = TYPE_DATE
        elif friendly_name in COLUMN_TYPES['datetime']:
            data_type = TYPE_DATETIME
        else:
            data_type = TYPE_STRING
        columns.append({'name': h, 'friendly_name': friendly_name, 'type': data_type})

    rows = []
    for num, row in enumerate(response['data']):
        res = {}
        for i, d in enumerate(row['dimensions']):
            res[columns[i]['name']] = d['name']
        for i, d in enumerate(row['metrics']):
            res[columns[dimensions_len + i]['name']] = d
            if num == 0 and isinstance(d, float):
                columns[dimensions_len + i]['type'] = TYPE_FLOAT
        rows.append(res)

    return {'columns': columns, 'rows': rows}


class YandexMetrica(BaseSQLQueryRunner):
    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        # This is written with a "k" for backward-compatibility. See #2874.
        return "yandex_metrika"

    @classmethod
    def name(cls):
        return "Yandex Metrica"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "token": {
                    "type": "string",
                    "title": "OAuth Token"
                }
            },
            "required": ["token"],
        }

    def __init__(self, configuration):
        super(YandexMetrica, self).__init__(configuration)
        self.syntax = 'yaml'
        self.host = 'https://api-metrica.yandex.com'
        self.list_path = 'counters'

    def _get_tables(self, schema):

        counters = self._send_query('management/v1/{0}'.format(self.list_path))

        for row in counters[self.list_path]:
            owner = row.get('owner_login')
            counter = '{0} | {1}'.format(
                row.get('name', 'Unknown').encode('utf-8'), row.get('id', 'Unknown')
            )
            if owner not in schema:
                schema[owner] = {'name': owner, 'columns': []}

            schema[owner]['columns'].append(counter)

        return schema.values()

    def test_connection(self):
        self._send_query('management/v1/{0}'.format(self.list_path))

    def _send_query(self, path='stat/v1/data', **kwargs):
        token = kwargs.pop('oauth_token', self.configuration['token'])
        r = requests.get('{0}/{1}'.format(self.host, path), params=dict(oauth_token=token, **kwargs))
        if r.status_code != 200:
            raise Exception(r.text)
        return r.json()

    def run_query(self, query, user):
        logger.debug("Metrica is about to execute query: %s", query)
        data = None
        query = query.strip()
        if query == "":
            error = "Query is empty"
            return data, error
        try:
            params = yaml.safe_load(query)
        except ValueError as e:
            logging.exception(e)
            error = unicode(e)
            return data, error

        if isinstance(params, dict):
            if 'url' in params:
                params = parse_qs(urlparse(params['url']).query, keep_blank_values=True)
        else:
            error = 'The query format must be JSON or YAML'
            return data, error

        try:
            data = json_dumps(parse_ym_response(self._send_query(**params)))
            error = None
        except Exception as e:
            logging.exception(e)
            error = unicode(e)
        return data, error


class YandexAppMetrica(YandexMetrica):
    @classmethod
    def type(cls):
        # This is written with a "k" for backward-compatibility. See #2874.
        return "yandex_appmetrika"

    @classmethod
    def name(cls):
        return "Yandex AppMetrica"

    def __init__(self, configuration):
        super(YandexAppMetrica, self).__init__(configuration)
        self.host = 'https://api.appmetrica.yandex.com'
        self.list_path = 'applications'


register(YandexMetrica)
register(YandexAppMetrica)
<EOF>
<BOF>
import datetime
import importlib
import logging
import sys

from redash.query_runner import *
from redash.utils import json_dumps, json_loads
from redash import models
from RestrictedPython import compile_restricted
from RestrictedPython.Guards import safe_builtins


logger = logging.getLogger(__name__)


class CustomPrint(object):
    """CustomPrint redirect "print" calls to be sent as "log" on the result object."""
    def __init__(self):
        self.enabled = True
        self.lines = []

    def write(self, text):
        if self.enabled:
            if text and text.strip():
                log_line = "[{0}] {1}".format(datetime.datetime.utcnow().isoformat(), text)
                self.lines.append(log_line)

    def enable(self):
        self.enabled = True

    def disable(self):
        self.enabled = False

    def __call__(self):
        return self


class Python(BaseQueryRunner):
    safe_builtins = (
        'sorted', 'reversed', 'map', 'reduce', 'any', 'all',
        'slice', 'filter', 'len', 'next', 'enumerate',
        'sum', 'abs', 'min', 'max', 'round', 'cmp', 'divmod',
        'str', 'unicode', 'int', 'float', 'complex',
        'tuple', 'set', 'list', 'dict', 'bool',
    )

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'allowedImportModules': {
                    'type': 'string',
                    'title': 'Modules to import prior to running the script'
                },
                'additionalModulesPaths': {
                    'type': 'string'
                }
            },
        }

    @classmethod
    def enabled(cls):
        return True

    @classmethod
    def annotate_query(cls):
        return False

    def __init__(self, configuration):
        super(Python, self).__init__(configuration)

        self.syntax = "python"

        self._allowed_modules = {}
        self._script_locals = {"result": {"rows": [], "columns": [], "log": []}}
        self._enable_print_log = True
        self._custom_print = CustomPrint()

        if self.configuration.get("allowedImportModules", None):
            for item in self.configuration["allowedImportModules"].split(","):
                self._allowed_modules[item] = None

        if self.configuration.get("additionalModulesPaths", None):
            for p in self.configuration["additionalModulesPaths"].split(","):
                if p not in sys.path:
                    sys.path.append(p)

    def custom_import(self, name, globals=None, locals=None, fromlist=(), level=0):
        if name in self._allowed_modules:
            m = None
            if self._allowed_modules[name] is None:
                m = importlib.import_module(name)
                self._allowed_modules[name] = m
            else:
                m = self._allowed_modules[name]

            return m

        raise Exception("'{0}' is not configured as a supported import module".format(name))

    @staticmethod
    def custom_write(obj):
        """
        Custom hooks which controls the way objects/lists/tuples/dicts behave in
        RestrictedPython
        """
        return obj

    @staticmethod
    def custom_get_item(obj, key):
        return obj[key]

    @staticmethod
    def custom_get_iter(obj):
        return iter(obj)

    @staticmethod
    def add_result_column(result, column_name, friendly_name, column_type):
        """Helper function to add columns inside a Python script running in Redash in an easier way

        Parameters:
        :result dict: The result dict
        :column_name string: Name of the column, which should be consisted of lowercase latin letters or underscore.
        :friendly_name string: Name of the column for display
        :column_type string: Type of the column. Check supported data types for details.
        """
        if column_type not in SUPPORTED_COLUMN_TYPES:
            raise Exception("'{0}' is not a supported column type".format(column_type))

        if "columns" not in result:
            result["columns"] = []

        result["columns"].append({
            "name": column_name,
            "friendly_name": friendly_name,
            "type": column_type
        })

    @staticmethod
    def add_result_row(result, values):
        """Helper function to add one row to results set.

        Parameters:
        :result dict: The result dict
        :values dict: One row of result in dict. The key should be one of the column names. The value is the value of the column in this row.
        """
        if "rows" not in result:
            result["rows"] = []

        result["rows"].append(values)

    @staticmethod
    def execute_query(data_source_name_or_id, query):
        """Run query from specific data source.

        Parameters:
        :data_source_name_or_id string|integer: Name or ID of the data source
        :query string: Query to run
        """
        try:
            if type(data_source_name_or_id) == int:
                data_source = models.DataSource.get_by_id(data_source_name_or_id)
            else:
                data_source = models.DataSource.get_by_name(data_source_name_or_id)
        except models.NoResultFound:
            raise Exception("Wrong data source name/id: %s." % data_source_name_or_id)

        # TODO: pass the user here...
        data, error = data_source.query_runner.run_query(query, None)
        if error is not None:
            raise Exception(error)

        # TODO: allow avoiding the JSON dumps/loads in same process
        return json_loads(data)

    @staticmethod
    def get_source_schema(data_source_name_or_id):
        """Get schema from specific data source.

        :param data_source_name_or_id: string|integer: Name or ID of the data source
        :return:
        """
        try:
            if type(data_source_name_or_id) == int:
                data_source = models.DataSource.get_by_id(data_source_name_or_id)
            else:
                data_source = models.DataSource.get_by_name(data_source_name_or_id)
        except models.NoResultFound:
            raise Exception("Wrong data source name/id: %s." % data_source_name_or_id)
        schema = data_source.query_runner.get_schema()
        return schema

    @staticmethod
    def get_query_result(query_id):
        """Get result of an existing query.

        Parameters:
        :query_id integer: ID of existing query
        """
        try:
            query = models.Query.get_by_id(query_id)
        except models.NoResultFound:
            raise Exception("Query id %s does not exist." % query_id)

        if query.latest_query_data is None:
            raise Exception("Query does not have results yet.")

        if query.latest_query_data.data is None:
            raise Exception("Query does not have results yet.")

        return json_loads(query.latest_query_data.data)

    def get_current_user(self):
        return self._current_user.to_dict()

    def test_connection(self):
        pass

    def run_query(self, query, user):
        self._current_user = user

        try:
            error = None

            code = compile_restricted(query, '<string>', 'exec')

            builtins = safe_builtins.copy()
            builtins["_write_"] = self.custom_write
            builtins["__import__"] = self.custom_import
            builtins["_getattr_"] = getattr
            builtins["getattr"] = getattr
            builtins["_setattr_"] = setattr
            builtins["setattr"] = setattr
            builtins["_getitem_"] = self.custom_get_item
            builtins["_getiter_"] = self.custom_get_iter
            builtins["_print_"] = self._custom_print

            # Layer in our own additional set of builtins that we have
            # considered safe.
            for key in self.safe_builtins:
                builtins[key] = __builtins__[key]

            restricted_globals = dict(__builtins__=builtins)
            restricted_globals["get_query_result"] = self.get_query_result
            restricted_globals["get_source_schema"] = self.get_source_schema
            restricted_globals["get_current_user"] = self.get_current_user
            restricted_globals["execute_query"] = self.execute_query
            restricted_globals["add_result_column"] = self.add_result_column
            restricted_globals["add_result_row"] = self.add_result_row
            restricted_globals["disable_print_log"] = self._custom_print.disable
            restricted_globals["enable_print_log"] = self._custom_print.enable

            # Supported data types
            restricted_globals["TYPE_DATETIME"] = TYPE_DATETIME
            restricted_globals["TYPE_BOOLEAN"] = TYPE_BOOLEAN
            restricted_globals["TYPE_INTEGER"] = TYPE_INTEGER
            restricted_globals["TYPE_STRING"] = TYPE_STRING
            restricted_globals["TYPE_DATE"] = TYPE_DATE
            restricted_globals["TYPE_FLOAT"] = TYPE_FLOAT


            # TODO: Figure out the best way to have a timeout on a script
            #       One option is to use ETA with Celery + timeouts on workers
            #       And replacement of worker process every X requests handled.

            exec((code), restricted_globals, self._script_locals)

            result = self._script_locals['result']
            result['log'] = self._custom_print.lines
            json_data = json_dumps(result)
        except KeyboardInterrupt:
            error = "Query cancelled by user."
            json_data = None
        except Exception as e:
            error = str(type(e)) + " " + str(e)
            json_data = None

        return json_data, error


register(Python)
<EOF>
<BOF>
import datetime
import logging

import requests

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)


def _transform_result(response):
    columns = ({'name': 'Time::x', 'type': TYPE_DATETIME},
               {'name': 'value::y', 'type': TYPE_FLOAT},
               {'name': 'name::series', 'type': TYPE_STRING})

    rows = []

    for series in response.json():
        for values in series['datapoints']:
            timestamp = datetime.datetime.fromtimestamp(int(values[1]))
            rows.append({'Time::x': timestamp, 'name::series': series['target'], 'value::y': values[0]})

    data = {'columns': columns, 'rows': rows}
    return json_dumps(data)


class Graphite(BaseQueryRunner):
    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'url': {
                    'type': 'string'
                },
                'username': {
                    'type': 'string'
                },
                'password': {
                    'type': 'string'
                },
                'verify': {
                    'type': 'boolean',
                    'title': 'Verify SSL certificate'
                }
            },
            'required': ['url'],
            'secret': ['password']
        }

    @classmethod
    def annotate_query(cls):
        return False

    def __init__(self, configuration):
        super(Graphite, self).__init__(configuration)

        if "username" in self.configuration and self.configuration["username"]:
            self.auth = (self.configuration["username"], self.configuration["password"])
        else:
            self.auth = None

        self.verify = self.configuration.get("verify", True)
        self.base_url = "%s/render?format=json&" % self.configuration['url']

    def test_connection(self):
        r = requests.get("{}/render".format(self.configuration['url']), auth=self.auth, verify=self.verify)
        if r.status_code != 200:
            raise Exception("Got invalid response from Graphite (http status code: {0}).".format(r.status_code))

    def run_query(self, query, user):
        url = "%s%s" % (self.base_url, "&".join(query.split("\n")))
        error = None
        data = None

        try:
            response = requests.get(url, auth=self.auth, verify=self.verify)

            if response.status_code == 200:
                data = _transform_result(response)
            else:
                error = "Failed getting results (%d)" % response.status_code
        except Exception as ex:
            data = None
            error = ex.message

        return data, error

register(Graphite)
<EOF>
<BOF>
import logging
import os

from redash.query_runner import *
from redash.settings import parse_boolean
from redash.utils import json_dumps, json_loads

logger = logging.getLogger(__name__)
types_map = {
    0: TYPE_FLOAT,
    1: TYPE_INTEGER,
    2: TYPE_INTEGER,
    3: TYPE_INTEGER,
    4: TYPE_FLOAT,
    5: TYPE_FLOAT,
    7: TYPE_DATETIME,
    8: TYPE_INTEGER,
    9: TYPE_INTEGER,
    10: TYPE_DATE,
    12: TYPE_DATETIME,
    15: TYPE_STRING,
    16: TYPE_INTEGER,
    246: TYPE_FLOAT,
    253: TYPE_STRING,
    254: TYPE_STRING,
}


class Mysql(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        show_ssl_settings = parse_boolean(os.environ.get('MYSQL_SHOW_SSL_SETTINGS', 'true'))

        schema = {
            'type': 'object',
            'properties': {
                'host': {
                    'type': 'string',
                    'default': '127.0.0.1'
                },
                'user': {
                    'type': 'string'
                },
                'passwd': {
                    'type': 'string',
                    'title': 'Password'
                },
                'db': {
                    'type': 'string',
                    'title': 'Database name'
                },
                'port': {
                    'type': 'number',
                    'default': 3306,
                }
            },
            "order": ['host', 'port', 'user', 'passwd', 'db'],
            'required': ['db'],
            'secret': ['passwd']
        }

        if show_ssl_settings:
            schema['properties'].update({
                'use_ssl': {
                    'type': 'boolean',
                    'title': 'Use SSL'
                },
                'ssl_cacert': {
                    'type': 'string',
                    'title': 'Path to CA certificate file to verify peer against (SSL)'
                },
                'ssl_cert': {
                    'type': 'string',
                    'title': 'Path to client certificate file (SSL)'
                },
                'ssl_key': {
                    'type': 'string',
                    'title': 'Path to private key file (SSL)'
                }
            })

        return schema

    @classmethod
    def name(cls):
        return "MySQL"

    @classmethod
    def enabled(cls):
        try:
            import MySQLdb
        except ImportError:
            return False

        return True

    def _get_tables(self, schema):
        query = """
        SELECT col.table_schema,
               col.table_name,
               col.column_name
        FROM `information_schema`.`columns` col
        WHERE col.table_schema NOT IN ('information_schema', 'performance_schema', 'mysql', 'sys');
        """

        results, error = self.run_query(query, None)

        if error is not None:
            raise Exception("Failed getting schema.")

        results = json_loads(results)

        for row in results['rows']:
            if row['table_schema'] != self.configuration['db']:
                table_name = u'{}.{}'.format(row['table_schema'], row['table_name'])
            else:
                table_name = row['table_name']

            if table_name not in schema:
                schema[table_name] = {'name': table_name, 'columns': []}

            schema[table_name]['columns'].append(row['column_name'])

        return schema.values()

    def run_query(self, query, user):
        import MySQLdb

        connection = None
        try:
            connection = MySQLdb.connect(host=self.configuration.get('host', ''),
                                         user=self.configuration.get('user', ''),
                                         passwd=self.configuration.get('passwd', ''),
                                         db=self.configuration['db'],
                                         port=self.configuration.get('port', 3306),
                                         charset='utf8', use_unicode=True,
                                         ssl=self._get_ssl_parameters(),
                                         connect_timeout=60)
            cursor = connection.cursor()
            logger.debug("MySQL running query: %s", query)
            cursor.execute(query)

            data = cursor.fetchall()

            while cursor.nextset():
                data = cursor.fetchall()

            # TODO - very similar to pg.py
            if cursor.description is not None:
                columns = self.fetch_columns([(i[0], types_map.get(i[1], None)) for i in cursor.description])
                rows = [dict(zip((c['name'] for c in columns), row)) for row in data]

                data = {'columns': columns, 'rows': rows}
                json_data = json_dumps(data)
                error = None
            else:
                json_data = None
                error = "No data was returned."

            cursor.close()
        except MySQLdb.Error as e:
            json_data = None
            error = e.args[1]
        except KeyboardInterrupt:
            cursor.close()
            error = "Query cancelled by user."
            json_data = None
        finally:
            if connection:
                connection.close()

        return json_data, error

    def _get_ssl_parameters(self):
        ssl_params = {}

        if self.configuration.get('use_ssl'):
            config_map = dict(ssl_cacert='ca',
                              ssl_cert='cert',
                              ssl_key='key')
            for key, cfg in config_map.items():
                val = self.configuration.get(key)
                if val:
                    ssl_params[cfg] = val

        return ssl_params


class RDSMySQL(Mysql):
    @classmethod
    def name(cls):
        return "MySQL (Amazon RDS)"

    @classmethod
    def type(cls):
        return 'rds_mysql'

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'host': {
                    'type': 'string',
                },
                'user': {
                    'type': 'string'
                },
                'passwd': {
                    'type': 'string',
                    'title': 'Password'
                },
                'db': {
                    'type': 'string',
                    'title': 'Database name'
                },
                'port': {
                    'type': 'number',
                    'default': 3306,
                },
                'use_ssl': {
                    'type': 'boolean',
                    'title': 'Use SSL'
                }
            },
            "order": ['host', 'port', 'user', 'passwd', 'db'],
            'required': ['db', 'user', 'passwd', 'host'],
            'secret': ['passwd']
        }

    def _get_ssl_parameters(self):
        if self.configuration.get('use_ssl'):
            ca_path = os.path.join(os.path.dirname(__file__), './files/rds-combined-ca-bundle.pem')
            return {'ca': ca_path}

        return {}


register(Mysql)
register(RDSMySQL)
<EOF>
<BOF>
import logging
import sys
import base64

from redash.query_runner import *
from redash.utils import json_dumps

logger = logging.getLogger(__name__)

try:
    from pyhive import hive
    enabled = True
except ImportError:
    enabled = False

COLUMN_NAME = 0
COLUMN_TYPE = 1

types_map = {
    'BIGINT_TYPE': TYPE_INTEGER,
    'TINYINT_TYPE': TYPE_INTEGER,
    'SMALLINT_TYPE': TYPE_INTEGER,
    'INT_TYPE': TYPE_INTEGER,
    'DOUBLE_TYPE': TYPE_FLOAT,
    'DECIMAL_TYPE': TYPE_FLOAT,
    'FLOAT_TYPE': TYPE_FLOAT,
    'REAL_TYPE': TYPE_FLOAT,
    'BOOLEAN_TYPE': TYPE_BOOLEAN,
    'TIMESTAMP_TYPE': TYPE_DATETIME,
    'DATE_TYPE': TYPE_DATETIME,
    'CHAR_TYPE': TYPE_STRING,
    'STRING_TYPE': TYPE_STRING,
    'VARCHAR_TYPE': TYPE_STRING
}


class Hive(BaseSQLQueryRunner):
    noop_query = "SELECT 1"

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "host": {
                    "type": "string"
                },
                "port": {
                    "type": "number"
                },
                "database": {
                    "type": "string"
                },
                "username": {
                    "type": "string"
                },
                "use_http": {
                    "type": "boolean",
                    "title": "Use HTTP transport"
                },
                "http_scheme": {
                    "type": "string",
                    "title": "Scheme when using HTTP transport",
                    "default": "https"
                },
                "http_path": {
                    "type": "string",
                    "title": "Path when using HTTP transport"
                },
                "http_password": {
                    "type": "string",
                    "title": "Password when using HTTP transport"
                },
            },
            "required": ["host"]
        }

    @classmethod
    def annotate_query(cls):
        return False

    @classmethod
    def type(cls):
        return "hive"

    @classmethod
    def enabled(cls):
        return enabled

    def _get_tables(self, schema):
        schemas_query = "show schemas"

        tables_query = "show tables in %s"

        columns_query = "show columns in %s.%s"

        for schema_name in filter(lambda a: len(a) > 0, map(lambda a: str(a['database_name']), self._run_query_internal(schemas_query))):
            for table_name in filter(lambda a: len(a) > 0, map(lambda a: str(a['tab_name']), self._run_query_internal(tables_query % schema_name))):
                columns = filter(lambda a: len(a) > 0, map(lambda a: str(a['field']), self._run_query_internal(columns_query % (schema_name, table_name))))

                if schema_name != 'default':
                    table_name = '{}.{}'.format(schema_name, table_name)

                schema[table_name] = {'name': table_name, 'columns': columns}
        return schema.values()

    def run_query(self, query, user):

        connection = None
        try:
            host = self.configuration['host']

            if self.configuration.get('use_http', False):
                # default to https
                scheme = self.configuration.get('http_scheme', 'https')

                # if path is set but is missing initial slash, append it
                path = self.configuration.get('http_path', '')
                if path and path[0] != '/':
                    path = '/' + path

                # if port is set prepend colon
                port = self.configuration.get('port', '')
                if port:
                    port = ':' + port

                http_uri = "{}://{}{}{}".format(scheme, host, port, path)

                # create transport
                transport = THttpClient.THttpClient(http_uri)

                # if username or password is set, add Authorization header
                username = self.configuration.get('username', '')
                password = self.configuration.get('http_password', '')
                if username | password:
                    auth = base64.b64encode(username + ':' + password)
                    transport.setCustomHeaders({'Authorization': 'Basic ' + auth})

                # create connection
                connection = hive.connect(thrift_transport=transport)
            else:
                connection = hive.connect(
                    host=host,
                    port=self.configuration.get('port', None),
                    database=self.configuration.get('database', 'default'),
                    username=self.configuration.get('username', None),
                )

            cursor = connection.cursor()

            cursor.execute(query)

            column_names = []
            columns = []

            for column in cursor.description:
                column_name = column[COLUMN_NAME]
                column_names.append(column_name)

                columns.append({
                    'name': column_name,
                    'friendly_name': column_name,
                    'type': types_map.get(column[COLUMN_TYPE], None)
                })

            rows = [dict(zip(column_names, row)) for row in cursor]

            data = {'columns': columns, 'rows': rows}
            json_data = json_dumps(data)
            error = None
        except KeyboardInterrupt:
            connection.cancel()
            error = "Query cancelled by user."
            json_data = None
        finally:
            if connection:
                connection.close()

        return json_data, error

register(Hive)
<EOF>
<BOF>
from __future__ import absolute_import

import logging
import socket
import time

from celery.signals import task_postrun, task_prerun
from redash import settings, statsd_client
from redash.utils import json_dumps

tasks_start_time = {}


@task_prerun.connect
def task_prerun_handler(signal, sender, task_id, task, args, kwargs, **kw):
    try:
        tasks_start_time[task_id] = time.time()
    except Exception:
        logging.exception("Exception during task_prerun handler:")


def metric_name(name, tags):
    # TODO: use some of the tags in the metric name if tags are not supported
    # TODO: support additional tag formats (this one is for InfluxDB)
    if not settings.STATSD_USE_TAGS:
        return name

    tags_string = ",".join(["{}={}".format(k, v) for k, v in tags.iteritems()])
    return "{},{}".format(name, tags_string)


@task_postrun.connect
def task_postrun_handler(signal, sender, task_id, task, args, kwargs, retval, state, **kw):
    try:
        run_time = 1000 * (time.time() - tasks_start_time.pop(task_id))

        state = (state or 'unknown').lower()
        tags = {'state': state, 'hostname': socket.gethostname()}
        if task.name == 'redash.tasks.execute_query':
            if isinstance(retval, Exception):
                tags['state'] = 'exception'
                state = 'exception'

            tags['data_source_id'] = args[1]

        normalized_task_name = task.name.replace('redash.tasks.', '').replace('.', '_')
        metric = "celery.task_runtime.{}".format(normalized_task_name)
        logging.debug("metric=%s", json_dumps({'metric': metric, 'tags': tags, 'value': run_time}))
        statsd_client.timing(metric_name(metric, tags), run_time)
        statsd_client.incr(metric_name('celery.task.{}.{}'.format(normalized_task_name, state), tags))
    except Exception:
        logging.exception("Exception during task_postrun handler.")
<EOF>
<BOF>
import logging
import time
from collections import namedtuple

from flask import g, request

from redash import statsd_client

metrics_logger = logging.getLogger("metrics")


def record_requets_start_time():
    g.start_time = time.time()


def calculate_metrics(response):
    if 'start_time' not in g:
        return response

    request_duration = (time.time() - g.start_time) * 1000
    queries_duration = g.get('queries_duration', 0.0)
    queries_count = g.get('queries_count', 0.0)
    endpoint = (request.endpoint or 'unknown').replace('.', '_')

    metrics_logger.info("method=%s path=%s endpoint=%s status=%d content_type=%s content_length=%d duration=%.2f query_count=%d query_duration=%.2f",
                        request.method,
                        request.path,
                        endpoint,
                        response.status_code,
                        response.content_type,
                        response.content_length or -1,
                        request_duration,
                        queries_count,
                        queries_duration)

    statsd_client.timing('requests.{}.{}'.format(endpoint, request.method.lower()), request_duration)

    return response

MockResponse = namedtuple('MockResponse', ['status_code', 'content_type', 'content_length'])


def calculate_metrics_on_exception(error):
    if error is not None:
        calculate_metrics(MockResponse(500, '?', -1))


def provision_app(app):
    app.before_request(record_requets_start_time)
    app.after_request(calculate_metrics)
    app.teardown_request(calculate_metrics_on_exception)
<EOF>
<BOF>
import logging
import time

from flask import g, has_request_context

from redash import statsd_client
from sqlalchemy.engine import Engine
from sqlalchemy.event import listens_for
from sqlalchemy.orm.util import _ORMJoin
from sqlalchemy.sql.selectable import Alias

metrics_logger = logging.getLogger("metrics")


def _table_name_from_select_element(elt):
    t = elt.froms[0]

    if isinstance(t, Alias):
        t = t.original.froms[0]

    while isinstance(t, _ORMJoin):
        t = t.left

    return t.name


@listens_for(Engine, "before_execute")
def before_execute(conn, elt, multiparams, params):
    conn.info.setdefault('query_start_time', []).append(time.time())


@listens_for(Engine, "after_execute")
def after_execute(conn, elt, multiparams, params, result):
    duration = 1000 * (time.time() - conn.info['query_start_time'].pop(-1))
    action = elt.__class__.__name__

    if action == 'Select':
        name = 'unknown'
        try:
            name = _table_name_from_select_element(elt)
        except Exception:
            logging.exception('Failed finding table name.')
    elif action in ['Update', 'Insert', 'Delete']:
        name = elt.table.name
    else:
        # create/drop tables, sqlalchemy internal schema queries, etc
        return

    action = action.lower()

    statsd_client.timing('db.{}.{}'.format(name, action), duration)
    metrics_logger.debug("table=%s query=%s duration=%.2f", name, action,
                         duration)

    if has_request_context():
        g.setdefault('queries_count', 0)
        g.setdefault('queries_duration', 0)
        g.queries_count += 1
        g.queries_duration += duration

    return result
<EOF>
<BOF>
from flask import request

from redash import models
from redash.handlers.base import BaseResource, get_object_or_404
from redash.serializers import serialize_visualization
from redash.permissions import (require_object_modify_permission,
                                require_permission)
from redash.utils import json_dumps


class VisualizationListResource(BaseResource):
    @require_permission('edit_query')
    def post(self):
        kwargs = request.get_json(force=True)

        query = get_object_or_404(models.Query.get_by_id_and_org, kwargs.pop('query_id'), self.current_org)
        require_object_modify_permission(query, self.current_user)

        kwargs['options'] = json_dumps(kwargs['options'])
        kwargs['query_rel'] = query

        vis = models.Visualization(**kwargs)
        models.db.session.add(vis)
        models.db.session.commit()
        return serialize_visualization(vis, with_query=False)


class VisualizationResource(BaseResource):
    @require_permission('edit_query')
    def post(self, visualization_id):
        vis = get_object_or_404(models.Visualization.get_by_id_and_org, visualization_id, self.current_org)
        require_object_modify_permission(vis.query_rel, self.current_user)

        kwargs = request.get_json(force=True)
        if 'options' in kwargs:
            kwargs['options'] = json_dumps(kwargs['options'])

        kwargs.pop('id', None)
        kwargs.pop('query_id', None)

        self.update_model(vis, kwargs)
        d = serialize_visualization(vis, with_query=False)
        models.db.session.commit()
        return d

    @require_permission('edit_query')
    def delete(self, visualization_id):
        vis = get_object_or_404(models.Visualization.get_by_id_and_org, visualization_id, self.current_org)
        require_object_modify_permission(vis.query_rel, self.current_user)
        self.record_event({
            'action': 'delete',
            'object_id': visualization_id,
            'object_type': 'Visualization'
        })
        models.db.session.delete(vis)
        models.db.session.commit()
<EOF>
<BOF>
from flask import make_response, request
from flask_restful import abort

from redash import models
from redash.destinations import (destinations,
                                 get_configuration_schema_for_destination_type)
from redash.handlers.base import BaseResource
from redash.permissions import require_admin
from redash.utils.configuration import ConfigurationContainer, ValidationError


class DestinationTypeListResource(BaseResource):
    @require_admin
    def get(self):
        return [q.to_dict() for q in destinations.values()]


class DestinationResource(BaseResource):
    @require_admin
    def get(self, destination_id):
        destination = models.NotificationDestination.get_by_id_and_org(destination_id, self.current_org)
        d = destination.to_dict(all=True)
        self.record_event({
            'action': 'view',
            'object_id': destination_id,
            'object_type': 'destination',
        })
        return d

    @require_admin
    def post(self, destination_id):
        destination = models.NotificationDestination.get_by_id_and_org(destination_id, self.current_org)
        req = request.get_json(True)

        schema = get_configuration_schema_for_destination_type(req['type'])
        if schema is None:
            abort(400)

        try:
            destination.type = req['type']
            destination.name = req['name']
            destination.options.set_schema(schema)
            destination.options.update(req['options'])
            models.db.session.add(destination)
            models.db.session.commit()
        except ValidationError:
            abort(400)

        return destination.to_dict(all=True)

    @require_admin
    def delete(self, destination_id):
        destination = models.NotificationDestination.get_by_id_and_org(destination_id, self.current_org)
        models.db.session.delete(destination)
        models.db.session.commit()

        self.record_event({
            'action': 'delete',
            'object_id': destination_id,
            'object_type': 'destination'
        })

        return make_response('', 204)


class DestinationListResource(BaseResource):
    def get(self):
        destinations = models.NotificationDestination.all(self.current_org)

        response = {}
        for ds in destinations:
            if ds.id in response:
                continue

            d = ds.to_dict()
            response[ds.id] = d

        self.record_event({
            'action': 'list',
            'object_id': 'admin/destinations',
            'object_type': 'destination',
        })

        return response.values()

    @require_admin
    def post(self):
        req = request.get_json(True)
        required_fields = ('options', 'name', 'type')
        for f in required_fields:
            if f not in req:
                abort(400)

        schema = get_configuration_schema_for_destination_type(req['type'])
        if schema is None:
            abort(400)

        config = ConfigurationContainer(req['options'], schema)
        if not config.is_valid():
            abort(400)

        destination = models.NotificationDestination(org=self.current_org,
                                                     name=req['name'],
                                                     type=req['type'],
                                                     options=config,
                                                     user=self.current_user)

        models.db.session.add(destination)
        models.db.session.commit()
        return destination.to_dict(all=True)
<EOF>
<BOF>
from flask import request

from redash import models
from redash.handlers.base import BaseResource
from redash.serializers import serialize_widget
from redash.permissions import (require_access,
                                require_object_modify_permission,
                                require_permission, view_only)
from redash.utils import json_dumps


class WidgetListResource(BaseResource):
    @require_permission('edit_dashboard')
    def post(self):
        """
        Add a widget to a dashboard.

        :<json number dashboard_id: The ID for the dashboard being added to
        :<json visualization_id: The ID of the visualization to put in this widget
        :<json object options: Widget options
        :<json string text: Text box contents
        :<json number width: Width for widget display

        :>json object widget: The created widget
        """
        widget_properties = request.get_json(force=True)
        dashboard = models.Dashboard.get_by_id_and_org(widget_properties.pop('dashboard_id'), self.current_org)
        require_object_modify_permission(dashboard, self.current_user)

        widget_properties['options'] = json_dumps(widget_properties['options'])
        widget_properties.pop('id', None)
        widget_properties['dashboard'] = dashboard

        visualization_id = widget_properties.pop('visualization_id')
        if visualization_id:
            visualization = models.Visualization.get_by_id_and_org(visualization_id, self.current_org)
            require_access(visualization.query_rel.groups, self.current_user, view_only)
        else:
            visualization = None

        widget_properties['visualization'] = visualization

        widget = models.Widget(**widget_properties)
        models.db.session.add(widget)
        models.db.session.commit()

        models.db.session.commit()
        return serialize_widget(widget)


class WidgetResource(BaseResource):
    @require_permission('edit_dashboard')
    def post(self, widget_id):
        """
        Updates a widget in a dashboard.
        This method currently handles Text Box widgets only.

        :param number widget_id: The ID of the widget to modify

        :<json string text: The new contents of the text box
        """
        widget = models.Widget.get_by_id_and_org(widget_id, self.current_org)
        require_object_modify_permission(widget.dashboard, self.current_user)
        widget_properties = request.get_json(force=True)
        widget.text = widget_properties['text']
        widget.options = json_dumps(widget_properties['options'])
        models.db.session.commit()
        return serialize_widget(widget)

    @require_permission('edit_dashboard')
    def delete(self, widget_id):
        """
        Remove a widget from a dashboard.

        :param number widget_id: ID of widget to remove
        """
        widget = models.Widget.get_by_id_and_org(widget_id, self.current_org)
        require_object_modify_permission(widget.dashboard, self.current_user)
        self.record_event({
            'action': 'delete',
            'object_id': widget_id,
            'object_type': 'widget',
        })
        models.db.session.delete(widget)
        models.db.session.commit()
<EOF>
<BOF>
from flask import request
from geoip import geolite2
from user_agents import parse as parse_ua

from redash.handlers.base import BaseResource, paginate
from redash.permissions import require_admin


def get_location(ip):
    if ip is None:
        return "Unknown"

    match = geolite2.lookup(ip)
    if match is None:
        return "Unknown"

    return match.country


def event_details(event):
    details = {}
    if event.object_type == 'data_source' and event.action == 'execute_query':
        details['query'] = event.additional_properties['query']
        details['data_source'] = event.object_id
    elif event.object_type == 'page' and event.action =='view':
        details['page'] = event.object_id
    else:
        details['object_id'] = event.object_id
        details['object_type'] = event.object_type

    return details


def serialize_event(event):
    d = {
        'org_id': event.org_id,
        'user_id': event.user_id,
        'action': event.action,
        'object_type': event.object_type,
        'object_id': event.object_id,
        'created_at': event.created_at
    }

    if event.user_id:
        d['user_name'] = event.additional_properties.get('user_name', 'User {}'.format(event.user_id))

    if not event.user_id:
        d['user_name'] = event.additional_properties.get('api_key', 'Unknown')

    d['browser'] = str(parse_ua(event.additional_properties.get('user_agent', '')))
    d['location'] = get_location(event.additional_properties.get('ip'))
    d['details'] = event_details(event)

    return d


class EventsResource(BaseResource):
    def post(self):
        events_list = request.get_json(force=True)
        for event in events_list:
            self.record_event(event)

    @require_admin
    def get(self):
        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)
        return paginate(self.current_org.events, page, page_size, serialize_event)
<EOF>
<BOF>
from __future__ import absolute_import
import logging
import time

from flask import request

from .authentication import current_org
from flask_login import current_user, login_required
from flask_restful import abort
from redash import models, utils
from redash.handlers import routes
from redash.handlers.base import (get_object_or_404, org_scoped_rule,
                                  record_event)
from redash.handlers.query_results import collect_query_parameters
from redash.handlers.static import render_index
from redash.utils import gen_query_hash, mustache_render


#
# Run a parameterized query synchronously and return the result
# DISCLAIMER: Temporary solution to support parameters in queries. Should be
#             removed once we refactor the query results API endpoints and handling
#             on the client side. Please don't reuse in other API handlers.
#
def run_query_sync(data_source, parameter_values, query_text, max_age=0):
    query_parameters = set(collect_query_parameters(query_text))
    missing_params = set(query_parameters) - set(parameter_values.keys())
    if missing_params:
        raise Exception('Missing parameter value for: {}'.format(", ".join(missing_params)))

    if query_parameters:
        query_text = mustache_render(query_text, parameter_values)

    if max_age <= 0:
        query_result = None
    else:
        query_result = models.QueryResult.get_latest(data_source, query_text, max_age)

    query_hash = gen_query_hash(query_text)

    if query_result:
        logging.info("Returning cached result for query %s" % query_hash)
        return query_result.data

    try:
        started_at = time.time()
        data, error = data_source.query_runner.run_query(query_text, current_user)

        if error:
            return None
        # update cache
        if max_age > 0:
            run_time = time.time() - started_at
            query_result, updated_query_ids = models.QueryResult.store_result(data_source.org_id, data_source.id,
                                                                              query_hash, query_text, data,
                                                                              run_time, utils.utcnow())

            models.db.session.commit()
        return data
    except Exception:
        if max_age > 0:
            abort(404, message="Unable to get result from the database, and no cached query result found.")
        else:
            abort(503, message="Unable to get result from the database.")
        return None


@routes.route(org_scoped_rule('/embed/query/<query_id>/visualization/<visualization_id>'), methods=['GET'])
@login_required
def embed(query_id, visualization_id, org_slug=None):
    record_event(current_org, current_user._get_current_object(), {
        'action': 'view',
        'object_id': visualization_id,
        'object_type': 'visualization',
        'query_id': query_id,
        'embed': True,
        'referer': request.headers.get('Referer')
    })

    return render_index()


@routes.route(org_scoped_rule('/public/dashboards/<token>'), methods=['GET'])
@login_required
def public_dashboard(token, org_slug=None):
    if current_user.is_api_user():
        dashboard = current_user.object
    else:
        api_key = get_object_or_404(models.ApiKey.get_by_api_key, token)
        dashboard = api_key.object

    record_event(current_org, current_user, {
        'action': 'view',
        'object_id': dashboard.id,
        'object_type': 'dashboard',
        'public': True,
        'headless': 'embed' in request.args,
        'referer': request.headers.get('Referer')
    })
    return render_index()
<EOF>
<BOF>
import time
import chromelogger
from flask import g, request
from flask_sqlalchemy import get_debug_queries


def log_queries():
    total_duration = 0.0
    queries_count = 0

    chromelogger.group("SQL Queries")

    for q in get_debug_queries():
        total_duration += q.duration
        queries_count += 1
        chromelogger.info(q.statement % q.parameters)
        chromelogger.info("Runtime: {:.2f}ms".format(1000 * q.duration))

    chromelogger.info("{} queries executed in {:.2f}ms.".format(queries_count, total_duration*1000))

    chromelogger.group_end("SQL Queries")


def chrome_log(response):
    request_duration = (time.time() - g.start_time) * 1000
    queries_duration = g.get('queries_duration', 0.0)
    queries_count = g.get('queries_count', 0)

    group_name = '{} {} ({}, {:.2f}ms runtime, {} queries in {:.2f}ms)'.format(
        request.method, request.path, response.status_code, request_duration, queries_count, queries_duration)

    chromelogger.group_collapsed(group_name)
    
    endpoint = (request.endpoint or 'unknown').replace('.', '_')
    chromelogger.info('Endpoint: {}'.format(endpoint))
    chromelogger.info('Content Type: {}'.format(response.content_type))
    chromelogger.info('Content Length: {}'.format(response.content_length or -1))

    log_queries()

    chromelogger.group_end(group_name)

    header = chromelogger.get_header()
    if header is not None:
        response.headers.add(*header)

    return response


def init_app(app):
    if not app.debug:
        return 

    app.after_request(chrome_log)
<EOF>
<BOF>
import time
from flask import request
from flask_restful import abort
from flask_login import current_user
from funcy import project
from sqlalchemy.exc import IntegrityError
from disposable_email_domains import blacklist
from funcy import partial

from redash import models
from redash.permissions import require_permission, require_admin_or_owner, is_admin_or_owner, \
    require_permission_or_owner, require_admin
from redash.handlers.base import BaseResource, require_fields, get_object_or_404, paginate, order_results as _order_results

from redash.authentication.account import invite_link_for_user, send_invite_email, send_password_reset_email


# Ordering map for relationships
order_map = {
    'name': 'name',
    '-name': '-name',
    'created_at': 'created_at',
    '-created_at': '-created_at',
    'groups': 'group_ids',
    '-groups': '-group_ids',
}

order_results = partial(
    _order_results,
    default_order='-created_at',
    allowed_orders=order_map,
)


def invite_user(org, inviter, user):
    invite_url = invite_link_for_user(user)
    send_invite_email(inviter, user, invite_url, org)
    return invite_url


class UserListResource(BaseResource):
    @require_permission('list_users')
    def get(self):
        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)

        groups = {group.id: group for group in models.Group.all(self.current_org)}

        def serialize_user(user):
            d = user.to_dict()
            user_groups = []
            for group_id in set(d['groups']):
                group = groups.get(group_id)

                if group:
                    user_groups.append({'id': group.id, 'name': group.name})

            d['groups'] = user_groups

            return d

        search_term = request.args.get('q', '')

        if request.args.get('disabled', None) is not None:
            users = models.User.all_disabled(self.current_org)
        else:
            users = models.User.all(self.current_org)

        if search_term:
            users = models.User.search(users, search_term)
            self.record_event({
                'action': 'search',
                'object_type': 'user',
                'term': search_term,
            })
        else:
            self.record_event({
                'action': 'list',
                'object_type': 'user',
            })

        # order results according to passed order parameter,
        # special-casing search queries where the database
        # provides an order by search rank
        ordered_users = order_results(users, fallback=bool(search_term))

        return paginate(ordered_users, page, page_size, serialize_user)

    @require_admin
    def post(self):
        req = request.get_json(force=True)
        require_fields(req, ('name', 'email'))

        name, domain = req['email'].split('@', 1)

        if domain.lower() in blacklist or domain.lower() == 'qq.com':
            abort(400, message='Bad email address.')

        user = models.User(org=self.current_org,
                           name=req['name'],
                           email=req['email'],
                           group_ids=[self.current_org.default_group.id])

        try:
            models.db.session.add(user)
            models.db.session.commit()
        except IntegrityError as e:
            if "email" in e.message:
                abort(400, message='Email already taken.')
            abort(500)

        self.record_event({
            'action': 'create',
            'object_id': user.id,
            'object_type': 'user'
        })

        if request.args.get('no_invite') is not None:
            invite_url = invite_link_for_user(user)
        else:
            invite_url = invite_user(self.current_org, self.current_user, user)

        d = user.to_dict()
        d['invite_link'] = invite_url

        return d


class UserInviteResource(BaseResource):
    @require_admin
    def post(self, user_id):
        user = models.User.get_by_id_and_org(user_id, self.current_org)
        invite_url = invite_user(self.current_org, self.current_user, user)

        d = user.to_dict()
        d['invite_link'] = invite_url

        return d


class UserResetPasswordResource(BaseResource):
    @require_admin
    def post(self, user_id):
        user = models.User.get_by_id_and_org(user_id, self.current_org)
        if user.is_disabled:
            abort(404, message='Not found')
        reset_link = send_password_reset_email(user)

        return {
            'reset_link': reset_link,
        }


class UserResource(BaseResource):
    def get(self, user_id):
        require_permission_or_owner('list_users', user_id)
        user = get_object_or_404(models.User.get_by_id_and_org, user_id, self.current_org)

        self.record_event({
            'action': 'view',
            'object_id': user_id,
            'object_type': 'user',
        })

        return user.to_dict(with_api_key=is_admin_or_owner(user_id))

    def post(self, user_id):
        require_admin_or_owner(user_id)
        user = models.User.get_by_id_and_org(user_id, self.current_org)

        req = request.get_json(True)

        params = project(req, ('email', 'name', 'password', 'old_password', 'groups'))

        if 'password' in params and 'old_password' not in params:
            abort(403, message="Must provide current password to update password.")

        if 'old_password' in params and not user.verify_password(params['old_password']):
            abort(403, message="Incorrect current password.")

        if 'password' in params:
            user.hash_password(params.pop('password'))
            params.pop('old_password')

        if 'groups' in params and not self.current_user.has_permission('admin'):
            abort(403, message="Must be admin to change groups membership.")
        
        if 'email' in params:
            _, domain = params['email'].split('@', 1)

            if domain.lower() in blacklist or domain.lower() == 'qq.com':
                abort(400, message='Bad email address.')

        try:
            self.update_model(user, params)
            models.db.session.commit()
        except IntegrityError as e:
            if "email" in e.message:
                message = "Email already taken."
            else:
                message = "Error updating record"

            abort(400, message=message)

        self.record_event({
            'action': 'edit',
            'object_id': user.id,
            'object_type': 'user',
            'updated_fields': params.keys()
        })

        return user.to_dict(with_api_key=is_admin_or_owner(user_id))


class UserDisableResource(BaseResource):
    @require_admin
    def post(self, user_id):
        user = models.User.get_by_id_and_org(user_id, self.current_org)
        # admin cannot disable self; current user is an admin (`@require_admin`)
        # so just check user id
        if user.id == current_user.id:
            abort(400, message="You cannot disable your own account. "
                               "Please ask another admin to do this for you.")
        user.disable()
        models.db.session.commit()

        return user.to_dict(with_api_key=is_admin_or_owner(user_id))

    @require_admin
    def delete(self, user_id):
        user = models.User.get_by_id_and_org(user_id, self.current_org)
        user.enable()
        models.db.session.commit()

        return user.to_dict(with_api_key=is_admin_or_owner(user_id))
<EOF>
<BOF>
from flask import request
from funcy import project

from redash import models
from redash.permissions import require_admin_or_owner
from redash.handlers.base import (BaseResource, require_fields,
                                  get_object_or_404)


class QuerySnippetResource(BaseResource):
    def get(self, snippet_id):
        snippet = get_object_or_404(models.QuerySnippet.get_by_id_and_org,
                                    snippet_id, self.current_org)

        self.record_event({
            'action': 'view',
            'object_id': snippet_id,
            'object_type': 'query_snippet',
        })

        return snippet.to_dict()

    def post(self, snippet_id):
        req = request.get_json(True)
        params = project(req, ('trigger', 'description', 'snippet'))
        snippet = get_object_or_404(models.QuerySnippet.get_by_id_and_org,
                                    snippet_id, self.current_org)
        require_admin_or_owner(snippet.user.id)

        self.update_model(snippet, params)
        models.db.session.commit()

        self.record_event({
            'action': 'edit',
            'object_id': snippet.id,
            'object_type': 'query_snippet'
        })
        return snippet.to_dict()

    def delete(self, snippet_id):
        snippet = get_object_or_404(models.QuerySnippet.get_by_id_and_org,
                                    snippet_id, self.current_org)
        require_admin_or_owner(snippet.user.id)
        models.db.session.delete(snippet)
        models.db.session.commit()

        self.record_event({
            'action': 'delete',
            'object_id': snippet.id,
            'object_type': 'query_snippet'
        })


class QuerySnippetListResource(BaseResource):
    def post(self):
        req = request.get_json(True)
        require_fields(req, ('trigger', 'description', 'snippet'))

        snippet = models.QuerySnippet(
            trigger=req['trigger'],
            description=req['description'],
            snippet=req['snippet'],
            user=self.current_user,
            org=self.current_org
        )

        models.db.session.add(snippet)
        models.db.session.commit()

        self.record_event({
            'action': 'create',
            'object_id': snippet.id,
            'object_type': 'query_snippet'
        })

        return snippet.to_dict()

    def get(self):
        self.record_event({
            'action': 'list',
            'object_type': 'query_snippet',
        })
        return [snippet.to_dict() for snippet in
                models.QuerySnippet.all(org=self.current_org)]
<EOF>
<BOF>
from flask import request
from redash import models
from redash.permissions import require_access, view_only
from redash.handlers.base import BaseResource, get_object_or_404, filter_by_tags, paginate
from redash.handlers.queries import order_results
from redash.serializers import QuerySerializer, serialize_dashboard

from sqlalchemy.exc import IntegrityError


class QueryFavoriteListResource(BaseResource):
    def get(self):
        search_term = request.args.get('q')

        if search_term:
            base_query = models.Query.search(search_term, self.current_user.group_ids, include_drafts=True, limit=None)
            favorites = models.Query.favorites(self.current_user, base_query=base_query)
        else:
            favorites = models.Query.favorites(self.current_user)

        favorites = filter_by_tags(favorites, models.Query.tags)

        # order results according to passed order parameter,
        # special-casing search queries where the database
        # provides an order by search rank
        ordered_favorites = order_results(favorites, fallback=bool(search_term))

        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)
        response = paginate(
            ordered_favorites,
            page,
            page_size,
            QuerySerializer,
            with_stats=True,
            with_last_modified_by=False,
        )

        self.record_event({
            'action': 'load_favorites',
            'object_type': 'query',
            'params': {
                'q': search_term,
                'tags': request.args.getlist('tags'),
                'page': page
            }
        })

        return response


class QueryFavoriteResource(BaseResource):
    def post(self, query_id):
        query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        require_access(query.groups, self.current_user, view_only)

        fav = models.Favorite(org_id=self.current_org.id, object=query, user=self.current_user)
        models.db.session.add(fav)

        try:
            models.db.session.commit()
        except IntegrityError as e:
            if 'unique_favorite' in e.message:
                models.db.session.rollback()
            else:
                raise e

        self.record_event({
            'action': 'favorite',
            'object_id': query.id,
            'object_type': 'query'
        })

    def delete(self, query_id):
        query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        require_access(query.groups, self.current_user, view_only)

        models.Favorite.query.filter(
            models.Favorite.object_id == query_id,
            models.Favorite.object_type == u'Query',
            models.Favorite.user==self.current_user,
        ).delete()
        models.db.session.commit()

        self.record_event({
            'action': 'favorite',
            'object_id': query.id,
            'object_type': 'query'
        })


class DashboardFavoriteListResource(BaseResource):
    def get(self):
        search_term = request.args.get('q')

        if search_term:
            base_query = models.Dashboard.search(self.current_org, self.current_user.group_ids, self.current_user.id, search_term)
            favorites = models.Dashboard.favorites(self.current_user, base_query=base_query)
        else:
            favorites = models.Dashboard.favorites(self.current_user)

        favorites = filter_by_tags(favorites, models.Dashboard.tags)

        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)
        response = paginate(favorites, page, page_size, serialize_dashboard)

        self.record_event({
            'action': 'load_favorites',
            'object_type': 'dashboard',
            'params': {
                'q': search_term,
                'tags': request.args.getlist('tags'),
                'page': page
            }
        })

        return response


class DashboardFavoriteResource(BaseResource):
    def post(self, object_id):
        dashboard = get_object_or_404(models.Dashboard.get_by_slug_and_org, object_id, self.current_org)
        fav = models.Favorite(org_id=self.current_org.id, object=dashboard, user=self.current_user)
        models.db.session.add(fav)

        try:
            models.db.session.commit()
        except IntegrityError as e:
            if 'unique_favorite' in e.message:
                models.db.session.rollback()
            else:
                raise e

        self.record_event({
            'action': 'favorite',
            'object_id': dashboard.id,
            'object_type': 'dashboard'
        })

    def delete(self, object_id):
        dashboard = get_object_or_404(models.Dashboard.get_by_slug_and_org, object_id, self.current_org)
        models.Favorite.query.filter(models.Favorite.object==dashboard, models.Favorite.user==self.current_user).delete()
        models.db.session.commit()
        self.record_event({
            'action': 'unfavorite',
            'object_id': dashboard.id,
            'object_type': 'dashboard'
        })
<EOF>
<BOF>
from flask import request
from flask_login import login_required, current_user

from redash import models, redis_connection
from redash.authentication import current_org
from redash.handlers import routes
from redash.handlers.base import json_response, record_event
from redash.permissions import require_super_admin
from redash.serializers import QuerySerializer
from redash.tasks.queries import QueryTaskTracker
from redash.utils import json_loads


@routes.route('/api/admin/queries/outdated', methods=['GET'])
@require_super_admin
@login_required
def outdated_queries():
    manager_status = redis_connection.hgetall('redash:status')
    query_ids = json_loads(manager_status.get('query_ids', '[]'))
    if query_ids:
        outdated_queries = (
            models.Query.query.outerjoin(models.QueryResult)
                              .filter(models.Query.id.in_(query_ids))
                              .order_by(models.Query.created_at.desc())
        )
    else:
        outdated_queries = []

    record_event(current_org, current_user._get_current_object(), {
        'action': 'list',
        'object_type': 'outdated_queries',
    })

    response = {
        'queries': QuerySerializer(outdated_queries, with_stats=True, with_last_modified_by=False).serialize(),
        'updated_at': manager_status['last_refresh_at'],
    }
    return json_response(response)


@routes.route('/api/admin/queries/tasks', methods=['GET'])
@require_super_admin
@login_required
def queries_tasks():
    record_event(current_org, current_user._get_current_object(), {
        'action': 'list',
        'object_id': 'admin/tasks',
        'object_type': 'celery_tasks'
    })

    global_limit = int(request.args.get('limit', 50))
    waiting_limit = int(request.args.get('waiting_limit', global_limit))
    progress_limit = int(request.args.get('progress_limit', global_limit))
    done_limit = int(request.args.get('done_limit', global_limit))

    waiting = QueryTaskTracker.all(QueryTaskTracker.WAITING_LIST, limit=waiting_limit)
    in_progress = QueryTaskTracker.all(QueryTaskTracker.IN_PROGRESS_LIST, limit=progress_limit)
    done = QueryTaskTracker.all(QueryTaskTracker.DONE_LIST, limit=done_limit)

    response = {
        'waiting': [t.data for t in waiting if t is not None],
        'in_progress': [t.data for t in in_progress if t is not None],
        'done': [t.data for t in done if t is not None]
    }

    return json_response(response)
<EOF>
<BOF>
import logging

from flask import make_response, request
from flask_restful import abort
from funcy import project
from six import text_type
from sqlalchemy.exc import IntegrityError

from redash import models
from redash.handlers.base import BaseResource, get_object_or_404
from redash.permissions import (require_access, require_admin,
                                require_permission, view_only)
from redash.query_runner import (get_configuration_schema_for_query_runner_type,
                                 query_runners, NotSupported)
from redash.utils import filter_none
from redash.utils.configuration import ConfigurationContainer, ValidationError


class DataSourceTypeListResource(BaseResource):
    @require_admin
    def get(self):
        return [q.to_dict() for q in sorted(query_runners.values(), key=lambda q: q.name())]


class DataSourceResource(BaseResource):
    @require_admin
    def get(self, data_source_id):
        data_source = models.DataSource.get_by_id_and_org(data_source_id, self.current_org)
        ds = data_source.to_dict(all=True)
        self.record_event({
            'action': 'view',
            'object_id': data_source_id,
            'object_type': 'datasource',
        })
        return ds

    @require_admin
    def post(self, data_source_id):
        data_source = models.DataSource.get_by_id_and_org(data_source_id, self.current_org)
        req = request.get_json(True)

        schema = get_configuration_schema_for_query_runner_type(req['type'])
        if schema is None:
            abort(400)
        try:
            data_source.options.set_schema(schema)
            data_source.options.update(filter_none(req['options']))
        except ValidationError:
            abort(400)

        data_source.type = req['type']
        data_source.name = req['name']
        models.db.session.add(data_source)

        try:
            models.db.session.commit()
        except IntegrityError as e:
            if req['name'] in e.message:
                abort(400, message="Data source with the name {} already exists.".format(req['name']))

            abort(400)

        return data_source.to_dict(all=True)

    @require_admin
    def delete(self, data_source_id):
        data_source = models.DataSource.get_by_id_and_org(data_source_id, self.current_org)
        data_source.delete()

        self.record_event({
            'action': 'delete',
            'object_id': data_source_id,
            'object_type': 'datasource',
        })

        return make_response('', 204)


class DataSourceListResource(BaseResource):
    @require_permission('list_data_sources')
    def get(self):
        if self.current_user.has_permission('admin'):
            data_sources = models.DataSource.all(self.current_org)
        else:
            data_sources = models.DataSource.all(self.current_org, group_ids=self.current_user.group_ids)

        response = {}
        for ds in data_sources:
            if ds.id in response:
                continue

            try:
                d = ds.to_dict()
                d['view_only'] = all(project(ds.groups, self.current_user.group_ids).values())
                response[ds.id] = d
            except AttributeError:
                logging.exception("Error with DataSource#to_dict (data source id: %d)", ds.id)

        self.record_event({
            'action': 'list',
            'object_id': 'admin/data_sources',
            'object_type': 'datasource',
        })

        return sorted(response.values(), key=lambda d: d['name'].lower())

    @require_admin
    def post(self):
        req = request.get_json(True)
        required_fields = ('options', 'name', 'type')
        for f in required_fields:
            if f not in req:
                abort(400)

        schema = get_configuration_schema_for_query_runner_type(req['type'])
        if schema is None:
            abort(400)

        config = ConfigurationContainer(filter_none(req['options']), schema)
        # from IPython import embed
        # embed()
        if not config.is_valid():
            abort(400)

        try:
            datasource = models.DataSource.create_with_group(org=self.current_org,
                                                             name=req['name'],
                                                             type=req['type'],
                                                             options=config)

            models.db.session.commit()
        except IntegrityError as e:
            if req['name'] in e.message:
                abort(400, message="Data source with the name {} already exists.".format(req['name']))

            abort(400)

        self.record_event({
            'action': 'create',
            'object_id': datasource.id,
            'object_type': 'datasource'
        })

        return datasource.to_dict(all=True)


class DataSourceSchemaResource(BaseResource):
    def get(self, data_source_id):
        data_source = get_object_or_404(models.DataSource.get_by_id_and_org, data_source_id, self.current_org)
        require_access(data_source.groups, self.current_user, view_only)
        refresh = request.args.get('refresh') is not None

        response = {}

        try:
            response['schema'] = data_source.get_schema(refresh)
        except NotSupported:
            response['error'] = {
                'code': 1,
                'message': 'Data source type does not support retrieving schema'
            }
        except Exception:
            response['error'] = {
                'code': 2,
                'message': 'Error retrieving schema.'
            }

        return response


class DataSourcePauseResource(BaseResource):
    @require_admin
    def post(self, data_source_id):
        data_source = get_object_or_404(models.DataSource.get_by_id_and_org, data_source_id, self.current_org)
        data = request.get_json(force=True, silent=True)
        if data:
            reason = data.get('reason')
        else:
            reason = request.args.get('reason')

        data_source.pause(reason)

        self.record_event({
            'action': 'pause',
            'object_id': data_source.id,
            'object_type': 'datasource'
        })
        return data_source.to_dict()

    @require_admin
    def delete(self, data_source_id):
        data_source = get_object_or_404(models.DataSource.get_by_id_and_org, data_source_id, self.current_org)
        data_source.resume()

        self.record_event({
            'action': 'resume',
            'object_id': data_source.id,
            'object_type': 'datasource'
        })
        return data_source.to_dict()


class DataSourceTestResource(BaseResource):
    @require_admin
    def post(self, data_source_id):
        data_source = get_object_or_404(models.DataSource.get_by_id_and_org, data_source_id, self.current_org)

        self.record_event({
            'action': 'test',
            'object_id': data_source_id,
            'object_type': 'datasource',
        })

        try:
            data_source.query_runner.test_connection()
        except Exception as e:
            return {"message": text_type(e), "ok": False}
        else:
            return {"message": "success", "ok": True}
<EOF>
<BOF>
import os
import simplejson
from flask import url_for

WEBPACK_MANIFEST_PATH = os.path.join(os.path.dirname(__file__), '../../client/dist/', 'asset-manifest.json')


def configure_webpack(app):
    app.extensions['webpack'] = {'assets': None}

    def get_asset(path):
        assets = app.extensions['webpack']['assets']
        # in debug we read in this file each request
        if assets is None or app.debug:
            try:
                with open(WEBPACK_MANIFEST_PATH) as fp:
                    assets = simplejson.load(fp)
            except IOError:
                app.logger.exception('Unable to load webpack manifest')
                assets = {}
            app.extensions['webpack']['assets'] = assets
        return url_for('static', filename=assets.get(path, path))

    @app.context_processor
    def webpack_assets():
        return {
            'asset_url': get_asset,
        }
<EOF>
<BOF>
from flask import jsonify
from flask_login import login_required

from redash.handlers.api import api
from redash.handlers.base import routes
from redash.monitor import get_status
from redash.permissions import require_super_admin


@routes.route('/ping', methods=['GET'])
def ping():
    return 'PONG.'


@routes.route('/status.json')
@login_required
@require_super_admin
def status_api():
    status = get_status()
    return jsonify(status)


def init_app(app):
    from redash.handlers import embed, queries, static, authentication, admin, setup, organization
    app.register_blueprint(routes)
    api.init_app(app)
<EOF>
<BOF>
from flask import request, url_for
from funcy import project, partial

from flask_restful import abort
from redash import models, serializers
from redash.handlers.base import (BaseResource, get_object_or_404, paginate,
                                  filter_by_tags,
                                  order_results as _order_results)
from redash.serializers import serialize_dashboard
from redash.permissions import (can_modify, require_admin_or_owner,
                                require_object_modify_permission,
                                require_permission)
from sqlalchemy.orm.exc import StaleDataError


# Ordering map for relationships
order_map = {
    'name': 'lowercase_name',
    '-name': '-lowercase_name',
    'created_at': 'created_at',
    '-created_at': '-created_at',
}

order_results = partial(
    _order_results,
    default_order='-created_at',
    allowed_orders=order_map,
)


class DashboardListResource(BaseResource):
    @require_permission('list_dashboards')
    def get(self):
        """
        Lists all accessible dashboards.

        :qparam number page_size: Number of queries to return per page
        :qparam number page: Page number to retrieve
        :qparam number order: Name of column to order by
        :qparam number q: Full text search term

        Responds with an array of :ref:`dashboard <dashboard-response-label>`
        objects.
        """
        search_term = request.args.get('q')

        if search_term:
            results = models.Dashboard.search(
                self.current_org,
                self.current_user.group_ids,
                self.current_user.id,
                search_term,
            )
        else:
            results = models.Dashboard.all(
                self.current_org,
                self.current_user.group_ids,
                self.current_user.id,
            )

        results = filter_by_tags(results, models.Dashboard.tags)

        # order results according to passed order parameter,
        # special-casing search queries where the database
        # provides an order by search rank
        ordered_results = order_results(results, fallback=bool(search_term))

        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)

        response = paginate(
            ordered_results,
            page=page,
            page_size=page_size,
            serializer=serialize_dashboard,
        )

        if search_term:
            self.record_event({
                'action': 'search',
                'object_type': 'dashboard',
                'term': search_term,
            })
        else:
            self.record_event({
                'action': 'list',
                'object_type': 'dashboard',
            })

        return response

    @require_permission('create_dashboard')
    def post(self):
        """
        Creates a new dashboard.

        :<json string name: Dashboard name

        Responds with a :ref:`dashboard <dashboard-response-label>`.
        """
        dashboard_properties = request.get_json(force=True)
        dashboard = models.Dashboard(name=dashboard_properties['name'],
                                     org=self.current_org,
                                     user=self.current_user,
                                     is_draft=True,
                                     layout='[]')
        models.db.session.add(dashboard)
        models.db.session.commit()
        return serialize_dashboard(dashboard)


class DashboardResource(BaseResource):
    @require_permission('list_dashboards')
    def get(self, dashboard_slug=None):
        """
        Retrieves a dashboard.

        :qparam string slug: Slug of dashboard to retrieve.

        .. _dashboard-response-label:

        :>json number id: Dashboard ID
        :>json string name:
        :>json string slug:
        :>json number user_id: ID of the dashboard creator
        :>json string created_at: ISO format timestamp for dashboard creation
        :>json string updated_at: ISO format timestamp for last dashboard modification
        :>json number version: Revision number of dashboard
        :>json boolean dashboard_filters_enabled: Whether filters are enabled or not
        :>json boolean is_archived: Whether this dashboard has been removed from the index or not
        :>json boolean is_draft: Whether this dashboard is a draft or not.
        :>json array layout: Array of arrays containing widget IDs, corresponding to the rows and columns the widgets are displayed in
        :>json array widgets: Array of arrays containing :ref:`widget <widget-response-label>` data

        .. _widget-response-label:

        Widget structure:

        :>json number widget.id: Widget ID
        :>json number widget.width: Widget size
        :>json object widget.options: Widget options
        :>json number widget.dashboard_id: ID of dashboard containing this widget
        :>json string widget.text: Widget contents, if this is a text-box widget
        :>json object widget.visualization: Widget contents, if this is a visualization widget
        :>json string widget.created_at: ISO format timestamp for widget creation
        :>json string widget.updated_at: ISO format timestamp for last widget modification
        """
        dashboard = get_object_or_404(models.Dashboard.get_by_slug_and_org, dashboard_slug, self.current_org)
        response = serialize_dashboard(dashboard, with_widgets=True, user=self.current_user)

        api_key = models.ApiKey.get_by_object(dashboard)
        if api_key:
            response['public_url'] = url_for('redash.public_dashboard', token=api_key.api_key, org_slug=self.current_org.slug, _external=True)
            response['api_key'] = api_key.api_key

        response['can_edit'] = can_modify(dashboard, self.current_user)

        self.record_event({
            'action': 'view',
            'object_id': dashboard.id,
            'object_type': 'dashboard',
        })

        return response

    @require_permission('edit_dashboard')
    def post(self, dashboard_slug):
        """
        Modifies a dashboard.

        :qparam string slug: Slug of dashboard to retrieve.

        Responds with the updated :ref:`dashboard <dashboard-response-label>`.

        :status 200: success
        :status 409: Version conflict -- dashboard modified since last read
        """
        dashboard_properties = request.get_json(force=True)
        # TODO: either convert all requests to use slugs or ids
        dashboard = models.Dashboard.get_by_id_and_org(dashboard_slug, self.current_org)

        require_object_modify_permission(dashboard, self.current_user)

        updates = project(dashboard_properties, ('name', 'layout', 'version', 'tags',
                                                 'is_draft', 'dashboard_filters_enabled'))

        # SQLAlchemy handles the case where a concurrent transaction beats us
        # to the update. But we still have to make sure that we're not starting
        # out behind.
        if 'version' in updates and updates['version'] != dashboard.version:
            abort(409)

        updates['changed_by'] = self.current_user

        self.update_model(dashboard, updates)
        models.db.session.add(dashboard)
        try:
            models.db.session.commit()
        except StaleDataError:
            abort(409)

        result = serialize_dashboard(dashboard, with_widgets=True, user=self.current_user)

        self.record_event({
            'action': 'edit',
            'object_id': dashboard.id,
            'object_type': 'dashboard',
        })

        return result

    @require_permission('edit_dashboard')
    def delete(self, dashboard_slug):
        """
        Archives a dashboard.

        :qparam string slug: Slug of dashboard to retrieve.

        Responds with the archived :ref:`dashboard <dashboard-response-label>`.
        """
        dashboard = models.Dashboard.get_by_slug_and_org(dashboard_slug, self.current_org)
        dashboard.is_archived = True
        dashboard.record_changes(changed_by=self.current_user)
        models.db.session.add(dashboard)
        d = serialize_dashboard(dashboard, with_widgets=True, user=self.current_user)
        models.db.session.commit()

        self.record_event({
            'action': 'archive',
            'object_id': dashboard.id,
            'object_type': 'dashboard',
        })

        return d


class PublicDashboardResource(BaseResource):
    def get(self, token):
        """
        Retrieve a public dashboard.

        :param token: An API key for a public dashboard.
        :>json array widgets: An array of arrays of :ref:`public widgets <public-widget-label>`, corresponding to the rows and columns the widgets are displayed in
        """
        if not isinstance(self.current_user, models.ApiUser):
            api_key = get_object_or_404(models.ApiKey.get_by_api_key, token)
            dashboard = api_key.object
        else:
            dashboard = self.current_user.object

        return serializers.public_dashboard(dashboard)


class DashboardShareResource(BaseResource):
    def post(self, dashboard_id):
        """
        Allow anonymous access to a dashboard.

        :param dashboard_id: The numeric ID of the dashboard to share.
        :>json string public_url: The URL for anonymous access to the dashboard.
        :>json api_key: The API key to use when accessing it.
        """
        dashboard = models.Dashboard.get_by_id_and_org(dashboard_id, self.current_org)
        require_admin_or_owner(dashboard.user_id)
        api_key = models.ApiKey.create_for_object(dashboard, self.current_user)
        models.db.session.flush()
        models.db.session.commit()

        public_url = url_for('redash.public_dashboard', token=api_key.api_key, org_slug=self.current_org.slug, _external=True)

        self.record_event({
            'action': 'activate_api_key',
            'object_id': dashboard.id,
            'object_type': 'dashboard',
        })

        return {'public_url': public_url, 'api_key': api_key.api_key}

    def delete(self, dashboard_id):
        """
        Disable anonymous access to a dashboard.

        :param dashboard_id: The numeric ID of the dashboard to unshare.
        """
        dashboard = models.Dashboard.get_by_id_and_org(dashboard_id, self.current_org)
        require_admin_or_owner(dashboard.user_id)
        api_key = models.ApiKey.get_by_object(dashboard)

        if api_key:
            api_key.active = False
            models.db.session.add(api_key)
            models.db.session.commit()

        self.record_event({
            'action': 'deactivate_api_key',
            'object_id': dashboard.id,
            'object_type': 'dashboard',
        })


class DashboardTagsResource(BaseResource):
    @require_permission('list_dashboards')
    def get(self):
        """
        Lists all accessible dashboards.
        """
        tags = models.Dashboard.all_tags(self.current_org, self.current_user)
        return {
            'tags': [
                {
                    'name': name,
                    'count': count,
                }
                for name, count in tags
            ]
        }
<EOF>
<BOF>
from flask_restful import Api
from werkzeug.wrappers import Response
from flask import make_response

from redash.utils import json_dumps
from redash.handlers.base import org_scoped_rule
from redash.handlers.permissions import ObjectPermissionsListResource, CheckPermissionResource
from redash.handlers.alerts import AlertResource, AlertListResource, AlertSubscriptionListResource, AlertSubscriptionResource
from redash.handlers.dashboards import DashboardListResource, DashboardResource, DashboardShareResource, PublicDashboardResource 
from redash.handlers.data_sources import DataSourceTypeListResource, DataSourceListResource, DataSourceSchemaResource, DataSourceResource, DataSourcePauseResource, DataSourceTestResource
from redash.handlers.events import EventsResource
from redash.handlers.queries import QueryForkResource, QueryRefreshResource, QueryListResource, QueryRecentResource, QuerySearchResource, QueryResource, MyQueriesResource
from redash.handlers.query_results import QueryResultListResource, QueryResultResource, JobResource
from redash.handlers.users import UserResource, UserListResource, UserInviteResource, UserResetPasswordResource, UserDisableResource
from redash.handlers.visualizations import VisualizationListResource
from redash.handlers.visualizations import VisualizationResource
from redash.handlers.widgets import WidgetResource, WidgetListResource
from redash.handlers.groups import GroupListResource, GroupResource, GroupMemberListResource, GroupMemberResource, \
    GroupDataSourceListResource, GroupDataSourceResource
from redash.handlers.destinations import DestinationTypeListResource, DestinationResource, DestinationListResource
from redash.handlers.query_snippets import QuerySnippetListResource, QuerySnippetResource
from redash.handlers.settings import OrganizationSettings
from redash.handlers.favorites import QueryFavoriteListResource, QueryFavoriteResource, DashboardFavoriteListResource, DashboardFavoriteResource
from redash.handlers.queries import QueryTagsResource
from redash.handlers.dashboards import DashboardTagsResource


class ApiExt(Api):
    def add_org_resource(self, resource, *urls, **kwargs):
        urls = [org_scoped_rule(url) for url in urls]
        return self.add_resource(resource, *urls, **kwargs)

api = ApiExt()


@api.representation('application/json')
def json_representation(data, code, headers=None):
    # Flask-Restful checks only for flask.Response but flask-login uses werkzeug.wrappers.Response
    if isinstance(data, Response):
        return data
    resp = make_response(json_dumps(data), code)
    resp.headers.extend(headers or {})
    return resp


api.add_org_resource(AlertResource, '/api/alerts/<alert_id>', endpoint='alert')
api.add_org_resource(AlertSubscriptionListResource, '/api/alerts/<alert_id>/subscriptions', endpoint='alert_subscriptions')
api.add_org_resource(AlertSubscriptionResource, '/api/alerts/<alert_id>/subscriptions/<subscriber_id>', endpoint='alert_subscription')
api.add_org_resource(AlertListResource, '/api/alerts', endpoint='alerts')

api.add_org_resource(DashboardListResource, '/api/dashboards', endpoint='dashboards')
api.add_org_resource(DashboardResource, '/api/dashboards/<dashboard_slug>', endpoint='dashboard')
api.add_org_resource(PublicDashboardResource, '/api/dashboards/public/<token>', endpoint='public_dashboard')
api.add_org_resource(DashboardShareResource, '/api/dashboards/<dashboard_id>/share', endpoint='dashboard_share')

api.add_org_resource(DataSourceTypeListResource, '/api/data_sources/types', endpoint='data_source_types')
api.add_org_resource(DataSourceListResource, '/api/data_sources', endpoint='data_sources')
api.add_org_resource(DataSourceSchemaResource, '/api/data_sources/<data_source_id>/schema')
api.add_org_resource(DataSourcePauseResource, '/api/data_sources/<data_source_id>/pause')
api.add_org_resource(DataSourceTestResource, '/api/data_sources/<data_source_id>/test')
api.add_org_resource(DataSourceResource, '/api/data_sources/<data_source_id>', endpoint='data_source')

api.add_org_resource(GroupListResource, '/api/groups', endpoint='groups')
api.add_org_resource(GroupResource, '/api/groups/<group_id>', endpoint='group')
api.add_org_resource(GroupMemberListResource, '/api/groups/<group_id>/members', endpoint='group_members')
api.add_org_resource(GroupMemberResource, '/api/groups/<group_id>/members/<user_id>', endpoint='group_member')
api.add_org_resource(GroupDataSourceListResource, '/api/groups/<group_id>/data_sources', endpoint='group_data_sources')
api.add_org_resource(GroupDataSourceResource, '/api/groups/<group_id>/data_sources/<data_source_id>', endpoint='group_data_source')

api.add_org_resource(EventsResource, '/api/events', endpoint='events')

api.add_org_resource(QueryFavoriteListResource, '/api/queries/favorites', endpoint='query_fovorites')
api.add_org_resource(QueryFavoriteResource, '/api/queries/<query_id>/favorite', endpoint='query_fovorite')
api.add_org_resource(DashboardFavoriteListResource, '/api/dashboards/favorites', endpoint='dashboard_fovorites')
api.add_org_resource(DashboardFavoriteResource, '/api/dashboards/<object_id>/favorite', endpoint='dashboard_fovorite')

api.add_org_resource(QueryTagsResource, '/api/queries/tags', endpoint='query_tags')
api.add_org_resource(DashboardTagsResource, '/api/dashboards/tags', endpoint='dashboard_tags')

api.add_org_resource(QuerySearchResource, '/api/queries/search', endpoint='queries_search')
api.add_org_resource(QueryRecentResource, '/api/queries/recent', endpoint='recent_queries')
api.add_org_resource(QueryListResource, '/api/queries', endpoint='queries')
api.add_org_resource(MyQueriesResource, '/api/queries/my', endpoint='my_queries')
api.add_org_resource(QueryRefreshResource, '/api/queries/<query_id>/refresh', endpoint='query_refresh')
api.add_org_resource(QueryResource, '/api/queries/<query_id>', endpoint='query')
api.add_org_resource(QueryForkResource, '/api/queries/<query_id>/fork', endpoint='query_fork')

api.add_org_resource(ObjectPermissionsListResource, '/api/<object_type>/<object_id>/acl', endpoint='object_permissions')
api.add_org_resource(CheckPermissionResource, '/api/<object_type>/<object_id>/acl/<access_type>', endpoint='check_permissions')

api.add_org_resource(QueryResultListResource, '/api/query_results', endpoint='query_results')
api.add_org_resource(QueryResultResource,
                     '/api/query_results/<query_result_id>.<filetype>',
                     '/api/query_results/<query_result_id>',
                     '/api/queries/<query_id>/results.<filetype>',
                     '/api/queries/<query_id>/results/<query_result_id>.<filetype>',
                     endpoint='query_result')
api.add_org_resource(JobResource, '/api/jobs/<job_id>', endpoint='job')

api.add_org_resource(UserListResource, '/api/users', endpoint='users')
api.add_org_resource(UserResource, '/api/users/<user_id>', endpoint='user')
api.add_org_resource(UserInviteResource, '/api/users/<user_id>/invite', endpoint='user_invite')
api.add_org_resource(UserResetPasswordResource, '/api/users/<user_id>/reset_password', endpoint='user_reset_password')
api.add_org_resource(UserDisableResource, '/api/users/<user_id>/disable', endpoint='user_disable')

api.add_org_resource(VisualizationListResource, '/api/visualizations', endpoint='visualizations')
api.add_org_resource(VisualizationResource, '/api/visualizations/<visualization_id>', endpoint='visualization')

api.add_org_resource(WidgetListResource, '/api/widgets', endpoint='widgets')
api.add_org_resource(WidgetResource, '/api/widgets/<int:widget_id>', endpoint='widget')

api.add_org_resource(DestinationTypeListResource, '/api/destinations/types', endpoint='destination_types')
api.add_org_resource(DestinationResource, '/api/destinations/<destination_id>', endpoint='destination')
api.add_org_resource(DestinationListResource, '/api/destinations', endpoint='destinations')

api.add_org_resource(QuerySnippetResource, '/api/query_snippets/<snippet_id>', endpoint='query_snippet')
api.add_org_resource(QuerySnippetListResource, '/api/query_snippets', endpoint='query_snippets')

api.add_org_resource(OrganizationSettings, '/api/settings/organization', endpoint='organization_settings')
<EOF>
<BOF>
import sqlparse
from flask import jsonify, request, url_for
from flask_login import login_required
from flask_restful import abort
from sqlalchemy.orm.exc import StaleDataError
from funcy import partial

from redash import models, settings
from redash.authentication.org_resolving import current_org
from redash.handlers.base import (BaseResource, filter_by_tags, get_object_or_404,
                                  org_scoped_rule, paginate, routes, order_results as _order_results)
from redash.handlers.query_results import run_query
from redash.permissions import (can_modify, not_view_only, require_access,
                                require_admin_or_owner,
                                require_object_modify_permission,
                                require_permission, view_only)
from redash.utils import collect_parameters_from_request
from redash.serializers import QuerySerializer


# Ordering map for relationships
order_map = {
    'name': 'lowercase_name',
    '-name': '-lowercase_name',
    'created_at': 'created_at',
    '-created_at': '-created_at',
    'schedule': 'schedule',
    '-schedule': '-schedule',
    'runtime': 'query_results-runtime',
    '-runtime': '-query_results-runtime',
    'executed_at': 'query_results-retrieved_at',
    '-executed_at': '-query_results-retrieved_at',
    'created_by': 'users-name',
    '-created_by': '-users-name',
}

order_results = partial(
    _order_results,
    default_order='-created_at',
    allowed_orders=order_map,
)


@routes.route(org_scoped_rule('/api/queries/format'), methods=['POST'])
@login_required
def format_sql_query(org_slug=None):
    """
    Formats an SQL query using the Python ``sqlparse`` formatter.

    :<json string query: The SQL text to format
    :>json string query: Formatted SQL text
    """
    arguments = request.get_json(force=True)
    query = arguments.get("query", "")

    return jsonify({'query': sqlparse.format(query, **settings.SQLPARSE_FORMAT_OPTIONS)})


class QuerySearchResource(BaseResource):
    @require_permission('view_query')
    def get(self):
        """
        Search query text, names, and descriptions.

        :qparam string q: Search term
        :qparam number include_drafts: Whether to include draft in results

        Responds with a list of :ref:`query <query-response-label>` objects.
        """
        term = request.args.get('q', '')
        if not term:
            return []

        include_drafts = request.args.get('include_drafts') is not None

        self.record_event({
            'action': 'search',
            'object_type': 'query',
            'term': term,
        })

        # this redirects to the new query list API that is aware of search
        new_location = url_for(
            'queries',
            q=term,
            org_slug=current_org.slug,
            drafts='true' if include_drafts else 'false',
        )
        return {}, 301, {'Location': new_location}


class QueryRecentResource(BaseResource):
    @require_permission('view_query')
    def get(self):
        """
        Retrieve up to 10 queries recently modified by the user.

        Responds with a list of :ref:`query <query-response-label>` objects.
        """

        results = models.Query.by_user(self.current_user).order_by(models.Query.updated_at.desc()).limit(10)
        return QuerySerializer(results, with_last_modified_by=False, with_user=False).serialize()


class QueryListResource(BaseResource):
    @require_permission('create_query')
    def post(self):
        """
        Create a new query.

        :<json number data_source_id: The ID of the data source this query will run on
        :<json string query: Query text
        :<json string name:
        :<json string description:
        :<json string schedule: Schedule interval, in seconds, for repeated execution of this query
        :<json object options: Query options

        .. _query-response-label:

        :>json number id: Query ID
        :>json number latest_query_data_id: ID for latest output data from this query
        :>json string name:
        :>json string description:
        :>json string query: Query text
        :>json string query_hash: Hash of query text
        :>json string schedule: Schedule interval, in seconds, for repeated execution of this query
        :>json string api_key: Key for public access to this query's results.
        :>json boolean is_archived: Whether this query is displayed in indexes and search results or not.
        :>json boolean is_draft: Whether this query is a draft or not
        :>json string updated_at: Time of last modification, in ISO format
        :>json string created_at: Time of creation, in ISO format
        :>json number data_source_id: ID of the data source this query will run on
        :>json object options: Query options
        :>json number version: Revision version (for update conflict avoidance)
        :>json number user_id: ID of query creator
        :>json number last_modified_by_id: ID of user who last modified this query
        :>json string retrieved_at: Time when query results were last retrieved, in ISO format (may be null)
        :>json number runtime: Runtime of last query execution, in seconds (may be null)
        """
        query_def = request.get_json(force=True)
        data_source = models.DataSource.get_by_id_and_org(query_def.pop('data_source_id'), self.current_org)
        require_access(data_source.groups, self.current_user, not_view_only)

        for field in ['id', 'created_at', 'api_key', 'visualizations', 'latest_query_data', 'last_modified_by']:
            query_def.pop(field, None)

        query_def['query_text'] = query_def.pop('query')
        query_def['user'] = self.current_user
        query_def['data_source'] = data_source
        query_def['org'] = self.current_org
        query_def['is_draft'] = True
        query = models.Query.create(**query_def)
        models.db.session.add(query)
        models.db.session.commit()

        self.record_event({
            'action': 'create',
            'object_id': query.id,
            'object_type': 'query'
        })

        return QuerySerializer(query).serialize()

    @require_permission('view_query')
    def get(self):
        """
        Retrieve a list of queries.

        :qparam number page_size: Number of queries to return per page
        :qparam number page: Page number to retrieve
        :qparam number order: Name of column to order by
        :qparam number q: Full text search term

        Responds with an array of :ref:`query <query-response-label>` objects.
        """
        # See if we want to do full-text search or just regular queries
        search_term = request.args.get('q', '')

        if search_term:
            results = models.Query.search(
                search_term,
                self.current_user.group_ids,
                self.current_user.id,
                include_drafts=True,
            )
        else:
            results = models.Query.all_queries(
                self.current_user.group_ids,
                self.current_user.id,
                drafts=True,
            )

        results = filter_by_tags(results, models.Query.tags)

        # order results according to passed order parameter,
        # special-casing search queries where the database
        # provides an order by search rank
        ordered_results = order_results(results, fallback=bool(search_term))

        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)

        response = paginate(
            ordered_results,
            page=page,
            page_size=page_size,
            serializer=QuerySerializer,
            with_stats=True,
            with_last_modified_by=False
        )

        if search_term:
            self.record_event({
                'action': 'search',
                'object_type': 'query',
                'term': search_term,
            })
        else:
            self.record_event({
                'action': 'list',
                'object_type': 'query',
            })

        return response


class MyQueriesResource(BaseResource):
    @require_permission('view_query')
    def get(self):
        """
        Retrieve a list of queries created by the current user.

        :qparam number page_size: Number of queries to return per page
        :qparam number page: Page number to retrieve
        :qparam number order: Name of column to order by
        :qparam number search: Full text search term

        Responds with an array of :ref:`query <query-response-label>` objects.
        """
        search_term = request.args.get('q', '')
        if search_term:
            results = models.Query.search_by_user(search_term, self.current_user)
        else:
            results = models.Query.by_user(self.current_user)

        results = filter_by_tags(results, models.Query.tags)

        # order results according to passed order parameter,
        # special-casing search queries where the database
        # provides an order by search rank
        ordered_results = order_results(results, fallback=bool(search_term))

        page = request.args.get('page', 1, type=int)
        page_size = request.args.get('page_size', 25, type=int)
        return paginate(
            ordered_results,
            page,
            page_size,
            QuerySerializer,
            with_stats=True,
            with_last_modified_by=False,
        )


class QueryResource(BaseResource):
    @require_permission('edit_query')
    def post(self, query_id):
        """
        Modify a query.

        :param query_id: ID of query to update
        :<json number data_source_id: The ID of the data source this query will run on
        :<json string query: Query text
        :<json string name:
        :<json string description:
        :<json string schedule: Schedule interval, in seconds, for repeated execution of this query
        :<json object options: Query options

        Responds with the updated :ref:`query <query-response-label>` object.
        """
        query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        query_def = request.get_json(force=True)

        require_object_modify_permission(query, self.current_user)

        for field in ['id', 'created_at', 'api_key', 'visualizations', 'latest_query_data', 'user', 'last_modified_by', 'org']:
            query_def.pop(field, None)

        if 'query' in query_def:
            query_def['query_text'] = query_def.pop('query')

        query_def['last_modified_by'] = self.current_user
        query_def['changed_by'] = self.current_user
        # SQLAlchemy handles the case where a concurrent transaction beats us
        # to the update. But we still have to make sure that we're not starting
        # out behind.
        if 'version' in query_def and query_def['version'] != query.version:
            abort(409)

        try:
            self.update_model(query, query_def)
            models.db.session.commit()
        except StaleDataError:
            abort(409)

        return QuerySerializer(query, with_visualizations=True).serialize()

    @require_permission('view_query')
    def get(self, query_id):
        """
        Retrieve a query.

        :param query_id: ID of query to fetch

        Responds with the :ref:`query <query-response-label>` contents.
        """
        q = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        require_access(q.groups, self.current_user, view_only)

        result = QuerySerializer(q, with_visualizations=True).serialize()
        result['can_edit'] = can_modify(q, self.current_user)

        self.record_event({
            'action': 'view',
            'object_id': query_id,
            'object_type': 'query',
        })

        return result

    # TODO: move to resource of its own? (POST /queries/{id}/archive)
    def delete(self, query_id):
        """
        Archives a query.

        :param query_id: ID of query to archive
        """
        query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        require_admin_or_owner(query.user_id)
        query.archive(self.current_user)
        models.db.session.commit()


class QueryForkResource(BaseResource):
    @require_permission('edit_query')
    def post(self, query_id):
        """
        Creates a new query, copying the query text from an existing one.

        :param query_id: ID of query to fork

        Responds with created :ref:`query <query-response-label>` object.
        """
        query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        require_access(query.data_source.groups, self.current_user, not_view_only)
        forked_query = query.fork(self.current_user)
        models.db.session.commit()

        self.record_event({
            'action': 'fork',
            'object_id': query_id,
            'object_type': 'query',
        })

        return QuerySerializer(forked_query, with_visualizations=True).serialize()


class QueryRefreshResource(BaseResource):
    def post(self, query_id):
        """
        Execute a query, updating the query object with the results.

        :param query_id: ID of query to execute

        Responds with query task details.
        """
        # TODO: this should actually check for permissions, but because currently you can only
        # get here either with a user API key or a query one, we can just check whether it's
        # an api key (meaning this is a query API key, which only grants read access).
        if self.current_user.is_api_user():
            abort(403, message="Please use a user API key.")

        query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)
        require_access(query.groups, self.current_user, not_view_only)

        parameter_values = collect_parameters_from_request(request.args)

        return run_query(query.data_source, parameter_values, query.query_text, query.id)


class QueryTagsResource(BaseResource):
    def get(self):
        """
        Returns all query tags including those for drafts.
        """
        tags = models.Query.all_tags(self.current_user, include_drafts=True)
        return {
            'tags': [
                {
                    'name': name,
                    'count': count,
                }
                for name, count in tags
            ]
        }
<EOF>
<BOF>
from flask import g, redirect, render_template, request, url_for

from flask_login import login_user
from redash import settings
from redash.authentication.org_resolving import current_org
from redash.handlers.base import routes
from redash.models import Group, Organization, User, db
from redash.tasks.general import subscribe
from wtforms import BooleanField, Form, PasswordField, StringField, validators
from wtforms.fields.html5 import EmailField


class SetupForm(Form):
    name = StringField('Name', validators=[validators.InputRequired()])
    email = EmailField('Email Address', validators=[validators.Email()])
    password = PasswordField('Password', validators=[validators.Length(6)])
    org_name = StringField("Organization Name", validators=[validators.InputRequired()])
    security_notifications = BooleanField()
    newsletter = BooleanField()


def create_org(org_name, user_name, email, password):
    default_org = Organization(name=org_name, slug='default', settings={})
    admin_group = Group(name='admin', permissions=['admin', 'super_admin'], org=default_org, type=Group.BUILTIN_GROUP)
    default_group = Group(name='default', permissions=Group.DEFAULT_PERMISSIONS, org=default_org, type=Group.BUILTIN_GROUP)

    db.session.add_all([default_org, admin_group, default_group])
    db.session.commit()

    user = User(org=default_org, name=user_name, email=email, group_ids=[admin_group.id, default_group.id])
    user.hash_password(password)

    db.session.add(user)
    db.session.commit()

    return default_org, user


@routes.route('/setup', methods=['GET', 'POST'])
def setup():
    if current_org != None or settings.MULTI_ORG:
        return redirect('/')

    form = SetupForm(request.form)
    form.newsletter.data = True
    form.security_notifications.data = True

    if request.method == 'POST' and form.validate():
        default_org, user = create_org(form.org_name.data, form.name.data, form.email.data, form.password.data)

        g.org = default_org
        login_user(user)

        # signup to newsletter if needed
        if form.newsletter.data or form.security_notifications:
            subscribe.delay(form.data)

        return redirect(url_for('redash.index', org_slug=None))

    return render_template('setup.html', form=form)
<EOF>
<BOF>
import time

from inspect import isclass
from flask import Blueprint, current_app, request

from flask_login import current_user, login_required
from flask_restful import Resource, abort
from redash import settings
from redash.authentication import current_org
from redash.models import db
from redash.tasks import record_event as record_event_task
from redash.utils import json_dumps
from sqlalchemy.orm.exc import NoResultFound
from sqlalchemy import cast
from sqlalchemy.dialects import postgresql
from sqlalchemy_utils import sort_query

routes = Blueprint('redash', __name__, template_folder=settings.fix_assets_path('templates'))


class BaseResource(Resource):
    decorators = [login_required]

    def __init__(self, *args, **kwargs):
        super(BaseResource, self).__init__(*args, **kwargs)
        self._user = None

    def dispatch_request(self, *args, **kwargs):
        kwargs.pop('org_slug', None)

        return super(BaseResource, self).dispatch_request(*args, **kwargs)

    @property
    def current_user(self):
        return current_user._get_current_object()

    @property
    def current_org(self):
        return current_org._get_current_object()

    def record_event(self, options):
        record_event(self.current_org, self.current_user, options)

    # TODO: this should probably be somewhere else
    def update_model(self, model, updates):
        for k, v in updates.items():
            setattr(model, k, v)


def record_event(org, user, options):
    if user.is_api_user():
        options.update({
            'api_key': user.name,
            'org_id': org.id
        })
    else:
        options.update({
            'user_id': user.id,
            'user_name': user.name,
            'org_id': org.id
        })

    options.update({
        'user_agent': request.user_agent.string,
        'ip': request.remote_addr
    })

    if 'timestamp' not in options:
        options['timestamp'] = int(time.time())

    record_event_task.delay(options)


def require_fields(req, fields):
    for f in fields:
        if f not in req:
            abort(400)


def get_object_or_404(fn, *args, **kwargs):
    try:
        rv = fn(*args, **kwargs)
        if rv is None:
            abort(404)
    except NoResultFound:
        abort(404)
    return rv


def paginate(query_set, page, page_size, serializer, **kwargs):
    count = query_set.count()

    if page < 1:
        abort(400, message='Page must be positive integer.')

    if (page - 1) * page_size + 1 > count > 0:
        abort(400, message='Page is out of range.')

    if page_size > 250 or page_size < 1:
        abort(400, message='Page size is out of range (1-250).')

    results = query_set.paginate(page, page_size)

    # support for old function based serializers
    if isclass(serializer):
        items = serializer(results.items, **kwargs).serialize()
    else:
        items = [serializer(result) for result in results.items]

    return {
        'count': count,
        'page': page,
        'page_size': page_size,
        'results': items,
    }


def org_scoped_rule(rule):
    if settings.MULTI_ORG:
        return "/<org_slug:org_slug>{}".format(rule)

    return rule


def json_response(response):
    return current_app.response_class(json_dumps(response), mimetype='application/json')


def filter_by_tags(result_set, column):
    if request.args.getlist('tags'):
        tags = request.args.getlist('tags')
        result_set = result_set.filter(cast(column, postgresql.ARRAY(db.Text)).contains(tags))
    return result_set


def order_results(results, default_order, allowed_orders, fallback=True):
    """
    Orders the given results with the sort order as requested in the
    "order" request query parameter or the given default order.
    """
    # See if a particular order has been requested
    requested_order = request.args.get('order', '').strip()

    # and if not (and no fallback is wanted) return results as is
    if not requested_order and not fallback:
        return results

    # and if it matches a long-form for related fields, falling
    # back to the default order
    selected_order = allowed_orders.get(requested_order, None)
    if selected_order is None and fallback:
        selected_order = default_order
    # The query may already have an ORDER BY statement attached
    # so we clear it here and apply the selected order
    return sort_query(results.order_by(None), selected_order)
<EOF>
<BOF>
from flask_login import current_user, login_required

from redash import models
from redash.handlers import routes
from redash.handlers.base import json_response, org_scoped_rule
from redash.authentication import current_org


@routes.route(org_scoped_rule('/api/organization/status'), methods=['GET'])
@login_required
def organization_status(org_slug=None):
    counters = {
        'users': models.User.all_not_disabled(current_org).count(),
        'alerts': models.Alert.all(group_ids=current_user.group_ids).count(),
        'data_sources': models.DataSource.all(current_org, group_ids=current_user.group_ids).count(),
        'queries': models.Query.all_queries(current_user.group_ids, current_user.id, drafts=True).count(),
        'dashboards': models.Dashboard.query.filter(models.Dashboard.org==current_org, models.Dashboard.is_archived==False).count(),
    }

    return json_response(dict(object_counters=counters))
<EOF>
<BOF>
import os

from flask import current_app, render_template, safe_join, send_file
from werkzeug.exceptions import NotFound

from flask_login import login_required
from redash import settings
from redash.handlers import routes
from redash.handlers.authentication import base_href
from redash.handlers.base import org_scoped_rule


def render_index():
    if settings.MULTI_ORG:
        response = render_template("multi_org.html", base_href=base_href())
    else:
        full_path = safe_join(settings.STATIC_ASSETS_PATH, 'index.html')
        response = send_file(full_path, **dict(cache_timeout=0, conditional=True))

    return response


@routes.route(org_scoped_rule('/<path:path>'))
@routes.route(org_scoped_rule('/'))
@login_required
def index(**kwargs):
    return render_index()
<EOF>
<BOF>
import logging

from flask import abort, flash, redirect, render_template, request, url_for

from flask_login import current_user, login_required, login_user, logout_user
from redash import __version__, limiter, models, settings
from redash.authentication import current_org, get_login_url, get_next_path
from redash.authentication.account import (BadSignature, SignatureExpired,
                                           send_password_reset_email,
                                           validate_token)
from redash.handlers import routes
from redash.handlers.base import json_response, org_scoped_rule
from redash.version_check import get_latest_version
from sqlalchemy.orm.exc import NoResultFound

logger = logging.getLogger(__name__)


def get_google_auth_url(next_path):
    if settings.MULTI_ORG:
        google_auth_url = url_for('google_oauth.authorize_org', next=next_path, org_slug=current_org.slug)
    else:
        google_auth_url = url_for('google_oauth.authorize', next=next_path)
    return google_auth_url


def render_token_login_page(template, org_slug, token):
    try:
        user_id = validate_token(token)
        org = current_org._get_current_object()
        user = models.User.get_by_id_and_org(user_id, org)
    except NoResultFound:
        logger.exception("Bad user id in token. Token= , User id= %s, Org=%s", user_id, token, org_slug)
        return render_template("error.html", error_message="Invalid invite link. Please ask for a new one."), 400
    except (SignatureExpired, BadSignature):
        logger.exception("Failed to verify invite token: %s, org=%s", token, org_slug)
        return render_template("error.html",
                               error_message="Your invite link has expired. Please ask for a new one."), 400
    status_code = 200
    if request.method == 'POST':
        if 'password' not in request.form:
            flash('Bad Request')
            status_code = 400
        elif not request.form['password']:
            flash('Cannot use empty password.')
            status_code = 400
        elif len(request.form['password']) < 6:
            flash('Password length is too short (<6).')
            status_code = 400
        else:
            # TODO: set active flag
            user.hash_password(request.form['password'])
            models.db.session.add(user)
            login_user(user)
            models.db.session.commit()
            return redirect(url_for('redash.index', org_slug=org_slug))

    google_auth_url = get_google_auth_url(url_for('redash.index', org_slug=org_slug))

    return render_template(template,
                           show_google_openid=settings.GOOGLE_OAUTH_ENABLED,
                           google_auth_url=google_auth_url,
                           show_saml_login=current_org.get_setting('auth_saml_enabled'),
                           show_remote_user_login=settings.REMOTE_USER_LOGIN_ENABLED,
                           show_ldap_login=settings.LDAP_LOGIN_ENABLED,
                           org_slug=org_slug,
                           user=user), status_code


@routes.route(org_scoped_rule('/invite/<token>'), methods=['GET', 'POST'])
def invite(token, org_slug=None):
    return render_token_login_page("invite.html", org_slug, token)


@routes.route(org_scoped_rule('/reset/<token>'), methods=['GET', 'POST'])
def reset(token, org_slug=None):
    return render_token_login_page("reset.html", org_slug, token)


@routes.route(org_scoped_rule('/forgot'), methods=['GET', 'POST'])
def forgot_password(org_slug=None):
    if not current_org.get_setting('auth_password_login_enabled'):
        abort(404)

    submitted = False
    if request.method == 'POST' and request.form['email']:
        submitted = True
        email = request.form['email']
        try:
            org = current_org._get_current_object()
            user = models.User.get_by_email_and_org(email, org)
            send_password_reset_email(user)
        except NoResultFound:
            logging.error("No user found for forgot password: %s", email)

    return render_template("forgot.html", submitted=submitted)


@routes.route(org_scoped_rule('/login'), methods=['GET', 'POST'])
@limiter.limit(settings.THROTTLE_LOGIN_PATTERN)
def login(org_slug=None):
    # We intentionally use == as otherwise it won't actually use the proxy. So weird :O
    # noinspection PyComparisonWithNone
    if current_org == None and not settings.MULTI_ORG:
        return redirect('/setup')
    elif current_org == None:
        return redirect('/')

    index_url = url_for('redash.index', org_slug=org_slug)
    unsafe_next_path = request.args.get('next', index_url)
    next_path = get_next_path(unsafe_next_path)
    if current_user.is_authenticated:
        return redirect(next_path)

    if request.method == 'POST':
        try:
            org = current_org._get_current_object()
            user = models.User.get_by_email_and_org(request.form['email'], org)
            if user and not user.is_disabled and user.verify_password(request.form['password']):
                remember = ('remember' in request.form)
                login_user(user, remember=remember)
                return redirect(next_path)
            else:
                flash("Wrong email or password.")
        except NoResultFound:
            flash("Wrong email or password.")

    google_auth_url = get_google_auth_url(next_path)

    return render_template("login.html",
                           org_slug=org_slug,
                           next=next_path,
                           email=request.form.get('email', ''),
                           show_google_openid=settings.GOOGLE_OAUTH_ENABLED,
                           google_auth_url=google_auth_url,
                           show_password_login=current_org.get_setting('auth_password_login_enabled'),
                           show_saml_login=current_org.get_setting('auth_saml_enabled'),
                           show_remote_user_login=settings.REMOTE_USER_LOGIN_ENABLED,
                           show_ldap_login=settings.LDAP_LOGIN_ENABLED)


@routes.route(org_scoped_rule('/logout'))
def logout(org_slug=None):
    logout_user()
    return redirect(get_login_url(next=None))


def base_href():
    if settings.MULTI_ORG:
        base_href = url_for('redash.index', _external=True, org_slug=current_org.slug)
    else:
        base_href = url_for('redash.index', _external=True)

    return base_href


def date_format_config():
    date_format = current_org.get_setting('date_format')
    date_format_list = set(["DD/MM/YY", "MM/DD/YY", "YYYY-MM-DD", settings.DATE_FORMAT])
    return {
        'dateFormat': date_format,
        'dateFormatList': list(date_format_list),
        'dateTimeFormat': "{0} HH:mm".format(date_format),
    }


def client_config():
    if not current_user.is_api_user() and current_user.is_authenticated:
        client_config = {
            'newVersionAvailable': bool(get_latest_version()),
            'version': __version__
        }
    else:
        client_config = {}

    defaults = {
        'allowScriptsInUserInput': settings.ALLOW_SCRIPTS_IN_USER_INPUT,
        'showPermissionsControl': current_org.get_setting("feature_show_permissions_control"),
        'allowCustomJSVisualizations': settings.FEATURE_ALLOW_CUSTOM_JS_VISUALIZATIONS,
        'autoPublishNamedQueries': settings.FEATURE_AUTO_PUBLISH_NAMED_QUERIES,
        'mailSettingsMissing': settings.MAIL_DEFAULT_SENDER is None,
        'dashboardRefreshIntervals': settings.DASHBOARD_REFRESH_INTERVALS,
        'queryRefreshIntervals': settings.QUERY_REFRESH_INTERVALS,
        'googleLoginEnabled': settings.GOOGLE_OAUTH_ENABLED,
        'pageSize': settings.PAGE_SIZE,
        'pageSizeOptions': settings.PAGE_SIZE_OPTIONS,
    }

    client_config.update(defaults)
    client_config.update({
        'basePath': base_href()
    })
    client_config.update(date_format_config())

    return client_config


@routes.route('/api/config', methods=['GET'])
def config(org_slug=None):
    return json_response({
        'org_slug': current_org.slug,
        'client_config': client_config()
    })


@routes.route(org_scoped_rule('/api/session'), methods=['GET'])
@login_required
def session(org_slug=None):
    if current_user.is_api_user():
        user = {
            'permissions': [],
            'apiKey': current_user.id
        }
    else:
        user = {
            'profile_image_url': current_user.profile_image_url,
            'id': current_user.id,
            'name': current_user.name,
            'email': current_user.email,
            'groups': current_user.group_ids,
            'permissions': current_user.permissions
        }

    return json_response({
        'user': user,
        'org_slug': current_org.slug,
        'client_config': client_config()
    })
<EOF>
<BOF>
from collections import defaultdict

from redash.handlers.base import BaseResource, get_object_or_404
from redash.models import AccessPermission, Query, Dashboard, User, db
from redash.permissions import require_admin_or_owner, ACCESS_TYPES
from flask import request
from flask_restful import abort
from sqlalchemy.orm.exc import NoResultFound


model_to_types = {
    'queries': Query,
    'dashboards': Dashboard
}


def get_model_from_type(type):
    model = model_to_types.get(type)
    if model is None:
        abort(404)
    return model


class ObjectPermissionsListResource(BaseResource):
    def get(self, object_type, object_id):
        model = get_model_from_type(object_type)
        obj = get_object_or_404(model.get_by_id_and_org, object_id, self.current_org)

        # TODO: include grantees in search to avoid N+1 queries
        permissions = AccessPermission.find(obj)

        result = defaultdict(list)

        for perm in permissions:
            result[perm.access_type].append(perm.grantee.to_dict())

        return result

    def post(self, object_type, object_id):
        model = get_model_from_type(object_type)
        obj = get_object_or_404(model.get_by_id_and_org, object_id, self.current_org)

        require_admin_or_owner(obj.user_id)

        req = request.get_json(True)

        access_type = req['access_type']

        if access_type not in ACCESS_TYPES:
            abort(400, message='Unknown access type.')

        try:
            grantee = User.get_by_id_and_org(req['user_id'], self.current_org)
        except NoResultFound:
            abort(400, message='User not found.')

        permission = AccessPermission.grant(obj, access_type, grantee, self.current_user)
        db.session.commit()

        self.record_event({
            'action': 'grant_permission',
            'object_id': object_id,
            'object_type': object_type,
            'grantee': grantee.id,
            'access_type': access_type,
        })

        return permission.to_dict()

    def delete(self, object_type, object_id):
        model = get_model_from_type(object_type)
        obj = get_object_or_404(model.get_by_id_and_org, object_id,
                                self.current_org)

        require_admin_or_owner(obj.user_id)

        req = request.get_json(True)
        grantee_id = req['user_id']
        access_type = req['access_type']

        grantee = User.query.get(req['user_id'])
        if grantee is None:
            abort(400, message='User not found.')

        AccessPermission.revoke(obj, grantee, access_type)
        db.session.commit()

        self.record_event({
            'action': 'revoke_permission',
            'object_id': object_id,
            'object_type': object_type,
            'access_type': access_type,
            'grantee_id': grantee_id
        })


class CheckPermissionResource(BaseResource):
    def get(self, object_type, object_id, access_type):
        model = get_model_from_type(object_type)
        obj = get_object_or_404(model.get_by_id_and_org, object_id,
                                self.current_org)

        has_access = AccessPermission.exists(obj, access_type,
                                             self.current_user)

        return {'response': has_access}
<EOF>
<BOF>
import logging
import time

from flask import make_response, request
from flask_login import current_user
from flask_restful import abort
from redash import models, settings
from redash.tasks import QueryTask, record_event
from redash.permissions import require_permission, not_view_only, has_access, require_access, view_only
from redash.handlers.base import BaseResource, get_object_or_404
from redash.utils import (collect_query_parameters,
                          collect_parameters_from_request,
                          gen_query_hash,
                          json_dumps,
                          utcnow,
                          mustache_render)
from redash.tasks.queries import enqueue_query


def error_response(message):
    return {'job': {'status': 4, 'error': message}}, 400


#
# Run a parameterized query synchronously and return the result
# DISCLAIMER: Temporary solution to support parameters in queries. Should be
#             removed once we refactor the query results API endpoints and handling
#             on the client side. Please don't reuse in other API handlers.
#
def run_query_sync(data_source, parameter_values, query_text, max_age=0):
    query_parameters = set(collect_query_parameters(query_text))
    missing_params = set(query_parameters) - set(parameter_values.keys())
    if missing_params:
        raise Exception('Missing parameter value for: {}'.format(", ".join(missing_params)))

    if query_parameters:
        query_text = mustache_render(query_text, parameter_values)

    if max_age <= 0:
        query_result = None
    else:
        query_result = models.QueryResult.get_latest(data_source, query_text, max_age)

    query_hash = gen_query_hash(query_text)

    if query_result:
        logging.info("Returning cached result for query %s" % query_hash)
        return query_result

    try:
        started_at = time.time()
        data, error = data_source.query_runner.run_query(query_text, current_user)

        if error:
            logging.info('got bak error')
            logging.info(error)
            return None

        run_time = time.time() - started_at
        query_result, updated_query_ids = models.QueryResult.store_result(data_source.org_id, data_source,
                                                                              query_hash, query_text, data,
                                                                              run_time, utcnow())

        models.db.session.commit()
        return query_result
    except Exception as e:
        if max_age > 0:
            abort(404, message="Unable to get result from the database, and no cached query result found.")
        else:
            abort(503, message="Unable to get result from the database.")
        return None

def run_query(data_source, parameter_values, query_text, query_id, max_age=0):
    query_parameters = set(collect_query_parameters(query_text))
    missing_params = set(query_parameters) - set(parameter_values.keys())
    if missing_params:
        return error_response('Missing parameter value for: {}'.format(", ".join(missing_params)))

    if data_source.paused:
        if data_source.pause_reason:
            message = '{} is paused ({}). Please try later.'.format(data_source.name, data_source.pause_reason)
        else:
            message = '{} is paused. Please try later.'.format(data_source.name)

        return error_response(message)

    if query_parameters:
        query_text = mustache_render(query_text, parameter_values)

    if max_age == 0:
        query_result = None
    else:
        query_result = models.QueryResult.get_latest(data_source, query_text, max_age)

    if query_result:
        return {'query_result': query_result.to_dict()}
    else:
        job = enqueue_query(query_text, data_source, current_user.id, metadata={"Username": current_user.email, "Query ID": query_id})
        return {'job': job.to_dict()}


class QueryResultListResource(BaseResource):
    @require_permission('execute_query')
    def post(self):
        """
        Execute a query (or retrieve recent results).

        :qparam string query: The query text to execute
        :qparam number query_id: The query object to update with the result (optional)
        :qparam number max_age: If query results less than `max_age` seconds old are available,
                                return them, otherwise execute the query; if omitted or -1, returns
                                any cached result, or executes if not available. Set to zero to
                                always execute.
        :qparam number data_source_id: ID of data source to query
        """
        params = request.get_json(force=True)
        parameter_values = collect_parameters_from_request(request.args)

        query = params['query']
        max_age = int(params.get('max_age', -1))
        query_id = params.get('query_id', 'adhoc')

        data_source = models.DataSource.get_by_id_and_org(params.get('data_source_id'), self.current_org)

        if not has_access(data_source.groups, self.current_user, not_view_only):
            return {'job': {'status': 4, 'error': 'You do not have permission to run queries with this data source.'}}, 403

        self.record_event({
            'action': 'execute_query',
            'object_id': data_source.id,
            'object_type': 'data_source',
            'query': query
        })
        return run_query(data_source, parameter_values, query, query_id, max_age)


ONE_YEAR = 60 * 60 * 24 * 365.25


class QueryResultResource(BaseResource):
    @staticmethod
    def add_cors_headers(headers):
        if 'Origin' in request.headers:
            origin = request.headers['Origin']

            if set(['*', origin]) & settings.ACCESS_CONTROL_ALLOW_ORIGIN:
                headers['Access-Control-Allow-Origin'] = origin
                headers['Access-Control-Allow-Credentials'] = str(settings.ACCESS_CONTROL_ALLOW_CREDENTIALS).lower()

    @require_permission('view_query')
    def options(self, query_id=None, query_result_id=None, filetype='json'):
        headers = {}
        self.add_cors_headers(headers)

        if settings.ACCESS_CONTROL_REQUEST_METHOD:
            headers['Access-Control-Request-Method'] = settings.ACCESS_CONTROL_REQUEST_METHOD

        if settings.ACCESS_CONTROL_ALLOW_HEADERS:
            headers['Access-Control-Allow-Headers'] = settings.ACCESS_CONTROL_ALLOW_HEADERS

        return make_response("", 200, headers)

    @require_permission('view_query')
    def get(self, query_id=None, query_result_id=None, filetype='json'):
        """
        Retrieve query results.

        :param number query_id: The ID of the query whose results should be fetched
        :param number query_result_id: the ID of the query result to fetch
        :param string filetype: Format to return. One of 'json', 'xlsx', or 'csv'. Defaults to 'json'.

        :<json number id: Query result ID
        :<json string query: Query that produced this result
        :<json string query_hash: Hash code for query text
        :<json object data: Query output
        :<json number data_source_id: ID of data source that produced this result
        :<json number runtime: Length of execution time in seconds
        :<json string retrieved_at: Query retrieval date/time, in ISO format
        """
        # TODO:
        # This method handles two cases: retrieving result by id & retrieving result by query id.
        # They need to be split, as they have different logic (for example, retrieving by query id
        # should check for query parameters and shouldn't cache the result).
        should_cache = query_result_id is not None

        parameter_values = collect_parameters_from_request(request.args)
        max_age = int(request.args.get('maxAge', 0))

        query_result = None

        if query_result_id:
            query_result = get_object_or_404(models.QueryResult.get_by_id_and_org, query_result_id, self.current_org)

        if query_id is not None:
            query = get_object_or_404(models.Query.get_by_id_and_org, query_id, self.current_org)

            if query_result is None and query is not None:
                if settings.ALLOW_PARAMETERS_IN_EMBEDS and parameter_values:
                    query_result = run_query_sync(query.data_source, parameter_values, query.query_text, max_age=max_age)
                elif query.latest_query_data_id is not None:
                    query_result = get_object_or_404(models.QueryResult.get_by_id_and_org, query.latest_query_data_id, self.current_org)

            if query is not None and query_result is not None and self.current_user.is_api_user():
                if query.query_hash != query_result.query_hash:
                    abort(404, message='No cached result found for this query.')

        if query_result:
            require_access(query_result.data_source.groups, self.current_user, view_only)

            if isinstance(self.current_user, models.ApiUser):
                event = {
                    'user_id': None,
                    'org_id': self.current_org.id,
                    'action': 'api_get',
                    'api_key': self.current_user.name,
                    'file_type': filetype,
                    'user_agent': request.user_agent.string,
                    'ip': request.remote_addr
                }

                if query_id:
                    event['object_type'] = 'query'
                    event['object_id'] = query_id
                else:
                    event['object_type'] = 'query_result'
                    event['object_id'] = query_result_id

                record_event.delay(event)

            if filetype == 'json':
                response = self.make_json_response(query_result)
            elif filetype == 'xlsx':
                response = self.make_excel_response(query_result)
            else:
                response = self.make_csv_response(query_result)

            if len(settings.ACCESS_CONTROL_ALLOW_ORIGIN) > 0:
                self.add_cors_headers(response.headers)

            if should_cache:
                response.headers.add_header('Cache-Control', 'private,max-age=%d' % ONE_YEAR)

            return response

        else:
            abort(404, message='No cached result found for this query.')

    def make_json_response(self, query_result):
        data = json_dumps({'query_result': query_result.to_dict()})
        headers = {'Content-Type': "application/json"}
        return make_response(data, 200, headers)

    @staticmethod
    def make_csv_response(query_result):
        headers = {'Content-Type': "text/csv; charset=UTF-8"}
        return make_response(query_result.make_csv_content(), 200, headers)

    @staticmethod
    def make_excel_response(query_result):
        headers = {'Content-Type': "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"}
        return make_response(query_result.make_excel_content(), 200, headers)


class JobResource(BaseResource):
    def get(self, job_id):
        """
        Retrieve info about a running query job.
        """
        job = QueryTask(job_id=job_id)
        return {'job': job.to_dict()}

    def delete(self, job_id):
        """
        Cancel a query job in progress.
        """
        job = QueryTask(job_id=job_id)
        job.cancel()
<EOF>
<BOF>
from flask import request

from redash.models import db, Organization
from redash.handlers.base import BaseResource, record_event
from redash.permissions import require_admin
from redash.settings.organization import settings as org_settings


def get_settings_with_defaults(defaults, org):
    values = org.settings.get('settings', {})
    settings = {}

    for setting, default_value in defaults.iteritems():
        current_value = values.get(setting)
        if current_value is None and default_value is None:
            continue

        if current_value is None:
            settings[setting] = default_value
        else:
            settings[setting] = current_value

    settings['auth_google_apps_domains'] = org.google_apps_domains

    return settings


class OrganizationSettings(BaseResource):
    @require_admin
    def get(self):
        settings = get_settings_with_defaults(org_settings, self.current_org)

        return {
            "settings": settings
        }

    @require_admin
    def post(self):
        new_values = request.json

        if self.current_org.settings.get('settings') is None:
            self.current_org.settings['settings'] = {}

        previous_values = {}
        for k, v in new_values.iteritems():
            if k == 'auth_google_apps_domains':
                previous_values[k] = self.current_org.google_apps_domains
                self.current_org.settings[Organization.SETTING_GOOGLE_APPS_DOMAINS] = v
            else:
                previous_values[k] = self.current_org.get_setting(k, raise_on_missing=False)
                self.current_org.set_setting(k, v)

        db.session.add(self.current_org)
        db.session.commit()

        self.record_event({
            'action': 'edit',
            'object_id': self.current_org.id,
            'object_type': 'settings',
            'new_values': new_values,
            'previous_values': previous_values
        })

        settings = get_settings_with_defaults(org_settings, self.current_org)

        return {
            "settings": settings
        }
<EOF>
<BOF>
import time
from flask import request
from flask_restful import abort
from redash import models
from redash.permissions import require_admin, require_permission
from redash.handlers.base import BaseResource, get_object_or_404


class GroupListResource(BaseResource):
    @require_admin
    def post(self):
        name = request.json['name']
        group = models.Group(name=name, org=self.current_org)
        models.db.session.add(group)
        models.db.session.commit()

        self.record_event({
            'action': 'create',
            'object_id': group.id,
            'object_type': 'group'
        })

        return group.to_dict()

    def get(self):
        if self.current_user.has_permission('admin'):
            groups = models.Group.all(self.current_org)
        else:
            groups = models.Group.query.filter(
                models.Group.id.in_(self.current_user.group_ids))

        self.record_event({
            'action': 'list',
            'object_id': 'groups',
            'object_type': 'group',
        })

        return [g.to_dict() for g in groups]


class GroupResource(BaseResource):
    @require_admin
    def post(self, group_id):
        group = models.Group.get_by_id_and_org(group_id, self.current_org)

        if group.type == models.Group.BUILTIN_GROUP:
            abort(400, message="Can't modify built-in groups.")

        group.name = request.json['name']
        models.db.session.commit()

        self.record_event({
            'action': 'edit',
            'object_id': group.id,
            'object_type': 'group'
        })

        return group.to_dict()

    def get(self, group_id):
        if not (self.current_user.has_permission('admin') or int(group_id) in self.current_user.group_ids):
            abort(403)

        group = models.Group.get_by_id_and_org(group_id, self.current_org)

        self.record_event({
            'action': 'view',
            'object_id': group_id,
            'object_type': 'group',
        })

        return group.to_dict()

    @require_admin
    def delete(self, group_id):
        group = models.Group.get_by_id_and_org(group_id, self.current_org)
        if group.type == models.Group.BUILTIN_GROUP:
            abort(400, message="Can't delete built-in groups.")

        members = models.Group.members(group_id)
        for member in members:
            member.group_ids.remove(int(group_id))
            models.db.session.add(member)

        models.db.session.delete(group)
        models.db.session.commit()


class GroupMemberListResource(BaseResource):
    @require_admin
    def post(self, group_id):
        user_id = request.json['user_id']
        user = models.User.get_by_id_and_org(user_id, self.current_org)
        group = models.Group.get_by_id_and_org(group_id, self.current_org)
        user.group_ids.append(group.id)
        models.db.session.commit()

        self.record_event({
            'action': 'add_member',
            'object_id': group.id,
            'object_type': 'group',
            'member_id': user.id
        })
        return user.to_dict()

    @require_permission('list_users')
    def get(self, group_id):
        if not (self.current_user.has_permission('admin') or int(group_id) in self.current_user.group_ids):
            abort(403)

        members = models.Group.members(group_id)
        return [m.to_dict() for m in members]


class GroupMemberResource(BaseResource):
    @require_admin
    def delete(self, group_id, user_id):
        user = models.User.get_by_id_and_org(user_id, self.current_org)
        user.group_ids.remove(int(group_id))
        models.db.session.commit()

        self.record_event({
            'action': 'remove_member',
            'object_id': group_id,
            'object_type': 'group',
            'member_id': user.id
        })


def serialize_data_source_with_group(data_source, data_source_group):
    d = data_source.to_dict()
    d['view_only'] = data_source_group.view_only
    return d


class GroupDataSourceListResource(BaseResource):
    @require_admin
    def post(self, group_id):
        data_source_id = request.json['data_source_id']
        data_source = models.DataSource.get_by_id_and_org(data_source_id, self.current_org)
        group = models.Group.get_by_id_and_org(group_id, self.current_org)

        data_source_group = data_source.add_group(group)
        models.db.session.commit()

        self.record_event({
            'action': 'add_data_source',
            'object_id': group_id,
            'object_type': 'group',
            'member_id': data_source.id
        })

        return serialize_data_source_with_group(data_source, data_source_group)

    @require_admin
    def get(self, group_id):
        group = get_object_or_404(models.Group.get_by_id_and_org, group_id,
                                  self.current_org)

        # TOOD: move to models
        data_sources = (models.DataSource.query
                        .join(models.DataSourceGroup)
                        .filter(models.DataSourceGroup.group == group))

        self.record_event({
            'action': 'list',
            'object_id': group_id,
            'object_type': 'group',
        })

        return [ds.to_dict(with_permissions_for=group) for ds in data_sources]


class GroupDataSourceResource(BaseResource):
    @require_admin
    def post(self, group_id, data_source_id):
        data_source = models.DataSource.get_by_id_and_org(data_source_id, self.current_org)
        group = models.Group.get_by_id_and_org(group_id, self.current_org)
        view_only = request.json['view_only']

        data_source_group = data_source.update_group_permission(group, view_only)
        models.db.session.commit()

        self.record_event({
            'action': 'change_data_source_permission',
            'object_id': group_id,
            'object_type': 'group',
            'member_id': data_source.id,
            'view_only': view_only
        })

        return serialize_data_source_with_group(data_source, data_source_group)

    @require_admin
    def delete(self, group_id, data_source_id):
        data_source = models.DataSource.get_by_id_and_org(data_source_id, self.current_org)
        group = models.Group.get_by_id_and_org(group_id, self.current_org)

        data_source.remove_group(group)
        models.db.session.commit()

        self.record_event({
            'action': 'remove_data_source',
            'object_id': group_id,
            'object_type': 'group',
            'member_id': data_source.id
        })
<EOF>
<BOF>
import time

from flask import request
from funcy import project

from redash import models
from redash.serializers import serialize_alert
from redash.handlers.base import (BaseResource, get_object_or_404,
                                  require_fields)
from redash.permissions import (require_access, require_admin_or_owner,
                                require_permission, view_only)


class AlertResource(BaseResource):
    def get(self, alert_id):
        alert = get_object_or_404(models.Alert.get_by_id_and_org, alert_id, self.current_org)
        require_access(alert.groups, self.current_user, view_only)
        self.record_event({
            'action': 'view',
            'object_id': alert.id,
            'object_type': 'alert'
        })
        return serialize_alert(alert)

    def post(self, alert_id):
        req = request.get_json(True)
        params = project(req, ('options', 'name', 'query_id', 'rearm'))
        alert = get_object_or_404(models.Alert.get_by_id_and_org, alert_id, self.current_org)
        require_admin_or_owner(alert.user.id)

        self.update_model(alert, params)
        models.db.session.commit()

        self.record_event({
            'action': 'edit',
            'object_id': alert.id,
            'object_type': 'alert'
        })

        return serialize_alert(alert)

    def delete(self, alert_id):
        alert = get_object_or_404(models.Alert.get_by_id_and_org, alert_id, self.current_org)
        require_admin_or_owner(alert.user_id)
        models.db.session.delete(alert)
        models.db.session.commit()


class AlertListResource(BaseResource):
    def post(self):
        req = request.get_json(True)
        require_fields(req, ('options', 'name', 'query_id'))

        query = models.Query.get_by_id_and_org(req['query_id'],
                                               self.current_org)
        require_access(query.groups, self.current_user, view_only)

        alert = models.Alert(
            name=req['name'],
            query_rel=query,
            user=self.current_user,
            rearm=req.get('rearm'),
            options=req['options']
        )

        models.db.session.add(alert)
        models.db.session.flush()
        models.db.session.commit()

        self.record_event({
            'action': 'create',
            'object_id': alert.id,
            'object_type': 'alert'
        })

        return serialize_alert(alert)

    @require_permission('list_alerts')
    def get(self):
        self.record_event({
            'action': 'list',
            'object_type': 'alert'
        })
        return [serialize_alert(alert) for alert in models.Alert.all(group_ids=self.current_user.group_ids)]


class AlertSubscriptionListResource(BaseResource):
    def post(self, alert_id):
        req = request.get_json(True)

        alert = models.Alert.get_by_id_and_org(alert_id, self.current_org)
        require_access(alert.groups, self.current_user, view_only)
        kwargs = {'alert': alert, 'user': self.current_user}

        if 'destination_id' in req:
            destination = models.NotificationDestination.get_by_id_and_org(req['destination_id'], self.current_org)
            kwargs['destination'] = destination

        subscription = models.AlertSubscription(**kwargs)
        models.db.session.add(subscription)
        models.db.session.commit()

        self.record_event({
            'action': 'subscribe',
            'object_id': alert_id,
            'object_type': 'alert',
            'destination': req.get('destination_id')
        })

        d = subscription.to_dict()
        return d

    def get(self, alert_id):
        alert_id = int(alert_id)
        alert = models.Alert.get_by_id_and_org(alert_id, self.current_org)
        require_access(alert.groups, self.current_user, view_only)

        subscriptions = models.AlertSubscription.all(alert_id)
        return [s.to_dict() for s in subscriptions]


class AlertSubscriptionResource(BaseResource):
    def delete(self, alert_id, subscriber_id):
        subscription = models.AlertSubscription.query.get_or_404(subscriber_id)
        require_admin_or_owner(subscription.user.id)
        models.db.session.delete(subscription)
        models.db.session.commit()

        self.record_event({
            'action': 'unsubscribe',
            'object_id': alert_id,
            'object_type': 'alert'
        })
<EOF>
<BOF>
import logging
import requests
from flask import redirect, url_for, Blueprint, flash, request, session
from flask_oauthlib.client import OAuth

from redash import models, settings
from redash.authentication import create_and_login_user, logout_and_redirect_to_index, get_next_path
from redash.authentication.org_resolving import current_org

logger = logging.getLogger('google_oauth')

oauth = OAuth()
blueprint = Blueprint('google_oauth', __name__)


def google_remote_app():
    if 'google' not in oauth.remote_apps:
        oauth.remote_app('google',
                         base_url='https://www.google.com/accounts/',
                         authorize_url='https://accounts.google.com/o/oauth2/auth',
                         request_token_url=None,
                         request_token_params={
                             'scope': 'https://www.googleapis.com/auth/userinfo.email https://www.googleapis.com/auth/userinfo.profile',
                         },
                         access_token_url='https://accounts.google.com/o/oauth2/token',
                         access_token_method='POST',
                         consumer_key=settings.GOOGLE_CLIENT_ID,
                         consumer_secret=settings.GOOGLE_CLIENT_SECRET)

    return oauth.google


def get_user_profile(access_token):
    headers = {'Authorization': 'OAuth {}'.format(access_token)}
    response = requests.get('https://www.googleapis.com/oauth2/v1/userinfo', headers=headers)

    if response.status_code == 401:
        logger.warning("Failed getting user profile (response code 401).")
        return None

    return response.json()


def verify_profile(org, profile):
    if org.is_public:
        return True

    email = profile['email']
    domain = email.split('@')[-1]

    if domain in org.google_apps_domains:
        return True

    if org.has_user(email) == 1:
        return True

    return False


@blueprint.route('/<org_slug>/oauth/google', endpoint="authorize_org")
def org_login(org_slug):
    session['org_slug'] = current_org.slug
    return redirect(url_for(".authorize", next=request.args.get('next', None)))


@blueprint.route('/oauth/google', endpoint="authorize")
def login():
    callback = url_for('.callback', _external=True)
    next_path = request.args.get('next', url_for("redash.index", org_slug=session.get('org_slug')))
    logger.debug("Callback url: %s", callback)
    logger.debug("Next is: %s", next_path)
    return google_remote_app().authorize(callback=callback, state=next_path)


@blueprint.route('/oauth/google_callback', endpoint="callback")
def authorized():
    resp = google_remote_app().authorized_response()
    access_token = resp['access_token']

    if access_token is None:
        logger.warning("Access token missing in call back request.")
        flash("Validation error. Please retry.")
        return redirect(url_for('redash.login'))

    profile = get_user_profile(access_token)
    if profile is None:
        flash("Validation error. Please retry.")
        return redirect(url_for('redash.login'))

    if 'org_slug' in session:
        org = models.Organization.get_by_slug(session.pop('org_slug'))
    else:
        org = current_org

    if not verify_profile(org, profile):
        logger.warning("User tried to login with unauthorized domain name: %s (org: %s)", profile['email'], org)
        flash("Your Google Apps account ({}) isn't allowed.".format(profile['email']))
        return redirect(url_for('redash.login', org_slug=org.slug))

    picture_url = "%s?sz=40" % profile['picture']
    user = create_and_login_user(org, profile['name'], profile['email'], picture_url)
    if user is None:
        return logout_and_redirect_to_index()

    unsafe_next_path = request.args.get('state') or url_for("redash.index", org_slug=org.slug)
    next_path = get_next_path(unsafe_next_path)

    return redirect(next_path)
<EOF>
<BOF>
import logging
logger = logging.getLogger('ldap_auth')

from redash import settings

from flask import flash, redirect, render_template, request, url_for, Blueprint
from flask_login import current_user, login_required, login_user, logout_user

try:
    from ldap3 import Server, Connection, SIMPLE
except ImportError:
    if settings.LDAP_LOGIN_ENABLED:
        logger.error("The ldap3 library was not found. This is required to use LDAP authentication (see requirements.txt).")
        exit()

from redash.authentication import create_and_login_user, logout_and_redirect_to_index, get_next_path
from redash.authentication.org_resolving import current_org


blueprint = Blueprint('ldap_auth', __name__)


@blueprint.route("/ldap/login", methods=['GET', 'POST'])
def login(org_slug=None):
    index_url = url_for("redash.index", org_slug=org_slug)
    unsafe_next_path = request.args.get('next', index_url)
    next_path = get_next_path(unsafe_next_path)

    if not settings.LDAP_LOGIN_ENABLED:
        logger.error("Cannot use LDAP for login without being enabled in settings")
        return redirect(url_for('redash.index', next=next_path))

    if current_user.is_authenticated:
        return redirect(next_path)

    if request.method == 'POST':
        ldap_user = auth_ldap_user(request.form['email'], request.form['password'])

        if ldap_user is not None:
            user = create_and_login_user(
                current_org,
                ldap_user[settings.LDAP_DISPLAY_NAME_KEY][0],
                ldap_user[settings.LDAP_EMAIL_KEY][0]
            )
            if user is None:
                return logout_and_redirect_to_index()

            return redirect(next_path or url_for('redash.index'))
        else:
            flash("Incorrect credentials.")

    return render_template("login.html",
                           org_slug=org_slug,
                           next=next_path,
                           email=request.form.get('email', ''),
                           show_password_login=True,
                           username_prompt=settings.LDAP_CUSTOM_USERNAME_PROMPT,
                           hide_forgot_password=True)


def auth_ldap_user(username, password):
    server = Server(settings.LDAP_HOST_URL)
    conn = Connection(server, settings.LDAP_BIND_DN, password=settings.LDAP_BIND_DN_PASSWORD, authentication=SIMPLE, auto_bind=True)

    conn.search(settings.LDAP_SEARCH_DN, settings.LDAP_SEARCH_TEMPLATE % {"username": username}, attributes=[settings.LDAP_DISPLAY_NAME_KEY, settings.LDAP_EMAIL_KEY])

    if len(conn.entries) == 0:
        return None

    user = conn.entries[0]

    if not conn.rebind(user=user.entry_dn, password=password):
        return None

    return user
<EOF>
<BOF>
import logging
from flask import redirect, url_for, Blueprint, request
from redash.authentication import create_and_login_user, logout_and_redirect_to_index, get_next_path
from redash.authentication.org_resolving import current_org
from redash.handlers.base import org_scoped_rule
from redash import settings

logger = logging.getLogger('remote_user_auth')

blueprint = Blueprint('remote_user_auth', __name__)

@blueprint.route(org_scoped_rule("/remote_user/login"))
def login(org_slug=None):
    unsafe_next_path = request.args.get('next')
    next_path = get_next_path(unsafe_next_path)

    if not settings.REMOTE_USER_LOGIN_ENABLED:
        logger.error("Cannot use remote user for login without being enabled in settings")
        return redirect(url_for('redash.index', next=next_path, org_slug=org_slug))

    email = request.headers.get(settings.REMOTE_USER_HEADER)

    # Some Apache auth configurations will, stupidly, set (null) instead of a
    # falsey value.  Special case that here so it Just Works for more installs.
    # '(null)' should never really be a value that anyone wants to legitimately
    # use as a redash user email.
    if email == '(null)':
        email = None

    if not email:
        logger.error("Cannot use remote user for login when it's not provided in the request (looked in headers['" + settings.REMOTE_USER_HEADER + "'])")
        return redirect(url_for('redash.index', next=next_path, org_slug=org_slug))

    logger.info("Logging in " + email + " via remote user")

    user = create_and_login_user(current_org, email, email)
    if user is None:
        return logout_and_redirect_to_index()

    return redirect(next_path or url_for('redash.index', org_slug=org_slug), code=302)
<EOF>
<BOF>
from flask_login import LoginManager, user_logged_in, login_user, logout_user
from sqlalchemy.orm.exc import NoResultFound
import hashlib
import hmac
import time
import logging

from flask import redirect, request, jsonify, url_for
from urlparse import urlsplit, urlunsplit
from werkzeug.exceptions import Unauthorized

from redash import models, settings
from redash.settings.organization import settings as org_settings
from redash.authentication import jwt_auth
from redash.authentication.org_resolving import current_org
from redash.tasks import record_event

login_manager = LoginManager()
logger = logging.getLogger('authentication')


def get_login_url(external=False, next="/"):
    if settings.MULTI_ORG and current_org == None:
        login_url = '/'
    elif settings.MULTI_ORG:
        login_url = url_for('redash.login', org_slug=current_org.slug, next=next, _external=external)
    else:
        login_url = url_for('redash.login', next=next, _external=external)

    return login_url


def sign(key, path, expires):
    if not key:
        return None

    h = hmac.new(str(key), msg=path, digestmod=hashlib.sha1)
    h.update(str(expires))

    return h.hexdigest()


@login_manager.user_loader
def load_user(user_id):
    org = current_org._get_current_object()
    try:
        user = models.User.get_by_id_and_org(user_id, org)
        if user.is_disabled:
            return None
        return user
    except models.NoResultFound:
        return None


def request_loader(request):
    user = None
    if settings.AUTH_TYPE == 'hmac':
        user = hmac_load_user_from_request(request)
    elif settings.AUTH_TYPE == 'api_key':
        user = api_key_load_user_from_request(request)
    else:
        logger.warning("Unknown authentication type ({}). Using default (HMAC).".format(settings.AUTH_TYPE))
        user = hmac_load_user_from_request(request)

    if org_settings['auth_jwt_login_enabled'] and user is None:
        user = jwt_token_load_user_from_request(request)
    return user


def hmac_load_user_from_request(request):
    signature = request.args.get('signature')
    expires = float(request.args.get('expires') or 0)
    query_id = request.view_args.get('query_id', None)
    user_id = request.args.get('user_id', None)

    # TODO: 3600 should be a setting
    if signature and time.time() < expires <= time.time() + 3600:
        if user_id:
            user = models.User.query.get(user_id)
            calculated_signature = sign(user.api_key, request.path, expires)

            if user.api_key and signature == calculated_signature:
                return user

        if query_id:
            query = models.db.session.query(models.Query).filter(models.Query.id == query_id).one()
            calculated_signature = sign(query.api_key, request.path, expires)

            if query.api_key and signature == calculated_signature:
                return models.ApiUser(query.api_key, query.org, query.groups.keys(), name="ApiKey: Query {}".format(query.id))

    return None


def get_user_from_api_key(api_key, query_id):
    if not api_key:
        return None

    user = None

    # TODO: once we switch all api key storage into the ApiKey model, this code will be much simplified
    org = current_org._get_current_object()
    try:
        user = models.User.get_by_api_key_and_org(api_key, org)
    except models.NoResultFound:
        try:
            api_key = models.ApiKey.get_by_api_key(api_key)
            user = models.ApiUser(api_key, api_key.org, [])
        except models.NoResultFound:
            if query_id:
                query = models.Query.get_by_id_and_org(query_id, org)
                if query and query.api_key == api_key:
                    user = models.ApiUser(api_key, query.org, query.groups.keys(), name="ApiKey: Query {}".format(query.id))

    return user


def get_api_key_from_request(request):
    api_key = request.args.get('api_key', None)

    if api_key is None and request.headers.get('Authorization'):
        auth_header = request.headers.get('Authorization')
        api_key = auth_header.replace('Key ', '', 1)

    if api_key is None and request.view_args.get('token'):
        api_key = request.view_args['token']

    return api_key


def api_key_load_user_from_request(request):
    api_key = get_api_key_from_request(request)
    query_id = request.view_args.get('query_id', None)
    user = get_user_from_api_key(api_key, query_id)
    return user


def jwt_token_load_user_from_request(request):
    org = current_org._get_current_object()

    payload = None

    if org_settings['auth_jwt_auth_cookie_name']:
        jwt_token = request.cookies.get(org_settings['auth_jwt_auth_cookie_name'], None)
    elif org_settings['auth_jwt_auth_header_name']:
        jwt_token = request.headers.get(org_settings['auth_jwt_auth_header_name'], None)
    else:
        return None

    if jwt_token:
        payload, token_is_valid = jwt_auth.verify_jwt_token(
            jwt_token,
            expected_issuer=org_settings['auth_jwt_auth_issuer'],
            expected_audience=org_settings['auth_jwt_auth_audience'],
            algorithms=org_settings['auth_jwt_auth_algorithms'],
            public_certs_url=org_settings['auth_jwt_auth_public_certs_url'],
        )
        if not token_is_valid:
            raise Unauthorized('Invalid JWT token')

    if not payload:
        return

    try:
        user = models.User.get_by_email_and_org(payload['email'], org)
    except models.NoResultFound:
        user = create_and_login_user(current_org, payload['email'], payload['email'])

    return user


def log_user_logged_in(app, user):
    event = {
        'org_id': current_org.id,
        'user_id': user.id,
        'action': 'login',
        'object_type': 'redash',
        'timestamp': int(time.time()),
        'user_agent': request.user_agent.string,
        'ip': request.remote_addr
    }

    record_event.delay(event)


@login_manager.unauthorized_handler
def redirect_to_login():
    if request.is_xhr or '/api/' in request.path:
        response = jsonify({'message': "Couldn't find resource. Please login and try again."})
        response.status_code = 404
        return response

    login_url = get_login_url(next=request.url, external=False)

    return redirect(login_url)


def logout_and_redirect_to_index():
    logout_user()

    if settings.MULTI_ORG and current_org == None:
        index_url = '/'
    elif settings.MULTI_ORG:
        index_url = url_for('redash.index', org_slug=current_org.slug, _external=False)
    else:
        index_url = url_for('redash.index', _external=False)

    return redirect(index_url)


def setup_authentication(app):
    from redash.authentication import google_oauth, saml_auth, remote_user_auth, ldap_auth

    login_manager.init_app(app)
    login_manager.anonymous_user = models.AnonymousUser

    app.secret_key = settings.COOKIE_SECRET
    app.register_blueprint(google_oauth.blueprint)
    app.register_blueprint(saml_auth.blueprint)
    app.register_blueprint(remote_user_auth.blueprint)
    app.register_blueprint(ldap_auth.blueprint)

    user_logged_in.connect(log_user_logged_in)
    login_manager.request_loader(request_loader)


def create_and_login_user(org, name, email, picture=None):
    try:
        user_object = models.User.get_by_email_and_org(email, org)
        if user_object.is_disabled:
            return None
        if user_object.name != name:
            logger.debug("Updating user name (%r -> %r)", user_object.name, name)
            user_object.name = name
            models.db.session.commit()
    except NoResultFound:
        logger.debug("Creating user object (%r)", name)
        user_object = models.User(org=org, name=name, email=email, _profile_image_url=picture,
                                  group_ids=[org.default_group.id])
        models.db.session.add(user_object)
        models.db.session.commit()

    login_user(user_object, remember=True)

    return user_object


def get_next_path(unsafe_next_path):
    if not unsafe_next_path:
        return ''

    # Preventing open redirection attacks
    parts = list(urlsplit(unsafe_next_path))
    parts[0] = ''  # clear scheme
    parts[1] = ''  # clear netloc
    safe_next_path = urlunsplit(parts)

    return safe_next_path
<EOF>
<BOF>
import logging
from flask import render_template

from redash import settings
from redash.tasks import send_mail
from redash.utils import base_url
from redash.models import User
# noinspection PyUnresolvedReferences
from itsdangerous import URLSafeTimedSerializer, SignatureExpired, BadSignature

logger = logging.getLogger(__name__)
serializer = URLSafeTimedSerializer(settings.COOKIE_SECRET)


def invite_token(user):
    return serializer.dumps(str(user.id))


def invite_link_for_user(user):
    token = invite_token(user)
    invite_url = "{}/invite/{}".format(base_url(user.org), token)

    return invite_url


def reset_link_for_user(user):
    token = invite_token(user)
    invite_url = "{}/reset/{}".format(base_url(user.org), token)

    return invite_url


def validate_token(token):
    max_token_age = settings.INVITATION_TOKEN_MAX_AGE
    return serializer.loads(token, max_age=max_token_age)


def send_invite_email(inviter, invited, invite_url, org):
    context = dict(inviter=inviter, invited=invited, org=org, invite_url=invite_url)
    html_content = render_template('emails/invite.html', **context)
    text_content = render_template('emails/invite.txt', **context)
    subject = u"{} invited you to join Redash".format(inviter.name)

    send_mail.delay([invited.email], subject, html_content, text_content)


def send_password_reset_email(user):
    reset_link = reset_link_for_user(user)
    context = dict(user=user, reset_link=reset_link)
    html_content = render_template('emails/reset.html', **context)
    text_content = render_template('emails/reset.txt', **context)
    subject = u"Reset your password"

    send_mail.delay([user.email], subject, html_content, text_content)
    return reset_link


<EOF>
<BOF>
import logging

from flask import g, request
from werkzeug.local import LocalProxy

from redash.models import Organization


def _get_current_org():
    if 'org' in g:
        return g.org

    slug = request.view_args.get('org_slug', g.get('org_slug', 'default'))
    g.org = Organization.get_by_slug(slug)
    logging.debug("Current organization: %s (slug: %s)", g.org, slug)
    return g.org

# TODO: move to authentication
current_org = LocalProxy(_get_current_org)
<EOF>
<BOF>
import logging
from flask import flash, redirect, url_for, Blueprint, request
from redash.authentication import create_and_login_user, logout_and_redirect_to_index
from redash.authentication.org_resolving import current_org
from redash.handlers.base import org_scoped_rule
from saml2 import BINDING_HTTP_POST, BINDING_HTTP_REDIRECT, entity
from saml2.client import Saml2Client
from saml2.config import Config as Saml2Config
from saml2.saml import NAMEID_FORMAT_TRANSIENT

logger = logging.getLogger('saml_auth')
blueprint = Blueprint('saml_auth', __name__)


def get_saml_client(org):
    """
    Return SAML configuration.

    The configuration is a hash for use by saml2.config.Config
    """
    metadata_url = org.get_setting("auth_saml_metadata_url")
    entity_id = org.get_setting("auth_saml_entity_id")
    acs_url = url_for("saml_auth.idp_initiated", org_slug=org.slug, _external=True)

    saml_settings = {
        'metadata': {
            "remote": [{
                "url": metadata_url
            }]
        },
        'service': {
            'sp': {
                'endpoints': {
                    'assertion_consumer_service': [
                        (acs_url, BINDING_HTTP_REDIRECT),
                        (acs_url, BINDING_HTTP_POST)
                    ],
                },
                # Don't verify that the incoming requests originate from us via
                # the built-in cache for authn request ids in pysaml2
                'allow_unsolicited': True,
                # Don't sign authn requests, since signed requests only make
                # sense in a situation where you control both the SP and IdP
                'authn_requests_signed': False,
                'logout_requests_signed': True,
                'want_assertions_signed': True,
                'want_response_signed': False,
            },
        },
    }

    if entity_id is not None and entity_id != "":
        saml_settings['entityid'] = entity_id

    sp_config = Saml2Config()
    sp_config.load(saml_settings)
    sp_config.allow_unknown_attributes = True
    saml_client = Saml2Client(config=sp_config)

    return saml_client


@blueprint.route(org_scoped_rule('/saml/callback'), methods=['POST'])
def idp_initiated(org_slug=None):
    if not current_org.get_setting("auth_saml_enabled"):
        logger.error("SAML Login is not enabled")
        return redirect(url_for('redash.index', org_slug=org_slug))

    saml_client = get_saml_client(current_org)
    try:
        authn_response = saml_client.parse_authn_request_response(
            request.form['SAMLResponse'],
            entity.BINDING_HTTP_POST)
    except Exception:
        logger.error('Failed to parse SAML response', exc_info=True)
        flash('SAML login failed. Please try again later.')
        return redirect(url_for('redash.login', org_slug=org_slug))

    authn_response.get_identity()
    user_info = authn_response.get_subject()
    email = user_info.text
    name = "%s %s" % (authn_response.ava['FirstName'][0], authn_response.ava['LastName'][0])

    # This is what as known as "Just In Time (JIT) provisioning".
    # What that means is that, if a user in a SAML assertion
    # isn't in the user store, we create that user first, then log them in
    user = create_and_login_user(current_org, name, email)
    if user is None:
        return logout_and_redirect_to_index()

    if 'RedashGroups' in authn_response.ava:
        group_names = authn_response.ava.get('RedashGroups')
        user.update_group_assignments(group_names)

    url = url_for('redash.index', org_slug=org_slug)

    return redirect(url)


@blueprint.route(org_scoped_rule("/saml/login"))
def sp_initiated(org_slug=None):
    if not current_org.get_setting("auth_saml_enabled"):
        logger.error("SAML Login is not enabled")
        return redirect(url_for('redash.index', org_slug=org_slug))

    saml_client = get_saml_client(current_org)
    nameid_format = current_org.get_setting('auth_saml_nameid_format')
    if nameid_format is None or nameid_format == "":
        nameid_format = NAMEID_FORMAT_TRANSIENT

    _, info = saml_client.prepare_for_authenticate(nameid_format=nameid_format)

    redirect_url = None
    # Select the IdP URL to send the AuthN request to
    for key, value in info['headers']:
        if key is 'Location':
            redirect_url = value
    response = redirect(redirect_url, code=302)

    # NOTE:
    #   I realize I _technically_ don't need to set Cache-Control or Pragma:
    #     https://stackoverflow.com/a/5494469
    #   However, Section 3.2.3.2 of the SAML spec suggests they are set:
    #     http://docs.oasis-open.org/security/saml/v2.0/saml-bindings-2.0-os.pdf
    #   We set those headers here as a "belt and suspenders" approach,
    #   since enterprise environments don't always conform to RFCs
    response.headers['Cache-Control'] = 'no-cache, no-store'
    response.headers['Pragma'] = 'no-cache'
    return response
<EOF>
<BOF>
import logging
import json
import jwt
import requests

logger = logging.getLogger('jwt_auth')


def get_public_keys(url):
    """
    Returns:
        List of RSA public keys usable by PyJWT.
    """
    key_cache = get_public_keys.key_cache
    if url in key_cache:
        return key_cache[url]
    else:
        r = requests.get(url)
        r.raise_for_status()
        data = r.json()
        if 'keys' in data:
            public_keys = []
            for key_dict in data['keys']:
                public_key = jwt.algorithms.RSAAlgorithm.from_jwk(json.dumps(key_dict))
                public_keys.append(public_key)

            get_public_keys.key_cache[url] = public_keys
            return public_keys
        else:
            get_public_keys.key_cache[url] = data
            return data


get_public_keys.key_cache = {}


def verify_jwt_token(jwt_token, expected_issuer, expected_audience, algorithms, public_certs_url):
    # https://developers.cloudflare.com/access/setting-up-access/validate-jwt-tokens/
    # https://cloud.google.com/iap/docs/signed-headers-howto
    # Loop through the keys since we can't pass the key set to the decoder
    keys = get_public_keys(public_certs_url)

    key_id = jwt.get_unverified_header(jwt_token).get('kid', '')
    if key_id and isinstance(keys, dict):
        keys = [keys.get(key_id)]

    valid_token = False
    payload = None
    for key in keys:
        try:
            # decode returns the claims which has the email if you need it
            payload = jwt.decode(
                jwt_token,
                key=key,
                audience=expected_audience,
                algorithms=algorithms
            )
            issuer = payload['iss']
            if issuer != expected_issuer:
                raise Exception('Wrong issuer: {}'.format(issuer))
            valid_token = True
            break
        except Exception as e:
            logging.exception(e)
    return payload, valid_token
<EOF>
<BOF>
from .general import record_event, version_check, send_mail
from .queries import QueryTask, refresh_queries, refresh_schemas, cleanup_tasks, cleanup_query_results, execute_query
from .alerts import check_alerts_for_query
<EOF>
<BOF>
import logging
import signal
import time

import redis
from celery.exceptions import SoftTimeLimitExceeded, TimeLimitExceeded
from celery.result import AsyncResult
from celery.utils.log import get_task_logger
from six import text_type

from redash import models, redis_connection, settings, statsd_client
from redash.query_runner import InterruptException
from redash.tasks.alerts import check_alerts_for_query
from redash.utils import gen_query_hash, json_dumps, json_loads, utcnow, mustache_render
from redash.worker import celery

logger = get_task_logger(__name__)


def _job_lock_id(query_hash, data_source_id):
    return "query_hash_job:%s:%s" % (data_source_id, query_hash)


def _unlock(query_hash, data_source_id):
    redis_connection.delete(_job_lock_id(query_hash, data_source_id))


# TODO:
# There is some duplication between this class and QueryTask, but I wanted to implement the monitoring features without
# much changes to the existing code, so ended up creating another object. In the future we can merge them.
class QueryTaskTracker(object):
    DONE_LIST = 'query_task_trackers:done'
    WAITING_LIST = 'query_task_trackers:waiting'
    IN_PROGRESS_LIST = 'query_task_trackers:in_progress'
    ALL_LISTS = (DONE_LIST, WAITING_LIST, IN_PROGRESS_LIST)

    def __init__(self, data):
        self.data = data

    @classmethod
    def create(cls, task_id, state, query_hash, data_source_id, scheduled, metadata):
        data = dict(task_id=task_id, state=state,
                    query_hash=query_hash, data_source_id=data_source_id,
                    scheduled=scheduled,
                    username=metadata.get('Username', 'unknown'),
                    query_id=metadata.get('Query ID', 'unknown'),
                    retries=0,
                    scheduled_retries=0,
                    created_at=time.time(),
                    started_at=None,
                    run_time=None)

        return cls(data)

    def save(self, connection=None):
        if connection is None:
            connection = redis_connection

        self.data['updated_at'] = time.time()
        key_name = self._key_name(self.data['task_id'])
        connection.set(key_name, json_dumps(self.data))
        connection.zadd(self._get_list(), time.time(), key_name)

        for l in self.ALL_LISTS:
            if l != self._get_list():
                connection.zrem(l, key_name)

    # TOOD: this is not thread/concurrency safe. In current code this is not an issue, but better to fix this.
    def update(self, **kwargs):
        self.data.update(kwargs)
        self.save()

    @staticmethod
    def _key_name(task_id):
        return 'query_task_tracker:{}'.format(task_id)

    def _get_list(self):
        if self.state in ('finished', 'failed', 'cancelled'):
            return self.DONE_LIST

        if self.state in ('created'):
            return self.WAITING_LIST

        return self.IN_PROGRESS_LIST

    @classmethod
    def get_by_task_id(cls, task_id, connection=None):
        if connection is None:
            connection = redis_connection

        key_name = cls._key_name(task_id)
        data = connection.get(key_name)
        return cls.create_from_data(data)

    @classmethod
    def create_from_data(cls, data):
        if data:
            data = json_loads(data)
            return cls(data)

        return None

    @classmethod
    def all(cls, list_name, offset=0, limit=-1):
        if limit != -1:
            limit -= 1

        if offset != 0:
            offset -= 1

        ids = redis_connection.zrevrange(list_name, offset, limit)
        pipe = redis_connection.pipeline()
        for id in ids:
            pipe.get(id)

        tasks = [cls.create_from_data(data) for data in pipe.execute()]
        return tasks

    @classmethod
    def prune(cls, list_name, keep_count, max_keys=100):
        count = redis_connection.zcard(list_name)
        if count <= keep_count:
            return 0

        remove_count = min(max_keys, count - keep_count)
        keys = redis_connection.zrange(list_name, 0, remove_count - 1)
        redis_connection.delete(*keys)
        redis_connection.zremrangebyrank(list_name, 0, remove_count - 1)
        return remove_count

    def __getattr__(self, item):
        return self.data[item]

    def __contains__(self, item):
        return item in self.data


class QueryTask(object):
    # TODO: this is mapping to the old Job class statuses. Need to update the client side and remove this
    STATUSES = {
        'PENDING': 1,
        'STARTED': 2,
        'SUCCESS': 3,
        'FAILURE': 4,
        'REVOKED': 4
    }

    def __init__(self, job_id=None, async_result=None):
        if async_result:
            self._async_result = async_result
        else:
            self._async_result = AsyncResult(job_id, app=celery)

    @property
    def id(self):
        return self._async_result.id

    def to_dict(self):
        task_info = self._async_result._get_task_meta()
        result, task_status = task_info['result'], task_info['status']
        if task_status == 'STARTED':
            updated_at = result.get('start_time', 0)
        else:
            updated_at = 0

        status = self.STATUSES[task_status]

        if isinstance(result, (TimeLimitExceeded, SoftTimeLimitExceeded)):
            error = "Query exceeded Redash query execution time limit."
            status = 4
        elif isinstance(result, Exception):
            error = result.message
            status = 4
        elif task_status == 'REVOKED':
            error = 'Query execution cancelled.'
        else:
            error = ''

        if task_status == 'SUCCESS' and not error:
            query_result_id = result
        else:
            query_result_id = None

        return {
            'id': self._async_result.id,
            'updated_at': updated_at,
            'status': status,
            'error': error,
            'query_result_id': query_result_id,
        }

    @property
    def is_cancelled(self):
        return self._async_result.status == 'REVOKED'

    @property
    def celery_status(self):
        return self._async_result.status

    def ready(self):
        return self._async_result.ready()

    def cancel(self):
        return self._async_result.revoke(terminate=True, signal='SIGINT')


def enqueue_query(query, data_source, user_id, scheduled_query=None, metadata={}):
    query_hash = gen_query_hash(query)
    logging.info("Inserting job for %s with metadata=%s", query_hash, metadata)
    try_count = 0
    job = None

    while try_count < 5:
        try_count += 1

        pipe = redis_connection.pipeline()
        try:
            pipe.watch(_job_lock_id(query_hash, data_source.id))
            job_id = pipe.get(_job_lock_id(query_hash, data_source.id))
            if job_id:
                logging.info("[%s] Found existing job: %s", query_hash, job_id)

                job = QueryTask(job_id=job_id)

                if job.ready():
                    logging.info("[%s] job found is ready (%s), removing lock", query_hash, job.celery_status)
                    redis_connection.delete(_job_lock_id(query_hash, data_source.id))
                    job = None

            if not job:
                pipe.multi()

                time_limit = None

                if scheduled_query:
                    queue_name = data_source.scheduled_queue_name
                    scheduled_query_id = scheduled_query.id
                else:
                    queue_name = data_source.queue_name
                    scheduled_query_id = None
                    time_limit = settings.ADHOC_QUERY_TIME_LIMIT

                result = execute_query.apply_async(args=(query, data_source.id, metadata, user_id, scheduled_query_id),
                                                   queue=queue_name,
                                                   time_limit=time_limit)
                job = QueryTask(async_result=result)
                tracker = QueryTaskTracker.create(
                    result.id, 'created', query_hash, data_source.id,
                    scheduled_query is not None, metadata)
                tracker.save(connection=pipe)

                logging.info("[%s] Created new job: %s", query_hash, job.id)
                pipe.set(_job_lock_id(query_hash, data_source.id), job.id, settings.JOB_EXPIRY_TIME)
                pipe.execute()
            break

        except redis.WatchError:
            continue

    if not job:
        logging.error("[Manager][%s] Failed adding job for query.", query_hash)

    return job


@celery.task(name="redash.tasks.refresh_queries")
def refresh_queries():
    logger.info("Refreshing queries...")

    outdated_queries_count = 0
    query_ids = []

    with statsd_client.timer('manager.outdated_queries_lookup'):
        for query in models.Query.outdated_queries():
            if settings.FEATURE_DISABLE_REFRESH_QUERIES:
                logging.info("Disabled refresh queries.")
            elif query.org.is_disabled:
                logging.info("Skipping refresh of %s because org is disabled.", query.id)
            elif query.data_source is None:
                logging.info("Skipping refresh of %s because the datasource is none.", query.id)
            elif query.data_source.paused:
                logging.info("Skipping refresh of %s because datasource - %s is paused (%s).", query.id, query.data_source.name, query.data_source.pause_reason)
            else:
                if query.options and len(query.options.get('parameters', [])) > 0:
                    query_params = {p['name']: p.get('value')
                                    for p in query.options['parameters']}
                    query_text = mustache_render(query.query_text, query_params)
                else:
                    query_text = query.query_text

                enqueue_query(query_text, query.data_source, query.user_id,
                              scheduled_query=query,
                              metadata={'Query ID': query.id, 'Username': 'Scheduled'})

                query_ids.append(query.id)
                outdated_queries_count += 1

    statsd_client.gauge('manager.outdated_queries', outdated_queries_count)

    logger.info("Done refreshing queries. Found %d outdated queries: %s" % (outdated_queries_count, query_ids))

    status = redis_connection.hgetall('redash:status')
    now = time.time()

    redis_connection.hmset('redash:status', {
        'outdated_queries_count': outdated_queries_count,
        'last_refresh_at': now,
        'query_ids': json_dumps(query_ids)
    })

    statsd_client.gauge('manager.seconds_since_refresh', now - float(status.get('last_refresh_at', now)))


@celery.task(name="redash.tasks.cleanup_tasks")
def cleanup_tasks():
    in_progress = QueryTaskTracker.all(QueryTaskTracker.IN_PROGRESS_LIST)
    for tracker in in_progress:
        result = AsyncResult(tracker.task_id)

        if result.ready():
            logging.info("in progress tracker %s finished", tracker.query_hash)
            _unlock(tracker.query_hash, tracker.data_source_id)
            tracker.update(state='finished')

    waiting = QueryTaskTracker.all(QueryTaskTracker.WAITING_LIST)
    for tracker in waiting:
        result = AsyncResult(tracker.task_id)

        if result.ready():
            logging.info("waiting tracker %s finished", tracker.query_hash)
            _unlock(tracker.query_hash, tracker.data_source_id)
            tracker.update(state='finished')

    # Maintain constant size of the finished tasks list:
    removed = 1000
    while removed > 0:
        removed = QueryTaskTracker.prune(QueryTaskTracker.DONE_LIST, 1000)


@celery.task(name="redash.tasks.cleanup_query_results")
def cleanup_query_results():
    """
    Job to cleanup unused query results -- such that no query links to them anymore, and older than
    settings.QUERY_RESULTS_MAX_AGE (a week by default, so it's less likely to be open in someone's browser and be used).

    Each time the job deletes only settings.QUERY_RESULTS_CLEANUP_COUNT (100 by default) query results so it won't choke
    the database in case of many such results.
    """

    logging.info("Running query results clean up (removing maximum of %d unused results, that are %d days old or more)",
                 settings.QUERY_RESULTS_CLEANUP_COUNT, settings.QUERY_RESULTS_CLEANUP_MAX_AGE)

    unused_query_results = models.QueryResult.unused(settings.QUERY_RESULTS_CLEANUP_MAX_AGE).limit(settings.QUERY_RESULTS_CLEANUP_COUNT)
    deleted_count = models.QueryResult.query.filter(
        models.QueryResult.id.in_(unused_query_results.subquery())
    ).delete(synchronize_session=False)
    models.db.session.commit()
    logger.info("Deleted %d unused query results.", deleted_count)


@celery.task(name="redash.tasks.refresh_schema", time_limit=90, soft_time_limit=60)
def refresh_schema(data_source_id):
    ds = models.DataSource.get_by_id(data_source_id)
    logger.info(u"task=refresh_schema state=start ds_id=%s", ds.id)
    start_time = time.time()
    try:
        ds.get_schema(refresh=True)
        logger.info(u"task=refresh_schema state=finished ds_id=%s runtime=%.2f", ds.id, time.time() - start_time)
        statsd_client.incr('refresh_schema.success')
    except SoftTimeLimitExceeded:
        logger.info(u"task=refresh_schema state=timeout ds_id=%s runtime=%.2f", ds.id, time.time() - start_time)
        statsd_client.incr('refresh_schema.timeout')
    except Exception:
        logger.warning(u"Failed refreshing schema for the data source: %s", ds.name, exc_info=1)
        statsd_client.incr('refresh_schema.error')
        logger.info(u"task=refresh_schema state=failed ds_id=%s runtime=%.2f", ds.id, time.time() - start_time)


@celery.task(name="redash.tasks.refresh_schemas")
def refresh_schemas():
    """
    Refreshes the data sources schemas.
    """
    blacklist = [int(ds_id) for ds_id in redis_connection.smembers('data_sources:schema:blacklist') if ds_id]
    global_start_time = time.time()

    logger.info(u"task=refresh_schemas state=start")

    for ds in models.DataSource.query:
        if ds.paused:
            logger.info(u"task=refresh_schema state=skip ds_id=%s reason=paused(%s)", ds.id, ds.pause_reason)
        elif ds.id in blacklist:
            logger.info(u"task=refresh_schema state=skip ds_id=%s reason=blacklist", ds.id)
        elif ds.org.is_disabled:
            logger.info(u"task=refresh_schema state=skip ds_id=%s reason=org_disabled", ds.id)
        else:
            refresh_schema.apply_async(args=(ds.id,), queue="schemas")

    logger.info(u"task=refresh_schemas state=finish total_runtime=%.2f", time.time() - global_start_time)


def signal_handler(*args):
    raise InterruptException


class QueryExecutionError(Exception):
    pass


# We could have created this as a celery.Task derived class, and act as the task itself. But this might result in weird
# issues as the task class created once per process, so decided to have a plain object instead.
class QueryExecutor(object):
    def __init__(self, task, query, data_source_id, user_id, metadata,
                 scheduled_query):
        self.task = task
        self.query = query
        self.data_source_id = data_source_id
        self.metadata = metadata
        self.data_source = self._load_data_source()
        if user_id is not None:
            self.user = models.User.query.get(user_id)
        else:
            self.user = None
        # Close DB connection to prevent holding a connection for a long time while the query is executing.
        models.db.session.close()
        self.query_hash = gen_query_hash(self.query)
        self.scheduled_query = scheduled_query
        # Load existing tracker or create a new one if the job was created before code update:
        self.tracker = (
            QueryTaskTracker.get_by_task_id(task.request.id) or
            QueryTaskTracker.create(
                task.request.id,
                'created',
                self.query_hash,
                self.data_source_id,
                False,
                metadata
            )
        )
        if self.tracker.scheduled:
            models.scheduled_queries_executions.update(self.tracker.query_id)

    def run(self):
        signal.signal(signal.SIGINT, signal_handler)
        self.tracker.update(started_at=time.time(), state='started')

        logger.debug("Executing query:\n%s", self.query)
        self._log_progress('executing_query')

        query_runner = self.data_source.query_runner
        annotated_query = self._annotate_query(query_runner)

        try:
            data, error = query_runner.run_query(annotated_query, self.user)
        except Exception as e:
            error = text_type(e)
            data = None
            logging.warning('Unexpected error while running query:', exc_info=1)

        run_time = time.time() - self.tracker.started_at
        self.tracker.update(error=error, run_time=run_time, state='saving_results')

        logger.info(u"task=execute_query query_hash=%s data_length=%s error=[%s]", self.query_hash, data and len(data), error)

        _unlock(self.query_hash, self.data_source.id)

        if error:
            self.tracker.update(state='failed')
            result = QueryExecutionError(error)
            if self.scheduled_query is not None:
                self.scheduled_query = models.db.session.merge(self.scheduled_query, load=False)
                self.scheduled_query.schedule_failures += 1
                models.db.session.add(self.scheduled_query)
            models.db.session.commit()
            raise result
        else:
            if (self.scheduled_query and self.scheduled_query.schedule_failures > 0):
                self.scheduled_query = models.db.session.merge(self.scheduled_query, load=False)
                self.scheduled_query.schedule_failures = 0
                models.db.session.add(self.scheduled_query)
            query_result, updated_query_ids = models.QueryResult.store_result(
                self.data_source.org_id, self.data_source,
                self.query_hash, self.query, data,
                run_time, utcnow())
            models.db.session.commit()  # make sure that alert sees the latest query result
            self._log_progress('checking_alerts')
            for query_id in updated_query_ids:
                check_alerts_for_query.delay(query_id)
            self._log_progress('finished')

            result = query_result.id
            models.db.session.commit()
            return result

    def _annotate_query(self, query_runner):
        if query_runner.annotate_query():
            self.metadata['Task ID'] = self.task.request.id
            self.metadata['Query Hash'] = self.query_hash
            self.metadata['Queue'] = self.task.request.delivery_info['routing_key']

            annotation = u", ".join([u"{}: {}".format(k, v) for k, v in self.metadata.iteritems()])
            annotated_query = u"/* {} */ {}".format(annotation, self.query)
        else:
            annotated_query = self.query
        return annotated_query

    def _log_progress(self, state):
        logger.info(
            u"task=execute_query state=%s query_hash=%s type=%s ds_id=%d  "
            "task_id=%s queue=%s query_id=%s username=%s",
            state, self.query_hash, self.data_source.type, self.data_source.id,
            self.task.request.id,
            self.task.request.delivery_info['routing_key'],
            self.metadata.get('Query ID', 'unknown'),
            self.metadata.get('Username', 'unknown'))
        self.tracker.update(state=state)

    def _load_data_source(self):
        logger.info("task=execute_query state=load_ds ds_id=%d", self.data_source_id)
        return models.DataSource.query.get(self.data_source_id)


# user_id is added last as a keyword argument for backward compatability -- to support executing previously submitted
# jobs before the upgrade to this version.
@celery.task(name="redash.tasks.execute_query", bind=True, track_started=True)
def execute_query(self, query, data_source_id, metadata, user_id=None,
                  scheduled_query_id=None):
    if scheduled_query_id is not None:
        scheduled_query = models.Query.query.get(scheduled_query_id)
    else:
        scheduled_query = None
    return QueryExecutor(self, query, data_source_id, user_id, metadata,
                         scheduled_query).run()
<EOF>
<BOF>
import requests

from celery.utils.log import get_task_logger
from flask_mail import Message
from redash import mail, models, settings
from redash.version_check import run_version_check
from redash.worker import celery

logger = get_task_logger(__name__)


@celery.task(name="redash.tasks.record_event")
def record_event(raw_event):
    event = models.Event.record(raw_event)
    models.db.session.commit()

    for hook in settings.EVENT_REPORTING_WEBHOOKS:
        logger.debug("Forwarding event to: %s", hook)
        try:
            data = {
                "schema": "iglu:io.redash.webhooks/event/jsonschema/1-0-0",
                "data": event.to_dict()
            }
            response = requests.post(hook, json=data)
            if response.status_code != 200:
                logger.error("Failed posting to %s: %s", hook, response.content)
        except Exception:
            logger.exception("Failed posting to %s", hook)


@celery.task(name="redash.tasks.version_check")
def version_check():
    run_version_check()


@celery.task(name="redash.tasks.subscribe")
def subscribe(form):
    logger.info("Subscribing to: [security notifications=%s], [newsletter=%s]", form['security_notifications'], form['newsletter'])
    data = {
        'admin_name': form['name'],
        'admin_email': form['email'],
        'org_name': form['org_name'],
        'security_notifications': form['security_notifications'],
        'newsletter': form['newsletter']
    }
    requests.post('https://beacon.redash.io/subscribe', json=data)


@celery.task(name="redash.tasks.send_mail")
def send_mail(to, subject, html, text):
    try:
        message = Message(recipients=to,
                          subject=subject,
                          html=html,
                          body=text)

        mail.send(message)
    except Exception:
        logger.exception('Failed sending message: %s', message.subject)
<EOF>
<BOF>
from celery.utils.log import get_task_logger
from flask import current_app
import datetime
from redash.worker import celery
from redash import utils
from redash import models, settings


logger = get_task_logger(__name__)


def base_url(org):
    if settings.MULTI_ORG:
        return "https://{}/{}".format(settings.HOST, org.slug)

    return settings.HOST


def notify_subscriptions(alert, new_state):
    host = base_url(alert.query_rel.org)
    for subscription in alert.subscriptions:
        try:
            subscription.notify(alert, alert.query_rel, subscription.user, new_state, current_app, host)
        except Exception as e:
            logger.exception("Error with processing destination")


def should_notify(alert, new_state):
    passed_rearm_threshold = False
    if alert.rearm and alert.last_triggered_at:
        passed_rearm_threshold = alert.last_triggered_at + datetime.timedelta(seconds=alert.rearm) < utils.utcnow()

    return new_state != alert.state or (alert.state == models.Alert.TRIGGERED_STATE and passed_rearm_threshold)


@celery.task(name="redash.tasks.check_alerts_for_query", time_limit=300, soft_time_limit=240)
def check_alerts_for_query(query_id):
    logger.debug("Checking query %d for alerts", query_id)

    query = models.Query.query.get(query_id)

    for alert in query.alerts:
        new_state = alert.evaluate()

        if should_notify(alert, new_state):
            logger.info("Alert %d new state: %s", alert.id, new_state)
            old_state = alert.state

            alert.state = new_state
            alert.last_triggered_at = utils.utcnow()
            models.db.session.commit()

            if old_state == models.Alert.UNKNOWN_STATE and new_state == models.Alert.OK_STATE:
                logger.debug("Skipping notification (previous state was unknown and now it's ok).")
                continue

            notify_subscriptions(alert, new_state)
<EOF>
<BOF>
import os


def fix_assets_path(path):
    fullpath = os.path.join(os.path.dirname(__file__), "../", path)
    return fullpath


def array_from_string(s):
    array = s.split(',')
    if "" in array:
        array.remove("")

    return array


def set_from_string(s):
    return set(array_from_string(s))


def parse_boolean(s):
    """Takes a string and returns the equivalent as a boolean value."""
    s = s.strip().lower()
    if s in ('yes', 'true', 'on', '1'):
        return True
    elif s in ('no', 'false', 'off', '0', 'none'):
        return False
    else:
        raise ValueError('Invalid boolean value %r' % s)


def int_or_none(value):
    if value is None:
        return value

    return int(value)
<EOF>
<BOF>
import os
from funcy import distinct, remove

from .helpers import fix_assets_path, array_from_string, parse_boolean, int_or_none, set_from_string
from .organization import DATE_FORMAT


def all_settings():
    from types import ModuleType

    settings = {}
    for name, item in globals().iteritems():
        if not callable(item) and not name.startswith("__") and not isinstance(item, ModuleType):
            settings[name] = item

    return settings

REDIS_URL = os.environ.get('REDASH_REDIS_URL', os.environ.get('REDIS_URL', "redis://localhost:6379/0"))
PROXIES_COUNT = int(os.environ.get('REDASH_PROXIES_COUNT', "1"))

STATSD_HOST = os.environ.get('REDASH_STATSD_HOST', "127.0.0.1")
STATSD_PORT = int(os.environ.get('REDASH_STATSD_PORT', "8125"))
STATSD_PREFIX = os.environ.get('REDASH_STATSD_PREFIX', "redash")
STATSD_USE_TAGS = parse_boolean(os.environ.get('REDASH_STATSD_USE_TAGS', "false"))

# Connection settings for Redash's own database (where we store the queries, results, etc)
SQLALCHEMY_DATABASE_URI = os.environ.get("REDASH_DATABASE_URL", os.environ.get('DATABASE_URL', "postgresql:///postgres"))
SQLALCHEMY_MAX_OVERFLOW = int_or_none(os.environ.get("SQLALCHEMY_MAX_OVERFLOW"))
SQLALCHEMY_POOL_SIZE = int_or_none(os.environ.get("SQLALCHEMY_POOL_SIZE"))
SQLALCHEMY_DISABLE_POOL = parse_boolean(os.environ.get("SQLALCHEMY_DISABLE_POOL", "false"))
SQLALCHEMY_TRACK_MODIFICATIONS = False
SQLALCHEMY_ECHO = False

# Celery related settings
CELERY_BROKER = os.environ.get("REDASH_CELERY_BROKER", REDIS_URL)
CELERY_RESULT_BACKEND = os.environ.get(
    "REDASH_CELERY_RESULT_BACKEND",
    os.environ.get("REDASH_CELERY_BACKEND", CELERY_BROKER))
CELERY_RESULT_EXPIRES = int(os.environ.get(
    "REDASH_CELERY_RESULT_EXPIRES",
    os.environ.get("REDASH_CELERY_TASK_RESULT_EXPIRES", 3600 * 4)))

# The following enables periodic job (every 5 minutes) of removing unused query results.
QUERY_RESULTS_CLEANUP_ENABLED = parse_boolean(os.environ.get("REDASH_QUERY_RESULTS_CLEANUP_ENABLED", "true"))
QUERY_RESULTS_CLEANUP_COUNT = int(os.environ.get("REDASH_QUERY_RESULTS_CLEANUP_COUNT", "100"))
QUERY_RESULTS_CLEANUP_MAX_AGE = int(os.environ.get("REDASH_QUERY_RESULTS_CLEANUP_MAX_AGE", "7"))

SCHEMAS_REFRESH_SCHEDULE = int(os.environ.get("REDASH_SCHEMAS_REFRESH_SCHEDULE", 30))

AUTH_TYPE = os.environ.get("REDASH_AUTH_TYPE", "api_key")
ENFORCE_HTTPS = parse_boolean(os.environ.get("REDASH_ENFORCE_HTTPS", "false"))
INVITATION_TOKEN_MAX_AGE = int(os.environ.get("REDASH_INVITATION_TOKEN_MAX_AGE", 60 * 60 * 24 * 7))

MULTI_ORG = parse_boolean(os.environ.get("REDASH_MULTI_ORG", "false"))

GOOGLE_CLIENT_ID = os.environ.get("REDASH_GOOGLE_CLIENT_ID", "")
GOOGLE_CLIENT_SECRET = os.environ.get("REDASH_GOOGLE_CLIENT_SECRET", "")
GOOGLE_OAUTH_ENABLED = bool(GOOGLE_CLIENT_ID and GOOGLE_CLIENT_SECRET)

# Enables the use of an externally-provided and trusted remote user via an HTTP
# header.  The "user" must be an email address.
#
# By default the trusted header is X-Forwarded-Remote-User.  You can change
# this by setting REDASH_REMOTE_USER_HEADER.
#
# Enabling this authentication method is *potentially dangerous*, and it is
# your responsibility to ensure that only a trusted frontend (usually on the
# same server) can talk to the redash backend server, otherwise people will be
# able to login as anyone they want by directly talking to the redash backend.
# You must *also* ensure that any special header in the original request is
# removed or always overwritten by your frontend, otherwise your frontend may
# pass it through to the backend unchanged.
#
# Note that redash will only check the remote user once, upon the first need
# for a login, and then set a cookie which keeps the user logged in.  Dropping
# the remote user header after subsequent requests won't automatically log the
# user out.  Doing so could be done with further work, but usually it's
# unnecessary.
#
# If you also set the organization setting auth_password_login_enabled to false,
# then your authentication will be seamless.  Otherwise a link will be presented
# on the login page to trigger remote user auth.
REMOTE_USER_LOGIN_ENABLED = parse_boolean(os.environ.get("REDASH_REMOTE_USER_LOGIN_ENABLED", "false"))
REMOTE_USER_HEADER = os.environ.get("REDASH_REMOTE_USER_HEADER", "X-Forwarded-Remote-User")

# If the organization setting auth_password_login_enabled is not false, then users will still be
# able to login through Redash instead of the LDAP server
LDAP_LOGIN_ENABLED = parse_boolean(os.environ.get('REDASH_LDAP_LOGIN_ENABLED', 'false'))
# The LDAP directory address (ex. ldap://10.0.10.1:389)
LDAP_HOST_URL = os.environ.get('REDASH_LDAP_URL', None)
# The DN & password used to connect to LDAP to determine the identity of the user being authenticated.
# For AD this should be "org\\user".
LDAP_BIND_DN = os.environ.get('REDASH_LDAP_BIND_DN', None)
LDAP_BIND_DN_PASSWORD = os.environ.get('REDASH_LDAP_BIND_DN_PASSWORD', '')
# AD/LDAP email and display name keys
LDAP_DISPLAY_NAME_KEY = os.environ.get('REDASH_LDAP_DISPLAY_NAME_KEY', 'displayName')
LDAP_EMAIL_KEY = os.environ.get('REDASH_LDAP_EMAIL_KEY', "mail")
# Prompt that should be shown above username/email field.
LDAP_CUSTOM_USERNAME_PROMPT = os.environ.get('REDASH_LDAP_CUSTOM_USERNAME_PROMPT', 'LDAP/AD/SSO username:')
# LDAP Search DN TEMPLATE (for AD this should be "(sAMAccountName=%(username)s)"")
LDAP_SEARCH_TEMPLATE = os.environ.get('REDASH_LDAP_SEARCH_TEMPLATE', '(cn=%(username)s)')
# The schema to bind to (ex. cn=users,dc=ORG,dc=local)
LDAP_SEARCH_DN = os.environ.get('REDASH_LDAP_SEARCH_DN', os.environ.get('REDASH_SEARCH_DN'))

STATIC_ASSETS_PATH = fix_assets_path(os.environ.get("REDASH_STATIC_ASSETS_PATH", "../client/dist/"))

JOB_EXPIRY_TIME = int(os.environ.get("REDASH_JOB_EXPIRY_TIME", 3600 * 12))
COOKIE_SECRET = os.environ.get("REDASH_COOKIE_SECRET", "c292a0a3aa32397cdb050e233733900f")
SESSION_COOKIE_SECURE = parse_boolean(os.environ.get("REDASH_SESSION_COOKIE_SECURE") or str(ENFORCE_HTTPS))

LOG_LEVEL = os.environ.get("REDASH_LOG_LEVEL", "INFO")
LOG_STDOUT = parse_boolean(os.environ.get('REDASH_LOG_STDOUT', 'false'))
LOG_PREFIX = os.environ.get('REDASH_LOG_PREFIX', '')
LOG_FORMAT = os.environ.get('REDASH_LOG_FORMAT', LOG_PREFIX + '[%(asctime)s][PID:%(process)d][%(levelname)s][%(name)s] %(message)s')
CELERYD_WORKER_LOG_FORMAT = os.environ.get(
    "REDASH_CELERYD_WORKER_LOG_FORMAT",
    os.environ.get('REDASH_CELERYD_LOG_FORMAT',
                   LOG_PREFIX + '[%(asctime)s][PID:%(process)d][%(levelname)s][%(processName)s] %(message)s'))
CELERYD_WORKER_TASK_LOG_FORMAT = os.environ.get(
    "REDASH_CELERYD_WORKER_TASK_LOG_FORMAT",
    os.environ.get('REDASH_CELERYD_TASK_LOG_FORMAT',
                   (LOG_PREFIX + '[%(asctime)s][PID:%(process)d][%(levelname)s][%(processName)s] '
                    'task_name=%(task_name)s '
                    'task_id=%(task_id)s %(message)s')))

# Mail settings:
MAIL_SERVER = os.environ.get('REDASH_MAIL_SERVER', 'localhost')
MAIL_PORT = int(os.environ.get('REDASH_MAIL_PORT', 25))
MAIL_USE_TLS = parse_boolean(os.environ.get('REDASH_MAIL_USE_TLS', 'false'))
MAIL_USE_SSL = parse_boolean(os.environ.get('REDASH_MAIL_USE_SSL', 'false'))
MAIL_USERNAME = os.environ.get('REDASH_MAIL_USERNAME', None)
MAIL_PASSWORD = os.environ.get('REDASH_MAIL_PASSWORD', None)
MAIL_DEFAULT_SENDER = os.environ.get('REDASH_MAIL_DEFAULT_SENDER', None)
MAIL_MAX_EMAILS = os.environ.get('REDASH_MAIL_MAX_EMAILS', None)
MAIL_ASCII_ATTACHMENTS = parse_boolean(os.environ.get('REDASH_MAIL_ASCII_ATTACHMENTS', 'false'))

HOST = os.environ.get('REDASH_HOST', '')

ALERTS_DEFAULT_MAIL_SUBJECT_TEMPLATE = os.environ.get('REDASH_ALERTS_DEFAULT_MAIL_SUBJECT_TEMPLATE', "({state}) {alert_name}")

# How many requests are allowed per IP to the login page before
# being throttled?
# See https://flask-limiter.readthedocs.io/en/stable/#rate-limit-string-notation

THROTTLE_LOGIN_PATTERN = os.environ.get('REDASH_THROTTLE_LOGIN_PATTERN', '50/hour')
LIMITER_STORAGE = os.environ.get("REDASH_LIMITER_STORAGE", REDIS_URL)

# CORS settings for the Query Result API (and possbily future external APIs).
# In most cases all you need to do is set REDASH_CORS_ACCESS_CONTROL_ALLOW_ORIGIN
# to the calling domain (or domains in a comma separated list).
ACCESS_CONTROL_ALLOW_ORIGIN = set_from_string(os.environ.get("REDASH_CORS_ACCESS_CONTROL_ALLOW_ORIGIN", ""))
ACCESS_CONTROL_ALLOW_CREDENTIALS = parse_boolean(os.environ.get("REDASH_CORS_ACCESS_CONTROL_ALLOW_CREDENTIALS", "false"))
ACCESS_CONTROL_REQUEST_METHOD = os.environ.get("REDASH_CORS_ACCESS_CONTROL_REQUEST_METHOD", "GET, POST, PUT")
ACCESS_CONTROL_ALLOW_HEADERS = os.environ.get("REDASH_CORS_ACCESS_CONTROL_ALLOW_HEADERS", "Content-Type")

# Query Runners
default_query_runners = [
    'redash.query_runner.athena',
    'redash.query_runner.big_query',
    'redash.query_runner.google_spreadsheets',
    'redash.query_runner.graphite',
    'redash.query_runner.mongodb',
    'redash.query_runner.mysql',
    'redash.query_runner.pg',
    'redash.query_runner.url',
    'redash.query_runner.influx_db',
    'redash.query_runner.elasticsearch',
    'redash.query_runner.presto',
    'redash.query_runner.databricks',
    'redash.query_runner.hive_ds',
    'redash.query_runner.impala_ds',
    'redash.query_runner.vertica',
    'redash.query_runner.clickhouse',
    'redash.query_runner.yandex_metrica',
    'redash.query_runner.rockset',
    'redash.query_runner.treasuredata',
    'redash.query_runner.sqlite',
    'redash.query_runner.dynamodb_sql',
    'redash.query_runner.mssql',
    'redash.query_runner.memsql_ds',
    'redash.query_runner.mapd',
    'redash.query_runner.jql',
    'redash.query_runner.google_analytics',
    'redash.query_runner.axibase_tsd',
    'redash.query_runner.salesforce',
    'redash.query_runner.query_results',
    'redash.query_runner.prometheus',
    'redash.query_runner.qubole',
    'redash.query_runner.db2',
    'redash.query_runner.druid',
    'redash.query_runner.kylin'
]

enabled_query_runners = array_from_string(os.environ.get("REDASH_ENABLED_QUERY_RUNNERS", ",".join(default_query_runners)))
additional_query_runners = array_from_string(os.environ.get("REDASH_ADDITIONAL_QUERY_RUNNERS", ""))
disabled_query_runners = array_from_string(os.environ.get("REDASH_DISABLED_QUERY_RUNNERS", ""))

QUERY_RUNNERS = remove(set(disabled_query_runners), distinct(enabled_query_runners + additional_query_runners))
ADHOC_QUERY_TIME_LIMIT = int_or_none(os.environ.get('REDASH_ADHOC_QUERY_TIME_LIMIT', None))

# Destinations
default_destinations = [
    'redash.destinations.email',
    'redash.destinations.slack',
    'redash.destinations.webhook',
    'redash.destinations.hipchat',
    'redash.destinations.mattermost',
    'redash.destinations.chatwork',
    'redash.destinations.pagerduty',
]

enabled_destinations = array_from_string(os.environ.get("REDASH_ENABLED_DESTINATIONS", ",".join(default_destinations)))
additional_destinations = array_from_string(os.environ.get("REDASH_ADDITIONAL_DESTINATIONS", ""))

DESTINATIONS = distinct(enabled_destinations + additional_destinations)

EVENT_REPORTING_WEBHOOKS = array_from_string(os.environ.get("REDASH_EVENT_REPORTING_WEBHOOKS", ""))

# Support for Sentry (https://getsentry.com/). Just set your Sentry DSN to enable it:
SENTRY_DSN = os.environ.get("REDASH_SENTRY_DSN", "")

# Client side toggles:
ALLOW_SCRIPTS_IN_USER_INPUT = parse_boolean(os.environ.get("REDASH_ALLOW_SCRIPTS_IN_USER_INPUT", "false"))
DASHBOARD_REFRESH_INTERVALS = map(int, array_from_string(os.environ.get("REDASH_DASHBOARD_REFRESH_INTERVALS", "60,300,600,1800,3600,43200,86400")))
QUERY_REFRESH_INTERVALS = map(int, array_from_string(os.environ.get("REDASH_QUERY_REFRESH_INTERVALS", "60, 300, 600, 900, 1800, 3600, 7200, 10800, 14400, 18000, 21600, 25200, 28800, 32400, 36000, 39600, 43200, 86400, 604800, 1209600, 2592000")))
PAGE_SIZE = int(os.environ.get('REDASH_PAGE_SIZE', 20))
PAGE_SIZE_OPTIONS = map(int, array_from_string(os.environ.get("REDASH_PAGE_SIZE_OPTIONS", "5,10,20,50,100")))

# Features:
VERSION_CHECK = parse_boolean(os.environ.get("REDASH_VERSION_CHECK", "true"))
FEATURE_DISABLE_REFRESH_QUERIES = parse_boolean(os.environ.get("REDASH_FEATURE_DISABLE_REFRESH_QUERIES", "false"))
FEATURE_SHOW_QUERY_RESULTS_COUNT = parse_boolean(os.environ.get("REDASH_FEATURE_SHOW_QUERY_RESULTS_COUNT", "true"))
FEATURE_ALLOW_CUSTOM_JS_VISUALIZATIONS = parse_boolean(os.environ.get("REDASH_FEATURE_ALLOW_CUSTOM_JS_VISUALIZATIONS", "false"))
FEATURE_AUTO_PUBLISH_NAMED_QUERIES = parse_boolean(os.environ.get("REDASH_FEATURE_AUTO_PUBLISH_NAMED_QUERIES", "true"))

# BigQuery
BIGQUERY_HTTP_TIMEOUT = int(os.environ.get("REDASH_BIGQUERY_HTTP_TIMEOUT", "600"))

# Enhance schema fetching
SCHEMA_RUN_TABLE_SIZE_CALCULATIONS = parse_boolean(os.environ.get("REDASH_SCHEMA_RUN_TABLE_SIZE_CALCULATIONS", "false"))

# Allow Parameters in Embeds
# WARNING: With this option enabled, Redash reads query parameters from the request URL (risk of SQL injection!)
ALLOW_PARAMETERS_IN_EMBEDS = parse_boolean(os.environ.get("REDASH_ALLOW_PARAMETERS_IN_EMBEDS", "false"))

# kylin
KYLIN_OFFSET = int(os.environ.get('REDASH_KYLIN_OFFSET', 0))
KYLIN_LIMIT = int(os.environ.get('REDASH_KYLIN_LIMIT', 50000))
KYLIN_ACCEPT_PARTIAL = parse_boolean(os.environ.get("REDASH_KYLIN_ACCEPT_PARTIAL", "false"))

# sqlparse
SQLPARSE_FORMAT_OPTIONS = {
    'reindent': parse_boolean(os.environ.get('SQLPARSE_FORMAT_REINDENT', 'true')),
    'keyword_case': os.environ.get('SQLPARSE_FORMAT_KEYWORD_CASE', 'upper'),
}
<EOF>
<BOF>
from __future__ import print_function
import os
from .helpers import parse_boolean

if os.environ.get("REDASH_SAML_LOCAL_METADATA_PATH") is not None:
    print("DEPRECATION NOTICE:\n")
    print("SAML_LOCAL_METADATA_PATH is no longer supported. Only URL metadata is supported now, please update")
    print("your configuration and reload.")
    raise SystemExit(1)


PASSWORD_LOGIN_ENABLED = parse_boolean(os.environ.get("REDASH_PASSWORD_LOGIN_ENABLED", "true"))

SAML_METADATA_URL = os.environ.get("REDASH_SAML_METADATA_URL", "")
SAML_ENTITY_ID = os.environ.get("REDASH_SAML_ENTITY_ID", "")
SAML_NAMEID_FORMAT = os.environ.get("REDASH_SAML_NAMEID_FORMAT", "")
SAML_LOGIN_ENABLED = SAML_METADATA_URL != ""

DATE_FORMAT = os.environ.get("REDASH_DATE_FORMAT", "DD/MM/YY")

JWT_LOGIN_ENABLED = parse_boolean(os.environ.get("REDASH_JWT_LOGIN_ENABLED", "false"))
JWT_AUTH_ISSUER = os.environ.get("REDASH_JWT_AUTH_ISSUER", "")
JWT_AUTH_PUBLIC_CERTS_URL = os.environ.get("REDASH_JWT_AUTH_PUBLIC_CERTS_URL", "")
JWT_AUTH_AUDIENCE = os.environ.get("REDASH_JWT_AUTH_AUDIENCE", "")
JWT_AUTH_ALGORITHMS = os.environ.get("REDASH_JWT_AUTH_ALGORITHMS", "HS256,RS256,ES256").split(',')
JWT_AUTH_COOKIE_NAME = os.environ.get("REDASH_JWT_AUTH_COOKIE_NAME", "")
JWT_AUTH_HEADER_NAME = os.environ.get("REDASH_JWT_AUTH_HEADER_NAME", "")

FEATURE_SHOW_PERMISSIONS_CONTROL = parse_boolean(os.environ.get("REDASH_FEATURE_SHOW_PERMISSIONS_CONTROL", "false"))

settings = {
    "auth_password_login_enabled": PASSWORD_LOGIN_ENABLED,
    "auth_saml_enabled": SAML_LOGIN_ENABLED,
    "auth_saml_entity_id": SAML_ENTITY_ID,
    "auth_saml_metadata_url": SAML_METADATA_URL,
    "auth_saml_nameid_format": SAML_NAMEID_FORMAT,
    "date_format": DATE_FORMAT,
    "auth_jwt_login_enabled": JWT_LOGIN_ENABLED,
    "auth_jwt_auth_issuer": JWT_AUTH_ISSUER,
    "auth_jwt_auth_public_certs_url": JWT_AUTH_PUBLIC_CERTS_URL,
    "auth_jwt_auth_audience": JWT_AUTH_AUDIENCE,
    "auth_jwt_auth_algorithms": JWT_AUTH_ALGORITHMS,
    "auth_jwt_auth_cookie_name": JWT_AUTH_COOKIE_NAME,
    "auth_jwt_auth_header_name": JWT_AUTH_HEADER_NAME,
    "feature_show_permissions_control": FEATURE_SHOW_PERMISSIONS_CONTROL,
}
<EOF>
<BOF>
from __future__ import print_function
from sys import exit

from click import BOOL, argument, option, prompt
from flask.cli import AppGroup
from six import string_types
from sqlalchemy.orm.exc import NoResultFound
from sqlalchemy.exc import IntegrityError

from redash import models
from redash.handlers.users import invite_user

manager = AppGroup(help="Users management commands.")


def build_groups(org, groups, is_admin):
    if isinstance(groups, string_types):
        groups = groups.split(',')
        groups.remove('')  # in case it was empty string
        groups = [int(g) for g in groups]

    if groups is None:
        groups = [org.default_group.id]

    if is_admin:
        groups += [org.admin_group.id]

    return groups


@manager.command()
@argument('email')
@option('--org', 'organization', default='default',
        help="the organization the user belongs to, (leave blank for "
        "'default').")
def grant_admin(email, organization='default'):
    """
    Grant admin access to user EMAIL.
    """
    try:
        org = models.Organization.get_by_slug(organization)
        admin_group = org.admin_group
        user = models.User.get_by_email_and_org(email, org)

        if admin_group.id in user.group_ids:
            print("User is already an admin.")
        else:
            user.group_ids = user.group_ids + [org.admin_group.id]
            models.db.session.add(user)
            models.db.session.commit()
            print("User updated.")
    except NoResultFound:
        print("User [%s] not found." % email)


@manager.command()
@argument('email')
@argument('name')
@option('--org', 'organization', default='default',
        help="The organization the user belongs to (leave blank for "
        "'default').")
@option('--admin', 'is_admin', is_flag=True, default=False,
        help="set user as admin")
@option('--google', 'google_auth', is_flag=True,
        default=False, help="user uses Google Auth to login")
@option('--password', 'password', default=None,
        help="Password for users who don't use Google Auth "
        "(leave blank for prompt).")
@option('--groups', 'groups', default=None,
        help="Comma separated list of groups (leave blank for "
        "default).")
def create(email, name, groups, is_admin=False, google_auth=False,
           password=None, organization='default'):
    """
    Create user EMAIL with display name NAME.
    """
    print("Creating user (%s, %s) in organization %s..." % (email, name,
                                                            organization))
    print("Admin: %r" % is_admin)
    print("Login with Google Auth: %r\n" % google_auth)

    org = models.Organization.get_by_slug(organization)
    groups = build_groups(org, groups, is_admin)

    user = models.User(org=org, email=email, name=name, group_ids=groups)
    if not password and not google_auth:
        password = prompt("Password", hide_input=True,
                          confirmation_prompt=True)
    if not google_auth:
        user.hash_password(password)

    try:
        models.db.session.add(user)
        models.db.session.commit()
    except Exception as e:
        print("Failed creating user: %s" % e.message)
        exit(1)


@manager.command()
@argument('email')
@argument('name')
@option('--org', 'organization', default='default',
        help="The organization the root user belongs to (leave blank for 'default').")
@option('--google', 'google_auth', is_flag=True,
        default=False, help="user uses Google Auth to login")
@option('--password', 'password', default=None,
        help="Password for root user who don't use Google Auth "
        "(leave blank for prompt).")
def create_root(email, name, google_auth=False, password=None, organization='default'):
    """
    Create root user.
    """
    print("Creating root user (%s, %s) in organization %s..." % (email, name, organization))
    print("Login with Google Auth: %r\n" % google_auth)

    user = models.User.query.filter(models.User.email == email).first()
    if user is not None:
        print("User [%s] is already exists." % email)
        exit(1)

    slug = 'default'
    default_org = models.Organization.query.filter(models.Organization.slug == slug).first()
    if default_org is None:
        default_org = models.Organization(name=organization, slug=slug, settings={})

    admin_group = models.Group(name='admin', permissions=['admin', 'super_admin'],
                               org=default_org, type=models.Group.BUILTIN_GROUP)
    default_group = models.Group(name='default', permissions=models.Group.DEFAULT_PERMISSIONS,
                                 org=default_org, type=models.Group.BUILTIN_GROUP)

    models.db.session.add_all([default_org, admin_group, default_group])
    models.db.session.commit()

    user = models.User(org=default_org, email=email, name=name,
                       group_ids=[admin_group.id, default_group.id])
    if not google_auth:
        user.hash_password(password)

    try:
        models.db.session.add(user)
        models.db.session.commit()
    except Exception as e:
        print("Failed creating root user: %s" % e.message)
        exit(1)


@manager.command()
@argument('email')
@option('--org', 'organization', default=None,
        help="The organization the user belongs to (leave blank for all"
        " organizations).")
def delete(email, organization=None):
    """
    Delete user EMAIL.
    """
    if organization:
        org = models.Organization.get_by_slug(organization)
        deleted_count = models.User.query.filter(
            models.User.email == email,
            models.User.org == org.id,
        ).delete()
    else:
        deleted_count = models.User.query.filter(models.User.email == email).delete(
            synchronize_session=False)
    models.db.session.commit()
    print("Deleted %d users." % deleted_count)


@manager.command()
@argument('email')
@argument('password')
@option('--org', 'organization', default=None,
        help="The organization the user belongs to (leave blank for all "
        "organizations).")
def password(email, password, organization=None):
    """
    Resets password for EMAIL to PASSWORD.
    """
    if organization:
        org = models.Organization.get_by_slug(organization)
        user = models.User.query.filter(
            models.User.email == email,
            models.User.org == org,
        ).first()
    else:
        user = models.User.query.filter(models.User.email == email).first()

    if user is not None:
        user.hash_password(password)
        models.db.session.add(user)
        models.db.session.commit()
        print("User updated.")
    else:
        print("User [%s] not found." % email)
        exit(1)


@manager.command()
@argument('email')
@argument('name')
@argument('inviter_email')
@option('--org', 'organization', default='default',
        help="The organization the user belongs to (leave blank for 'default')")
@option('--admin', 'is_admin', type=BOOL, default=False,
        help="set user as admin")
@option('--groups', 'groups', default=None,
        help="Comma seperated list of groups (leave blank for default).")
def invite(email, name, inviter_email, groups, is_admin=False,
           organization='default'):
    """
    Sends an invitation to the given NAME and EMAIL from INVITER_EMAIL.
    """
    org = models.Organization.get_by_slug(organization)
    groups = build_groups(org, groups, is_admin)
    try:
        user_from = models.User.get_by_email_and_org(inviter_email, org)
        user = models.User(org=org, name=name, email=email, group_ids=groups)
        models.db.session.add(user)
        try:
            models.db.session.commit()
            invite_user(org, user_from, user)
            print("An invitation was sent to [%s] at [%s]." % (name, email))
        except IntegrityError as e:
            if "email" in e.message:
                print("Cannot invite. User already exists [%s]" % email)
            else:
                print(e)
    except NoResultFound:
        print("The inviter [%s] was not found." % inviter_email)


@manager.command()
@option('--org', 'organization', default=None,
        help="The organization the user belongs to (leave blank for all"
        " organizations)")
def list(organization=None):
    """List all users"""
    if organization:
        org = models.Organization.get_by_slug(organization)
        users = models.User.query.filter(models.User.org == org)
    else:
        users = models.User.query
    for i, user in enumerate(users.order_by(models.User.name)):
        if i > 0:
            print("-" * 20)

        print("Id: {}\nName: {}\nEmail: {}\nOrganization: {}\nActive: {}".format(
            user.id, user.name.encode('utf-8'), user.email, user.org.name, not(user.is_disabled)))

        groups = models.Group.query.filter(models.Group.id.in_(user.group_ids)).all()
        group_names = [group.name for group in groups]
        print("Groups: {}".format(", ".join(group_names)))
<EOF>
<BOF>
from __future__ import print_function
from sys import exit

import click
from flask.cli import AppGroup
from six import text_type
from sqlalchemy.orm.exc import NoResultFound

from redash import models
from redash.query_runner import (get_configuration_schema_for_query_runner_type,
                                 query_runners)
from redash.utils import json_loads
from redash.utils.configuration import ConfigurationContainer

manager = AppGroup(help="Data sources management commands.")


@manager.command()
@click.option('--org', 'organization', default=None,
              help="The organization the user belongs to (leave blank for "
              "all organizations).")
def list(organization=None):
    """List currently configured data sources."""
    if organization:
        org = models.Organization.get_by_slug(organization)
        data_sources = models.DataSource.query.filter(
            models.DataSource.org == org)
    else:
        data_sources = models.DataSource.query
    for i, ds in enumerate(data_sources.order_by(models.DataSource.name)):
        if i > 0:
            print("-" * 20)

        print("Id: {}\nName: {}\nType: {}\nOptions: {}".format(
            ds.id, ds.name, ds.type, ds.options.to_json()))


def validate_data_source_type(type):
    if type not in query_runners.keys():
        print ("Error: the type \"{}\" is not supported (supported types: {})."
               .format(type, ", ".join(query_runners.keys())))
        print("OJNK")
        exit(1)


@manager.command()
@click.argument('name')
@click.option('--org', 'organization', default='default',
              help="The organization the user belongs to "
              "(leave blank for 'default').")
def test(name, organization='default'):
    """Test connection to data source by issuing a trivial query."""
    try:
        org = models.Organization.get_by_slug(organization)
        data_source = models.DataSource.query.filter(
            models.DataSource.name == name,
            models.DataSource.org == org).one()
        print("Testing connection to data source: {} (id={})".format(
            name, data_source.id))
        try:
            data_source.query_runner.test_connection()
        except Exception as e:
            print("Failure: {}".format(e))
            exit(1)
        else:
            print("Success")
    except NoResultFound:
        print("Couldn't find data source named: {}".format(name))
        exit(1)


@manager.command()
@click.argument('name', default=None, required=False)
@click.option('--type', default=None,
              help="new type for the data source")
@click.option('--options', default=None,
              help="updated options for the data source")
@click.option('--org', 'organization', default='default',
              help="The organization the user belongs to (leave blank for "
              "'default').")
def new(name=None, type=None, options=None, organization='default'):
    """Create new data source."""

    if name is None:
        name = click.prompt("Name")

    if type is None:
        print("Select type:")
        for i, query_runner_name in enumerate(query_runners.keys()):
            print("{}. {}".format(i + 1, query_runner_name))

        idx = 0
        while idx < 1 or idx > len(query_runners.keys()):
            idx = click.prompt("[{}-{}]".format(1, len(query_runners.keys())),
                               type=int)

        type = query_runners.keys()[idx - 1]
    else:
        validate_data_source_type(type)

    query_runner = query_runners[type]
    schema = query_runner.configuration_schema()

    if options is None:
        types = {
            'string': text_type,
            'number': int,
            'boolean': bool
        }

        options_obj = {}

        for k, prop in schema['properties'].iteritems():
            required = k in schema.get('required', [])
            default_value = "<<DEFAULT_VALUE>>"
            if required:
                default_value = None

            prompt = prop.get('title', k.capitalize())
            if required:
                prompt = "{} (required)".format(prompt)
            else:
                prompt = "{} (optional)".format(prompt)

            value = click.prompt(prompt, default=default_value,
                                 type=types[prop['type']], show_default=False)
            if value != default_value:
                options_obj[k] = value

        options = ConfigurationContainer(options_obj, schema)
    else:
        options = ConfigurationContainer(json_loads(options), schema)

    if not options.is_valid():
        print("Error: invalid configuration.")
        exit()

    print("Creating {} data source ({}) with options:\n{}".format(
        type, name, options.to_json()))

    data_source = models.DataSource.create_with_group(
        name=name, type=type, options=options,
        org=models.Organization.get_by_slug(organization))
    models.db.session.commit()
    print("Id: {}".format(data_source.id))


@manager.command()
@click.argument('name')
@click.option('--org', 'organization', default='default',
              help="The organization the user belongs to (leave blank for "
              "'default').")
def delete(name, organization='default'):
    """Delete data source by name."""
    try:
        org = models.Organization.get_by_slug(organization)
        data_source = models.DataSource.query.filter(
            models.DataSource.name == name,
            models.DataSource.org == org).one()
        print("Deleting data source: {} (id={})".format(name, data_source.id))
        models.db.session.delete(data_source)
        models.db.session.commit()
    except NoResultFound:
        print("Couldn't find data source named: {}".format(name))
        exit(1)


def update_attr(obj, attr, new_value):
    if new_value is not None:
        old_value = getattr(obj, attr)
        print("Updating {}: {} -> {}".format(attr, old_value, new_value))
        setattr(obj, attr, new_value)


@manager.command()
@click.argument('name')
@click.option('--name', 'new_name', default=None,
              help="new name for the data source")
@click.option('--options', default=None,
              help="updated options for the data source")
@click.option('--type', default=None,
              help="new type for the data source")
@click.option('--org', 'organization', default='default',
              help="The organization the user belongs to (leave blank for "
              "'default').")
def edit(name, new_name=None, options=None, type=None, organization='default'):
    """Edit data source settings (name, options, type)."""
    try:
        if type is not None:
            validate_data_source_type(type)
        org = models.Organization.get_by_slug(organization)
        data_source = models.DataSource.query.filter(
            models.DataSource.name == name,
            models.DataSource.org == org).one()
        update_attr(data_source, "name", new_name)
        update_attr(data_source, "type", type)

        if options is not None:
            schema = get_configuration_schema_for_query_runner_type(
                data_source.type)
            options = json_loads(options)
            data_source.options.set_schema(schema)
            data_source.options.update(options)

        models.db.session.add(data_source)
        models.db.session.commit()

    except NoResultFound:
        print("Couldn't find data source named: {}".format(name))
<EOF>
<BOF>
from __future__ import print_function
import click
import simplejson
from flask.cli import FlaskGroup, run_command
from flask import current_app

from redash import create_app, settings, __version__
from redash.cli import users, groups, database, data_sources, organization
from redash.monitor import get_status


def create(group):
    app = current_app or create_app()
    group.app = app

    @app.shell_context_processor
    def shell_context():
        from redash import models
        return dict(models=models)

    return app


@click.group(cls=FlaskGroup, create_app=create)
def manager():
    """Management script for Redash"""


manager.add_command(database.manager, "database")
manager.add_command(users.manager, "users")
manager.add_command(groups.manager, "groups")
manager.add_command(data_sources.manager, "ds")
manager.add_command(organization.manager, "org")
manager.add_command(run_command, "runserver")


@manager.command()
def version():
    """Displays Redash version."""
    print(__version__)


@manager.command()
def status():
    print(simplejson.dumps(get_status(), indent=2))


@manager.command()
def check_settings():
    """Show the settings as Redash sees them (useful for debugging)."""
    for name, item in settings.all_settings().iteritems():
        print("{} = {}".format(name, item))


@manager.command()
@click.argument('email', default=settings.MAIL_DEFAULT_SENDER, required=False)
def send_test_mail(email=None):
    """
    Send test message to EMAIL (default: the address you defined in MAIL_DEFAULT_SENDER)
    """
    from redash import mail
    from flask_mail import Message

    if email is None:
        email = settings.MAIL_DEFAULT_SENDER

    mail.send(Message(subject="Test Message from Redash", recipients=[email],
                      body="Test message."))


@manager.command()
def ipython():
    """Starts IPython shell instead of the default Python shell."""
    import sys
    import IPython
    from flask.globals import _app_ctx_stack
    app = _app_ctx_stack.top.app

    banner = 'Python %s on %s\nIPython: %s\nRedash version: %s\n' % (
        sys.version,
        sys.platform,
        IPython.__version__,
        __version__
    )

    ctx = {}
    ctx.update(app.make_shell_context())

    IPython.embed(banner1=banner, user_ns=ctx)
<EOF>
<BOF>
from __future__ import print_function
from click import argument
from flask.cli import AppGroup

from redash import models

manager = AppGroup(help="Organization management commands.")


@manager.command()
@argument('domains')
def set_google_apps_domains(domains):
    """
    Sets the allowable domains to the comma separated list DOMAINS.
    """
    organization = models.Organization.query.first()
    k = models.Organization.SETTING_GOOGLE_APPS_DOMAINS
    organization.settings[k] = domains.split(',')
    models.db.session.add(organization)
    models.db.session.commit()
    print("Updated list of allowed domains to: {}".format(
        organization.google_apps_domains))


@manager.command()
def show_google_apps_domains():
    organization = models.Organization.query.first()
    print("Current list of Google Apps domains: {}".format(
        ', '.join(organization.google_apps_domains)))


@manager.command()
def list():
    """List all organizations"""
    orgs = models.Organization.query
    for i, org in enumerate(orgs.order_by(models.Organization.name)):
        if i > 0:
            print("-" * 20)

        print("Id: {}\nName: {}\nSlug: {}".format(org.id, org.name, org.slug))
<EOF>
<BOF>
from __future__ import print_function
from sys import exit

from sqlalchemy.orm.exc import NoResultFound
from flask.cli import AppGroup
from click import argument, option

from redash import models

manager = AppGroup(help="Groups management commands.")


@manager.command()
@argument('name')
@option('--org', 'organization', default='default',
        help="The organization the user belongs to (leave blank for "
        "'default').")
@option('--permissions', default=None,
        help="Comma separated list of permissions ('create_dashboard',"
        " 'create_query', 'edit_dashboard', 'edit_query', "
        "'view_query', 'view_source', 'execute_query', 'list_users',"
        " 'schedule_query', 'list_dashboards', 'list_alerts',"
        " 'list_data_sources') (leave blank for default).")
def create(name, permissions=None, organization='default'):
    print("Creating group (%s)..." % (name))

    org = models.Organization.get_by_slug(organization)

    permissions = extract_permissions_string(permissions)

    print("permissions: [%s]" % ",".join(permissions))

    try:
        models.db.session.add(models.Group(
            name=name, org=org,
            permissions=permissions))
        models.db.session.commit()
    except Exception as e:
        print("Failed create group: %s" % e.message)
        exit(1)


@manager.command()
@argument('group_id')
@option('--permissions', default=None,
        help="Comma separated list of permissions ('create_dashboard',"
        " 'create_query', 'edit_dashboard', 'edit_query',"
        " 'view_query', 'view_source', 'execute_query', 'list_users',"
        " 'schedule_query', 'list_dashboards', 'list_alerts',"
        " 'list_data_sources') (leave blank for default).")
def change_permissions(group_id, permissions=None):
    print("Change permissions of group %s ..." % group_id)

    try:
        group = models.Group.query.get(group_id)
    except NoResultFound:
        print("User [%s] not found." % group_id)
        exit(1)

    permissions = extract_permissions_string(permissions)
    print("current permissions [%s] will be modify to [%s]" % (
        ",".join(group.permissions), ",".join(permissions)))

    group.permissions = permissions

    try:
        models.db.session.add(group)
        models.db.session.commit()
    except Exception as e:
        print("Failed change permission: %s" % e.message)
        exit(1)


def extract_permissions_string(permissions):
    if permissions is None:
        permissions = models.Group.DEFAULT_PERMISSIONS
    else:
        permissions = permissions.split(',')
        permissions = [p.strip() for p in permissions]
    return permissions


@manager.command()
@option('--org', 'organization', default=None,
        help="The organization to limit to (leave blank for all).")
def list(organization=None):
    """List all groups"""
    if organization:
        org = models.Organization.get_by_slug(organization)
        groups = models.Group.query.filter(models.Group.org == org)
    else:
        groups = models.Group.query

    for i, group in enumerate(groups.order_by(models.Group.name)):
        if i > 0:
            print("-" * 20)

        print("Id: {}\nName: {}\nType: {}\nOrganization: {}\nPermissions: [{}]".format(
            group.id, group.name, group.type, group.org.slug, ",".join(group.permissions)))

        members = models.Group.members(group.id)
        user_names = [m.name for m in members]
        print("Users: {}".format(", ".join(user_names)))
<EOF>
<BOF>
import time

from flask.cli import AppGroup
from flask_migrate import stamp
from sqlalchemy.exc import DatabaseError

manager = AppGroup(help="Manage the database (create/drop tables).")


def _wait_for_db_connection(db):
    retried = False
    while not retried:
        try:
            db.engine.execute('SELECT 1;')
            return
        except DatabaseError:
            time.sleep(30)

        retried = True


@manager.command()
def create_tables():
    """Create the database tables."""
    from redash.models import db

    _wait_for_db_connection(db)
    db.create_all()

    # Need to mark current DB as up to date
    stamp()


@manager.command()
def drop_tables():
    """Drop the database tables."""
    from redash.models import db

    _wait_for_db_connection(db)
    db.drop_all()
<EOF>
<BOF>
import jsonschema
from jsonschema import ValidationError
from sqlalchemy.ext.mutable import Mutable

from redash.utils import json_dumps, json_loads

SECRET_PLACEHOLDER = '--------'


class ConfigurationContainer(Mutable):
    @classmethod
    def coerce(cls, key, value):
        if not isinstance(value, ConfigurationContainer):
            if isinstance(value, dict):
                return ConfigurationContainer(value)

            # this call will raise ValueError
            return Mutable.coerce(key, value)
        else:
            return value

    def __init__(self, config, schema=None):
        self._config = config
        self.set_schema(schema)

    def set_schema(self, schema):
        self._schema = schema

    @property
    def schema(self):
        if self._schema is None:
            raise RuntimeError("Schema missing.")

        return self._schema

    def is_valid(self):
        try:
            self.validate()
        except (ValidationError, ValueError):
            return False

        return True

    def validate(self):
        jsonschema.validate(self._config, self._schema)

    def to_json(self):
        return json_dumps(self._config, sort_keys=True)

    def iteritems(self):
        return self._config.iteritems()

    def to_dict(self, mask_secrets=False):
        if mask_secrets is False or 'secret' not in self.schema:
            return self._config

        config = self._config.copy()
        for key in config:
            if key in self.schema['secret']:
                config[key] = SECRET_PLACEHOLDER

        return config

    def update(self, new_config):
        jsonschema.validate(new_config, self.schema)

        config = {}
        for k, v in new_config.iteritems():
            if k in self.schema.get('secret', []) and v == SECRET_PLACEHOLDER:
                config[k] = self[k]
            else:
                config[k] = v

        self._config = config
        self.changed()

    def get(self, *args, **kwargs):
        return self._config.get(*args, **kwargs)

    def __setitem__(self, key, value):
        self._config[key] = value
        self.changed()

    def __getitem__(self, item):
        if item in self._config:
            return self._config[item]

        raise KeyError(item)

    def __contains__(self, item):
        return item in self._config

    @classmethod
    def from_json(cls, config_in_json):
        return cls(json_loads(config_in_json))
<EOF>
<BOF>
import cStringIO
import csv
import codecs
import datetime
import decimal
import hashlib
import os
import random
import re
import uuid

import pystache
import pytz
import simplejson
from funcy import distinct, select_values
from six import string_types
from sqlalchemy.orm.query import Query

from .human_time import parse_human_time
from redash import settings

COMMENTS_REGEX = re.compile("/\*.*?\*/")
WRITER_ENCODING = os.environ.get('REDASH_CSV_WRITER_ENCODING', 'utf-8')
WRITER_ERRORS = os.environ.get('REDASH_CSV_WRITER_ERRORS', 'strict')


def utcnow():
    """Return datetime.now value with timezone specified.

    Without the timezone data, when the timestamp stored to the database it gets the current timezone of the server,
    which leads to errors in calculations.
    """
    return datetime.datetime.now(pytz.utc)


def dt_from_timestamp(timestamp, tz_aware=True):
    timestamp = datetime.datetime.utcfromtimestamp(float(timestamp))

    if tz_aware:
        timestamp = timestamp.replace(tzinfo=pytz.utc)

    return timestamp


def slugify(s):
    return re.sub('[^a-z0-9_\-]+', '-', s.lower())


def gen_query_hash(sql):
    """Return hash of the given query after stripping all comments, line breaks
    and multiple spaces, and lower casing all text.

    TODO: possible issue - the following queries will get the same id:
        1. SELECT 1 FROM table WHERE column='Value';
        2. SELECT 1 FROM table where column='value';
    """
    sql = COMMENTS_REGEX.sub("", sql)
    sql = "".join(sql.split()).lower()
    return hashlib.md5(sql.encode('utf-8')).hexdigest()


def generate_token(length):
    chars = ('abcdefghijklmnopqrstuvwxyz'
             'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
             '0123456789')

    rand = random.SystemRandom()
    return ''.join(rand.choice(chars) for x in range(length))


class JSONEncoder(simplejson.JSONEncoder):
    """Adapter for `simplejson.dumps`."""

    def default(self, o):
        # Some SQLAlchemy collections are lazy.
        if isinstance(o, Query):
            return list(o)
        elif isinstance(o, decimal.Decimal):
            return float(o)
        elif isinstance(o, (datetime.timedelta, uuid.UUID)):
            return str(o)
        elif isinstance(o, (datetime.date, datetime.time)):
            return o.isoformat()
        else:
            return super(JSONEncoder, self).default(o)


def json_loads(data, *args, **kwargs):
    """A custom JSON loading function which passes all parameters to the
    simplejson.loads function."""
    return simplejson.loads(data, *args, **kwargs)


def json_dumps(data, *args, **kwargs):
    """A custom JSON dumping function which passes all parameters to the
    simplejson.dumps function."""
    kwargs.setdefault('cls', JSONEncoder)
    return simplejson.dumps(data, *args, **kwargs)


def mustache_render(template, context=None, **kwargs):
    renderer = pystache.Renderer(escape=lambda u: u)
    return renderer.render(template, context, **kwargs)


def build_url(request, host, path):
    parts = request.host.split(':')
    if len(parts) > 1:
        port = parts[1]
        if (port, request.scheme) not in (('80', 'http'), ('443', 'https')):
            host = '{}:{}'.format(host, port)

    return "{}://{}{}".format(request.scheme, host, path)


class UnicodeWriter:
    """
    A CSV writer which will write rows to CSV file "f",
    which is encoded in the given encoding.
    """

    def __init__(self, f, dialect=csv.excel, encoding=WRITER_ENCODING, **kwds):
        # Redirect output to a queue
        self.queue = cStringIO.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f
        self.encoder = codecs.getincrementalencoder(encoding)()

    def _encode_utf8(self, val):
        if isinstance(val, string_types):
            return val.encode(WRITER_ENCODING, WRITER_ERRORS)

        return val

    def writerow(self, row):
        self.writer.writerow([self._encode_utf8(s) for s in row])
        # Fetch UTF-8 output from the queue ...
        data = self.queue.getvalue()
        data = data.decode(WRITER_ENCODING)
        # ... and reencode it into the target encoding
        data = self.encoder.encode(data)
        # write to the target stream
        self.stream.write(data)
        # empty queue
        self.queue.truncate(0)

    def writerows(self, rows):
        for row in rows:
            self.writerow(row)


def _collect_key_names(nodes):
    keys = []
    for node in nodes._parse_tree:
        if isinstance(node, pystache.parser._EscapeNode):
            keys.append(node.key)
        elif isinstance(node, pystache.parser._SectionNode):
            keys.append(node.key)
            keys.extend(_collect_key_names(node.parsed))

    return distinct(keys)


def collect_query_parameters(query):
    nodes = pystache.parse(query)
    keys = _collect_key_names(nodes)
    return keys


def collect_parameters_from_request(args):
    parameters = {}

    for k, v in args.iteritems():
        if k.startswith('p_'):
            parameters[k[2:]] = v

    return parameters


def base_url(org):
    if settings.MULTI_ORG:
        return "https://{}/{}".format(settings.HOST, org.slug)

    return settings.HOST


def filter_none(d):
    return select_values(lambda v: v is not None, d)
<EOF>
<BOF>
import parsedatetime
from time import mktime
from datetime import datetime

cal = parsedatetime.Calendar()


def parse_human_time(s):
    time_struct, _ = cal.parse(s)
    return datetime.fromtimestamp(mktime(time_struct))
<EOF>
<BOF>
import logging
import requests

from redash.destinations import *
from redash.models import Alert
from redash.utils import json_dumps


colors = {
    Alert.OK_STATE: 'green',
    Alert.TRIGGERED_STATE: 'red',
    Alert.UNKNOWN_STATE: 'yellow'
}


class HipChat(BaseDestination):
    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                    "title": "HipChat Notification URL (get it from the Integrations page)"
                },
            },
            "required": ["url"]
        }

    @classmethod
    def icon(cls):
        return 'fa-comment-o'

    def notify(self, alert, query, user, new_state, app, host, options):
        try:
            alert_url = '{host}/alerts/{alert_id}'.format(host=host, alert_id=alert.id)
            query_url = '{host}/queries/{query_id}'.format(host=host, query_id=query.id)

            message = u'<a href="{alert_url}">{alert_name}</a> changed state to {new_state} (based on <a href="{query_url}">this query</a>).'.format(
                alert_name=alert.name, new_state=new_state.upper(),
                alert_url=alert_url,
                query_url=query_url)

            data = {
                'message': message,
                'color': colors.get(new_state, 'green')
            }
            headers = {'Content-Type': 'application/json'}
            response = requests.post(options['url'], data=json_dumps(data), headers=headers, timeout=5.0)

            if response.status_code != 204:
                logging.error('Bad status code received from HipChat: %d', response.status_code)
        except Exception:
            logging.exception("HipChat Send ERROR.")


register(HipChat)
<EOF>
<BOF>
import logging

logger = logging.getLogger(__name__)

__all__ = [
    'BaseDestination',
    'register',
    'get_destination',
    'import_destinations'
]


class BaseDestination(object):
    def __init__(self, configuration):
        self.configuration = configuration

    @classmethod
    def name(cls):
        return cls.__name__

    @classmethod
    def type(cls):
        return cls.__name__.lower()

    @classmethod
    def icon(cls):
        return 'fa-bullseye'

    @classmethod
    def enabled(cls):
        return True

    @classmethod
    def configuration_schema(cls):
        return {}

    def notify(self, alert, query, user, new_state, app, host, options):
        raise NotImplementedError()

    @classmethod
    def to_dict(cls):
        return {
            'name': cls.name(),
            'type': cls.type(),
            'icon': cls.icon(),
            'configuration_schema': cls.configuration_schema()
        }


destinations = {}


def register(destination_class):
    global destinations
    if destination_class.enabled():
        logger.debug("Registering %s (%s) destinations.", destination_class.name(), destination_class.type())
        destinations[destination_class.type()] = destination_class
    else:
        logger.warning("%s destination enabled but not supported, not registering. Either disable or install missing dependencies.", destination_class.name())


def get_destination(destination_type, configuration):
    destination_class = destinations.get(destination_type, None)
    if destination_class is None:
        return None
    return destination_class(configuration)


def get_configuration_schema_for_destination_type(destination_type):
    destination_class = destinations.get(destination_type, None)
    if destination_class is None:
        return None

    return destination_class.configuration_schema()


def import_destinations(destination_imports):
    for destination_import in destination_imports:
        __import__(destination_import)
<EOF>
<BOF>
import logging
import requests

from redash.destinations import *
from redash.utils import json_dumps


class Mattermost(BaseDestination):
    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'url': {
                    'type': 'string',
                    'title': 'Mattermost Webhook URL'
                },
                'username': {
                    'type': 'string',
                    'title': 'Username'
                },
                'icon_url': {
                    'type': 'string',
                    'title': 'Icon (URL)'
                },
                'channel': {
                    'type': 'string',
                    'title': 'Channel'
                }
            }
        }

    @classmethod
    def icon(cls):
        return 'fa-bolt'

    def notify(self, alert, query, user, new_state, app, host, options):
        if new_state == "triggered":
            text = "####" + alert.name + " just triggered"
        else:
            text = "####" + alert.name + " went back to normal"

        payload = {'text': text}
        if options.get('username'): payload['username'] = options.get('username')
        if options.get('icon_url'): payload['icon_url'] = options.get('icon_url')
        if options.get('channel'): payload['channel'] = options.get('channel')

        try:
            resp = requests.post(options.get('url'), data=json_dumps(payload), timeout=5.0)
            logging.warning(resp.text)

            if resp.status_code != 200:
                logging.error("Mattermost webhook send ERROR. status_code => {status}".format(status=resp.status_code))
        except Exception:
            logging.exception("Mattermost webhook send ERROR.")


register(Mattermost)
<EOF>
<BOF>
import logging
import requests

from redash.destinations import *


class ChatWork(BaseDestination):
    ALERTS_DEFAULT_MESSAGE_TEMPLATE = u'{alert_name} changed state to {new_state}.\\n{alert_url}\\n{query_url}'

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'api_token': {
                    'type': 'string',
                    'title': 'API Token'
                },
                'room_id': {
                    'type': 'string',
                    'title': 'Room ID'
                },
                'message_template': {
                    'type': 'string',
                    'default': ChatWork.ALERTS_DEFAULT_MESSAGE_TEMPLATE,
                    'title': 'Message Template'
                }
            },
            'required': ['message_template', 'api_token', 'room_id']
        }

    @classmethod
    def icon(cls):
        return 'fa-comment'

    def notify(self, alert, query, user, new_state, app, host, options):
        try:
            # Documentation: http://developer.chatwork.com/ja/endpoint_rooms.html#POST-rooms-room_id-messages
            url = 'https://api.chatwork.com/v2/rooms/{room_id}/messages'.format(room_id=options.get('room_id'))

            alert_url = '{host}/alerts/{alert_id}'.format(host=host, alert_id=alert.id)
            query_url = '{host}/queries/{query_id}'.format(host=host, query_id=query.id)

            message_template = options.get('message_template', ChatWork.ALERTS_DEFAULT_MESSAGE_TEMPLATE)

            message = message_template.replace('\\n', '\n').format(
                alert_name=alert.name, new_state=new_state.upper(),
                alert_url=alert_url,
                query_url=query_url)

            headers = {'X-ChatWorkToken': options.get('api_token')}
            payload = {'body': message}

            resp = requests.post(url, headers=headers, data=payload, timeout=5.0)
            logging.warning(resp.text)
            if resp.status_code != 200:
                logging.error('ChatWork send ERROR. status_code => {status}'.format(status=resp.status_code))
        except Exception:
            logging.exception('ChatWork send ERROR.')

register(ChatWork)
<EOF>
<BOF>
import logging
import requests
from requests.auth import HTTPBasicAuth

from redash.destinations import *
from redash.utils import json_dumps
from redash.serializers import serialize_alert


class Webhook(BaseDestination):
    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "url": {
                    "type": "string",
                },
                "username": {
                    "type": "string"
                },
                "password": {
                    "type": "string"
                }
            },
            "required": ["url"],
            "secret": ["password"]
        }

    @classmethod
    def icon(cls):
        return 'fa-bolt'

    def notify(self, alert, query, user, new_state, app, host, options):
        try:
            data = {
                'event': 'alert_state_change',
                'alert': serialize_alert(alert, full=False),
                'url_base': host 
            }
            headers = {'Content-Type': 'application/json'}
            auth = HTTPBasicAuth(options.get('username'), options.get('password')) if options.get('username') else None
            resp = requests.post(options.get('url'), data=json_dumps(data), auth=auth, headers=headers, timeout=5.0)
            if resp.status_code != 200:
                logging.error("webhook send ERROR. status_code => {status}".format(status=resp.status_code))
        except Exception:
            logging.exception("webhook send ERROR.")


register(Webhook)
<EOF>
<BOF>
import logging

from flask_mail import Message
from redash import mail, settings
from redash.destinations import *


class Email(BaseDestination):

    @classmethod
    def configuration_schema(cls):
        return {
            "type": "object",
            "properties": {
                "addresses": {
                    "type": "string"
                },
                "subject_template": {
                    "type": "string",
                    "default": settings.ALERTS_DEFAULT_MAIL_SUBJECT_TEMPLATE,
                    "title": "Subject Template"
                }
            },
            "required": ["addresses"]
        }

    @classmethod
    def icon(cls):
        return 'fa-envelope'

    def notify(self, alert, query, user, new_state, app, host, options):
        recipients = [email for email in options.get('addresses', '').split(',') if email]

        if not recipients:
            logging.warning("No emails given. Skipping send.")

        html = """
        Check <a href="{host}/alerts/{alert_id}">alert</a> / check <a href="{host}/queries/{query_id}">query</a>.
        """.format(host=host, alert_id=alert.id, query_id=query.id)
        logging.debug("Notifying: %s", recipients)

        try:
            alert_name = alert.name.encode('utf-8', 'ignore')
            state = new_state.upper()
            subject_template = options.get('subject_template', settings.ALERTS_DEFAULT_MAIL_SUBJECT_TEMPLATE)
            message = Message(
                recipients=recipients,
                subject=subject_template.format(alert_name=alert_name, state=state),
                html=html
            )
            mail.send(message)
        except Exception:
            logging.exception("Mail send error.")

register(Email)
<EOF>
<BOF>
import logging
import requests

from redash.destinations import *
from redash.utils import json_dumps


class Slack(BaseDestination):
    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'url': {
                    'type': 'string',
                    'title': 'Slack Webhook URL'
                },
                'username': {
                    'type': 'string',
                    'title': 'Username'
                },
                'icon_emoji': {
                    'type': 'string',
                    'title': 'Icon (Emoji)'
                },
                'icon_url': {
                    'type': 'string',
                    'title': 'Icon (URL)'
                },
                'channel': {
                    'type': 'string',
                    'title': 'Channel'
                }
            }
        }

    @classmethod
    def icon(cls):
        return 'fa-slack'

    def notify(self, alert, query, user, new_state, app, host, options):
        # Documentation: https://api.slack.com/docs/attachments
        fields = [
            {
                "title": "Query",
                "value": "{host}/queries/{query_id}".format(host=host, query_id=query.id),
                "short": True
            },
            {
                "title": "Alert",
                "value": "{host}/alerts/{alert_id}".format(host=host, alert_id=alert.id),
                "short": True
            }
        ]
        if new_state == "triggered":
            text = alert.name + " just triggered"
            color = "#c0392b"
        else:
            text = alert.name + " went back to normal"
            color = "#27ae60"
        
        payload = {'attachments': [{'text': text, 'color': color, 'fields': fields}]}

        if options.get('username'): payload['username'] = options.get('username')
        if options.get('icon_emoji'): payload['icon_emoji'] = options.get('icon_emoji')
        if options.get('icon_url'): payload['icon_url'] = options.get('icon_url')
        if options.get('channel'): payload['channel'] = options.get('channel')

        try:
            resp = requests.post(options.get('url'), data=json_dumps(payload), timeout=5.0)
            logging.warning(resp.text)
            if resp.status_code != 200:
                logging.error("Slack send ERROR. status_code => {status}".format(status=resp.status_code))
        except Exception:
            logging.exception("Slack send ERROR.")

register(Slack)
<EOF>
<BOF>
import logging
from redash.destinations import *

enabled = True

try:
    import pypd
except ImportError:
    enabled = False


class PagerDuty(BaseDestination):

    KEY_STRING = '{alert_id}_{query_id}'
    DESCRIPTION_STR = 'Alert - Redash Query #{query_id}: {query_name}'

    @classmethod
    def enabled(cls):
        return enabled

    @classmethod
    def configuration_schema(cls):
        return {
            'type': 'object',
            'properties': {
                'integration_key': {
                    'type': 'string',
                    'title': 'PagerDuty Service Integration Key'
                },
                'description': {
                    'type': 'string',
                    'title': 'Description for the event, defaults to query',
                }
            },
            "required": ["integration_key"]
        }

    @classmethod
    def icon(cls):
        return 'creative-commons-pd-alt'

    def notify(self, alert, query, user, new_state, app, host, options):

        default_desc = self.DESCRIPTION_STR.format(query_id=query.id, query_name=query.name)

        if options.get('description'):
            default_desc = options.get('description')

        incident_key = self.KEY_STRING.format(alert_id=alert.id, query_id=query.id)
        data = {
            'routing_key': options.get('integration_key'),
            'incident_key': incident_key,
            'dedup_key': incident_key,
            'payload': {
                'summary': default_desc,
                'severity': 'error',
                'source': 'redash',
            }
        }

        if new_state == 'triggered':
            data['event_action'] = 'trigger'
        elif new_state == "unknown":
            logging.info('Unknown state, doing nothing')
            return
        else:
            data['event_action'] = 'resolve'

        try:

            ev = pypd.EventV2.create(data=data)
            logging.warning(ev)

        except Exception:
            logging.exception("PagerDuty trigger failed!")


register(PagerDuty)
<EOF>
<BOF>
from __future__ import with_statement
from alembic import context
from sqlalchemy import engine_from_config, pool
from logging.config import fileConfig
import logging

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)
logger = logging.getLogger('alembic.env')

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
from flask import current_app
config.set_main_option('sqlalchemy.url',
                       current_app.config.get('SQLALCHEMY_DATABASE_URI'))
target_metadata = current_app.extensions['migrate'].db.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(url=url)

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """

    # this callback is used to prevent an auto-migration from being generated
    # when there are no changes to the schema
    # reference: http://alembic.readthedocs.org/en/latest/cookbook.html
    def process_revision_directives(context, revision, directives):
        if getattr(config.cmd_opts, 'autogenerate', False):
            script = directives[0]
            if script.upgrade_ops.is_empty():
                directives[:] = []
                logger.info('No changes in schema detected.')

    engine = engine_from_config(config.get_section(config.config_ini_section),
                                prefix='sqlalchemy.',
                                poolclass=pool.NullPool)

    connection = engine.connect()
    context.configure(connection=connection,
                      target_metadata=target_metadata,
                      process_revision_directives=process_revision_directives,
                      **current_app.extensions['migrate'].configure_args)

    try:
        with context.begin_transaction():
            context.run_migrations()
    finally:
        connection.close()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
<EOF>
<BOF>
from __future__ import print_function
# This is here just to print a warning for users who use the old Fabric upgrade script.

if __name__ == '__main__':
    warning = "You're using an outdated upgrade script that is running migrations the wrong way. Please upgrade to " \
              "newer version of the script before continuning the upgrade process."
    print("*" * 20)
    print(warning)
    print("*" * 20)
    exit(1)
<EOF>
<BOF>
"""inline_tags

Revision ID: 0f740a081d20
Revises: a92d92aa678e
Create Date: 2018-05-10 15:47:56.120338

"""
import re
from funcy import flatten, compact
from alembic import op
import sqlalchemy as sa
from sqlalchemy.sql import text
from redash import models


# revision identifiers, used by Alembic.
revision = '0f740a081d20'
down_revision = 'a92d92aa678e'
branch_labels = None
depends_on = None


def upgrade():
    tags_regex = re.compile('^([\w\s]+):|#([\w-]+)', re.I | re.U)
    connection = op.get_bind()

    dashboards = connection.execute("SELECT id, name FROM dashboards")

    update_query = text("UPDATE dashboards SET tags = :tags WHERE id = :id")
    
    for dashboard in dashboards:
        tags = compact(flatten(tags_regex.findall(dashboard[1])))
        if tags:
            connection.execute(update_query, tags=tags, id=dashboard[0])


def downgrade():
    pass
<EOF>
<BOF>
"""favorites_unique_constraint

Revision ID: 71477dadd6ef
Revises: 0f740a081d20
Create Date: 2018-07-11 12:49:52.792123

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = '71477dadd6ef'
down_revision = '0f740a081d20'
branch_labels = None
depends_on = None


def upgrade():
    op.create_unique_constraint('unique_favorite', 'favorites', ['object_type', 'object_id', 'user_id'])


def downgrade():
    op.drop_constraint('unique_favorite', 'favorites', type_='unique')
<EOF>
<BOF>
"""add_org_id_to_favorites

Revision ID: e7004224f284
Revises: d4c798575877
Create Date: 2018-05-10 09:46:31.169938

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'e7004224f284'
down_revision = 'd4c798575877'
branch_labels = None
depends_on = None


def upgrade():
    op.add_column('favorites', sa.Column('org_id', sa.Integer(), nullable=False))
    op.create_foreign_key(None, 'favorites', 'organizations', ['org_id'], ['id'])


def downgrade():
    op.drop_constraint(None, 'favorites', type_='foreignkey')
    op.drop_column('favorites', 'org_id')
<EOF>
<BOF>
"""Add is_draft status to queries and dashboards

Revision ID: 65fc9ede4746
Revises: 
Create Date: 2016-12-07 18:08:13.395586

"""
from __future__ import print_function
from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
from sqlalchemy.exc import ProgrammingError

revision = '65fc9ede4746'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    try:
        op.add_column('queries', sa.Column('is_draft', sa.Boolean, default=True, index=True))
        op.add_column('dashboards', sa.Column('is_draft', sa.Boolean, default=True, index=True))
        op.execute("UPDATE queries SET is_draft = (name = 'New Query')")
        op.execute("UPDATE dashboards SET is_draft = false")
    except ProgrammingError as e:
        # The columns might exist if you ran the old migrations.
        if 'column "is_draft" of relation "queries" already exists' in e.message:
            print("Can't run this migration as you already have is_draft columns, please run:")
            print("./manage.py db stamp {} # you might need to alter the command to match your environment.".format(revision))
            exit()


def downgrade():
    op.drop_column('queries', 'is_draft')
    op.drop_column('dashboards', 'is_draft')
<EOF>
<BOF>
"""empty message

Revision ID: 7671dca4e604
Revises: d1eae8b9893e
Create Date: 2017-11-22 22:20:25.166045

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = '7671dca4e604'
down_revision = 'd1eae8b9893e'
branch_labels = None
depends_on = None


def upgrade():
    op.add_column('users', sa.Column('profile_image_url', sa.String(),
                                     nullable=True, server_default=None))


def downgrade():
    op.drop_column('users', 'profile_image_url')
<EOF>
<BOF>
"""Add Query.search_vector field for full text search.

Revision ID: 5ec5c84ba61e
Revises: 7671dca4e604
Create Date: 2017-10-17 18:21:00.174015

"""
from alembic import op
import sqlalchemy as sa
import sqlalchemy_utils as su
import sqlalchemy_searchable as ss


# revision identifiers, used by Alembic.
revision = '5ec5c84ba61e'
down_revision = '7671dca4e604'
branch_labels = None
depends_on = None


def upgrade():
    conn = op.get_bind()
    op.add_column('queries', sa.Column('search_vector', su.TSVectorType()))
    op.create_index('ix_queries_search_vector', 'queries', ['search_vector'],
                    unique=False, postgresql_using='gin')
    ss.sync_trigger(conn, 'queries', 'search_vector',
                    ['name', 'description', 'query'])


def downgrade():
    conn = op.get_bind()

    ss.drop_trigger(conn, 'queries', 'search_vector')
    op.drop_index('ix_queries_search_vector', table_name='queries')
    op.drop_column('queries', 'search_vector')
<EOF>
<BOF>
"""add Query.schedule_failures

Revision ID: d1eae8b9893e
Revises: 65fc9ede4746
Create Date: 2017-02-03 01:45:02.954923

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'd1eae8b9893e'
down_revision = '65fc9ede4746'
branch_labels = None
depends_on = None


def upgrade():
    op.add_column('queries', sa.Column('schedule_failures', sa.Integer(),
                                       nullable=False, server_default='0'))


def downgrade():
    op.drop_column('queries', 'schedule_failures')
<EOF>
<BOF>
"""empty message

Revision ID: d4c798575877
Revises: 1daa601d3ae5
Create Date: 2018-05-09 10:28:22.931442

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = 'd4c798575877'
down_revision = '1daa601d3ae5'
branch_labels = None
depends_on = None


def upgrade():
    op.create_table('favorites',
        sa.Column('updated_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), nullable=False),
        sa.Column('id', sa.Integer(), nullable=False),
        sa.Column('object_type', sa.Unicode(length=255), nullable=False),
        sa.Column('object_id', sa.Integer(), nullable=False),
        sa.Column('user_id', sa.Integer(), nullable=False),
        sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
        sa.PrimaryKeyConstraint('id')
    )


def downgrade():
    op.drop_table('favorites')
<EOF>
<BOF>
"""Re-index Query.search_vector with existing queries.

Revision ID: 6b5be7e0a0ef
Revises: 5ec5c84ba61e
Create Date: 2017-11-02 20:42:13.356360

"""
from alembic import op
import sqlalchemy as sa
import sqlalchemy_searchable as ss


# revision identifiers, used by Alembic.
revision = '6b5be7e0a0ef'
down_revision = '5ec5c84ba61e'
branch_labels = None
depends_on = None


def upgrade():
    ss.vectorizer.clear()

    conn = op.get_bind()

    metadata = sa.MetaData(bind=conn)
    queries = sa.Table('queries', metadata, autoload=True)

    @ss.vectorizer(queries.c.id)
    def integer_vectorizer(column):
        return sa.func.cast(column, sa.Text)

    ss.sync_trigger(
        conn,
        'queries',
        'search_vector',
        ['id', 'name', 'description', 'query'],
        metadata=metadata
    )


def downgrade():
    conn = op.get_bind()
    ss.drop_trigger(conn, 'queries', 'search_vector')
    op.drop_index('ix_queries_search_vector', table_name='queries')
    op.create_index('ix_queries_search_vector', 'queries', ['search_vector'],
                    unique=False, postgresql_using='gin')
    ss.sync_trigger(conn, 'queries', 'search_vector',
                    ['name', 'description', 'query'])
<EOF>
<BOF>
"""Update widget's position data based on dashboard layout.

Revision ID: 969126bd800f
Revises: 6b5be7e0a0ef
Create Date: 2018-01-31 15:20:30.396533

"""
from __future__ import print_function
import simplejson
from alembic import op
import sqlalchemy as sa

from redash.models import Dashboard, Widget, db


# revision identifiers, used by Alembic.
revision = '969126bd800f'
down_revision = '6b5be7e0a0ef'
branch_labels = None
depends_on = None


def upgrade():
    # Update widgets position data:
    column_size = 3
    print("Updating dashboards position data:")
    for dashboard in Dashboard.query:
        print("  Updating dashboard: {}".format(dashboard.id))
        layout = simplejson.loads(dashboard.layout)

        print("    Building widgets map:")
        widgets = {}
        for w in dashboard.widgets:
            print("    Widget: {}".format(w.id))
            widgets[w.id] = w

        print("    Iterating over layout:")
        for row_index, row in enumerate(layout):
            print("      Row: {} - {}".format(row_index, row))
            if row is None:
                continue

            for column_index, widget_id in enumerate(row):
                print("      Column: {} - {}".format(column_index, widget_id))
                widget = widgets.get(widget_id)

                if widget is None:
                    continue

                options = simplejson.loads(widget.options) or {}
                options['position'] = {
                    "row": row_index,
                    "col": column_index * column_size,
                    "sizeX": column_size * widget.width
                }

                widget.options = simplejson.dumps(options)

                db.session.add(widget)

    db.session.commit()

    # Remove legacy columns no longer in use.
    op.drop_column('widgets', 'type')
    op.drop_column('widgets', 'query_id')


def downgrade():
    op.add_column('widgets', sa.Column('query_id', sa.INTEGER(), autoincrement=False, nullable=True))
    op.add_column('widgets', sa.Column('type', sa.VARCHAR(length=100), autoincrement=False, nullable=True))
<EOF>
<BOF>
"""inline_tags

Revision ID: a92d92aa678e
Revises: e7004224f284
Create Date: 2018-05-10 15:41:28.053237

"""
import re
from funcy import flatten, compact
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from redash import models

# revision identifiers, used by Alembic.
revision = 'a92d92aa678e'
down_revision = 'e7004224f284'
branch_labels = None
depends_on = None


def upgrade():
    op.add_column('dashboards', sa.Column('tags', postgresql.ARRAY(sa.Unicode()), nullable=True))
    op.add_column('queries', sa.Column('tags', postgresql.ARRAY(sa.Unicode()), nullable=True))


def downgrade():
    op.drop_column('queries', 'tags')
    op.drop_column('dashboards', 'tags')
<EOF>
<BOF>
"""add columns for disabled users

Revision ID: 1daa601d3ae5
Revises: 969126bd800f
Create Date: 2018-03-07 10:20:10.410159

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = '1daa601d3ae5'
down_revision = '969126bd800f'
branch_labels = None
depends_on = None


def upgrade():
    op.add_column(
        'users',
        sa.Column('disabled_at', sa.DateTime(True), nullable=True)
    )


def downgrade():
    op.drop_column('users', 'disabled_at')
<EOF>
<BOF>
from __future__ import print_function
import os
import sys
import re
import subprocess
import requests
import simplejson

github_token = os.environ['GITHUB_TOKEN']
auth = (github_token, 'x-oauth-basic')
repo = 'getredash/redash'

def _github_request(method, path, params=None, headers={}):
    if not path.startswith('https://api.github.com'):
        url = "https://api.github.com/{}".format(path)
    else:
        url = path

    if params is not None:
        params = simplejson.dumps(params)

    response = requests.request(method, url, data=params, auth=auth)
    return response

def exception_from_error(message, response):
    return Exception("({}) {}: {}".format(response.status_code, message, response.json().get('message', '?')))

def rc_tag_name(version):
    return "v{}-rc".format(version)

def get_rc_release(version):
    tag = rc_tag_name(version)
    response = _github_request('get', 'repos/{}/releases/tags/{}'.format(repo, tag))

    if response.status_code == 404:
        return None
    elif response.status_code == 200:
        return response.json()

    raise exception_from_error("Unknown error while looking RC release: ", response)

def create_release(version, commit_sha):
    tag = rc_tag_name(version)

    params = {
        'tag_name': tag,
        'name': "{} - RC".format(version),
        'target_commitish': commit_sha,
        'prerelease': True
    }

    response = _github_request('post', 'repos/{}/releases'.format(repo), params)

    if response.status_code != 201:
        raise exception_from_error("Failed creating new release", response)

    return response.json()

def upload_asset(release, filepath):
    upload_url = release['upload_url'].replace('{?name,label}', '')
    filename = filepath.split('/')[-1]

    with open(filepath) as file_content:
        headers = {'Content-Type': 'application/gzip'}
        response = requests.post(upload_url, file_content, params={'name': filename}, headers=headers, auth=auth, verify=False)

    if response.status_code != 201:  # not 200/201/...
        raise exception_from_error('Failed uploading asset', response)

    return response

def remove_previous_builds(release):
    for asset in release['assets']:
        response = _github_request('delete', asset['url'])
        if response.status_code != 204:
            raise exception_from_error("Failed deleting asset", response)

def get_changelog(commit_sha):
    latest_release = _github_request('get', 'repos/{}/releases/latest'.format(repo))
    if latest_release.status_code != 200:
        raise exception_from_error('Failed getting latest release', latest_release)

    latest_release = latest_release.json()
    previous_sha = latest_release['target_commitish']

    args = ['git', '--no-pager', 'log', '--merges', '--grep', 'Merge pull request', '--pretty=format:"%h|%s|%b|%p"', '{}...{}'.format(previous_sha, commit_sha)]
    log = subprocess.check_output(args)
    changes = ["Changes since {}:".format(latest_release['name'])]

    for line in log.split('\n'):
        try:
            sha, subject, body, parents = line[1:-1].split('|')
        except ValueError:
            continue

        try:
            pull_request = re.match("Merge pull request #(\d+)", subject).groups()[0]
            pull_request = " #{}".format(pull_request)
        except Exception as ex:
            pull_request = ""

        author = subprocess.check_output(['git', 'log', '-1', '--pretty=format:"%an"', parents.split(' ')[-1]])[1:-1]

        changes.append("{}{}: {} ({})".format(sha, pull_request, body.strip(), author))

    return "\n".join(changes)

def update_release_commit_sha(release, commit_sha):
    params = {
        'target_commitish': commit_sha,
    }

    response = _github_request('patch', 'repos/{}/releases/{}'.format(repo, release['id']), params)

    if response.status_code != 200:
        raise exception_from_error("Failed updating commit sha for existing release", response)

    return response.json()

def update_release(version, build_filepath, commit_sha):
    try:
        release = get_rc_release(version)
        if release:
            release = update_release_commit_sha(release, commit_sha)
        else:
            release = create_release(version, commit_sha)

        print("Using release id: {}".format(release['id']))

        remove_previous_builds(release)
        response = upload_asset(release, build_filepath)

        changelog = get_changelog(commit_sha)

        response = _github_request('patch', release['url'], {'body': changelog})
        if response.status_code != 200:
            raise exception_from_error("Failed updating release description", response)

    except Exception as ex:
        print(ex)

if __name__ == '__main__':
    commit_sha = sys.argv[1]
    version = sys.argv[2]
    filepath = sys.argv[3]

    # TODO: make sure running from git directory & remote = repo
    update_release(version, filepath, commit_sha)
<EOF>
<BOF>
#!/bin/env python
from __future__ import print_function
import sys
import re
import subprocess

def get_change_log(previous_sha):
    args = ['git', '--no-pager', 'log', '--merges', '--grep', 'Merge pull request', '--pretty=format:"%h|%s|%b|%p"', 'master...{}'.format(previous_sha)]
    log = subprocess.check_output(args)
    changes = []

    for line in log.split('\n'):
        try:
            sha, subject, body, parents = line[1:-1].split('|')
        except ValueError:
            continue

        try:
            pull_request = re.match("Merge pull request #(\d+)", subject).groups()[0]
            pull_request = " #{}".format(pull_request)
        except Exception as ex:
            pull_request = ""

        author = subprocess.check_output(['git', 'log', '-1', '--pretty=format:"%an"', parents.split(' ')[-1]])[1:-1]

        changes.append("{}{}: {} ({})".format(sha, pull_request, body.strip(), author))

    return changes


if __name__ == '__main__':
    previous_sha = sys.argv[1]
    changes = get_change_log(previous_sha)

    for change in changes:
        print(change)
<EOF>
<BOF>
__author__ = 'lior'

from redash.models import DataSource

if __name__ == '__main__':

    for ds in DataSource.select(DataSource.id, DataSource.type):
        if ds.type == 'elasticsearch':
            ds.type = 'kibana'
            ds.save(only=ds.dirty_fields)
<EOF>
<BOF>
from __future__ import print_function
import simplejson
from redash import models

if __name__ == '__main__':
    for vis in models.Visualization.select():
        if vis.type == 'COUNTER':
            options = simplejson.loads(vis.options)
            print("Before: ", options)
            if 'rowNumber' in options and options['rowNumber'] is not None:
                options['rowNumber'] += 1
            else:
                options['rowNumber'] = 1

            if 'counterColName' not in options:
                options['counterColName'] = 'counter'

            if 'targetColName' not in options:
                options['targetColName'] = 'target'
            options['targetRowNumber'] = options['rowNumber']

            print("After: ", options)
            vis.options = simplejson.dumps(options)
            vis.save()
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.add_column('queries', 'is_archived', models.Query.is_archived)
        )

    db.close_db(None)
<EOF>
<BOF>
from redash.models import db

if __name__ == '__main__':
    db.connect_db()

    with db.database.transaction():
        # Make sure all data sources names are unique.
        db.database.execute_sql("""
        UPDATE data_sources
        SET name = new_names.name
        FROM (
            SELECT id, name || ' ' || id as name
            FROM (SELECT id, name, rank() OVER (PARTITION BY name ORDER BY created_at ASC) FROM data_sources) ds WHERE rank > 1
        ) AS new_names
        WHERE data_sources.id = new_names.id;
        """)
        # Add unique constraint on data_sources.name.
        db.database.execute_sql("ALTER TABLE data_sources ADD CONSTRAINT unique_name UNIQUE (name);")

    db.close_db(None)
<EOF>
<BOF>
from redash import models

if __name__ == '__main__':
    default_group = models.Group.select(models.Group.id, models.Group.permissions).where(models.Group.name=='default').first()
    default_group.permissions.append('schedule_query')
    default_group.save(only=[models.Group.permissions])
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.add_column('queries', 'is_draft', models.Query.is_draft)
        )
        migrate(
            migrator.add_column('dashboards', 'is_draft', models.Query.is_draft)
        )
        db.database.execute_sql("UPDATE queries SET is_draft = (name = 'New Query')")
    db.close_db(None)
<EOF>
<BOF>
from __future__ import print_function
import simplejson
import jsonschema
from jsonschema import ValidationError

from six import string_types
from redash import query_runner
from redash.models import DataSource


def validate_configuration(query_runner_type, configuration_json):
    query_runner_class = query_runner.query_runners.get(query_runner_type, None)
    if query_runner_class is None:
        return False

    try:
        if isinstance(configuration_json, string_types):
            configuration = simplejson.loads(configuration_json)
        else:
            configuration = configuration_json
        jsonschema.validate(configuration, query_runner_class.configuration_schema())
    except (ValidationError, ValueError):
        return False

    return True


def update(data_source):
    print("[%s] Old options: %s" % (data_source.name, data_source.options))

    if validate_configuration(data_source.type, data_source.options):
        print("[%s] configuration already valid. skipping." % data_source.name)
        return

    if data_source.type == 'pg':
        values = data_source.options.split(" ")
        configuration = {}
        for value in values:
            k, v = value.split("=", 1)
            configuration[k] = v
            if k == 'port':
                configuration[k] = int(v)

        data_source.options = simplejson.dumps(configuration)

    elif data_source.type == 'mysql':
        mapping = {
            'Server': 'host',
            'User': 'user',
            'Pwd': 'passwd',
            'Database': 'db'
        }

        values = data_source.options.split(";")
        configuration = {}
        for value in values:
            k, v = value.split("=", 1)
            configuration[mapping[k]] = v
        data_source.options = simplejson.dumps(configuration)

    elif data_source.type == 'graphite':
        old_config = simplejson.loads(data_source.options)

        configuration = {
            "url": old_config["url"]
        }

        if "verify" in old_config:
            configuration['verify'] = old_config['verify']

        if "auth" in old_config:
            configuration['username'], configuration['password'] = old_config["auth"]

        data_source.options = simplejson.dumps(configuration)

    elif data_source.type == 'url':
        data_source.options = simplejson.dumps({"url": data_source.options})

    elif data_source.type == 'script':
        data_source.options = simplejson.dumps({"path": data_source.options})

    elif data_source.type == 'mongo':
        data_source.type = 'mongodb'

    else:
        print("[%s] No need to convert type of: %s" % (data_source.name, data_source.type))

    print("[%s] New options: %s" % (data_source.name, data_source.options))
    data_source.save(only=data_source.dirty_fields)


if __name__ == '__main__':
    for data_source in DataSource.select(DataSource.id, DataSource.name, DataSource.type, DataSource.options):
        update(data_source)
<EOF>
<BOF>
from redash.models import db

if __name__ == '__main__':
    db.connect_db()
    columns = (
        ('activity_log', 'created_at'),
        ('dashboards', 'created_at'),
        ('data_sources', 'created_at'),
        ('events', 'created_at'),
        ('groups', 'created_at'),
        ('queries', 'created_at'),
        ('widgets', 'created_at'),
        ('query_results', 'retrieved_at')
    )

    with db.database.transaction():
        for column in columns:
            db.database.execute_sql("ALTER TABLE {} ALTER COLUMN {} TYPE timestamp with time zone;".format(*column))

    db.close_db(None)
<EOF>
<BOF>
from collections import defaultdict
from redash.models import db, DataSourceGroup, DataSource, Group, Organization, User
from playhouse.migrate import PostgresqlMigrator, migrate
import peewee

if __name__ == '__main__':
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        # Add type to groups
        migrate(
            migrator.add_column('groups', 'type', Group.type)
        )

        for name in ['default', 'admin']:
            group = Group.get(Group.name==name)
            group.type = Group.BUILTIN_GROUP
            group.save()

        # Create association table between data sources and groups
        DataSourceGroup.create_table()

        # add default to existing data source:
        default_org = Organization.get_by_id(1)
        default_group = Group.get(Group.name=="default")
        for ds in DataSource.all(default_org):
            DataSourceGroup.create(data_source=ds, group=default_group)

        # change the groups list on a user object to be an ids list
        migrate(
            migrator.rename_column('users', 'groups', 'old_groups'),
        )

        migrate(migrator.add_column('users', 'groups', User.groups))

        group_map = dict(map(lambda g: (g.name, g.id), Group.select()))
        user_map = defaultdict(list)
        for user in User.select(User, peewee.SQL('old_groups')):
            group_ids = [group_map[group] for group in user.old_groups]
            user.update_instance(groups=group_ids)

        migrate(migrator.drop_column('users', 'old_groups'))

    db.close_db(None)
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.drop_not_null('queries', 'data_source_id'),
        )

    db.close_db(None)
<EOF>
<BOF>
from redash import models

if __name__ == '__main__':
    with models.db.database.transaction():
        groups = models.Group.select(models.Group.id, models.Group.type).where(models.Group.name=='default')
        for group in groups:
            group.type = models.Group.BUILTIN_GROUP
            group.save(only=[models.Group.type])
<EOF>
<BOF>
from redash import redis_connection

if __name__ == '__main__':
    redis_connection.delete('query_task_trackers')
<EOF>
<BOF>
from redash import models

if __name__ == '__main__':

    default_group = models.Group.select(models.Group.id, models.Group.permissions).where(models.Group.name=='default').first()
    default_group.permissions.append('list_users')
    default_group.save(only=[models.Group.permissions])
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.add_column('queries', 'schedule', models.Query.schedule),
        )

        db.database.execute_sql("UPDATE queries SET schedule = ttl WHERE ttl > 0;")

        migrate(
            migrator.drop_column('queries', 'ttl')
        )

    db.close_db(None)
<EOF>
<BOF>
from base64 import b64encode
import simplejson
from redash.models import DataSource


def convert_p12_to_pem(p12file):
    from OpenSSL import crypto
    with open(p12file, 'rb') as f:
        p12 = crypto.load_pkcs12(f.read(), "notasecret")

    return crypto.dump_privatekey(crypto.FILETYPE_PEM, p12.get_privatekey())

if __name__ == '__main__':

    for ds in DataSource.select(DataSource.id, DataSource.type, DataSource.options):

        if ds.type == 'bigquery':
            options = simplejson.loads(ds.options)

            if 'jsonKeyFile' in options:
                continue

            new_options = {
                'projectId': options['projectId'],
                'jsonKeyFile': b64encode(simplejson.dumps({
                    'client_email': options['serviceAccount'],
                    'private_key': convert_p12_to_pem(options['privateKey'])
                }))
            }

            ds.options = simplejson.dumps(new_options)
            ds.save(only=ds.dirty_fields)
        elif ds.type == 'google_spreadsheets':
            options = simplejson.loads(ds.options)
            if 'jsonKeyFile' in options:
                continue

            with open(options['credentialsFilePath']) as f:
                new_options = {
                    'jsonKeyFile': b64encode(f.read())
                }

            ds.options = simplejson.dumps(new_options)
            ds.save(only=ds.dirty_fields)
<EOF>
<BOF>
from redash.models import db, QuerySnippet

if __name__ == '__main__':
    with db.database.transaction():
        if not QuerySnippet.table_exists():
            QuerySnippet.create_table()

    db.close_db(None)
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.add_column('queries', 'last_modified_by_id', models.Query.last_modified_by)
        )

        db.database.execute_sql("UPDATE queries SET last_modified_by_id = user_id;")

    db.close_db(None)
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        column = models.User.api_key
        column.null = True
        migrate(
            migrator.add_column('users', 'api_key', models.User.api_key),
        )

        for user in models.User.select(models.User.id, models.User.api_key):
            user.save(only=user.dirty_fields)

        migrate(
            migrator.add_not_null('users', 'api_key')
        )

    db.close_db(None)
<EOF>
<BOF>
from redash.models import db, Query
from playhouse.migrate import PostgresqlMigrator, migrate

if __name__ == '__main__':
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.add_column('queries', 'options', Query.options),
        )
<EOF>
<BOF>
from __future__ import print_function
import peewee
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    cursor = db.database.execute_sql("SELECT column_name FROM information_schema.columns WHERE table_name='alerts' and column_name='rearm';")
    if cursor.rowcount > 0:
        print("Column exists. Skipping.")
        exit()

    with db.database.transaction():
        migrate(
            migrator.add_column('alerts', 'rearm', models.Alert.rearm),
        )

    db.close_db(None)
<EOF>
<BOF>
from redash.models import db
import peewee
from playhouse.migrate import PostgresqlMigrator, migrate

if __name__ == '__main__':
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        # Change the uniqueness constraint on data source name to be (org, name):
        # In some cases it's a constraint:
        db.database.execute_sql('ALTER TABLE data_sources DROP CONSTRAINT IF EXISTS unique_name')
        # In others only an index:
        db.database.execute_sql('DROP INDEX IF EXISTS data_sources_name')

        migrate(
            migrator.add_index('data_sources', ('org_id', 'name'), unique=True)
        )

    db.close_db(None)
<EOF>
<BOF>
import os
from redash.models import db, Organization, Group
from redash import settings
from playhouse.migrate import PostgresqlMigrator, migrate

# The following is deprecated and should be defined with the Organization object
GOOGLE_APPS_DOMAIN = settings.set_from_string(os.environ.get("REDASH_GOOGLE_APPS_DOMAIN", ""))

if __name__ == '__main__':
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        Organization.create_table()

        default_org = Organization.create(name="Default", slug='default', settings={
            Organization.SETTING_GOOGLE_APPS_DOMAINS: list(GOOGLE_APPS_DOMAIN)
        })

        column = Group.org
        column.default = default_org

        migrate(
            migrator.add_column('groups', 'org_id', column),
            migrator.add_column('events', 'org_id', column),
            migrator.add_column('data_sources', 'org_id', column),
            migrator.add_column('users', 'org_id', column),
            migrator.add_column('dashboards', 'org_id', column),
            migrator.add_column('queries', 'org_id', column),
            migrator.add_column('query_results', 'org_id', column),
        )

        # Change the uniqueness constraint on user email to be (org, email):
        migrate(
            migrator.drop_index('users', 'users_email'),
            migrator.add_index('users', ('org_id', 'email'), unique=True)
        )

    db.close_db(None)
<EOF>
<BOF>
from redash.models import ApiKey


if __name__ == '__main__':
    ApiKey.create_table()
<EOF>
<BOF>
from __future__ import print_function
from redash.models import db, Change, AccessPermission, Query, Dashboard
from playhouse.migrate import PostgresqlMigrator, migrate

if __name__ == '__main__':

    if not Change.table_exists():
        Change.create_table()

    if not AccessPermission.table_exists():
        AccessPermission.create_table()

    migrator = PostgresqlMigrator(db.database)

    try:
        migrate(
            migrator.add_column('queries', 'version', Query.version),
            migrator.add_column('dashboards', 'version', Dashboard.version)
        )
    except Exception as ex:
        print("Error while adding version column to queries/dashboards. Maybe it already exists?")
        print(ex)

<EOF>
<BOF>
from redash.models import db, Alert, AlertSubscription

if __name__ == '__main__':
    with db.database.transaction():
        Alert.create_table()
        AlertSubscription.create_table()

    db.close_db(None)
<EOF>
<BOF>
from redash import models

if __name__ == '__main__':
    admin_group = models.Group.get(models.Group.name=='admin')
    admin_group.permissions.append('super_admin')
    admin_group.save()
<EOF>
<BOF>
from redash.models import db, Alert, AlertSubscription

if __name__ == '__main__':
    with db.database.transaction():
        # There was an AWS/GCE image created without this table, to make sure this exists we run this migration.
        if not AlertSubscription.table_exists():
            AlertSubscription.create_table()

    db.close_db(None)
<EOF>
<BOF>
from __future__ import print_function
import os
import peewee
from redash.models import db, NotificationDestination, AlertSubscription, Alert, Organization, User
from redash.destinations import get_configuration_schema_for_destination_type
from redash.utils.configuration import ConfigurationContainer
from playhouse.migrate import PostgresqlMigrator, migrate

HIPCHAT_API_TOKEN = os.environ.get('REDASH_HIPCHAT_API_TOKEN', None)
HIPCHAT_API_URL = os.environ.get('REDASH_HIPCHAT_API_URL', None)
HIPCHAT_ROOM_ID = os.environ.get('REDASH_HIPCHAT_ROOM_ID', None)

WEBHOOK_ENDPOINT = os.environ.get('REDASH_WEBHOOK_ENDPOINT', None)
WEBHOOK_USERNAME = os.environ.get('REDASH_WEBHOOK_USERNAME', None)
WEBHOOK_PASSWORD = os.environ.get('REDASH_WEBHOOK_PASSWORD', None)

if __name__ == '__main__':
    migrator = PostgresqlMigrator(db.database)
    with db.database.transaction():

        if not NotificationDestination.table_exists():
            NotificationDestination.create_table()
            
            # Update alert subscription fields
            migrate(
                migrator.add_column('alert_subscriptions', 'destination_id', AlertSubscription.destination)
            )

            try:
                org = Organization.get_by_slug('default')
                user = User.select().where(User.org==org, peewee.SQL("%s = ANY(groups)", org.admin_group.id)).get()
            except Exception:
                print("!!! Warning: failed finding default organization or admin user, won't migrate Webhook/HipChat alert subscriptions.")
                exit()

            if WEBHOOK_ENDPOINT:
                # Have all existing alerts send to webhook if already configured
                schema = get_configuration_schema_for_destination_type('webhook')
                conf = {'url': WEBHOOK_ENDPOINT}
                if WEBHOOK_USERNAME:
                    conf['username'] = WEBHOOK_USERNAME
                    conf['password'] = WEBHOOK_PASSWORD
                options = ConfigurationContainer(conf, schema)

                webhook = NotificationDestination.create(
                    org=org,
                    user=user,
                    name="Webhook",
                    type="webhook",
                    options=options
                )

                for alert in Alert.select():
                    AlertSubscription.create(
                        user=user,
                        destination=webhook,
                        alert=alert
                    )

            if HIPCHAT_API_TOKEN:
                # Have all existing alerts send to HipChat if already configured
                schema = get_configuration_schema_for_destination_type('hipchat')

                conf = {}

                if HIPCHAT_API_URL:
                    conf['url'] = '{url}/room/{room_id}/notification?auth_token={token}'.format(
                        url=HIPCHAT_API_URL, room_id=HIPCHAT_ROOM_ID, token=HIPCHAT_API_TOKEN)
                else:
                    conf['url'] = 'https://hipchat.com/v2/room/{room_id}/notification?auth_token={token}'.format(
                        room_id=HIPCHAT_ROOM_ID, token=HIPCHAT_API_TOKEN)

                options = ConfigurationContainer(conf, schema)

                hipchat = NotificationDestination.create(
                    org=org,
                    user=user,
                    name="HipChat",
                    type="hipchat",
                    options=options
                )

                for alert in Alert.select():
                    AlertSubscription.create(
                        user=user,
                        destination=hipchat,
                        alert=alert
                    )

    db.close_db(None)
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.drop_not_null('events', 'user_id')
        )
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db
from redash import models

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.add_column('queries', 'updated_at', models.Query.updated_at),
            migrator.add_column('dashboards', 'updated_at', models.Dashboard.updated_at),
            migrator.add_column('widgets', 'updated_at', models.Widget.updated_at),
            migrator.add_column('users', 'created_at', models.User.created_at),
            migrator.add_column('users', 'updated_at', models.User.updated_at),
            migrator.add_column('visualizations', 'created_at', models.Visualization.created_at),
            migrator.add_column('visualizations', 'updated_at', models.Visualization.updated_at)
        )

        db.database.execute_sql("UPDATE queries SET updated_at = created_at;")
        db.database.execute_sql("UPDATE dashboards SET updated_at = created_at;")
        db.database.execute_sql("UPDATE widgets SET updated_at = created_at;")

    db.close_db(None)
<EOF>
<BOF>
from redash import models

if __name__ == '__main__':
    with models.db.database.transaction():
        groups = models.Group.select(models.Group.id, models.Group.permissions).where(models.Group.name=='default')
        for group in groups:
            group.permissions.append('list_dashboards')
            group.permissions.append('list_alerts')
            group.permissions.append('list_data_sources')
            group.save(only=[models.Group.permissions])
<EOF>
<BOF>
from playhouse.migrate import PostgresqlMigrator, migrate

from redash.models import db

if __name__ == '__main__':
    db.connect_db()
    migrator = PostgresqlMigrator(db.database)

    with db.database.transaction():
        migrate(
            migrator.drop_column('groups', 'tables')
        )

    db.close_db(None)
<EOF>
